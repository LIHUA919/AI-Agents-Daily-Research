<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Main category: cs.AI

TL;DR: The paper explores AI's role in shifting human-nature dynamics from dominance to interdependence, showcasing AI's application in ecological design and restoration.


<details>
  <summary>Details</summary>
Motivation: To investigate if AI can mediate a shift from human dominance over nature to a relationship of interdependence, and how this can be achieved through ecological design.

Method: Case studies of AI applications in ecological design, including data analysis, image recognition, and ecological restoration, plus a prototype for AI-assisted water remediation using reinforcement learning and phytoremediation.

Result: AI expands creative methods and reframes ecological design theory and practice, linking scientific insight, artistic practice, and environmental stewardship.

Conclusion: AI offers a promising pathway for sustainable, technology-enabled ecosystems, providing a roadmap for future research in this field.

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [2] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: MLLMs show promise as verifiers for agent behavior but suffer from agreement bias. Self-Grounded Verification (SGV) improves accuracy by leveraging MLLMs' own sampling mechanisms.


<details>
  <summary>Details</summary>
Motivation: Extending verifiers to domains without clear success criteria (e.g., computer use) is challenging. MLLMs offer potential due to their world knowledge and reasoning skills.

Method: Proposes Self-Grounded Verification (SGV), a two-step method: eliciting broad priors from MLLMs, then evaluating trajectories conditioned on these priors.

Result: SGV improves MLLM verifiers by up to 20 points in accuracy and failure detection, achieving state-of-the-art performance on benchmarks.

Conclusion: SGV effectively addresses agreement bias in MLLMs, enhancing their utility as verifiers in diverse domains.

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [3] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: A modular harness design for LLM agents improves performance in diverse gaming environments by integrating perception, memory, and reasoning components.


<details>
  <summary>Details</summary>
Motivation: To enable a single LLM or VLM backbone to handle varied multi-turn gaming scenarios without domain-specific adjustments.

Method: The framework uses perception, memory, and reasoning modules, tested on classic and modern game suites to analyze performance impacts.

Result: The harness consistently outperforms baselines, with memory excelling in long-horizon puzzles and perception in vision-noisy arcades.

Conclusion: The modular design effectively advances general-purpose agents, leveraging the familiarity of games for broader applicability.

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [4] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Main category: cs.AI

TL;DR: ClarifAI enhances AI transparency using Case-Based Reasoning and ontologies for better decision-making.


<details>
  <summary>Details</summary>
Motivation: To improve AI interpretability for stakeholders in high-stake applications.

Method: Combines Case-Based Reasoning (CBR) and ontology-driven approaches for detailed explanations.

Result: ClarifAI improves AI interpretability across sectors and critical processes.

Conclusion: ClarifAI advances AI transparency, enabling deployment in critical decision-making.

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [5] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Main category: cs.AI

TL;DR: The paper introduces DP-Bench, a benchmark for dynamic programming (DP) problems, and DPLM, a specialized 7B-parameter model. It uses DualReflect, a synthetic data generation pipeline, to overcome data scarcity and achieve strong performance.


<details>
  <summary>Details</summary>
Motivation: Automating DP model formulation is challenging due to stochastic transitions and limited training data. Existing LLMs are not directly applicable, necessitating a specialized approach.

Method: Developed DP-Bench for evaluation and DPLM, a specialized model. Introduced DualReflect, a pipeline combining forward and backward synthetic data generation.

Result: DPLM matches state-of-the-art LLMs and outperforms them on hard problems. DualReflect's backward generation excels in low-data regimes, while forward generation adds diversity at scale.

Conclusion: Combining forward and backward generation in DualReflect is key for effective DP problem-solving, balancing correctness and diversity.

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [6] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.AI

TL;DR: A survey on Swarm Intelligence (SI) algorithms for document search based on semantic similarity, highlighting their effectiveness and suggesting future research.


<details>
  <summary>Details</summary>
Motivation: Swarm Intelligence is popular in AI for solving optimization problems, and this survey explores its application in semantic document search.

Method: Review of latest developments in SI algorithms for semantic similarity-based document search.

Result: Identifies effectiveness of SI in solving such problems and current advancements.

Conclusion: Recommends future research directions for improving SI applications in semantic document search.

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [7] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: The paper introduces a method for batching GPU computations in depth-first search (DFS), proposing cost-bounded DFS (CB-DFS) to leverage CPU and GPU parallelism. It extends algorithms like Batch IDA* and Batch BTS, maintaining optimality. Evaluations on Rubik's Cube and sliding tile puzzles show efficient GPU batching in DFS.


<details>
  <summary>Details</summary>
Motivation: GPU advancements enable parallel processing, but few search algorithms exploit GPUs. The paper aims to address this gap by batching GPU computations in DFS.

Method: Proposes CB-DFS, a cost-bounded DFS method, extending algorithms like Batch IDA* and Batch BTS. Builds on Asynchronous Parallel IDA* (AIDA*) while ensuring optimality.

Result: Demonstrates efficient GPU batching in DFS on 3x3 Rubik's Cube and 4x4 sliding tile puzzles. Analyzes hyperparameters, heuristic size, and hardware impact.

Conclusion: The method successfully integrates GPU parallelism into DFS, enhancing performance while preserving optimality.

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [8] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Main category: cs.AI

TL;DR: Aime is a dynamic multi-agent framework that improves adaptability and robustness in MAS by replacing rigid planning with reactive planning, dynamic actor instantiation, and centralized progress management.


<details>
  <summary>Details</summary>
Motivation: Current MAS with LLMs are limited by rigid planning, static capabilities, and inefficient communication, hindering adaptability in dynamic environments.

Method: Aime introduces a Dynamic Planner, Actor Factory for on-demand agent creation, and a Progress Management Module for system-wide state awareness.

Result: Aime outperforms specialized state-of-the-art agents in benchmarks like GAIA, SWE-bench, and WebVoyager.

Conclusion: Aime provides a more resilient and effective foundation for multi-agent collaboration.

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [9] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Main category: cs.AI

TL;DR: Bio-inspired UAV navigation uses optic flow, mimicking honeybees. A Reinforcement Learning agent trained with optic flow focuses on flow discontinuities and large magnitudes, resembling insect behavior.


<details>
  <summary>Details</summary>
Motivation: Bio-inspired design leverages biological systems' efficient navigation despite limited sensory input, aiming to improve UAV navigation.

Method: Train a Reinforcement Learning agent using optic flow for tunnel navigation, analyzing attention patterns.

Result: Agents focus on optic flow discontinuities and large magnitudes, resembling insect behavior, suggesting a viable UAV control strategy.

Conclusion: The findings support bio-inspired optic flow as a simple, effective strategy for UAV navigation, replicating insect-like behavior.

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [10] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: TPE-MARL is a topology-enhanced MARL method for CAVs in mixed traffic, compressing state info and improving decision-making via QMIX, visit counts, and mutual info. It outperforms in efficiency, safety, and rationality.


<details>
  <summary>Details</summary>
Motivation: Addressing the exploration-exploitation trade-off in MARL for CAVs, exacerbated by high-dimensional state-action spaces in dynamic traffic.

Method: Constructs a game topology tensor to compress traffic state info and uses QMIX with visit counts and agent mutual info for MARL.

Result: TPE-MARL balances exploration-exploitation, excelling in traffic efficiency, safety, and decision smoothness, even matching human rationality.

Conclusion: TPE-MARL effectively optimizes CAV decision-making in mixed traffic, demonstrating superior performance and practical applicability.

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [11] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Main category: cs.AI

TL;DR: A novel online POMDP solver, Partially Observable Reference Policy Programming, improves performance by sampling deeply and ensuring gradual policy updates, with bounded performance loss based on average sampling errors.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of online POMDP planning in dynamically evolving environments, where traditional methods suffer from high sampling sparsity and performance loss.

Method: Proposes an anytime online approximate POMDP solver that samples future histories deeply and enforces gradual policy updates, with theoretical guarantees on bounded performance loss.

Result: Empirical tests on large-scale problems, including a helicopter emergency scenario, show the solver outperforms current online benchmarks.

Conclusion: The solver is effective for complex, dynamic environments, offering improved performance and theoretical robustness.

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [12] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: BuildEvo uses LLMs to design interpretable energy prediction heuristics, outperforming benchmarks with improved generalization and transparency.


<details>
  <summary>Details</summary>
Motivation: Traditional heuristics lack precision, and advanced models are opaque and neglect physical principles, necessitating a better solution.

Method: BuildEvo employs an evolutionary process with LLMs to create and refine heuristics using physical insights from building data.

Result: BuildEvo achieves state-of-the-art performance on benchmarks, with better generalization and transparent logic.

Conclusion: The framework advances automated design of robust, physically grounded heuristics for trustworthy energy system models.

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [13] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Main category: cs.AI

TL;DR: The paper explores LLMs' spatial strategic reasoning in Chinese Chess (Xiangqi), proposing a tailored training framework and Xiangqi-R1 model, which outperforms general-purpose LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the gap in LLMs' spatial strategic reasoning, using Xiangqi as a complex testbed due to its intricate rules and spatial demands.

Method: A multi-stage training framework: fine-tuning for legal moves, adding strategic annotations, and reinforcement learning with GRPO for stable reasoning.

Result: Xiangqi-R1 improves move legality by 18% and analysis accuracy by 22% over general-purpose LLMs.

Conclusion: The approach shows promise for developing general strategic intelligence in spatially complex domains.

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: STAGED integrates agent-based modeling with deep learning to model cellular dynamics, capturing intercellular and intracellular interactions using graph ODE networks.


<details>
  <summary>Details</summary>
Motivation: Current methods treat cells as independent points, missing dynamic interactions. Spatial transcriptomics reveals cellular organization, but computational advances are needed for data-driven learning of interactive dynamics.

Method: STAGED combines agent-based modeling with deep learning, using graph ODE networks with shared weights per cell type and an attention mechanism to dynamically learn interaction strengths.

Result: The model accurately captures intercellular and intracellular interactions, matching simulated and inferred trajectories from spatial transcriptomics data.

Conclusion: STAGED provides a more adaptive and accurate representation of cellular dynamics by integrating data-driven approaches with agent-based modeling.

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [15] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: A new graph neural network (RUGNN) is proposed for accurate 3D deformation predictions in material forming, outperforming traditional AI models and baseline GNNs.


<details>
  <summary>Details</summary>
Motivation: Traditional AI surrogate models struggle with 3D spatial relationships and permutation invariance, prompting the need for advanced graph-based solutions.

Method: Developed RUGNN with GRUs for temporal dynamics and U-Net-inspired spatial handling, plus a novel 'node-to-surface' contact method for efficiency.

Result: RUGNN accurately predicts deformation fields in cold and hot forming, matching ground truth FE simulations and outperforming baseline GNNs.

Conclusion: RUGNN is a reliable tool for manufacturability predictions in sheet material forming, validated by case studies and hyperparameter tuning.

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [16] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: SurgeryLSTM, a masked BiLSTM with attention, outperforms traditional ML models in predicting LOS in elective spine surgery, offering higher accuracy (R2=0.86) and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve LOS prediction in elective spine surgery by leveraging temporal modeling and enhancing interpretability for clinical adoption.

Method: Compared traditional ML models (linear regression, random forest, SVM, XGBoost) with SurgeryLSTM (masked BiLSTM with attention) using EHR data. Evaluated performance via R2 and explainable AI.

Result: SurgeryLSTM achieved the highest accuracy (R2=0.86). Attention mechanism identified key predictors (bone disorder, chronic kidney disease, lumbar fusion) and influential temporal segments.

Conclusion: SurgeryLSTM is an effective, interpretable AI solution for LOS prediction, supporting its integration into clinical workflows for better discharge planning and patient care.

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [17] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: CMCO introduces a framework for robust uncertainty quantification in deep learning for virtual sensing, combining Monte Carlo dropout and conformal prediction to provide calibrated, distribution-free prediction intervals without retraining or custom loss design.


<details>
  <summary>Details</summary>
Motivation: The need for reliable uncertainty quantification in high-stakes domains with sparse or noisy sensor data drives the development of CMCO.

Method: CMCO unifies Monte Carlo dropout and split conformal prediction in a DeepONet architecture, enabling spatially resolved uncertainty estimates.

Result: CMCO achieves near-nominal empirical coverage across turbulent flow, elastoplastic deformation, and cosmic radiation dose estimation, even in challenging settings.

Conclusion: CMCO offers a plug-and-play UQ solution for neural operators, enabling trustworthy real-time inference in safety-critical applications.

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [18] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: Einstein Fields is a neural representation for compressing 4D numerical relativity simulations into compact neural network weights, enabling efficient modeling of spacetime geometry and physical quantities via automatic differentiation.


<details>
  <summary>Details</summary>
Motivation: To address the computational intensity of numerical relativity simulations by leveraging neural networks for compact, efficient, and accurate modeling of spacetime dynamics.

Method: Uses Neural Tensor Fields to encode spacetime geometry, naturally capturing dynamics as a byproduct, and employs automatic differentiation for deriving physical quantities.

Result: Demonstrates potential in continuum modeling, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use across canonical test beds.

Conclusion: Einstein Fields offer a scalable and expressive approach to numerical relativity, supported by an open-source JAX-based library.

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [19] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: A survey on synthetic tabular data generation, focusing on preserving feature relationships, statistical fidelity, and privacy, with a novel taxonomy and benchmark framework.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for synthetic data due to strict privacy regulations and limited access to real-world data, especially in finance, healthcare, and social sciences.

Method: Comprehensive review of recent advances, introducing a taxonomy based on generation objectives (downstream applications, privacy, utility) and proposing a benchmark framework.

Result: A roadmap for future research and practical deployment of synthetic tabular data in privacy-critical environments.

Conclusion: The survey bridges theory and practice, guiding methodological design and evaluation for synthetic data generation.

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [20] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: The paper introduces tensor representations and sparse autoencoders for analyzing irregular event time series, demonstrating effectiveness in X-ray astronomy.


<details>
  <summary>Details</summary>
Motivation: Event time series are unstructured and irregular, making pattern extraction challenging with conventional methods.

Method: Proposes 2D/3D tensor representations and sparse autoencoders to learn meaningful latent embeddings.

Result: Successfully captures temporal/spectral signatures and classifies X-ray transients in real-world data.

Conclusion: The framework is flexible, scalable, and generalizable for analyzing event time series across domains.

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [21] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: The paper evaluates five deep generative models for industrial tire design, finding diffusion models perform best overall, with specific strengths in conditional and constrained scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of clarity on which deep generative models are best for complex manufacturing design tasks, particularly in tire architecture generation.

Method: Comparative study of five models (VAE, GAN, MMVAE, DDPM, MDM) across three industrial scenarios: unconditional generation, component-conditioned generation, and dimension-constrained generation. Introduces categorical inpainting for conditional scenarios.

Result: Diffusion models excel overall; masking-trained VAE outperforms MMVAE in component-conditioned tasks, and MDM leads in-distribution while DDPM generalizes better to out-of-distribution constraints.

Conclusion: Diffusion models are most effective for industrial tire design, with specific models suited to different scenarios, highlighting the importance of tailored evaluation metrics.

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [22] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: The paper introduces metrics like dropout robustness, embedding similarity, and sparsity to predict grokking—delayed generalization in neural networks. These metrics reveal patterns during the transition from memorization to generalization.


<details>
  <summary>Details</summary>
Motivation: To understand and forecast grokking behavior in neural networks, which involves delayed generalization after training accuracy improves.

Method: Proposes metrics such as Dropout Robustness Curve (DRC), variance under dropout, embedding similarity, and sparsity measures to analyze grokking.

Result: Metrics like DRC and inactive neuron percentage show patterns during grokking, with embeddings tending to a bimodal distribution. These correlate with dataset symmetries.

Conclusion: The introduced metrics effectively forecast grokking and provide insights into its behavior and origins.

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [23] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: Proposes a privacy-preserving FL evaluation protocol using Zero-Knowledge Proofs (ZKPs) to verify local loss thresholds without revealing raw data.


<details>
  <summary>Details</summary>
Motivation: Addresses privacy leakage risks in FL evaluation by sharing performance metrics.

Method: Uses ZKPs for verifiable evaluation, with self-contained modules for FL simulation, ZKP circuit design, and testing on MNIST and HAR datasets.

Result: Evaluated computational overhead, communication cost, and verifiability for CNN (MNIST) and MLP (HAR) models.

Conclusion: The protocol ensures privacy-preserving and verifiable FL evaluation without external APIs.

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [24] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: The paper explores the compositional structure of linear layers in large models using Clifford algebra, identifying bivectors as fundamental primitives and introducing a rotor-based decomposition method with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To understand and synthesize the fundamental building blocks (geometric primitives) of linear transformations in large models, which remain poorly understood.

Method: Uses Clifford algebra to express linear layers as compositions of bivectors and introduces a differentiable algorithm to decompose them into rotors, reducing parameter count from O(d^2) to O(log^2 d).

Result: Rotor-based layers match the performance of strong baselines like block-Hadamard and low-rank approximations in LLM attention layers.

Conclusion: The study provides an algebraic perspective on how geometric primitives compose into higher-level functions in deep models, offering a parameter-efficient alternative to dense matrices.

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [25] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: The paper analyzes how coreset selection methods impact dataset biases and model robustness, revealing nuanced interactions and limited guarantees for bias reduction.


<details>
  <summary>Details</summary>
Motivation: To understand if and how dataset reduction methods like coreset selection affect biases in datasets and downstream model robustness.

Method: Comprehensive analysis using ten spurious correlation benchmarks, five score metrics, and five data selection policies across various coreset sizes.

Result: Embedding-based scores reduce bias risk compared to learning dynamics. Difficult samples may lower bias but don't ensure robustness.

Conclusion: Coreset selection methods can influence bias but lack reliability in guaranteeing robust models.

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [26] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: A study proposes an LSTM-based model using satellite and ground data to predict leaf-fall timings for UK railroads, reducing prediction errors and aiding mitigation efforts.


<details>
  <summary>Details</summary>
Motivation: Leaf-fall disrupts UK rail traffic, costing £300M yearly. Current prediction methods lack scalability and reliability, necessitating a better solution.

Method: An LSTM network trained on ground-truth leaf-fall data, multispectral, and meteorological satellite data.

Result: The model achieved RMSE of 6.32 days for start and 9.31 days for end of leaf-fall, outperforming prior methods.

Conclusion: The system improves leaf-fall prediction, aiding rail operators in scheduling mitigation and advancing ecological understanding.

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [27] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: The paper introduces preference-based MDPs (PbMDPs) with adversarial preferences and Borda scores, establishes regret lower bounds, and proposes algorithms achieving sublinear regret.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding episodic MDPs with adversarial preferences, where learners observe preferences rather than numerical losses, focusing on Borda scores for reward functions.

Method: The work derives regret lower bounds for PbMDPs and proposes two algorithms: a global optimization approach and a policy optimization method, both achieving sublinear regret.

Result: A regret lower bound of Ω((H²SK)¹/³T²/³) is established, and algorithms achieve regret bounds of Õ((H²S²K)¹/³T²/³) and Õ((H⁶SK⁵)¹/³T²/³) under known and unknown transitions.

Conclusion: The paper provides theoretical foundations and practical algorithms for PbMDPs with adversarial preferences, demonstrating sublinear regret and addressing computational challenges.

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [28] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: FLEX is a GGM framework combining structurally-conditioned graph generation and adversarial co-training to improve GNN link-prediction in OOD scenarios without expert knowledge.


<details>
  <summary>Details</summary>
Motivation: Current GNNs for link prediction assume uniform data distribution, while GGMs are underutilized. FLEX bridges this gap for OOD scenarios.

Method: FLEX uses structurally-conditioned graph generation and adversarial co-training between an auto-encoder and GNN to align sample distributions.

Result: FLEX enhances link-prediction performance in OOD settings, validated by experiments in synthetic and real-world scenarios.

Conclusion: FLEX effectively improves GNN performance in OOD scenarios without requiring expert knowledge, demonstrating the value of graph data augmentation.

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [29] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: The paper explores global forecasting models (GFMs) for load forecasting in power networks, addressing limitations of local models (LFMs) like generalizability and scalability. It examines feature- and target-transforming models, proposing clustering methods to handle data heterogeneity, with GFMs showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Traditional local forecasting models (LFMs) struggle with generalizability, scalability, and data drift. GFMs offer a promising alternative, but their effectiveness under data heterogeneity and drift needs investigation.

Method: The study compares feature-transforming and target-transforming GFMs, proposing time series clustering (TSC) methods to manage data heterogeneity. Experiments use Alberta's electricity load data.

Result: Global target-transforming models outperform local ones, especially with global features and clustering. Feature-transforming models require TSC to balance local and global dynamics.

Conclusion: GFMs, particularly target-transforming ones, enhance load forecasting accuracy and robustness, with clustering techniques mitigating data heterogeneity challenges.

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [30] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: The paper introduces a GEE-powered GNN (GG) framework, using one-hot graph encoder embedding (GEE) for better initial node features, improving GNN performance in node clustering and classification.


<details>
  <summary>Details</summary>
Motivation: GNNs often suffer from suboptimal performance due to poor initial feature representations. This work aims to enhance GNNs by using a statistically grounded method (GEE) for feature initialization.

Method: The proposed GG framework integrates GEE to generate high-quality initial node features. An enhanced variant, GG-C, concatenates GG and GEE outputs for node classification.

Result: GG achieves state-of-the-art performance in node clustering and faster convergence. GG-C outperforms baselines in node classification.

Conclusion: Principled, structure-aware feature initialization (via GEE) is crucial for maximizing GNN potential, as demonstrated by GG and GG-C.

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [31] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: The paper explores integrating Conformal Prediction with Ensemble-SINDy (E-SINDy) to quantify uncertainty in SINDy models, demonstrating improved reliability in time series forecasting, feature importance, and model coefficients.


<details>
  <summary>Details</summary>
Motivation: Quantifying uncertainty in SINDy models is crucial for reliability, especially in safety-critical applications. Existing methods lack robustness under non-Gaussian noise.

Method: Integrates Conformal Prediction with E-SINDy for three applications: time series prediction uncertainty, model selection via feature importance, and coefficient uncertainty quantification.

Result: Conformal Prediction with E-SINDy achieves desired coverage in forecasting, robustly quantifies feature importance, and provides better uncertainty intervals for coefficients under non-Gaussian noise.

Conclusion: The integration of Conformal Prediction with E-SINDy enhances uncertainty quantification in SINDy models, offering more reliable and robust results.

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [32] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: A novel GNN-based framework, Graph-in-Graph (GiG), integrates transductive and inductive learning to improve drug-target interaction prediction by leveraging molecular and network-level features.


<details>
  <summary>Details</summary>
Motivation: Current machine learning approaches struggle to effectively integrate diverse drug and target features for DTI prediction.

Method: The GiG model represents drug and target molecular structures as meta-nodes in a DTI graph, combining transductive and inductive learning.

Result: GiG outperforms existing methods across all evaluation metrics on a specialized benchmark.

Conclusion: Integrating different learning paradigms and interaction data enhances DTI prediction accuracy.

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [33] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: Torsional-GFN, a conditional GFlowNet, efficiently samples molecular conformations proportional to the Boltzmann distribution, enabling zero-shot generalization to unseen molecular structures.


<details>
  <summary>Details</summary>
Motivation: Generating stable molecular conformations is essential for drug discovery, particularly for estimating binding affinity. Traditional methods like molecular dynamics are inefficient, motivating the use of generative machine learning.

Method: Torsional-GFN conditions on molecular graphs and local structures (bond lengths, angles) to sample torsion angle rotations, using only a reward function for training.

Result: The model successfully samples conformations proportional to the Boltzmann distribution for multiple molecules, with zero-shot generalization to unseen bond lengths and angles from MD simulations.

Conclusion: Torsional-GFN offers a scalable approach for larger molecular systems, potential zero-shot generalization to new molecules, and future integration of local structure generation.

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [34] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: The paper investigates the effectiveness of Contrastive Activation Addition (CAA) across different scales of Llama 2 models, finding it works best in early-mid layers but loses efficacy with larger models. Negative steering is more impactful than positive.


<details>
  <summary>Details</summary>
Motivation: To understand how alignment techniques like CAA perform as LLMs scale in size and complexity, given uncertainty about their efficacy.

Method: Applied CAA to Llama 2 models (7B, 13B, 70B) by manipulating the residual stream using contrastive pairs (e.g., hate to love) and evaluated refusal behavior.

Result: CAA is most effective in early-mid layers, less effective with larger models, and negative steering has stronger effects than positive.

Conclusion: CAA's effectiveness varies with model scale and layer placement, with negative steering being more influential, suggesting limitations for scaling alignment techniques.

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [35] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: The study uses an XGBoost Classifier to predict delays in the Dutch railway network, focusing on topological features and comparing multiple classifiers, but results show limited performance.


<details>
  <summary>Details</summary>
Motivation: Addresses a gap in delay prediction studies for the Dutch railway network, emphasizing broader network-wide patterns to mitigate ripple effects.

Method: Employs an XGBoost Classifier with Node Centrality Measures and compares it with RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression.

Result: Limited performance, especially in non-simultaneous testing, indicating a need for context-specific adaptations.

Conclusion: Contributes to transportation network evaluation and suggests future directions for more robust delay prediction models.

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [36] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: The paper introduces CLID-MU, a meta-learning method for noisy label scenarios that doesn't rely on clean labeled data, leveraging cross-layer data structure consistency to guide training.


<details>
  <summary>Details</summary>
Motivation: Existing meta-learning methods for noisy labels require clean labeled meta-datasets, which are hard to obtain. This work addresses the challenge of noisy label learning without such dependencies.

Method: CLID-MU uses cross-layer information divergence to evaluate model performance by aligning data structures between the last hidden and final layers, bypassing the need for labels.

Result: CLID-MU outperforms state-of-the-art methods on benchmark datasets with synthetic and real-world noise.

Conclusion: CLID-MU effectively addresses noisy label learning without clean labeled data, demonstrating superior performance.

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [37] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: FlatVI is a training framework for variational autoencoders that enforces Euclidean geometry in latent space, improving trajectory reconstruction in single-cell RNA sequencing data.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume linear shifts and Euclidean geometry in latent space, which may not align with geodesic paths on the data manifold, limiting accuracy.

Method: FlatVI regularizes the latent manifold of variational autoencoders to encourage Euclidean geometry, ensuring straight lines in latent space approximate geodesic interpolations.

Result: Experiments show improved trajectory reconstruction and manifold interpolation in synthetic and single-cell RNA sequencing data.

Conclusion: FlatVI enhances compatibility with downstream methods by enforcing Euclidean latent geometry, improving accuracy in modeling cellular state transitions.

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [38] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen is an automated framework for generating MNIST-style datasets tailored to specific domains, combining CLIP-based semantics, reinforcement learning, and human feedback for efficient and accurate categorization.


<details>
  <summary>Details</summary>
Motivation: Standard datasets like MNIST are too generic for domain-specific tasks, and creating custom datasets is often impractical due to time, legal, or scope constraints.

Method: MNIST-Gen uses hierarchical semantic categorization, CLIP-based understanding, reinforcement learning, and human feedback to generate datasets. It offers multiple processing modes (individual review, smart batch, fast batch) and models transformations as composable morphisms.

Result: MNIST-Gen achieved 85% automatic categorization accuracy and 80% time savings, demonstrated by generating Tree-MNIST and Food-MNIST datasets.

Conclusion: MNIST-Gen provides a practical solution for creating domain-specific datasets efficiently, addressing limitations of standard benchmarks.

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [39] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: Kevin, a model trained with multi-turn RL, improves CUDA kernel generation, boosting correctness from 56% to 82% and speedup from 0.53x to 1.10x over PyTorch Eager.


<details>
  <summary>Details</summary>
Motivation: GPU kernel writing is iterative and critical for AI efficiency, making it suitable for RL due to verifiable rewards like correctness and speedup.

Method: Developed a multi-turn RL recipe to address challenges like learning from long trajectories and reward attribution across turns.

Result: Kevin outperforms its base model (QwQ-32B) and frontier models (o4-mini), achieving higher correctness and speedup.

Conclusion: Multi-turn RL is effective for kernel optimization, with serial refinement proving more beneficial than parallel sampling.

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [40] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: SynCoGen is a framework for synthesizable 3D molecule generation, combining masked graph diffusion and flow matching, achieving state-of-the-art results in molecule and conformer generation.


<details>
  <summary>Details</summary>
Motivation: The challenge of synthesizability in generative small molecule design, especially in 3D representations, limits geometry-based conditional generation.

Method: SynCoGen uses simultaneous masked graph diffusion and flow matching to sample from joint distributions of building blocks, reactions, and atomic coordinates, trained on the SynSpace dataset.

Result: Achieves top performance in molecule and conformer generation and competitive results in zero-shot molecular linker design for drug discovery.

Conclusion: SynCoGen provides a foundation for non-autoregressive molecular generation, enabling applications like analog expansion and lead optimization.

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [41] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: The paper proposes integrating simultaneous training and pruning in RL, specifically for OFENet-enhanced RL, using stochastic optimization and variational Bernoulli distributions to prune inactive units, achieving efficient RL agents with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the computational and memory complexity of scaling deep neural networks in RL by applying pruning methods, which are underexplored in RL compared to supervised learning.

Method: Combines training and pruning in RL by formulating a stochastic optimization problem over network weights and variational Bernoulli distributions for pruning. Uses cost-aware regularization tailored to OFENet architectures.

Result: Demonstrates significant pruning of OFENets with minimal performance loss on MuJoCo benchmarks, showing pruned networks outperform smaller networks trained from scratch.

Conclusion: Pruning during training in RL produces more efficient and higher-performing agents, validating the proposed approach.

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [42] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: MLED is a multi-level LLM-enhanced framework for graph fraud detection, integrating textual and graph data to improve fraudster identification.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore raw textual information and struggle with multimodal fusion of text and graph data.

Method: MLED uses LLMs to extract textual knowledge and integrates it with graph structures via type-level and relation-level enhancers.

Result: MLED outperforms existing methods on four real-world datasets.

Conclusion: MLED is a generalized, effective framework for enhancing graph fraud detection with LLMs.

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [43] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: HyperEvent reframes dynamic link prediction as hyper-event recognition, capturing structural cohesion of causally related events, and outperforms state-of-the-art methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture structural cohesion of composite hyper-events, limiting their effectiveness in dynamic link prediction.

Method: HyperEvent dynamically constructs association sequences using event correlation vectors to quantify dependencies and predict hyper-events.

Result: HyperEvent outperforms state-of-the-art methods on 4/5 datasets and achieves a 6.95% improvement in Mean Reciprocal Rank on the Flight dataset with 10.17% training time.

Conclusion: HyperEvent is a scalable, accurate framework for dynamic link prediction, validated by superior performance and efficiency on large-scale graphs.

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [44] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: DUSE is a data expansion framework for AMR that uses uncertainty scoring and active learning to address data scarcity, outperforming baselines and showing cross-architecture generalization.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for AMR require large labeled datasets, but target domain data is often scarce. Manual collection or augmentation are costly or insufficient.

Method: DUSE employs an uncertainty scoring function to filter useful samples from AMR datasets and refines the scorer using active learning.

Result: DUSE outperforms 8 coreset selection baselines in balanced and imbalanced settings and generalizes well to unseen models.

Conclusion: DUSE effectively tackles data scarcity in AMR by dynamically expanding samples and demonstrates strong performance and generalization.

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [45] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: Protenix-Mini is a lightweight, optimized model for protein structure prediction, reducing computational overhead while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for efficient biomolecular structure prediction in real-world applications drives the development of a lightweight model without significant accuracy loss.

Method: Key modifications include replacing the Multi-step AF3 sampler with a few-step ODE sampler, pruning redundant Transformer blocks, and substituting the MSA module with an ESM module.

Result: Protenix-Mini achieves high-fidelity predictions with only a 1-5% performance drop compared to the full-scale model.

Conclusion: Protenix-Mini is ideal for resource-limited applications requiring accurate protein structure prediction.

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [46] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM is a communication-efficient and compute-aware strategy for deploying foundation models on edge devices, reducing communication and computation overhead with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Foundation models (FMs) face challenges in edge deployment due to high communication and computation demands. PRISM addresses these issues for practical edge use.

Method: PRISM uses Segment Means for feature approximation, restructures self-attention to avoid redundant computations, and introduces partition-aware causal masking for autoregressive models.

Result: PRISM reduces communication overhead by up to 99.2% and computation by 51.24% for BERT, with minor accuracy degradation.

Conclusion: PRISM provides a scalable and efficient solution for deploying foundation models in resource-constrained edge environments.

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [47] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: A jointly efficient algorithm for generalized linear bandits (GLB) achieves near-optimal regret with low computational cost.


<details>
  <summary>Details</summary>
Motivation: GLBs model diverse reward distributions but face trade-offs between computational and statistical efficiency. Existing methods compromise one for the other.

Method: Proposes an algorithm using a tight confidence set for online mirror descent (OMD) estimator, leveraging mix loss analysis for efficiency.

Result: The method achieves nearly optimal regret with constant-time updates, matching statistical efficiency of maximum likelihood estimation.

Conclusion: The algorithm balances computational and statistical efficiency, offering a practical solution for GLB problems.

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [48] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: TuneQn is a suite for selective quantization of ONNX models, optimizing performance and size across hardware, reducing accuracy loss by up to 54.14% and model size by 72.9%.


<details>
  <summary>Details</summary>
Motivation: Fully quantized models often suffer from accuracy loss and deployment challenges on low-end hardware, necessitating selective quantization.

Method: TuneQn selectively quantizes ONNX models, deploys them on CPUs/GPUs, profiles performance, and uses multi-objective optimization to identify optimal models.

Result: TuneQn achieved up to 54.14% less accuracy loss and 72.9% model size reduction compared to fully quantized and original models, respectively.

Conclusion: TuneQn effectively balances accuracy and model size through selective quantization, making it practical for deployment on diverse hardware.

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [49] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: OrdShap is a novel attribution method that disentangles feature value and position effects in sequential deep learning models, providing deeper insights into model behavior.


<details>
  <summary>Details</summary>
Motivation: Existing feature attribution methods assume fixed feature ordering, conflating feature values and their positions, limiting interpretability.

Method: Introduces OrdShap, which quantifies how predictions change with feature position permutations, leveraging game-theoretic principles.

Result: Empirical results show OrdShap effectively captures feature value and position attributions across health, NLP, and synthetic datasets.

Conclusion: OrdShap offers a theoretically grounded and practical solution for position-sensitive attribution in sequential models.

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [50] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: A framework using nonstationary kernels with neural network-parameterized kernels improves Gaussian process regression, outperforming stationary and hierarchical models in accuracy and log-score.


<details>
  <summary>Details</summary>
Motivation: Stationary kernels in Gaussian processes limit expressiveness and may not suit many datasets. Nonstationary kernels, parameterized by neural networks, offer flexibility and better performance.

Method: Proposes a framework where nonstationary kernel parameters vary across feature space, modeled by a neural network. Joint training of the neural network and Gaussian process is done using the chain rule for derivatives.

Result: Outperforms stationary and hierarchical models in accuracy and log-score on machine learning datasets. Demonstrates ability to recover nonstationary parameters in spatial datasets.

Conclusion: The method is flexible, scalable, and adaptable to various nonstationary kernels, providing improved performance and interpretability.

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [51] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: The paper proposes a policy-improved deep deterministic policy gradient (pi-DDPG) framework to dynamically manage drivers' acceptance of Discount Express services, addressing challenges like high stochasticity and limited historical data.


<details>
  <summary>Details</summary>
Motivation: To mitigate market fragmentation and improve matching efficiency while managing reduced profit margins for ride-hailing platforms.

Method: Develops a pi-DDPG framework with a refiner module, convolutional LSTM, and prioritized experience replay to optimize driver acceptance behavior.

Result: Numerical experiments show pi-DDPG improves learning efficiency and reduces early-stage training losses.

Conclusion: The pi-DDPG framework effectively manages driver acceptance behavior, enhancing platform performance under uncertain conditions.

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [52] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: A novel prompt-based method for online continual learning (OCL) is proposed, addressing data privacy and catastrophic forgetting. It outperforms SOTAs with fewer parameters and moderate efficiency.


<details>
  <summary>Details</summary>
Motivation: Data privacy constraints in OCL complicate catastrophic forgetting. Existing methods either replay data (violating privacy) or use growing parameters (reducing throughput).

Method: The method includes: (1) a lightweight prompt generator, (2) trainable scaler-shifter, (3) PTM generalization preservation, and (4) hard-soft updates.

Result: Achieves higher performance on CIFAR100, ImageNet-R, ImageNet-A, and CUB datasets with fewer parameters and moderate efficiency.

Conclusion: The proposed method balances performance and efficiency, offering a practical solution for OCL. Code is available on GitHub.

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [53] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: Meta-IR is a meta-learning framework for imbalanced regression, recommending optimal resampling and learning model pipelines per task, outperforming AutoML and baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of imbalanced regression tasks, where rare target values complicate model performance, by avoiding exhaustive testing of resampling and learning model combinations.

Method: Proposes Meta-IR with Independent and Chained formulations. Meta-classifiers map meta-features to best pipelines, with Chained showing superior performance by modeling relationships between resampling and learning models.

Result: Meta-IR outperformed AutoML and 42 baseline configurations, with the Chained formulation proving more effective.

Conclusion: Meta-IR provides an efficient, zero-shot solution for imbalanced regression, leveraging meta-learning to recommend optimal pipelines without extensive testing.

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [54] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: The paper introduces Thought Purity (TP), a defense mechanism against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs), addressing security vulnerabilities while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LRMs like Deepseek-R1 are vulnerable to adversarial attacks (e.g., CoTA) that exploit Chain-of-Thought (CoT) processes, compromising reasoning and task performance.

Method: Proposes TP with three components: safety-optimized data processing, reinforcement learning-enhanced rules, and adaptive monitoring metrics.

Result: TP effectively defends against CoTA, balancing security and functionality in LRMs.

Conclusion: TP is the first comprehensive defense against CoTA, enhancing security in next-gen AI architectures.

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [55] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: The paper conducts an experimental study on imbalanced regression tasks, proposing a taxonomy and evaluating balancing and predictive models with user-focused metrics.


<details>
  <summary>Details</summary>
Motivation: Imbalanced data is common in real-world scenarios, but most research focuses on classification, leaving regression tasks understudied.

Method: The study involves extensive experiments with various balancing and predictive models, using tailored metrics for evaluation. A taxonomy for imbalanced regression is proposed based on regression model, learning process, and evaluation metrics.

Result: The study provides insights into the effectiveness of balancing strategies in regression, highlighting their benefits for model learning.

Conclusion: The work advances understanding of imbalanced regression, offering practical guidance and directions for future research.

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [56] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: NOCTA is a method for cost-effective feature acquisition in dynamic prediction tasks, outperforming baselines with its non-greedy approach.


<details>
  <summary>Details</summary>
Motivation: Resource constraints in critical applications like healthcare require efficient feature acquisition, balancing information gain with cost and risk.

Method: NOCTA includes two estimators: NOCTA-NP (non-parametric, nearest neighbors) and NOCTA-P (parametric, utility prediction).

Result: NOCTA variants outperform existing baselines on synthetic and real-world medical datasets.

Conclusion: NOCTA effectively addresses the challenge of dynamic feature acquisition with cost-awareness.

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [57] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: The paper addresses the challenge of replicable learning in reinforcement learning (RL), showing that exploration in RL is not inherently more expensive than batch learning. It presents an algorithm with near-optimal sample efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the replicability crisis in empirical science and machine learning, particularly the gap in understanding replicable learning in control settings like RL compared to batch settings.

Method: The authors develop a replicable RL algorithm for low-horizon tabular MDPs, achieving sample efficiency of $	ilde{O}(S^2A)$. They also provide matching lower bounds to demonstrate near-optimality.

Result: The main result is a replicable RL algorithm requiring $	ilde{O}(S^2A)$ samples, bridging the gap between generative and episodic settings. Lower bounds confirm the algorithm's near-optimality.

Conclusion: The work resolves the question of whether replicable exploration is inherently more expensive, showing it is not, and provides a near-optimal solution for replicable RL.

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [58] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: A Mixture of Raytraced Experts (MoE) architecture dynamically selects expert sequences, enabling variable computation depth and width, improving accuracy and reducing training epochs by 10-40%.


<details>
  <summary>Details</summary>
Motivation: Existing MoE architectures use fixed computation per sample, limiting flexibility and efficiency. This work aims to enhance accuracy and training speed by dynamically selecting expert sequences.

Method: The model trains by iteratively sampling experts, unfolding sequences like Recurrent Neural Networks, eliminating the need for load-balancing mechanisms.

Result: Preliminary experiments show 10-40% fewer training epochs with comparable/higher accuracy.

Conclusion: The approach opens new research directions for faster, more expressive MoE models, with code available for further exploration.

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [59] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: A machine learning framework reduces RF power amplifier simulation needs by 65% while maintaining high accuracy, using MaxMin Latin Hypercube Sampling and CatBoost.


<details>
  <summary>Details</summary>
Motivation: To minimize simulation time and resources in RF power amplifier design without sacrificing accuracy.

Method: Combines MaxMin Latin Hypercube Sampling with CatBoost to strategically select 35% of critical simulation points, predicting performance across the design space.

Result: Achieves 65% simulation reduction with ±0.3 to ±0.4 dBm accuracy, 0.901 average R², and 58.24% to 77.78% time savings.

Conclusion: The framework enables rapid, accurate RF power amplifier design iterations, meeting production standards efficiently.

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [60] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: A computational approach using smartwatch data (audio and inertial) to detect in-person conversations, achieving 82.0% and 77.2% F1-scores in lab and semi-naturalistic settings.


<details>
  <summary>Details</summary>
Motivation: Social interactions are vital for human behavior, and detecting conversations is foundational. Current methods struggle in acoustically-challenging scenarios.

Method: Leveraged audio and inertial data from smartwatches, tested with 11 lab and 24 semi-naturalistic participants. Evaluated ML/DL models with 3 fusion methods.

Result: Achieved 82.0% (lab) and 77.2% (semi-naturalistic) macro F1-scores, showing benefits of multimodal data fusion.

Conclusion: Multimodal sensing (audio + inertial) improves conversation detection, especially in challenging environments.

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [61] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: The paper explores how feedback granularity affects the accuracy of predicting population feedback distributions, showing sophisticated methods outperform regularized averaging for granular feedback.


<details>
  <summary>Details</summary>
Motivation: To improve predictions of population feedback distributions when limited by small sample sizes, especially as feedback granularity increases.

Method: Compare regularized averaging with more sophisticated methods for combining feedback, testing on binary and five-point feedback scales.

Result: Sophisticated methods significantly outperform regularized averaging with granular feedback (e.g., five-point scale), requiring fewer individuals for the same accuracy.

Conclusion: Feedback granularity determines the effectiveness of sophisticated methods, with greater granularity enabling substantial improvements over simple averaging.

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [62] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: The paper presents a theoretical framework for replay-based continual learning, deriving information-theoretic bounds to explain generalization behavior and mitigate catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of generalization in replay-based continual learning methods, despite their empirical success.

Method: Establishes a unified theoretical framework with information-theoretic bounds, analyzing the interaction between memory buffers and current tasks. Uses stochastic gradient Langevin dynamics (SGLD) as an example.

Result: Derived bounds show that limited exemplars of previous tasks improve generalization and mitigate forgetting. Prediction-based bounds offer tighter, tractable upper bounds.

Conclusion: The framework effectively captures generalization dynamics in replay-based continual learning, validated by experiments.

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [63] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: A data-driven method using cGANs to generate urban mobility flows, incorporating dynamic factors like land use and population density, outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: Urban planners need accurate mobility simulations for sustainable development, but existing models lack adaptability to evolving urban factors.

Method: Uses conditional generative adversarial networks (cGANs) to blend historical data with dynamic parameters like land use and region sizes.

Result: Demonstrated effectiveness with Singapore mobile phone data, showing superior performance over static or historical-data-dependent models.

Conclusion: The approach offers a scalable, adaptable solution for urban mobility simulation without extensive calibration, aiding future urban planning.

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [64] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: A novel method for assessing representational alignment reveals that algebraic symmetries in network primitives predict task-agnostic structure. Activation functions' algebraic properties influence whether representations discretize or remain continuous, impacting interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: To understand how functional form choices in neural networks introduce unintended inductive biases, leading to task-independent structural artifacts in representations.

Method: Developed a tool based on Spotlight Resonance to analyze representational alignment, focusing on activation functions' algebraic symmetries through an ablation study.

Result: Discrete activation functions lead to discretized representations, while continuous ones maintain continuity. Quantization correlates with increased reconstruction error.

Conclusion: Functional form choices causally influence representation structure, offering insights into emergent interpretability and potential performance trade-offs.

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [65] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: The paper introduces the 'informativeness gap' to compare miscalibrated predictors, generalizing existing notions like U-Calibration and Blackwell informativeness. It provides a dual characterization and a relaxed EMD-based measure, proving its soundness, completeness, and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Decision-makers often face multiple miscalibrated models; the goal is to determine which is more useful for downstream tasks.

Method: Introduces the informativeness gap, a framework comparing predictors, and a dual characterization leading to a relaxed EMD-based measure.

Result: The measure is sound, complete, and sample-efficient, with novel combinatorial insights for calibrated predictors.

Conclusion: The informativeness gap generalizes prior work and offers a practical, theoretically grounded tool for comparing predictors.

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [66] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: The paper proposes a semi-supervised Federated Learning (FL) approach for spectrum sensing (FLSS) to address labeled data scarcity and introduces a novel defense against data poisoning attacks, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of wireless devices exacerbates spectrum scarcity, and centralized ML-based DSA systems face privacy and regulatory challenges. FL offers a promising alternative but lacks labeled data and is vulnerable to attacks.

Method: A semi-supervised FL approach combined with energy detection trains models on unlabeled data. A novel defense mechanism, inspired by vaccination, counters data poisoning attacks without majority-based assumptions.

Result: FLSS achieves near-perfect accuracy on unlabeled datasets and maintains robustness against data poisoning attacks, even with malicious participants.

Conclusion: The proposed FLSS approach effectively addresses labeled data scarcity and security vulnerabilities, making it a viable solution for dynamic spectrum allocation.

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [67] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: HyDRA is a hybrid RF architecture combining VMD, CNNs, Transformers, and Mamba for secure wireless device recognition, excelling in both closed-set and open-set classification with real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing wireless security through non-cryptographic RFFI by leveraging hardware-induced signal distortions for device recognition.

Method: Integrates optimized VMD for preprocessing and a novel architecture fusing CNNs, Transformers, and Mamba for classification, supporting both closed-set and open-set tasks.

Result: Achieves SOTA accuracy in closed-set scenarios and robust open-set performance, with millisecond-level inference on NVIDIA Jetson Xavier NX.

Conclusion: HyDRA provides a practical, efficient solution for real-time wireless authentication in diverse environments.

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [68] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: RiemannLoRA improves LoRA by treating LoRA matrices as a smooth manifold, addressing overparametrization and initialization challenges, enhancing convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in LoRA include optimal initialization and overparametrization in low-rank matrix factorization, which hinder efficiency and performance.

Method: Proposes RiemannLoRA, treating LoRA matrices as a smooth manifold to remove overparametrization and determine optimal initialization via fastest loss decrease direction.

Result: RiemannLoRA outperforms standard LoRA and its variants in convergence speed and final performance on LLMs and diffusion models.

Conclusion: RiemannLoRA provides a unified, efficient solution to LoRA's challenges, improving both training dynamics and model performance.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [69] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3 improves global weather forecasting using a scalable ML approach, outperforming conventional models in accuracy and speed while maintaining realistic dynamics and probabilistic calibration.


<details>
  <summary>Details</summary>
Motivation: To advance weather modeling by addressing the need for scalable, geometrically accurate, and probabilistically calibrated ensemble forecasting.

Method: Uses a convolutional neural network tailored for spherical geometry, with a novel training paradigm for large-scale efficiency.

Result: Achieves faster forecasts (8-60x speedup), superior accuracy, and realistic spectra even at 60-day lead times.

Conclusion: FourCastNet 3 is a promising tool for meteorological forecasting and early warning systems due to its efficiency and performance.

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [70] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: The paper introduces GMRF MCVAE, a generative model using Gaussian Markov Random Fields to improve structural coherence in multi-component datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-component generative modeling oversimplify dependencies, compromising structural coherence.

Method: The GMRF MCVAE embeds Gaussian Markov Random Fields into prior and posterior distributions to model cross-component relationships.

Result: The model achieves state-of-the-art performance on synthetic and real-world datasets, enhancing structural coherence.

Conclusion: GMRF MCVAE is effective for applications requiring robust modeling of multi-component coherence.

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [71] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: UrbanRadio3D introduces a large-scale 3D radio map dataset with pathloss, DoA, and ToA metrics, addressing limitations of 2D methods. A 3D UNet and diffusion-based RadioDiff-3D framework are proposed, achieving superior performance in high-dimensional radio map construction.


<details>
  <summary>Details</summary>
Motivation: Existing radio map methods focus on 2D pathloss prediction, neglecting key parameters like DoA, ToA, and vertical variations due to static learning paradigms. This limits generalization beyond training data.

Method: UrbanRadio3D is created via ray tracing in urban environments, offering a 3D dataset with pathloss, DoA, and ToA. A 3D UNet and RadioDiff-3D (a diffusion-model-based framework) are proposed for 3D radio map construction.

Result: RadioDiff-3D outperforms existing methods, constructing rich, high-dimensional radio maps under diverse conditions. UrbanRadio3D is significantly larger and more detailed than prior datasets.

Conclusion: UrbanRadio3D and RadioDiff-3D provide a foundational dataset and benchmark for 3D environment-aware communication research, advancing beyond 2D limitations.

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [72] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: The paper addresses the challenge of explaining evidential clustering results, introducing representativity as a key condition for decision trees to serve as explainers. It proposes the IEMM algorithm for interpretable explanations, validated with high satisfaction rates.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with imperfect real-world data. Evidential clustering handles uncertainty but lacks explainability, which is critical in high-stakes domains like healthcare.

Method: The paper generalizes representativity for partial labeling using utility functions, defines evidential mistakeness, and introduces the IEMM algorithm for cautious decision tree explanations.

Result: The IEMM algorithm achieved up to 93% satisfaction in providing interpretable explanations for evidential clustering.

Conclusion: The work successfully bridges the gap in explaining evidential clustering, offering practical and interpretable solutions for real-world applications.

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [73] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed linear model (PILM) for solving PDEs and estimating coefficients or boundary conditions using linear combinations of basis functions, validated on forward and inverse problems and applied to crustal strain rate estimation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving PDEs and estimating their parameters from data, leveraging linear models for analytical solutions and comparing physical vs. mathematical regularization.

Method: Developed PILM, a linear model using basis functions to represent PDE solutions, tested on forward/inverse problems and applied to geodetic data for strain rate estimation.

Result: PILM provided analytical solutions for linear problems, with mathematical regularization outperforming physical regularization in Bayesian analysis.

Conclusion: PILM offers a solvable framework for linear PDE problems, underdetermined systems, and regularization, with mathematical regularization being more effective.

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [74] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: The paper argues that optimizers in DNNs influence not just convergence rates but also the qualitative properties of learned solutions, advocating for a shift in focus to understand and design optimizers for specific inductive biases.


<details>
  <summary>Details</summary>
Motivation: Early skepticism about DNNs due to nonlinearity and lack of global convergence guarantees has been disproven by empirical success. However, the community's focus on convex optimization as a mental model overlooks the optimizer's role in shaping solution properties.

Method: The paper critiques the current focus on training efficiency (e.g., iteration, FLOPs, time) and proposes a shift toward understanding and designing optimizers to encode specific inductive biases and desired solution properties.

Result: The paper highlights that optimizers can change the effective expressivity of models and encode desiderata in learning, suggesting a need for deeper understanding and intentional design.

Conclusion: The community should prioritize understanding optimizer biases and designing optimizers to induce specific solution properties, recognizing their critical role alongside architecture and data in shaping model outcomes.

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [75] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: A robust causal discovery method leveraging power-law spectral features to improve accuracy in noisy time series data.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods are sensitive to noise, leading to unreliable inferences in real-world applications.

Method: Extracts power-law spectral features from time series to amplify genuine causal signals.

Result: Outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: The proposed method is robust and practically relevant for causal discovery in noisy time series.

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [76] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: RegCL is a continual learning framework for SAM, merging domain-specific modules to avoid catastrophic forgetting and maintain efficiency.


<details>
  <summary>Details</summary>
Motivation: Address performance limitations and scalability issues of SAM in multi-domain adaptation.

Method: Uses model merging (e.g., LoRA modules) with weight optimization to integrate multi-domain knowledge without replay.

Result: Achieves efficient continual learning across multiple datasets without increasing model size or storing historical data.

Conclusion: RegCL effectively integrates multi-domain knowledge while maintaining scalability and efficiency.

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [77] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [78] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: A novel topological framework using Laplacian operators on combinatorial complexes (CCs) for efficient computation of heat kernels, outperforming existing methods in efficiency and expressiveness.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of higher-order message passing in topological neural networks by introducing an efficient and expressive method.

Method: Introduces a Laplacian operator on combinatorial complexes (CCs) to compute heat kernels as node descriptors, enabling multiscale information capture and permutation-equivariant representations.

Result: The method is theoretically maximally expressive, outperforms existing topological methods in efficiency, and shows competitive performance on molecular datasets.

Conclusion: Advances topological deep learning with scalable and expressive representations, opening new possibilities for molecular tasks.

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [79] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: The paper derives sharper PAC bounds for RL in infinite-horizon MDPs, reducing sample complexity by a logarithmic factor to O(SA log A) timesteps.


<details>
  <summary>Details</summary>
Motivation: Existing PAC bounds for RL in MDPs are suboptimal. The study aims to exploit state transition dependencies to improve sample complexity.

Method: Approximates the original MDP using smaller MDPs with subsets of the state-space, extending results to a model-free setting with a PAC-MDP algorithm.

Result: Achieves a logarithmic reduction in sample complexity (O(SA log A)) and demonstrates significant improvement over prior work experimentally.

Conclusion: The proposed method provides sharper PAC bounds and practical efficiency, validated by experimental comparisons.

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [80] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: A novel hardware-software co-design using $MoS_2$ Flash-based analog CAM with soft boundaries improves robustness and accuracy for tree-based models, addressing interpretability and efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about AI trustworthiness, particularly interpretability and robustness, while overcoming computational inefficiencies and device variation vulnerabilities in tree-based models.

Method: A hardware-software co-design approach leveraging $MoS_2$ Flash-based analog CAM with soft boundaries for efficient inference with soft tree-based models.

Result: Achieves 96% accuracy on WDBC and minimal accuracy drop (0.6%) on MNIST under device variation, outperforming traditional methods (45.3% drop).

Conclusion: The approach enhances AI trustworthiness and efficiency, paving the way for specialized hardware solutions.

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [81] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: The paper analyzes how verifier imperfection impacts test-time scaling methods like Best-of-N (BoN) and rejection sampling, linking performance to the verifier's ROC curve geometry.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of how verifier quality affects test-time scaling techniques.

Method: Theoretical analysis of BoN and rejection sampling, relating their accuracy to the verifier's ROC curve geometry, supported by experiments on GSM8K using Llama and Qwen models.

Result: Rejection sampling outperforms BoN for fixed compute, but both converge to the same accuracy in the infinite-compute limit, determined by the ROC curve's slope near the origin.

Conclusion: Verifier imperfection's impact on test-time scaling is precisely characterized by ROC curve geometry, with practical implications for method selection.

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [82] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: Targeted Deep Architectures (TDA) embeds TMLE into neural networks for unbiased causal inference, improving bias reduction and coverage for multi-parameter targets.


<details>
  <summary>Details</summary>
Motivation: Existing neural implementations lack guarantees for solving efficient influence function equations or are computationally expensive for multi-parameter settings.

Method: TDA partitions model parameters, freezing most and iteratively updating a small subset along a targeting gradient derived from influence functions.

Result: TDA reduces bias and improves coverage on benchmark datasets compared to standard neural-network estimators and prior post-hoc methods.

Conclusion: TDA provides a scalable, direct pathway for rigorous causal inference in deep architectures for complex multi-parameter targets.

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [83] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: A proactive, economic defense for federated learning using a Bayesian incentive mechanism to deter data-poisoning attacks, ensuring robustness and high accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning's open-participation nature makes it vulnerable to data-poisoning attacks, and existing reactive defenses are computationally expensive and assume an honest majority.

Method: Introduces a lightweight Bayesian incentive mechanism where the server verifies updates using a private validation dataset before issuing payments, ensuring IR and IC.

Result: Maintains 96.7% accuracy with 50% label-flipping adversaries on MNIST, outperforming FedAvg by 51.7 percentage points.

Conclusion: The mechanism is computationally efficient, budget-bounded, and integrates easily into existing FL frameworks, providing a practical solution for robust FL ecosystems.

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [84] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: A cost-aware stopping rule for Bayesian optimization is proposed, ensuring efficient evaluation without excessive costs, backed by theoretical guarantees and outperforming other methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of guarantees in existing adaptive stopping rules for Bayesian optimization to prevent excessive evaluation costs.

Method: Proposes a cost-aware stopping rule adapting to varying costs, grounded in theoretical connections to state-of-the-art cost-aware acquisition functions (PBGI and log expected improvement per cost).

Result: Theoretical guarantees bound cumulative evaluation costs, and experiments show superior performance in cost-adjusted simple regret.

Conclusion: The proposed stopping rule, especially with PBGI, effectively balances solution quality and evaluation costs, outperforming alternatives.

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [85] [A Cellular Automata Approach to Donation Game](https://arxiv.org/abs/2507.11744)
*Marcin Kowalik,Przemysław Stokłosa,Mateusz Grabowski,Janusz Starzyk,Paweł Raif*

Main category: cs.MA

TL;DR: The paper explores cooperation dynamics in the donation game using binary cellular automata, focusing on neighbor-limited interactions and noise models.


<details>
  <summary>Details</summary>
Motivation: To study how cooperation evolves when interactions are spatially localized, unlike traditional random interaction models.

Method: Uses binary cellular automata rules aligned with donation game mechanics, incorporating perceptual/action noise and strategy mutation.

Result: Cooperation is significantly influenced by agent mobility and spatial locality, differing from random interaction models.

Conclusion: Spatial interactions and noise models critically impact cooperation, emphasizing the need to distinguish between random and localized interaction systems.

Abstract: The donation game is a well-established framework for studying the emergence
and evolution of cooperation in multi-agent systems. The cooperative behavior
can be influenced by the environmental noise in partially observable settings
and by the decision-making strategies of agents, which may incorporate not only
reputation but also traits such as generosity and forgiveness. Traditional
simulations often assume fully random interactions, where cooperation is tested
between randomly selected agent pairs. In this paper, we investigate
cooperation dynamics using the concept of Stephen Wolfram's one-dimensional
binary cellular automata. This approach allows us to explore how cooperation
evolves when interactions are limited to neighboring agents. We define binary
cellular automata rules that conform to the donation game mechanics.
Additionally, we introduce models of perceptual and action noise, along with a
mutation matrix governing the probabilistic evolution of agent strategies. Our
empirical results demonstrate that cooperation is significantly affected by
agents' mobility and their spatial locality on the game board. These findings
highlight the importance of distinguishing between entirely random multi-agent
systems and those in which agents are more likely to interact with their
nearest neighbors.

</details>


### [86] [CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models](https://arxiv.org/abs/2507.11906)
*Tadahiro Taniguchi,Masatoshi Nagano,Haruumi Omoto,Yoshiki Hayashi*

Main category: cs.MA

TL;DR: CoCre-Sam models collective linguistic outputs (e.g., Ouija board) as Langevin dynamics sampling from fused language models, showing how decentralized knowledge combines through shared interaction.


<details>
  <summary>Details</summary>
Motivation: To computationally understand how decentralized, implicit linguistic knowledge fuses through shared physical interaction, beyond psychological explanations like the ideomotor effect.

Method: Introduces CoCre-Sam, a framework where participants are agents with energy landscapes from internal language models. Collective pointer motion is modeled as Langevin MCMC sampling from summed energy landscapes.

Result: Simulations show CoCre-Sam fuses models and generates meaningful sequences, with collective interaction and stochasticity being essential.

Conclusion: CoCre-Sam links individual implicit knowledge, collective action, and emergent linguistic phenomena through probabilistic sampling.

Abstract: Collective human activities like using an Ouija board (or Kokkuri-san) often
produce emergent, coherent linguistic outputs unintended by any single
participant. While psychological explanations such as the ideomotor effect
exist, a computational understanding of how decentralized, implicit linguistic
knowledge fuses through shared physical interaction remains elusive. We
introduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this
phenomenon as collective Langevin dynamics sampling from implicitly fused
language models. Each participant is represented as an agent associated with an
energy landscape derived from an internal language model reflecting linguistic
priors, and agents exert stochastic forces based on local energy gradients. We
theoretically prove that the collective motion of the shared pointer
(planchette) corresponds to Langevin MCMC sampling from the sum of individual
energy landscapes, representing fused collective knowledge. Simulations
validate that CoCre-Sam dynamics effectively fuse different models and generate
meaningful character sequences, while ablation studies confirm the essential
roles of collective interaction and stochasticity. Altogether, CoCre-Sam
provides a novel computational mechanism linking individual implicit knowledge,
embodied collective action, and emergent linguistic phenomena, grounding these
complex interactions in the principles of probabilistic sampling.

</details>


### [87] [Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment](https://arxiv.org/abs/2507.12400)
*Noble Harasha,Cristina Gava,Nancy Lynch,Claudia Contini,Frederik Mallmann-Trenn*

Main category: cs.MA

TL;DR: The paper models nanobot swarms for cancer treatment, focusing on locomotion in a chemical gradient. Two variants are explored: one with a fixed gradient and another where nanobots amplify the gradient, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To enhance drug delivery precision and reduce side effects by using nanobots to locate and treat cancerous regions, despite their limited individual capabilities.

Method: A general model for nanobot behavior in colloidal environments, with two variants: fixed chemical gradient and dynamic gradient amplified by nanobots. Simulations and analytical results validate performance.

Result: The fixed gradient variant shows bounded time for cancer site location, while the dynamic variant improves runtime performance through collective signal amplification.

Conclusion: Nanobots can effectively locate and treat cancer sites using chemical gradients, with dynamic amplification offering superior performance.

Abstract: Deploying motile nanosized particles, also known as ``nanobots'', in the
human body promises to improve selectivity in drug delivery and reduce side
effects. We consider a swarm of nanobots locating a single cancerous region and
treating it by releasing an onboard payload of drugs at the site. At nanoscale,
the computation, communication, sensing, and locomotion capabilities of
individual agents are extremely limited, noisy, and/or nonexistent.
  We present a general model to formally describe the individual and collective
behavior of agents in a colloidal environment, such as the bloodstream, for
cancer detection and treatment by nanobots. This includes a feasible and
precise model of agent locomotion, inspired by actual nanoparticles that, in
the presence of an external chemical gradient, move towards areas of higher
concentration by means of self-propulsion. We present two variants of our
general model: The first assumes an endogenous chemical gradient that is fixed
over time and centered at the targeted cancer site; the second is a more
speculative and dynamic variant in which agents themselves create and amplify a
chemical gradient centered at the cancer site. In both settings, agents can
sense the gradient and ascend it noisily, locating the cancer site more quickly
than via simple Brownian motion.
  For the first variant of the model, we present simulation results to show the
behavior of agents under our locomotion model, as well as {analytical results}
to bound the time it takes for the agents to reach the cancer site. For the
second variant, simulation results highlight the collective benefit in having
agents issue their own chemical signal. While arguably more speculative in its
agent capability assumptions, this variant shows a significant improvement in
runtime performance over the first variant, resulting from its chemical signal
amplification mechanism.

</details>
