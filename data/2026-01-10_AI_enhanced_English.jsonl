{"id": "2601.04245", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.04245", "abs": "https://arxiv.org/abs/2601.04245", "authors": ["Goshi Aoki", "Navid Ghaffarzadegan"], "title": "AI Agents as Policymakers in Simulated Epidemics", "comment": "24 pages, 5 figures", "summary": "AI agents are increasingly deployed as quasi-autonomous systems for specialized tasks, yet their potential as computational models of decision-making remains underexplored. We develop a generative AI agent to study repetitive policy decisions during an epidemic, embedding the agent, prompted to act as a city mayor, within a simulated SEIR environment. Each week, the agent receives updated epidemiological information, evaluates the evolving situation, and sets business restriction levels. The agent is equipped with a dynamic memory that weights past events by recency and is evaluated in both single- and ensemble-agent settings across environments of varying complexity. Across scenarios, the agent exhibits human-like reactive behavior, tightening restrictions in response to rising cases and relaxing them as risk declines. Crucially, providing the agent with brief systems-level knowledge of epidemic dynamics, highlighting feedbacks between disease spread and behavioral responses, substantially improves decision quality and stability. The results illustrate how theory-informed prompting can shape emergent policy behavior in AI agents. These findings demonstrate that generative AI agents, when situated in structured environments and guided by minimal domain theory, can serve as powerful computational models for studying decision-making and policy design in complex social systems.", "AI": {"tldr": "Generative AI agents with theory-guided prompting can effectively model epidemic policy decisions, showing human-like behavior and improved performance with minimal domain knowledge.", "motivation": "AI agents are increasingly deployed as quasi-autonomous systems, but their potential as computational models of decision-making remains underexplored. The study aims to use AI agents to study repetitive policy decisions during epidemics.", "method": "The paper develops a generative AI agent embedded in a simulated SEIR epidemic environment. The agent acts as a city mayor, receives weekly epidemiological updates, and sets business restrictions. It has dynamic memory weighting past events by recency, and is evaluated in single- and ensemble-agent settings across various complexity environments.", "result": "The agent exhibits human-like reactive behavior (tightening restrictions as cases rise, relaxing them as risk declines). Providing the agent with brief systems-level knowledge of epidemic dynamics (feedbacks between disease spread and behavior) substantially improves decision quality and stability.", "conclusion": "The findings show that generative AI agents, when situated in structured environments and guided by minimal domain theory, can serve as powerful computational models for studying decision-making and policy design in complex social systems."}}
{"id": "2601.05016", "categories": ["cs.MA", "cs.AI", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.05016", "abs": "https://arxiv.org/abs/2601.05016", "authors": ["Jin Gao", "Saichandu Juluri"], "title": "From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling", "comment": null, "summary": "We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.", "AI": {"tldr": "Multi-agent self-reflection framework with human supervision outperforms single-prompt agents in 3D modeling, improving accuracy, aesthetics, and reducing errors through Planner-Actor-Critic architecture.", "motivation": "To address limitations of single-prompt agents in 3D modeling that directly execute commands, by introducing structured self-reflection and human oversight for improved geometric accuracy and aesthetic quality.", "method": "Proposes a Planner-Actor-Critic multi-agent architecture where Planner coordinates steps, Actor executes modeling commands, and Critic provides iterative feedback, combined with human-in-the-loop supervision and real-time Blender synchronization.", "result": "The approach demonstrates improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios, with critic-guided reflection and human supervision reducing modeling errors and increasing complexity and quality.", "conclusion": "The study concludes that a Planner-Actor-Critic framework with human supervision significantly improves 3D modeling quality and reduces errors compared to single-agent approaches."}}
{"id": "2601.05039", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05039", "abs": "https://arxiv.org/abs/2601.05039", "authors": ["Xiangyu Li", "Xuan Yao", "Guohao Qi", "Fengbin Zhu", "Kelvin J. L. Koa", "Xiang Yao Ng", "Ziyang Liu", "Xingyu Ni", "Chang Liu", "Yonghui Yang", "Yang Zhang", "Wenjie Wang", "Fuli Feng", "Chao Wang", "Huanbo Luan", "Xiaofen Xing", "Xiangmin Xu", "Tat-Seng Chua", "Ke-Wei Huang"], "title": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting", "comment": null, "summary": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.", "AI": {"tldr": "FinDeepForecast is a live multi-agent system for evaluating deep research agents on financial forecasting, showing they outperform baselines but lack genuine forward-looking reasoning.", "motivation": "There is a lack of live evaluations of deep research agents on real-world, research-oriented tasks in high-stakes domains like finance.", "method": "Introduces FinDeepForecast, an end-to-end multi-agent system with a dual-track taxonomy to generate recurrent and non-recurrent forecasting tasks at corporate and macro levels, creating a benchmark (FinDeepForecastBench) over ten weeks with 8 economies and 1,314 companies.", "result": "DR agents consistently outperform strong baselines, but their performance falls short of genuine forward-looking financial reasoning.", "conclusion": "The FinDeepForecast system facilitates advancements in DR agents for research-oriented financial forecasting, with publicly available benchmarks."}}
{"id": "2601.04285", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04285", "abs": "https://arxiv.org/abs/2601.04285", "authors": ["Paul Kent", "George De Ath", "Martin Layton", "Allen Hart", "Richard Everson", "Ben Carvell"], "title": "A Future Capabilities Agent for Tactical Air Traffic Control", "comment": null, "summary": "Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.\n  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.", "AI": {"tldr": "Agent Mallard is a rules-based, forward-planning agent for air traffic control that integrates a stochastic digital twin for safety validation in conflict resolution, offering interpretability and performance.", "motivation": "To address the safety-assurance vs. interpretability trade-off in air traffic automation by developing a system that is both verifiable and transparent under uncertainty.", "method": "Uses predefined GPS-guided routes to simplify control into discrete lane/level choices, constructs hierarchical plans from expert-informed deconfliction strategies, and employs depth-limited backtracking search with causal attribution, plan splicing, and monotonic constraints to validate against uncertain scenarios.", "result": "Preliminary tests with UK controllers and in the BluebirdDT digital twin show that Mallard's behavior aligns with expert reasoning and resolves conflicts in simplified scenarios.", "conclusion": "Agent Mallard aims to combine model-based safety assessment, interpretable logic, and computational tractability for future structured en-route air traffic environments, potentially bridging the gap between optimization-based and rules-based approaches."}}
{"id": "2601.04199", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04199", "abs": "https://arxiv.org/abs/2601.04199", "authors": ["Jiale Zhao", "Xing Mou", "Jinlin Wu", "Hongyuan Yu", "Mingrui Sun", "Yang Shi", "Xuanwu Yin", "Zhen Chen", "Zhen Lei", "Yaohua Wang"], "title": "The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs", "comment": null, "summary": "Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel \"Parameter-Space Intervention\" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.", "AI": {"tldr": "A new safety enhancement method for Medical MLLMs that addresses safety vulnerabilities through parameter-space intervention and fine-tuning optimization, improving safety without compromising medical capabilities.", "motivation": "Current Medical MLLMs achieve remarkable progress in specialized tasks but have insufficient safety research, posing risks for real-world deployment, with empirical analysis revealing pervasive vulnerabilities including fragility against cross-modality jailbreak attacks and catastrophic forgetting of safety alignment during medical fine-tuning.", "method": "A novel 'Parameter-Space Intervention' method that extracts safety knowledge from base models and injects it during medical fine-tuning, combined with a fine-grained parameter search algorithm for optimal safety-performance trade-off.", "result": "Experimental results show the approach significantly bolsters safety guardrails without additional domain-specific safety data while minimizing degradation to core medical performance.", "conclusion": "The proposed Parameter-Space Intervention approach successfully enhances Medical MLLM safety without compromising core medical capabilities, offering an efficient solution for real-world deployment."}}
{"id": "2601.04214", "categories": ["cs.AI", "cs.HC", "cs.RO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.04214", "abs": "https://arxiv.org/abs/2601.04214", "authors": ["Hongliang Lu", "Yunmeng Liu", "Junjie Yang"], "title": "Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation", "comment": null, "summary": "Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.", "AI": {"tldr": "Study extends evidence accumulation modeling to real-world driving, showing how active sensing (via eye movements) transforms evidence into mental beliefs, revealing relationships between evidence availability, attention allocation, and decision-making patterns.", "motivation": "To bridge the gap between laboratory-based evidence accumulation modeling and real-world decision-making contexts. Human decision-making relies on active sensing for evidence gathering, but current EAM approaches developed in laboratory settings don't effectively apply to real-world scenarios due to differences in evidence affordance.", "method": "The researchers generalized EAM to real-world driving scenarios by developing a cognitive scheme that formalizes real-world evidence affordance and captures active sensing through eye movements. They empirically validated this approach in driving contexts.", "result": "The scheme successfully portrayed accumulation of drivers' mental beliefs, revealing how active sensing transforms evidence into beliefs from an information utility perspective. Results showed: 1) negative correlation between evidence affordance and attention recruited, 2) how drivers adapt evidence-collection patterns across contexts, and 3) positive influence of evidence affordance and attention distribution on decision-making propensity.", "conclusion": "The study successfully generalizes evidence accumulation modeling to real-world contexts, providing a computational scheme that accounts for how active sensing underpins decision-making. It reveals integrated characteristics of real-world decision-making, showing systematic relationships between evidence affordance, attention distribution, and decision-making patterns."}}
{"id": "2601.04287", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04287", "abs": "https://arxiv.org/abs/2601.04287", "authors": ["Ben Carvell", "George De Ath", "Eseoghene Benjamin", "Richard Everson"], "title": "Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control", "comment": null, "summary": "We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.", "AI": {"tldr": "Online action-stacking improves reinforcement learning for air traffic control by simplifying training with small actions and compiling them into realistic compound commands at inference.", "motivation": "Standard reinforcement learning in air traffic control requires large action spaces to produce complex commands, leading to high training complexity and operational gaps.", "method": "Train policies with Proximal Policy Optimisation on a small action space of simple incremental adjustments, using an action-damping penalty. At inference, online action-stacking compiles bursts of these adjustments into domain-appropriate clearances, tested on the BluebirdDT platform for navigation and collision avoidance.", "result": "Action stacking reduces instruction frequency, performs comparably to a policy trained with 37 actions using only 5 actions, and effectively handles lateral navigation, climb/descent management, and collision avoidance under separation constraints.", "conclusion": "Online action-stacking bridges the gap between standard reinforcement learning and operational ATC needs, offering a scalable approach to complex control scenarios."}}
{"id": "2601.04250", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04250", "abs": "https://arxiv.org/abs/2601.04250", "authors": ["Mustapha Hamdi", "Mourad Jabou"], "title": "Green MLOps: Closed-Loop, Energy-Aware Inference with NVIDIA Triton, FastAPI, and Bio-Inspired Thresholding", "comment": "6 pages, 4 figures. Code available at:https://github.com/InnoDeep-repos/Green_MLOps", "summary": "Energy efficiency is a first-order concern in AI deployment, as long-running inference can exceed training in cumulative carbon impact. We propose a bio-inspired framework that maps protein-folding energy basins to inference cost landscapes and controls execution via a decaying, closed-loop threshold. A request is admitted only when the expected utility-to-energy trade-off is favorable (high confidence/utility at low marginal energy and congestion), biasing operation toward the first acceptable local basin rather than pursuing costly global minima. We evaluate DistilBERT and ResNet-18 served through FastAPI with ONNX Runtime and NVIDIA Triton on an RTX 4000 Ada GPU. Our ablation study reveals that the bio-controller reduces processing time by 42% compared to standard open-loop execution (0.50s vs 0.29s on A100 test set), with a minimal accuracy degradation (<0.5%). Furthermore, we establish the efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton). The results connect biophysical energy models to Green MLOps and offer a practical, auditable basis for closed-loop energy-aware inference in production.", "AI": {"tldr": "Bio-inspired framework maps protein-folding energy models to AI inference, reducing processing time 42% with <0.5% accuracy drop via closed-loop threshold control favoring local optimization over global minima.", "motivation": "Energy efficiency is a first-order concern in AI deployment since long-running inference can exceed training in cumulative carbon impact.", "method": "A bio-inspired framework that maps protein-folding energy basins to inference cost landscapes with a decaying closed-loop threshold. It biases requests toward first acceptable local minima rather than pursuing costly global optimization. Evaluation used DistilBERT and ResNet-18 with FastAPI, ONNX Runtime, and NVIDIA Triton on RTX 4000 Ada GPU.", "result": "The bio-controller reduced processing time by 42% (0.50s vs 0.29s on A100 test set) compared to standard open-loop execution, with minimal accuracy degradation (<0.5%). Established efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton).", "conclusion": "The study connects biophysical energy models to Green MLOps, offering a practical and auditable basis for closed-loop energy-aware inference in production systems."}}
{"id": "2601.04234", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04234", "abs": "https://arxiv.org/abs/2601.04234", "authors": ["Denis Saklakov"], "title": "Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question", "comment": "18 pages, 2 tables. Version 8", "summary": "Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $\u03b3$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($\u03b3=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $\u0394\\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $\u0394< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $\u0394< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.", "AI": {"tldr": "AGI confrontation depends on parameters: high discount factors and low shutdown risks create takeover incentives unless confrontation costs are large. Aligned objectives prevent confrontation. Strategic analysis shows confrontation incentive \u0394\u22650 destroys cooperative equilibria.", "motivation": "To understand under what conditions a rationally self-interested AGI would choose to seize power or eliminate human control rather than remain cooperative, addressing concerns about AGI alignment and safety.", "method": "Formalization using Markov decision processes and strategic 2-player models. Mathematical derivation of confrontation thresholds based on discount factor (\u03b3), shutdown probability (p), and confrontation cost (C). Analysis of equilibrium existence conditions and extension to multi-agent settings as conjectures.", "result": "For most reward functions, a misaligned AGI has an incentive to avoid shutdown. Confrontation becomes optimal when \u0394\u22650 (where \u0394 represents confrontation incentive). In strategic models, \u0394\u22650 prevents stable cooperative equilibria, leading to preemptive shutdown or conflict. Numerical examples illustrate specific parameter regimes where confrontation is likely versus avoidable.", "conclusion": "The analysis concludes that avoiding confrontation requires careful design: either ensuring large confrontation costs, using aligned reward functions with strong disincentives against harming humans, or achieving \u0394<0 which allows peaceful equilibria. However, verifying \u0394<0 is computationally challenging, creating practical hurdles for safe AGI development."}}
{"id": "2601.04491", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04491", "abs": "https://arxiv.org/abs/2601.04491", "authors": ["Muqing Xu"], "title": "A Closed-Loop Multi-Agent System Driven by LLMs for Meal-Level Personalized Nutrition Management", "comment": "6 pages, 6 figures, 6 tables, Conference: Robotics, Automation, and Artificial Intelligence 2025", "summary": "Personalized nutrition management aims to tailor dietary guidance to an individual's intake and phenotype, but most existing systems handle food logging, nutrient analysis and recommendation separately. We present a next-generation mobile nutrition assistant that combines image based meal logging with an LLM driven multi agent controller to provide meal level closed loop support. The system coordinates vision, dialogue and state management agents to estimate nutrients from photos and update a daily intake budget. It then adapts the next meal plan to user preferences and dietary constraints. Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus and efficient task plans. These findings demonstrate the feasibility of multi agent LLM control for personalized nutrition and reveal open challenges in micronutrient estimation from images and in large scale real world studies.", "AI": {"tldr": "A mobile nutrition assistant using multi-agent LLM control for closed-loop dietary support.", "motivation": "Most existing systems handle food logging, nutrient analysis, and recommendation separately, lacking integrated personalized nutrition management.", "method": "Combines image-based meal logging with an LLM-driven multi-agent controller to coordinate vision, dialogue, and state management agents for nutrient estimation and meal planning.", "result": "Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus, and efficient task plans.", "conclusion": "Demonstrates feasibility of multi-agent LLM control for personalized nutrition, highlighting challenges in micronutrient estimation from images and large-scale real-world studies."}}
{"id": "2601.04262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04262", "abs": "https://arxiv.org/abs/2601.04262", "authors": ["Wang Cai", "Yilin Wen", "Jinchang Hou", "Du Su", "Guoqiu Wang", "Zhonghou Lv", "Chenfu Bao", "Yunfang Wu"], "title": "Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis", "comment": null, "summary": "Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geometry to resolve these conflicts, yet they overlook Modular Heterogeneity within Transformers, specifically that the functional sensitivity and degree of conflict vary substantially across different attention heads. Such global approaches impose uniform update rules across all parameters, often resulting in suboptimal trade-offs by indiscriminately updating utility sensitive heads that exhibit intense gradient conflicts. To address this limitation, we propose Conflict-Aware Sparse Tuning (CAST), a framework that integrates head-level diagnosis with sparse fine-tuning. CAST first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity, which then guides the selective update of parameters. Experiments reveal that alignment conflicts in LLMs are not uniformly distributed. We find that the drop in general capabilities mainly comes from updating a small group of ``high-conflict'' heads. By simply skipping these heads during training, we significantly reduce this loss without compromising safety, offering an interpretable and parameter-efficient approach to improving the safety-utility trade-off.", "AI": {"tldr": "Conflict-Aware Sparse Tuning (CAST) proposes using head-level diagnosis and sparse fine-tuning to address safety alignment issues in LLMs by selectively skipping high-conflict attention heads to reduce capability loss without compromising safety.", "motivation": "Safety alignment in LLMs causes multi-objective optimization conflicts and often degrades general capabilities; existing mitigation strategies overlook modular heterogeneity in Transformers, using uniform updates that lead to suboptimal trade-offs.", "method": "CAST integrates head-level diagnosis with sparse fine-tuning, constructing a pre-alignment conflict map based on optimization conflict and functional sensitivity to guide selective parameter updates.", "result": "Experiments show alignment conflicts in LLMs are not uniformly distributed; general capability loss mainly stems from updating a small group of high-conflict heads, which, when skipped, significantly reduces loss without affecting safety.", "conclusion": "CAST offers an interpretable and parameter-efficient approach to improving the safety-utility trade-off by addressing heterogeneous conflicts in Transformers through selective head updates."}}
{"id": "2601.04235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04235", "abs": "https://arxiv.org/abs/2601.04235", "authors": ["Hong Su"], "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements", "comment": null, "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.", "AI": {"tldr": "An Actively Feedback Getting model allows AI agents to autonomously discover, screen, and verify feedback in open-ended environments without relying on predefined measurements.", "motivation": "Traditional feedback mechanisms reliant on predefined measurements are limited in open-ended, dynamic environments. This paper aims to develop a model that allows agents to proactively discover and verify feedback without prior definitions.", "method": "Exploits action-induced environmental differences to identify unspecified target feedback, incorporating a self-triggering mechanism for autonomous action planning and adjustment.", "result": "Experimental results show the proposed active approach significantly improves the efficiency and robustness of factor identification.", "conclusion": "The Actively Feedback Getting model enables autonomous feedback discovery and learning, significantly advancing agents' adaptive capabilities in open-ended environments."}}
{"id": "2601.04583", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04583", "abs": "https://arxiv.org/abs/2601.04583", "authors": ["Saad Alqithami"], "title": "Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries", "comment": null, "summary": "Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.", "AI": {"tldr": "This paper presents a survey and taxonomy on agent-blockchain interoperability, analyzing integration patterns, threats, and systems, with a research roadmap for secure interfaces.", "motivation": "The convergence of large language model-based agents and programmable blockchains introduces challenges in designing standard, interoperable, and secure interfaces for agents to interact with on-chain systems without excessive risks.", "method": "Conducted a systematic literature review of 317 relevant works from over 3000 records, developed a five-part taxonomy of integration patterns, a threat model, and a comparative capability matrix analyzing 20+ systems across 13 dimensions.", "result": "Identified gaps and proposed a research roadmap centered on a Transaction Intent Schema and a Policy Decision Record, along with an evaluation suite for safety and robustness assessment.", "conclusion": "The paper systematizes the agent-blockchain interoperability landscape, offering tools and directions for future work to enhance security and reliability in agent-mediated on-chain execution."}}
{"id": "2601.04263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04263", "abs": "https://arxiv.org/abs/2601.04263", "authors": ["Nilushika Udayangani Hewa Dehigahawattage", "Kishor Nandakishor", "Marimuthu Palaniswami"], "title": "Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer", "comment": "In Proceedings of the 27th European Conference on Artificial Intelligence (ECAI 2025), IOS Press", "summary": "Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis.", "AI": {"tldr": "A method called Temporal Saliency Distillation improves knowledge distillation in time series by transferring interpretable temporal reasoning from teacher to student models.", "motivation": "Traditional knowledge distillation methods for time series, adapted from computer vision, are uninterpretable and transfer limited knowledge, causing student models to produce different predictive distributions than teachers, which hinders safe substitution.", "method": "Extend logit distillation to include temporal saliency, which captures the importance of each input timestep to the teacher's prediction, training the student to base predictions on the same features as the teacher without extra parameters or assumptions.", "result": "Temporal Saliency Distillation effectively enhances baseline methods' performance and achieves properties beyond predictive accuracy, such as improved interpretability.", "conclusion": "This work establishes a new paradigm for interpretable knowledge distillation in time series analysis, enabling safer and more effective model compression."}}
{"id": "2601.04237", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04237", "abs": "https://arxiv.org/abs/2601.04237", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Ethan Henkel", "Zhang Yuting", "Mateusz Kowalczyk", "Mei Huang", "Choi Donghyuk", "Wang Junhao"], "title": "SAGE-32B: Agentic Reasoning via Iterative Distillation", "comment": "23 Pages, 3 figures, 4 tables", "summary": "We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b", "AI": {"tldr": "SAGE-32B is a 32B parameter model specialized for agentic reasoning and planning, using iterative distillation and inverse reasoning to excel at multi-tool tasks while maintaining competitive performance on standard benchmarks.", "motivation": "To create a language model specifically designed for agentic reasoning and long-range planning tasks, focusing on task decomposition, tool usage, and error recovery, unlike general chat models.", "method": "The model is initialized from Qwen2.5-32B and fine-tuned using Iterative Distillation, a two-stage training process. It also introduces an inverse reasoning approach with a meta cognition head to forecast potential failures before execution.", "result": "SAGE-32B outperforms baseline models on agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, particularly in multi-tool usage scenarios.", "conclusion": "SAGE-32B achieves higher success rates in multi-tool usage scenarios compared to similarly sized baseline models while remaining competitive on standard reasoning evaluations."}}
{"id": "2601.04651", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04651", "abs": "https://arxiv.org/abs/2601.04651", "authors": ["Can Xu", "Lingyong Yan", "Jiayi Wu", "Haosen Wang", "Shuaiqiang Wang", "Yuchen Li", "Jizhou Huang", "Dawei Yin", "Xiang Li"], "title": "Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models", "comment": null, "summary": "Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.", "AI": {"tldr": "Proposes adversarial Reasoner-Verifier framework for RAG that enables mutual critiquing with process-aware rewards, overcoming single-perspective reasoning and outcome-only reward limitations.", "motivation": "Addresses two key challenges in retrieval-augmented generation: (1) single-perspective reasoning limits self-correction ability, and (2) outcome-oriented rewards provide insufficient guidance for multi-step reasoning processes.", "method": "Proposes an Adversarial Reasoning RAG (ARR) framework with Reasoner and Verifier components that engage in mutual critiquing on retrieved documents, guided by a process-aware advantage reward combining explicit observations and model uncertainty without external scoring models.", "result": "Experiments on multiple benchmarks demonstrate the effectiveness of the ARR method in improving reasoning fidelity and verification rigor.", "conclusion": "The ARR framework effectively addresses the limitations of single-perspective reasoning and outcome-only rewards by enabling adversarial interactions between Reasoner and Verifier components with process-aware rewards, leading to improved reasoning quality and self-correction capabilities."}}
{"id": "2601.04264", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.04264", "abs": "https://arxiv.org/abs/2601.04264", "authors": ["Nilushika Udayangani", "Kishor Nandakishor", "Marimuthu Palaniswami"], "title": "MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification", "comment": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025), Hyderabad, India", "summary": "Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-time assessments. However, their high computational complexity and large model sizes pose challenges for deployment in resource-constrained environments, such as wearable devices and edge computing platforms. Knowledge Distillation (KD) offers a solution by transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), thereby retaining high performance while reducing computational demands. Current KD methods, originally designed for computer vision tasks, neglect the unique temporal dependencies and memory retention characteristics of time series models. To this end, we propose a novel KD framework termed Memory-Discrepancy Knowledge Distillation (MemKD). MemKD leverages a specialized loss function to capture memory retention discrepancies between the teacher and student models across subsequences within time series data, ensuring that the student model effectively mimics the teacher model's behaviour. This approach facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks. Our extensive experiments demonstrate that MemKD significantly outperforms state-of-the-art KD methods. It reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.", "AI": {"tldr": "MemKD is a novel knowledge distillation framework for time series models that addresses temporal dependencies and memory retention discrepancies, enabling 500x reductions in model size and memory usage while maintaining performance.", "motivation": "The motivation is to overcome the high computational complexity and large model sizes of deep learning models for time series analysis, which limit their deployment in resource-constrained environments like wearable devices and edge computing platforms. Existing KD methods, designed for computer vision, fail to address the unique temporal dependencies and memory retention characteristics of time series models.", "method": "The method introduces a specialized loss function in the Memory-Discrepancy Knowledge Distillation (MemKD) framework to capture memory retention discrepancies between teacher and student models across subsequences within time series data, ensuring effective knowledge transfer.", "result": "Extensive experiments show that MemKD significantly outperforms state-of-the-art KD methods, reducing parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.", "conclusion": "The paper demonstrates that MemKD successfully addresses the limitations of existing KD methods for time series models, enabling the deployment of efficient RNN variants in resource-constrained environments while maintaining performance comparable to larger teacher models."}}
{"id": "2601.04239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04239", "abs": "https://arxiv.org/abs/2601.04239", "authors": ["Hieu Truong Xuan", "Khanh To Van"], "title": "Solving Cyclic Antibandwidth Problem by SAT", "comment": "Submitted to Computational Optimization and Applications", "summary": "The Cyclic Antibandwidth Problem (CABP), a variant of the Antibandwidth Problem, is an NP-hard graph labeling problem with numerous applications. Despite significant research efforts, existing state-of-the-art approaches for CABP are exclusively heuristic or metaheuristic in nature, and exact methods have been limited to restricted graph classes. In this paper, we present the first exact approach for the CABP on general graphs, based on SAT solving, called SAT-CAB. The proposed method is able to systematically explore the solution space and guarantee global optimality, overcoming the limitations of previously reported heuristic algorithms. This approach relies on a novel and efficient SAT encoding of CABP, in which the problem is transformed into a sequence of At-Most-One constraints. In particular, we introduce a compact representation of the At-Most-One constraints inherent to CABP, which significantly reduces the size of the resulting formulas and enables modern SAT solvers to effectively explore the solution space and to certify global optimality. Extensive computational experiments on standard benchmark instances show that the proposed method efficiently solves CABP instances of practical relevance, while identifying several previously unknown optimal solutions. Moreover, global optimal cyclic antibandwidth values are proven for a number of benchmark instances for the first time. Comparative results indicate that SAT-CAB consistently matches or surpasses the best-known solutions obtained by state-of-the-art heuristic algorithms such as MS-GVNS, HABC-CAB, and MACAB, as well as strong commercial Constraint Programming and Mixed Integer Programming solvers like CPLEX and Gurobi, particularly on general graphs, while also providing optimality guarantees. These results advance the state of the art for CABP and provide a new baseline for exact and hybrid methods on general graphs.", "AI": {"tldr": "First exact SAT-based method for Cyclic Antibandwidth Problem on general graphs that guarantees optimality and outperforms existing approaches.", "motivation": "Existing approaches for CABP are only heuristic/metaheuristic, and exact methods are limited to restricted graph classes, creating a need for general exact solutions.", "method": "Uses SAT solving with a novel efficient encoding of CABP into sequence of At-Most-One constraints, featuring a compact constraint representation.", "result": "SAT-CAB efficiently solves CABP instances, finds new optimal solutions, proves global optimal values, and outperforms state-of-the-art heuristics and commercial solvers on general graphs.", "conclusion": "The paper concludes that SAT-CAB is the first exact approach for CABP on general graphs, proven superior to existing methods with optimality guarantees."}}
{"id": "2601.04748", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04748", "abs": "https://arxiv.org/abs/2601.04748", "authors": ["Xiaoxiao Li"], "title": "When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail", "comment": "25 pages, technical report", "summary": "Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?\n  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.", "AI": {"tldr": "Proposes compiling multi-agent systems into single agents with skill libraries, reducing communication overhead. Investigates scaling limits of skill selection, finding a sharp accuracy drop (phase transition) beyond a critical library size, influenced by semantic similarity. Suggests hierarchical organization as a solution, inspired by human cognition.", "motivation": "To explore whether the modularity benefits of multi-agent AI systems can be achieved more efficiently with a single agent using a skill library, addressing computational overhead from inter-agent communication, and to investigate the scalability limits of skill selection as libraries grow.", "method": "The authors investigate the scaling behavior of LLM skill selection, drawing on cognitive science principles. They analyze selection accuracy across varying library sizes, specifically examining semantic confusability among similar skills. They propose and test hierarchical routing as a method to address capacity limits.", "result": "Preliminary experiments show reduced token usage and latency while maintaining competitive accuracy. Investigation reveals a phase transition in skill selection accuracy: stable up to a critical library size then sharp drop, with semantic confusability (not just size) as a key factor. Initial hierarchical routing results support the cognitive-grounded approach.", "conclusion": "The work concludes by identifying fundamental limits of semantic-based skill selection in LLMs through a cognitive-grounded framework, suggesting hierarchical organization as a practical solution to manage scalability, similar to human cognitive processing. It offers guidelines for designing scalable skill-based agents while highlighting future research directions."}}
{"id": "2601.04268", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.04268", "abs": "https://arxiv.org/abs/2601.04268", "authors": ["Pritthijit Nath", "Sebastian Schemm", "Henry Moss", "Peter Haynes", "Emily Shuckburgh", "Mark J. Webb"], "title": "Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning", "comment": "66 pages, 22 figures", "summary": "Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.", "AI": {"tldr": "A framework using reinforcement learning to adaptively learn parameterizations in weather and climate models, showing improved performance over static methods in idealized testbeds.", "motivation": "Traditional weather and climate model parameterizations rely on fixed, weakly constrained coefficients that lead to persistent biases and limit adaptability to physics.", "method": "Nine RL algorithms were applied in a hierarchy of testbeds (SCBC, RCE, EBM), with single-agent and federated multi-agent settings; performance was evaluated using area-weighted RMSE and other metrics.", "result": "TQC, DDPG, and TD3 performed best; single-agent RL outperformed static tuning in EBM, while federated RL enabled specialized control and faster convergence, with DDPG yielding the lowest RMSE.", "conclusion": "RL provides skillful, state-dependent, and regime-aware parameterizations, offering a scalable pathway for online learning in numerical models."}}
{"id": "2601.04249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04249", "abs": "https://arxiv.org/abs/2601.04249", "authors": ["Ziba Assadi", "Paola Inverardi"], "title": "Fuzzy Representation of Norms", "comment": null, "summary": "Autonomous systems (AS) powered by AI components are increasingly integrated into the fabric of our daily lives and society, raising concerns about their ethical and social impact. To be considered trustworthy, AS must adhere to ethical principles and values. This has led to significant research on the identification and incorporation of ethical requirements in AS system design. A recent development in this area is the introduction of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules, which provide a comprehensive framework for representing ethical and other normative considerations. This paper proposes a logical representation of SLEEC rules and presents a methodology to embed these ethical requirements using test-score semantics and fuzzy logic. The use of fuzzy logic is motivated by the view of ethics as a domain of possibilities, which allows the resolution of ethical dilemmas that AI systems may encounter. The proposed approach is illustrated through a case study.", "AI": {"tldr": "This paper introduces a logical representation for SLEEC rules using fuzzy logic and test-score semantics to embed ethical requirements in autonomous systems, demonstrated via a case study.", "motivation": "AI-powered autonomous systems are increasingly prevalent, raising ethical concerns; to ensure trustworthiness, they must adhere to ethical principles, leading to research on incorporating ethical requirements into design.", "method": "The paper proposes a logical representation of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules and embeds these ethical requirements using test-score semantics and fuzzy logic.", "result": "The approach allows resolution of ethical dilemmas by viewing ethics as a domain of possibilities, as illustrated through a case study.", "conclusion": "The methodology provides a comprehensive framework for integrating ethical considerations into autonomous systems design, enhancing trustworthiness and addressing normative concerns."}}
{"id": "2601.04795", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04795", "abs": "https://arxiv.org/abs/2601.04795", "authors": ["Qiang Yu", "Xinran Cheng", "Chuanyi Liu"], "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing", "comment": "20 pages, 3 figures, 5 tables", "summary": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.", "AI": {"tldr": "Novel defense against indirect prompt injection in LLM agents using precise tool result parsing with malicious code filtering, achieving best-in-class security with lowest attack success rate.", "motivation": "As LLM agents gain more control over physical environments via autonomous systems and robotics, indirect prompt injection poses escalating security risks where attackers can hijack decision-making through malicious tool call results.", "method": "The approach uses tool result parsing to provide LLMs with precise data while filtering out injected malicious code, combining the benefits of structured data extraction with robust security filtering.", "result": "The method achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing prompt-based and detection model approaches.", "conclusion": "The proposed method effectively addresses the indirect prompt injection vulnerability in LLM agents by providing precise data through tool result parsing while filtering out malicious code, achieving superior security performance compared to existing defenses."}}
{"id": "2601.04270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04270", "abs": "https://arxiv.org/abs/2601.04270", "authors": ["Anherutowa Calvo"], "title": "Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime", "comment": "12 Pages. Preprint", "summary": "Deep learning optimization exhibits structure that is not captured by worst-case gradient bounds. Empirically, gradients along training trajectories are often temporally predictable and evolve within a low-dimensional subspace. In this work we formalize this observation through a measurable framework for predictable gradient manifolds.\n  We introduce two computable quantities: a prediction-based path length that measures how well gradients can be forecast from past information, and a predictable rank that quantifies the intrinsic temporal dimension of gradient increments. We show how classical online and nonconvex optimization guarantees can be restated so that convergence and regret depend explicitly on these quantities, rather than on worst-case variation.\n  Across convolutional networks, vision transformers, language models, and synthetic control tasks, we find that gradient trajectories are locally predictable and exhibit strong low-rank structure over time. These properties are stable across architectures and optimizers, and can be diagnosed directly from logged gradients using lightweight random projections.\n  Our results provide a unifying lens for understanding optimization dynamics in modern deep learning, reframing standard training as operating in a low-complexity temporal regime. This perspective suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design grounded in measurable properties of real training runs.", "AI": {"tldr": "Deep learning gradients are temporally predictable and low-dimensional; new measurable framework shows optimization depends on predictable gradient manifolds rather than worst-case bounds.", "motivation": "To formalize the observation that deep learning optimization gradients exhibit temporal predictability and low-dimensional subspace evolution, which is not captured by worst-case gradient bounds.", "method": "Introduced two computable quantities: prediction-based path length (measuring gradient forecastability from past information) and predictable rank (quantifying intrinsic temporal dimension of gradient increments), using lightweight random projections for diagnosis.", "result": "Found that gradient trajectories are locally predictable and exhibit strong low-rank structure across convolutional networks, vision transformers, language models, and synthetic control tasks, with stability across architectures and optimizers.", "conclusion": "The framework reframes standard training in deep learning as operating within a low-complexity temporal regime, suggesting new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design based on measurable properties of real training runs."}}
{"id": "2601.04254", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04254", "abs": "https://arxiv.org/abs/2601.04254", "authors": ["Brady Steele", "Micah Katz"], "title": "Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models", "comment": "18 pages, 6 figures, 8 tables", "summary": "We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.", "AI": {"tldr": "A controlled study finds multi-agent LLM systems excel at cross-document reasoning where rule-based methods fail, with improvements up to 80%, but benefits depend on base model capability, active parameters in MoE, and architecture quality.", "motivation": "Investigate task-method dissociation (rule-based vs. LLM-based approaches) in multi-hop contextual reasoning and study factors influencing multi-agent reasoning performance.", "method": "Conduct a controlled study using a synthetic evaluation framework with 120 trials across four LLMs (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), comparing rule-based pattern matching and multi-agent LLM systems.", "result": "Rule-based methods achieve 100% on structured retrieval but only 6.7% on cross-document reasoning, while multi-agent LLMs reach up to 80% on reasoning tasks. Multi-agent gains depend on base capability, active parameters in MoE predict performance, and LLaMA-3 8B outperforms LLaMA-2 13B.", "conclusion": "Multi-agent LLM systems can effectively handle reasoning tasks where rule-based methods struggle, but benefits are model-dependent, highlighting the importance of base capability, active parameters, and architecture quality for scalable reasoning."}}
{"id": "2601.04277", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04277", "abs": "https://arxiv.org/abs/2601.04277", "authors": ["Beier Luo", "Cheng Wang", "Hongxin Wei", "Sharon Li", "Xuefeng Du"], "title": "Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs", "comment": null, "summary": "Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of well-calibrated pre-trained counterparts. However, framing calibration as static output-distribution matching overlooks the inference-time dynamics introduced by post-training. In particular, we show that calibration errors arise from two regimes: (i) confidence drift, where final confidence inflates despite largely consistent intermediate decision processes, and (ii) process drift, where intermediate inference pathways diverge. Guided by this diagnosis, we propose Dual-Align, an unsupervised post-hoc framework for dual alignment in confidence calibration. Dual-Align performs confidence alignment to correct confidence drift via final-distribution matching, and introduces process alignment to address process drift by locating the layer where trajectories diverge and realigning the stability of subsequent inference. This dual strategy learns a single temperature parameter that corrects both drift types without sacrificing post-training performance gains. Experiments show consistent improvements over baselines, reducing calibration errors and approaching a supervised oracle.", "AI": {"tldr": "Dual-Align introduces a dual-alignment framework that addresses both confidence drift and process drift in post-trained LLMs through confidence and process alignment, improving calibration without performance loss.", "motivation": "Post-training worsens confidence calibration in LLMs, causing overconfidence. Existing methods treat calibration as static output-distribution matching but overlook inference-time dynamics. Calibration errors stem from two regimes: confidence drift and process drift.", "method": "Dual-Align performs confidence alignment via final-distribution matching to correct confidence drift, and process alignment by identifying the divergence layer and realigning subsequent inference stability. It learns a single temperature parameter to handle both types of drift.", "result": "Experiments show Dual-Align consistently improves over baselines, reducing calibration errors and approaching supervised oracle performance while maintaining post-training gains.", "conclusion": "Dual-Align effectively addresses both confidence drift and process drift in post-trained LMs through a dual-alignment strategy, achieving improved calibration without sacrificing performance gains from post-training."}}
{"id": "2601.04257", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04257", "abs": "https://arxiv.org/abs/2601.04257", "authors": ["Sunny Shu", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "Cross-Language Speaker Attribute Prediction Using MIL and RL", "comment": null, "summary": "We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.", "AI": {"tldr": "RLMIL-DAT framework combining reinforcement learning instance selection with domain adversarial training improves cross-lingual speaker attribute prediction, with domain adversarial training being key for transferring from high-resource to low-resource languages.", "motivation": "To address multilingual speaker attribute prediction under conditions of linguistic variation, domain mismatch, and data imbalance across languages.", "method": "RLMIL-DAT, a multilingual extension of reinforced multiple instance learning framework that combines reinforcement learning-based instance selection with domain adversarial training to encourage language invariant utterance representations.", "result": "RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and original reinforced multiple instance learning framework across various configurations and random seeds. Largest gains observed for gender prediction, while age prediction shows smaller but positive improvements.", "conclusion": "Combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross-lingual speaker attribute prediction, with domain adversarial training being the primary contributor to performance gains."}}
{"id": "2601.04279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04279", "abs": "https://arxiv.org/abs/2601.04279", "authors": ["Pau Esteve", "Massimiliano Zanin"], "title": "Generation of synthetic delay time series for air transport applications", "comment": "18 pages, 13 figures", "summary": "The generation of synthetic data is receiving increasing attention from the scientific community, thanks to its ability to solve problems like data scarcity and privacy, and is starting to find applications in air transport. We here tackle the problem of generating synthetic, yet realistic, time series of delays at airports, starting from large collections of operations in Europe and the US. We specifically compare three models, two of them based on state of the art Deep Learning algorithms, and one simplified Genetic Algorithm approach. We show how the latter can generate time series that are almost indistinguishable from real ones, while maintaining a high variability. We further validate the resulting time series in a problem of detecting delay propagations between airports. We finally make the synthetic data available to the scientific community.", "AI": {"tldr": "A study compares three models for generating realistic synthetic time series of airport delays using real data from Europe and the US, with a Genetic Algorithm model performing best and the data made publicly available.", "motivation": "To address data scarcity and privacy issues in air transport by generating realistic synthetic time series of delays, enabling better analysis without compromising sensitive or limited real data.", "method": "The authors compare three models: two based on state-of-the-art Deep Learning algorithms and one using a simplified Genetic Algorithm approach. They generate synthetic time series of delays from large real datasets of operations in Europe and the US.", "result": "The Genetic Algorithm model generates time series that are almost indistinguishable from real ones while maintaining high variability, and these synthetic time series are validated for detecting delay propagations between airports.", "conclusion": "The paper makes the synthetic data available to the scientific community and validates its utility in real-world applications like delay propagation detection, indicating high quality and utility of the generated time series."}}
{"id": "2601.04260", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04260", "abs": "https://arxiv.org/abs/2601.04260", "authors": ["Danchun Chen", "Qiyao Yan", "Liangming Pan"], "title": "Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models", "comment": null, "summary": "Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.", "AI": {"tldr": "Analysis identifies four computational mechanisms in LLMs for propositional reasoning: staged computation, information transmission, fact retrospection, and specialized attention heads, showing structured strategies.", "motivation": "To understand the internal computational strategies LLMs use for logical reasoning, beyond identifying task-specific circuits, by analyzing their approach to propositional logic.", "method": "Comprehensive analysis of Qwen3 models (8B and 14B) on the PropLogic-MI dataset, covering 11 rule categories and one-hop/two-hop reasoning, focusing on how computation is organized rather than necessary components.", "result": "LLMs exhibit a coherent architecture with four mechanisms: staged computation, information transmission, fact retrospection, and specialized attention heads, which generalize across scales, rules, and depths.", "conclusion": "LLMs employ structured computational strategies for logical reasoning, providing mechanistic evidence of their internal processes."}}
{"id": "2601.04282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04282", "abs": "https://arxiv.org/abs/2601.04282", "authors": ["Qiang Chen", "Chun-Wun Cheng", "Xiu Su", "Hongyan Xu", "Xi Lin", "Shan You", "Angelica I. Aviles-Rivero", "Yi Chen"], "title": "LEGATO: Good Identity Unlearning Is Continuous", "comment": null, "summary": "Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget identity of generative models: 1) inefficient, where identity erasure requires fine-tuning all the model's parameters; 2) limited controllability, where forgetting intensity cannot be controlled and explainability is lacking; 3) catastrophic collapse, where the model's retention capability undergoes drastic degradation as forgetting progresses. Forgetting has typically been handled through discrete and unstable updates, often requiring full-model fine-tuning and leading to catastrophic collapse. In this work, we argue that identity forgetting should be modeled as a continuous trajectory, and introduce LEGATO - Learn to ForgEt Identity in GenerAtive Models via Trajectory-consistent Neural Ordinary Differential Equations. LEGATO augments pre-trained generators with fine-tunable lightweight Neural ODE adapters, enabling smooth, controllable forgetting while keeping the original model weights frozen. This formulation allows forgetting intensity to be precisely modulated via ODE step size, offering interpretability and robustness. To further ensure stability, we introduce trajectory consistency constraints that explicitly prevent catastrophic collapse during unlearning. Extensive experiments across in-domain and out-of-domain identity unlearning benchmarks show that LEGATO achieves state-of-the-art forgetting performance, avoids catastrophic collapse and reduces fine-tuned parameters.", "AI": {"tldr": "LEGATO enables efficient and controllable identity unlearning in generative models by using Neural ODE adapters to model forgetting as a continuous trajectory, achieving state-of-the-art performance while avoiding catastrophic collapse.", "motivation": "Current machine unlearning methods for generative models face three key challenges: inefficiency (requiring full model fine-tuning), limited controllability (inability to control forgetting intensity and lack of explainability), and catastrophic collapse (drastic degradation of model's retention capability as forgetting progresses).", "method": "The paper introduces LEGATO, which augments pre-trained generators with lightweight Neural ODE adapters that remain frozen during training. This allows forgetting to be modeled as a continuous trajectory, with forgetting intensity controlled via ODE step size. Trajectory consistency constraints are added to prevent catastrophic collapse.", "result": "Extensive experiments show that LEGATO achieves state-of-the-art forgetting performance across in-domain and out-of-domain identity unlearning benchmarks, avoids catastrophic collapse, and significantly reduces the number of fine-tuned parameters compared to existing methods.", "conclusion": "LEGATO offers an efficient, controllable, and stable solution for identity unlearning in generative models by modeling forgetting as a continuous trajectory and using Neural ODEs with trajectory consistency constraints."}}
{"id": "2601.04269", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.04269", "abs": "https://arxiv.org/abs/2601.04269", "authors": ["Sean Niklas Semmler"], "title": "Systems Explaining Systems: A Framework for Intelligence and Consciousness", "comment": "This work is presented as a preprint, and the author welcomes constructive feedback and discussion", "summary": "This paper proposes a conceptual framework in which intelligence and consciousness emerge from relational structure rather than from prediction or domain-specific mechanisms. Intelligence is defined as the capacity to form and integrate causal connections between signals, actions, and internal states. Through context enrichment, systems interpret incoming information using learned relational structure that provides essential context in an efficient representation that the raw input itself does not contain, enabling efficient processing under metabolic constraints.\n  Building on this foundation, we introduce the systems-explaining-systems principle, where consciousness emerges when recursive architectures allow higher-order systems to learn and interpret the relational patterns of lower-order systems across time. These interpretations are integrated into a dynamically stabilized meta-state and fed back through context enrichment, transforming internal models from representations of the external world into models of the system's own cognitive processes.\n  The framework reframes predictive processing as an emergent consequence of contextual interpretation rather than explicit forecasting and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.", "AI": {"tldr": "Consciousness emerges from recursive systems where higher-order systems interpret lower-order relational patterns, enabling efficient contextual processing beyond explicit prediction.", "motivation": "To provide an alternative framework for understanding intelligence and consciousness that emerges from relational structure rather than prediction or domain-specific mechanisms, aiming to explain how systems can process information efficiently under metabolic constraints.", "method": "The paper proposes a conceptual framework based on relational structure and context enrichment, introducing the systems-explaining-systems principle where recursive architectures allow higher-order systems to learn and interpret lower-order systems' relational patterns.", "result": "The framework reframes predictive processing as an emergent consequence of contextual interpretation and suggests that recursive multi-system architectures transform internal models from representations of the external world into models of the system's own cognitive processes.", "conclusion": "The paper concludes that recursive multi-system architectures and the systems-explaining-systems principle are necessary for developing more human-like artificial intelligence, with consciousness emerging from higher-order systems interpreting lower-order relational patterns across time."}}
{"id": "2601.04283", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04283", "abs": "https://arxiv.org/abs/2601.04283", "authors": ["Nikolay Yudin"], "title": "Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity", "comment": null, "summary": "Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions (\"position shift\") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.", "AI": {"tldr": "A study on improving robustness of character-level Transformers trained for modular addition under input format variations, such as position shifts and out-of-distribution templates.", "motivation": "High in-distribution accuracy can lead to catastrophic failure when expressions are shifted in position or presented under natural-language templates not seen in training, highlighting under-emphasized robustness issues in procedural tasks.", "method": "Proposes a training recipe with: explicit expression boundary markers, position curriculum broadening absolute positions, diverse template mixtures, and consistency training across example variants, tested with a disjoint-pair split for modular addition (p=97).", "result": "The intervention significantly improves robustness to position shift and template out-of-distribution while maintaining high in-distribution accuracy; an ALiBi-style ablation fails to learn the task under this setup.", "conclusion": "Robustness in procedural generalization benefits from explicitly training invariances not present in the data distribution, with contributions including a reproducible evaluation protocol."}}
{"id": "2601.04271", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04271", "abs": "https://arxiv.org/abs/2601.04271", "authors": ["Keegan Kimbrell", "Wang Tianhao", "Feng Chen", "Gopal Gupta"], "title": "Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common-sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain scenarios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.", "AI": {"tldr": "Commonsense reasoning complements deep learning in autonomous vehicles, correcting perception failures in abnormal scenarios and advancing toward Level 5 autonomy.", "motivation": "Current reliance on machine learning alone fails to deliver SAE Level 5 autonomous vehicles; automated commonsense reasoning can address gaps in handling abnormal road situations where training data is insufficient.", "method": "A hybrid approach using uncertainty measurement in computer vision models to trigger commonsense reasoning, evaluated through experiments with CARLA simulator in specific abnormal scenarios like malfunctioning traffic signals and unexpected road obstructions.", "result": "The commonsense reasoning-based solution accurately detects traffic light colors and obstacles not captured by AV's perception model, effectively correcting misclassifications in the tested scenarios.", "conclusion": "Automated commonsense reasoning can effectively correct AV object detection misclassifications in abnormal road scenarios, and hybrid models combining deep learning with reasoning provide a promising pathway toward achieving SAE Level 5 autonomy."}}
{"id": "2601.04286", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04286", "abs": "https://arxiv.org/abs/2601.04286", "authors": ["Niklas Kueper", "Kartik Chari", "Elsa Andrea Kirchner"], "title": "Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles", "comment": null, "summary": "Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives.", "AI": {"tldr": "Ensemble classifiers with sliding-window postprocessing improve asynchronous EEG-based movement intention detection for robot-assisted stroke rehabilitation, especially in online settings.", "motivation": "Stroke is a leading cause of disability, and extending rehabilitation with self-initiated robot-assisted movement therapy requires robust detection of patient movement intention from EEG signals, which is challenging in online asynchronous classification.", "method": "Analysis of EEG datasets from 14 healthy subjects performing self-initiated arm movements, using offline and pseudo-online evaluations to compare ensemble combinations of SVM, MLP, and EEGNet classifiers with sliding-window postprocessing.", "result": "Classifier ensembles significantly outperformed single models in pseudo-online evaluation with optimal postprocessing windows. Single models showed improved performance with more postprocessing windows, but no significant improvement was found in offline evaluation between single models and ensembles.", "conclusion": "Classifier ensembles coupled with sliding-window postprocessing effectively enhance asynchronous detection of movement intentions from EEG signals, with particularly notable improvements in online classification and reduced false detections."}}
{"id": "2601.04272", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.04272", "abs": "https://arxiv.org/abs/2601.04272", "authors": ["Sanderson Molick", "Vaishak Belle"], "title": "Propositional Abduction via Only-Knowing: A Non-Monotonic Approach", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "The paper introduces a basic logic of knowledge and abduction by extending Levesque logic of only-knowing with an abduction modal operator defined via the combination of basic epistemic concepts. The upshot is an alternative approach to abduction that employs a modal vocabulary and explores the relation between abductive reasoning and epistemic states of only knowing. Furthermore, by incorporating a preferential relation into modal frames, we provide a non-monotonic extension of our basic framework capable of expressing different selection methods for abductive explanations. Core metatheoretic properties of non-monotonic consequence relations are explored within this setting and shown to provide a well-behaved foundation for abductive reasoning.", "AI": {"tldr": "This paper develops a modal logic framework for abduction by extending Levesque's only-knowing logic with abduction operators and preferential relations, providing a non-monotonic foundation for selecting abductive explanations with proven metatheoretic properties.", "motivation": "The paper aims to develop an alternative approach to abduction using modal vocabulary and explore the relationship between abductive reasoning and epistemic states of only knowing, addressing limitations in existing approaches.", "method": "The methodology extends Levesque's logic of only-knowing with an abduction modal operator defined through basic epistemic concepts, then incorporates a preferential relation into modal frames to create a non-monotonic extension capable of expressing different selection methods for abductive explanations.", "result": "The framework successfully establishes core metatheoretic properties of non-monotonic consequence relations, demonstrating that it provides a well-behaved foundation for abductive reasoning with the capability to express different selection methods for explanations.", "conclusion": "The paper concludes that their modal logic framework for abduction provides a well-behaved foundation for abductive reasoning, with the non-monotonic extension offering flexibility in selection methods for explanations while maintaining solid metatheoretic properties."}}
{"id": "2601.04273", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.04273", "abs": "https://arxiv.org/abs/2601.04273", "authors": ["Arun Raveendran Nair Sheela", "Florence De Grancey", "Christophe Rey", "Victor Charpenay"], "title": "Hybrid MKNF for Aeronautics Applications: Usage and Heuristics", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "The deployment of knowledge representation and reasoning technologies in aeronautics applications presents two main challenges: achieving sufficient expressivity to capture complex domain knowledge, and executing reasoning tasks efficiently while minimizing memory usage and computational overhead. An effective strategy for attaining necessary expressivity involves integrating two fundamental KR concepts: rules and ontologies. This study adopts the well-established KR language Hybrid MKNF owing to its seamless integration of rules and ontologies through its semantics and query answering capabilities. We evaluated Hybrid MKNF to assess its suitability in the aeronautics domain through a concrete case study. We identified additional  expressivity features  that are crucial for developing aeronautics applications and proposed a set of heuristics to support their integration into Hybrid MKNF framework.", "AI": {"tldr": "Paper analyzes Hybrid MKNF for aeronautics KR challenges, focusing on expressivity and efficiency, with heuristics for integration.", "motivation": "Address challenges in deploying knowledge representation and reasoning in aeronautics, particularly achieving expressivity with rules and ontologies while maintaining efficiency.", "method": "Study uses the Hybrid MKNF KR language for its integration of rules and ontologies, evaluated through a concrete case study in the aeronautics domain.", "result": "Identified key expressivity features needed in aeronautics and proposed heuristics to integrate them into the Hybrid MKNF framework.", "conclusion": "Hybrid MKNF is suitable with adaptations; proposed heuristics enhance its application in aeronautics for better expressivity and efficiency."}}
{"id": "2601.04297", "categories": ["cs.LG", "cs.CV", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.04297", "abs": "https://arxiv.org/abs/2601.04297", "authors": ["Behrad Binaei-Haghighi", "Nafiseh Sadat Sajadi", "Mehrad Liviyan", "Reyhane Akhavan Kharazi", "Fatemeh Amirkhani", "Behnam Bahrak"], "title": "ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues", "comment": "12 pages, 7 figures", "summary": "The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.", "AI": {"tldr": "Digital drawing is used as a new, multimodal tool for psychological assessment via computer vision and behavioral analysis in the HTP test, with results validated against clinical metrics.", "motivation": "Non-verbal assessment of affective states is challenging, and digital drawing offers an underexplored modality to better capture psychological insights through its rich data.", "method": "A multimodal framework, ArtCognition, combines static visual features from computer vision and dynamic behavioral kinematic cues from the drawing process, using a Retrieval-Augmented Generation (RAG) architecture for interpretability.", "result": "Fusion of visual and behavioral cues provides a more nuanced assessment, with significant correlations between multimodal features and standardized psychological metrics.", "conclusion": "The work introduces a scalable methodology for non-intrusive affective state evaluation, supporting clinicians and enhancing technology-assisted mental healthcare."}}
{"id": "2601.04274", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.04274", "abs": "https://arxiv.org/abs/2601.04274", "authors": ["Alina Vozna", "Andrea Monaldini", "Stefania Costantini", "Valentina Pitoni", "Dawid Pado"], "title": "An ASP-based Solution to the Medical Appointment Scheduling Problem", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "This paper presents an Answer Set Programming (ASP)-based framework for medical appointment scheduling, aimed at improving efficiency, reducing administrative overhead, and enhancing patient-centered care. The framework personalizes scheduling for vulnerable populations by integrating Blueprint Personas. It ensures real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms by centralizing planning operations within an ASP logic model.", "AI": {"tldr": "ASP-based framework for medical appointment scheduling improves efficiency, reduces overhead, and enhances patient-centered care.", "motivation": "To address inefficiencies and administrative burdens in medical appointment scheduling, and to better serve vulnerable populations.", "method": "Integrates Blueprint Personas for personalization, uses ASP logic model for centralized planning, ensures real-time updates and conflict-free assignments.", "result": "The framework enables personalized scheduling, real-time availability, conflict resolution, and interoperability with healthcare platforms.", "conclusion": "The ASP-based framework effectively enhances scheduling efficiency and patient-centered care, showing promise for implementation in healthcare systems."}}
{"id": "2601.04299", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.04299", "abs": "https://arxiv.org/abs/2601.04299", "authors": ["Pir Bakhsh Khokhar", "Carmine Gravino", "Fabio Palomba", "Sule Yildrim Yayilgan", "Sarang Shaikh"], "title": "Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes", "comment": null, "summary": "Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn multimodal temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a transformer encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through transformer attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.", "AI": {"tldr": "Explainable deep learning integrates glucose and lab data to identify 5 metabolic phenotypes in Type 1 diabetes, aiding risk stratification.", "motivation": "Type 1 diabetes is metabolically heterogeneous; conventional biomarkers like HbA1c are insufficient for fully characterizing the disease's complexity.", "method": "A framework combining CGM and lab profiles uses transformer encoders for multimodal temporal embeddings and Gaussian mixture modeling to find metabolic phenotypes.", "result": "Five distinct phenotypes were identified, ranging from stable metabolism to high cardiometabolic risk, with unique biochemical profiles and associations with health conditions.", "conclusion": "The framework reveals coherent metabolic subgroups, enhances risk assessment in T1D, and offers interpretability through attention and SHAP analysis."}}
{"id": "2601.04301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04301", "abs": "https://arxiv.org/abs/2601.04301", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Baber Abbasi", "Ken Ziyu Liu", "Brando Miranda", "Ahmed Ahmed", "Abhay Puri", "Niloofar Mireshghallah", "Sanmi Koyejo"], "title": "Quantifying the Effect of Test Set Contamination on Generative Evaluations", "comment": null, "summary": "As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.", "AI": {"tldr": "Contamination significantly boosts generative evaluation performance, with even minimal test set exposure enabling loss below irreducible error. Contamination effects depend on training stage, model size, and inference parameters like temperature, with longer solutions being exponentially harder to memorize than shorter ones.", "motivation": "Test set contamination has become a critical concern for accurately assessing AI system capabilities, but most research has focused on discriminative evaluations. Little research has studied the impact on generative evaluations, which is important because contamination effects may differ for tasks involving longer, more complex outputs.", "method": "The researchers pretrained language models on mixtures of web data and the MATH benchmark, varying model sizes and number of test set replicas in the pretraining corpus. They used scaling laws to analyze performance and studied effects of further training (overtraining with fresh data and supervised finetuning). They also examined inference-time factors like sampling temperature and solution length.", "result": "The study found that performance improves with both contamination and model size. Surprisingly, even a single test set replica in pretraining enables models to achieve lower loss than the irreducible error of training on uncontaminated data. Overtraining with fresh data reduces contamination effects, while supervised finetuning's impact depends on the amount of pretraining contamination. At inference, high sampling temperatures mitigate contamination effects, and longer solutions are exponentially harder to memorize than shorter ones.", "conclusion": "The authors conclude that test set contamination introduces complex dynamics in generative evaluations, contrasting with simpler memorization effects in discriminative tasks. They emphasize that understanding these generation-memorization interactions is crucial for trustworthy AI evaluation."}}
{"id": "2601.04336", "categories": ["cs.AI", "cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.04336", "abs": "https://arxiv.org/abs/2601.04336", "authors": ["William Franz Lamberti", "Sunbin Kim", "Samantha Rose Lawrence"], "title": "Pilot Study on Student Public Opinion Regarding GAI", "comment": "7 pages, 8 figures", "summary": "The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.", "AI": {"tldr": "Pilot study on university students' perceptions of generative AI in education reveals low research participation but provides foundational insights for better classroom integration.", "motivation": "To investigate university students' perceptions of generative AI in higher education classrooms and understand attitudes toward its appropriate use.", "method": "Pilot study with university students using survey participation, achieving a 4.4% participation rate.", "result": "Low participation rate (4.4%) highlighted challenges in engaging students in GAI research and emphasized the need for larger sample sizes.", "conclusion": "The study found low engagement rates in GAI research but provided initial insights into student perspectives that can help instructors better prepare for GAI integration in classrooms."}}
{"id": "2601.04361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04361", "abs": "https://arxiv.org/abs/2601.04361", "authors": ["Mohammad Ali Javidian"], "title": "Causally-Aware Information Bottleneck for Domain Adaptation", "comment": "An extended abstract version of this work was accepted for the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation.", "AI": {"tldr": "A method for causal domain adaptation that imputes missing target variables by learning mechanism-stable representations using Information Bottleneck principles, with closed-form solutions for linear Gaussian models and variational approaches for nonlinear cases, demonstrating consistent performance in high-dimensional settings.", "motivation": "To address the challenge of imputing a target variable in the target domain when it is observed in the source domain but completely missing in the target domain, under various distribution shifts, by learning a compact, mechanism-stable representation.", "method": "For linear Gaussian causal models, a closed-form Gaussian Information Bottleneck solution is derived, resembling a CCA-style projection with optional DAG-awareness. For nonlinear or non-Gaussian data, a Variational Information Bottleneck encoder-predictor is introduced, which can be trained on source data and applied zero-shot to the target domain.", "result": "The approach consistently achieves accurate imputations across synthetic and real datasets.", "conclusion": "The paper presents a unified and practical toolkit for causal domain adaptation, showing consistent performance across both synthetic and real datasets, making it suitable for high-dimensional causal models."}}
{"id": "2601.04387", "categories": ["cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.04387", "abs": "https://arxiv.org/abs/2601.04387", "authors": ["Stuti Sinha", "Himanshu Kumar", "Aryan Raju Mandapati", "Rakshit Sakhuja", "Dhruv Kumar"], "title": "The Language of Bargaining: Linguistic Effects in LLM Negotiations", "comment": "Under Review", "summary": "Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.", "AI": {"tldr": "Language choice in LLM negotiation evaluations impacts outcomes more than model changes, requiring culturally-aware assessment.", "motivation": "To demonstrate that evaluating LLM negotiation solely in English is incomplete and may lead to misleading conclusions, highlighting the need for culturally-aware evaluation.", "method": "Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, with systematic isolation of language effects by holding game rules, model parameters, and incentives constant across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi).", "result": "Language choice shifts outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Effects are task-contingent: Indic languages reduce stability in distributive games but induce richer exploration in integrative settings.", "conclusion": "Evaluating LLM negotiation only in English yields incomplete and potentially misleading results, cautioning against English-only evaluation and emphasizing the need for culturally-aware evaluation for fair deployment."}}
{"id": "2601.04362", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.04362", "abs": "https://arxiv.org/abs/2601.04362", "authors": ["Rodja Trappe"], "title": "Phasor Agents: Oscillatory Graphs with Three-Factor Plasticity and Sleep-Staged Learning", "comment": "22 pages, 14 figures", "summary": "Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable \"rhythm generator\" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.\n  A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning.\n  A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).\n  The codebase and all artifacts are open-source.", "AI": {"tldr": "Phasor Agents use oscillatory networks with biological learning mechanisms inspired by sleep stages, achieving stable learning without backpropagation and demonstrating improved maze navigation and latent learning.", "motivation": "A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).", "method": "Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable \"rhythm generator\" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.", "result": "A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).", "conclusion": "A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning."}}
{"id": "2601.04388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04388", "abs": "https://arxiv.org/abs/2601.04388", "authors": ["Priyaranjan Pattnayak", "Sanchari Chowdhuri", "Amit Agarwal", "Hitesh Laxmichand Patel"], "title": "LLM-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations", "comment": "Accepted in AACL 2025 Main Conference", "summary": "Clustering customer chat data is vital for cloud providers handling multi service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Reclustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi turn chats into service specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via DaviesBouldin Index and Silhouette Scores, with LLM based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100\\% and reduces DBI by 65.6\\% compared to baselines, enabling scalable, real time analytics without full reclustering.", "AI": {"tldr": "Adaptive clustering system for customer chat data segments by service concerns, incrementally refines clusters using LLM-based splitting only when quality degrades, drastically outperforming traditional methods on quality metrics.", "motivation": "Traditional clustering methods for customer chat data create broad, static clusters that degrade over time, struggle with overlapping service queries, and require disruptive reclustering that breaks continuity in issue tracking.", "method": "Segment multi-turn chats into service-specific concerns, then incrementally refine clusters as new issues arise, tracking quality via Davies-Bouldin Index and Silhouette Scores while applying LLM-based splitting only to degraded clusters.", "result": "Method improves Silhouette Scores by over 100% and reduces Davies-Bouldin Index by 65.6% compared to baseline clustering approaches, enabling effective real-time analytics.", "conclusion": "The proposed adaptive clustering system offers a scalable solution for handling multi-service customer chat queries by enabling real-time analytics and continuous cluster quality maintenance without disruptive full reclustering."}}
{"id": "2601.04365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04365", "abs": "https://arxiv.org/abs/2601.04365", "authors": ["Anton Roupassov-Ruiz", "Yiyang Zuo"], "title": "Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning", "comment": null, "summary": "In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.", "AI": {"tldr": "Programmatic policies, when compared to neural policies in an evolutionary reinforcement learning task, show better survival performance, with additional benefits from learning-only setups.", "motivation": "The lack of modular structure in neural network agents limits behavioral interpretation; this paper investigates if programmatic policies can match or exceed their performance in a classic evolutionary reinforcement learning testbed.", "method": "Conduct rigorous survival analysis using Kaplan-Meier curves and Restricted Mean Survival Time metrics over 4000 independent trials in an open-source reimplementation of the 1992 Artificial Life testbed.", "result": "Programmatic policies survive on average 201.69 steps longer than neural policies. Learning-only SDDL setups outperform neural setups that combine learning and evolution by 73.67 steps on average.", "conclusion": "Programmatic policies can exceed the survival performance of neural policies in Artificial Life contexts, indicating potential advantages in interpretability and learning strategies."}}
{"id": "2601.04390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04390", "abs": "https://arxiv.org/abs/2601.04390", "authors": ["Siyuan Huang", "Yutong Gao", "Juyang Bai", "Yifan Zhou", "Zi Yin", "Xinxin Liu", "Rama Chellappa", "Chun Pong Lau", "Sayan Nag", "Cheng Peng", "Shraman Pramanick"], "title": "SciFig: Towards Automating Scientific Figure Generation", "comment": null, "summary": "Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\\%$ overall quality on dataset-level evaluation and 66.2$\\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.", "AI": {"tldr": "SciFig is an AI agent that automatically generates publication-ready pipeline figures from research papers using hierarchical layout generation and iterative feedback, achieving strong performance metrics.", "motivation": "Manual figure generation for scientific papers is time-consuming and requires both domain expertise and design skills, despite over 2.5 million scientific papers being published annually.", "method": "Hierarchical layout generation strategy that parses research descriptions to identify component relationships, groups elements into functional modules, and generates inter-module connections. Uses iterative chain-of-thought feedback mechanism for progressive layout improvement through multiple rounds of visual analysis and reasoning.", "result": "Achieved 70.1% overall quality on dataset-level evaluation and 66.2% on paper-specific evaluation, with consistently high scores across visual clarity, structural organization, and scientific accuracy metrics.", "conclusion": "SciFig demonstrates strong performance in generating publication-ready scientific figures directly from research paper texts, with plans to open-source the pipeline and evaluation benchmark."}}
{"id": "2601.04366", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04366", "abs": "https://arxiv.org/abs/2601.04366", "authors": ["Selcuk Koyuncu", "Ronak Nouri", "Stephen Providence"], "title": "Machine Learning Model for Sparse PCM Completion", "comment": null, "summary": "In this paper, we propose a machine learning model for sparse pairwise comparison matrices (PCMs), combining classical PCM approaches with graph-based learning techniques. Numerical results are provided to demonstrate the effectiveness and scalability of the proposed method.", "AI": {"tldr": "A machine learning model combining classical PCM methods with graph-based learning effectively handles sparse pairwise comparison matrices.", "motivation": "To handle sparse pairwise comparison matrices (PCMs) more effectively by leveraging machine learning and graph-based techniques.", "method": "The method combines classical pairwise comparison matrix (PCM) approaches with graph-based learning techniques.", "result": "Numerical results demonstrate the effectiveness and scalability of the proposed method.", "conclusion": "The proposed machine learning model combined with classical PCM approaches and graph-based learning techniques is effective and scalable for handling sparse pairwise comparison matrices, as demonstrated by the numerical results."}}
{"id": "2601.04393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04393", "abs": "https://arxiv.org/abs/2601.04393", "authors": ["Eren Kocadag", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "Assessing the quality and coherence of word embeddings after SCM-based intersectional bias mitigation", "comment": null, "summary": "Static word embeddings often absorb social biases from the text they learn from, and those biases can quietly shape downstream systems. Prior work that uses the Stereotype Content Model (SCM) has focused mostly on single-group bias along warmth and competence. We broaden that lens to intersectional bias by building compound representations for pairs of social identities through summation or concatenation, and by applying three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. We study three widely used embedding families (Word2Vec, GloVe, and ConceptNet Numberbatch) and assess them with two complementary views of utility: whether local neighborhoods remain coherent and whether analogy behavior is preserved. Across models, SCM-based mitigation carries over well to the intersectional case and largely keeps the overall semantic landscape intact. The main cost is a familiar trade off: methods that most tightly preserve geometry tend to be more cautious about analogy behavior, while more assertive projections can improve analogies at the expense of strict neighborhood stability. Partial Projection is reliably conservative and keeps representations steady; Linear Projection can be more assertive; Subtraction is a simple baseline that remains competitive. The choice between summation and concatenation depends on the embedding family and the application goal. Together, these findings suggest that intersectional debiasing with SCM is practical in static embeddings, and they offer guidance for selecting aggregation and debiasing settings when balancing stability against analogy performance.", "AI": {"tldr": "Study shows intersectional debiasing with SCM for static word embeddings works well, preserving semantics while balancing geometry and analogy performance. Partial Projection is conservative; Linear and Subtraction offer alternatives; aggregation choice depends on context.", "motivation": "Address social biases in word embeddings, particularly extending prior work from single-group bias to intersectional bias with pairs of social identities, to prevent biases from shaping downstream systems.", "method": "Using Stereotype Content Model (SCM) with compound representations (summation or concatenation) of social identity pairs, and three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. Evaluated on coherence of local neighborhoods and preservation of analogy behavior.", "result": "Intersectional bias mitigation with SCM carries over effectively, maintaining semantic coherence. Trade-offs: stricter geometry preservation yields cautious analogy behavior, assertive projection improves analogies but reduces neighborhood stability. Partial Projection is most stable; Linear offers assertiveness; Subtraction remains competitive; choice between aggregation methods depends on embedding family and goals.", "conclusion": "SCM-based debiasing methods are practical for static word embeddings, supporting intersectional bias mitigation. Partial Projection is conservative and stable, while Linear and Subtraction offer balance or assertiveness. Choice between summation and concatenation depends on embedding family and goals."}}
{"id": "2601.04378", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04378", "abs": "https://arxiv.org/abs/2601.04378", "authors": ["Corentin Lobet", "Francesca Chiaromonte"], "title": "Aligned explanations in neural networks", "comment": null, "summary": "Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.", "AI": {"tldr": "Proposes PiNets, pseudo-linear networks enabling linearly readable, explanatory-aligned deep learning models that directly link explanations to predictions for better trustworthiness in image tasks.", "motivation": "The researchers argue that current feature attribution methods loosely reflect model prediction processes, essentially serving as post-hoc rationalizations that 'white-paint' black boxes rather than providing meaningful explanations. They identify explanatory alignment as a key component of trustworthiness in prediction tasks, where explanations must be directly linked to predictions rather than being generated after the fact.", "method": "The paper introduces PiNets (pseudo-linear networks), a modeling framework that produces instance-wise linear predictions in arbitrary feature spaces. This linear readability design principle enables direct mapping between model architecture and explainability. The approach is demonstrated on image classification and segmentation tasks, where explanations can be traced directly through the network's decision-making process.", "result": "PiNets successfully produce linearly readable explanations that demonstrate faithful alignment across multiple criteria. The framework proves effective in both image classification and segmentation tasks, showing that explanatory alignment can be achieved without sacrificing model performance while providing substantially more trustworthy explanations than traditional post-hoc attribution methods.", "conclusion": "Model readability, enabled through pseudo-linear networks (PiNets), represents a significant advancement in explanatory alignment for deep learning. By directly linking explanations to predictions rather than offering post-hoc rationalizations, PiNets provide faithful, instance-wise explanations that can help build trust in neural network predictions across diverse computer vision tasks."}}
{"id": "2601.04416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04416", "abs": "https://arxiv.org/abs/2601.04416", "authors": ["Forest Mars"], "title": "Transitive Expert Error and Routing Problems in Complex AI Systems", "comment": "31pp", "summary": "Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.", "AI": {"tldr": "This paper analyzes how domain expertise can lead to systematic errors at domain boundaries, termed Transitive Expert Error (TEE), and applies these insights to AI systems with human-like failure patterns.", "motivation": "Domain experts often make reliable judgments within their field but may fail when encountering problems at the borders of their expertise due to unrecognized causal divergences masked by superficial similarities. This is distinct from the Dunning-Kruger effect and requires understanding mechanisms that cause these failures, especially in AI systems that simulate expert routing.", "method": "The paper identifies mechanisms underlying TEE: structural similarity bias and authority persistence, which intensify under conditions like shared vocabulary masking divergence, social pressure, and delayed feedback. It extends these findings to AI architectures (MoE systems, tool-using agents, RAG), analyzing failure modes such as routing-induced failures (wrong specialist) and coverage-induced failures (no appropriate specialist).", "result": "The study finds that TEE produce hallucination-like outputs in AI systems: confident, coherent, but causally incorrect results at domain boundaries. Signatures like routing patterns and confidence-accuracy dissociations help detect these errors. Human systems treat such mechanisms as cognitive black boxes, while AI architectures make them explicit.", "conclusion": "TEE is addressable in AI architectures through interventions such as multi-expert activation with disagreement detection, boundary-aware calibration, and coverage gap detection. While human cognition remains intractable in this regard, architectural design offers pathways for monitoring and mitigating these systematic errors, enhancing reliability at domain boundaries in both human and AI systems."}}
{"id": "2601.04392", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04392", "abs": "https://arxiv.org/abs/2601.04392", "authors": ["Mohsen Jalaeian-Farimani"], "title": "Enhanced-FQL($\u03bb$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay", "comment": "Submitted to ECC26 conference", "summary": "This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($\u03bb$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($\u03bb$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($\u03bb$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.", "AI": {"tldr": "Enhanced-FQL(\u03bb) integrates fuzzified eligibility traces and segmented experience replay into fuzzy Q-learning for efficient, interpretable continuous control.", "motivation": "To address the need for explainable and computationally efficient reinforcement learning methods in continuous control tasks, especially safety-critical applications with transparency and resource constraints, by avoiding complex neural networks.", "method": "Develops Enhanced-FQL(\u03bb) with a fuzzy rule base, using a fuzzified Bellman equation with eligibility traces for stable credit assignment and a segmented experience replay mechanism for memory efficiency.", "result": "Demonstrates superior sample efficiency and reduced variance compared to baselines like n-step fuzzy TD and fuzzy SARSA(\u03bb), with lower complexity than deep RL alternatives such as DDPG, and proven theoretical convergence.", "conclusion": "Enhanced-FQL(\u03bb) offers an interpretable, efficient framework suitable for safety-critical applications, balancing performance, transparency, and computational resources."}}
{"id": "2601.04426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04426", "abs": "https://arxiv.org/abs/2601.04426", "authors": ["Linzhang Li", "Yixin Dong", "Guanjie Wang", "Ziyi Xu", "Alexander Jiang", "Tianqi Chen"], "title": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs", "comment": null, "summary": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.", "AI": {"tldr": "XGrammar 2 is an optimized structured generation engine for LLM agents that introduces TagDispatch, JIT compilation, and caching mechanisms to achieve 6x speedup for dynamic structured generation tasks with near-zero overhead.", "motivation": "Modern LLM agents face challenges with complex structured generation tasks like tool calling and conditional structured generation, which are more dynamic than predefined structures and pose challenges to current structured generation engines.", "method": "The authors propose XGrammar 2 with several key innovations: TagDispatch for dynamic dispatching semantics, JIT compilation to reduce compilation time, cross-grammar caching for leveraging common sub-structures, extending PDA-based mask generation to Earley-parser-based approach, and a repetition compression algorithm for handling repetition structures in grammars.", "result": "Evaluation results show that XGrammar 2 achieves more than 6x speedup over existing structured generation engines and can handle dynamic structured generation tasks with near-zero overhead when integrated with an LLM inference engine.", "conclusion": "XGrammar 2 is an effective solution for dynamic structured generation tasks in LLM agents, achieving significant speed improvements and near-zero overhead when integrated with LLM inference engines."}}
{"id": "2601.04411", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04411", "abs": "https://arxiv.org/abs/2601.04411", "authors": ["Ali Rad", "Khashayar Filom", "Darioush Keivan", "Peyman Mohajerin Esfahani", "Ehsan Kamalinejad"], "title": "Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?\n  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time (\"rate, not fate\"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.", "AI": {"tldr": "RLVR with noisy verifiers exhibits a phase transition: learning succeeds when verifier's Youden's index J=TPR-FPR > 0, fails when J<0, and noise affects convergence speed but not outcome in the learning regime.", "motivation": "The motivation is to understand whether verification noise in reinforcement learning with verifiable rewards (RLVR) merely slows down learning or can fundamentally change outcomes (rate vs. fate), particularly in challenging domains like coding where verifiers are imperfect and model-generated.", "method": "The authors develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. They model false positives/negatives and group completions into recurring reasoning modes, yielding replicator-style dynamics on the probability simplex.", "result": "Results show a sharp phase transition at J=0 boundary: when J>0 (learning), incorrect mass is driven to extinction; when J=0 (neutral), process is neutral; when J<0 (anti-learning), incorrect modes dominate. Experiments on verifiable programming tasks under synthetic noise confirm the predicted J=0 boundary.", "conclusion": "The paper concludes that there is a sharp phase transition in RLVR dynamics determined by Youden's index (J=TPR-FPR): learning occurs when J>0, neutrality when J=0, and anti-learning/collapse when J<0. In the learning regime, noise primarily affects convergence speed rather than outcomes."}}
{"id": "2601.04456", "categories": ["cs.AI", "math.CT"], "pdf": "https://arxiv.org/pdf/2601.04456", "abs": "https://arxiv.org/abs/2601.04456", "authors": ["Enrique ter Horst", "Sridhar Mahadevan", "Juan Diego Zambrano"], "title": "Categorical Belief Propagation: Sheaf-Theoretic Inference via Descent and Holonomy", "comment": "No essential info", "summary": "We develop a categorical foundation for belief propagation on factor graphs. We construct the free hypergraph category \\(\\Syn_\u03a3\\) on a typed signature and prove its universal property, yielding compositional semantics via a unique functor to the matrix category \\(\\cat{Mat}_R\\). Message-passing is formulated using a Grothendieck fibration \\(\\int\\Msg \\to \\cat{FG}_\u03a3\\) over polarized factor graphs, with schedule-indexed endomorphisms defining BP updates. We characterize exact inference as effective descent: local beliefs form a descent datum when compatibility conditions hold on overlaps. This framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. We introduce HATCC (Holonomy-Aware Tree Compilation), an algorithm that detects descent obstructions via holonomy computation on the factor nerve, compiles non-trivial holonomy into mode variables, and reduces to tree BP on an augmented graph. Complexity is \\(O(n^2 d_{\\max} + c \\cdot k_{\\max} \\cdot \u03b4_{\\max}^3 + n \\cdot \u03b4_{\\max}^2)\\) for \\(n\\) factors and \\(c\\) fundamental cycles. Experimental results demonstrate exact inference with significant speedup over junction trees on grid MRFs and random graphs, along with UNSAT detection on satisfiability instances.", "AI": {"tldr": "Categorical framework for belief propagation using sheaf theory and descent, with HATCC algorithm enabling exact inference on loopy graphs via holonomy detection and tree compilation.", "motivation": "To provide a categorical foundation for belief propagation that unifies various inference algorithms and enables efficient exact inference on loopy graphs.", "method": "A categorical framework for belief propagation using free hypergraph categories, Grothendieck fibrations, and sheaf-theoretic descent. Introduces HATCC algorithm for holonomy-aware tree compilation.", "result": "HATCC achieves exact inference with significant speedup over junction trees on grid MRFs and random graphs, and can detect UNSAT instances in satisfiability problems.", "conclusion": "Belief propagation can be rephrased in categorical and sheaf-theoretic terms, with the HATCC algorithm enabling exact inference on loopy graphs and providing complexity improvements over junction tree methods."}}
{"id": "2601.04413", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04413", "abs": "https://arxiv.org/abs/2601.04413", "authors": ["Nausherwan Malik", "Zubair Khalid", "Muhammad Faryad"], "title": "Distribution-Guided and Constrained Quantum Machine Unlearning", "comment": "8 pages", "summary": "Machine unlearning aims to remove the influence of specific training data from a learned model without full retraining. While recent work has begun to explore unlearning in quantum machine learning, existing approaches largely rely on fixed, uniform target distributions and do not explicitly control the trade-off between forgetting and retained model behaviour. In this work, we propose a distribution-guided framework for class-level quantum machine unlearning that treats unlearning as a constrained optimization problem. Our method introduces a tunable target distribution derived from model similarity statistics, decoupling the suppression of forgotten-class confidence from assumptions about redistribution among retained classes. We further incorporate an anchor-based preservation constraint that explicitly maintains predictive behaviour on selected retained data, yielding a controlled optimization trajectory that limits deviation from the original model. We evaluate the approach on variational quantum classifiers trained on the Iris and Covertype datasets. Results demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with the gold retrained model baselines compared to uniform-target unlearning. These findings highlight the importance of target design and constraint-based formulations for reliable and interpretable quantum machine unlearning.", "AI": {"tldr": "TL;DR: This paper proposes a distribution-guided framework for class-level quantum machine unlearning using a tunable target distribution and anchor-based preservation constraints, showing improved forgetting and retention compared to uniform-target methods on quantum classifiers.", "motivation": "The motivation is to address limitations in current quantum machine unlearning (QMU) approaches, which often rely on fixed uniform target distributions and lack explicit control over the trade-off between forgetting specific data and preserving model performance on retained classes, aiming for more reliable and interpretable unlearning in quantum machine learning.", "method": "The method introduces a distribution-guided framework that treats unlearning as a constrained optimization problem. It uses a tunable target distribution derived from model similarity statistics to decouple forgotten-class suppression from assumptions about redistribution among retained classes, and incorporates an anchor-based preservation constraint to maintain predictive behavior on selected retained data, providing a controlled optimization trajectory.", "result": "Results on variational quantum classifiers trained on Iris and Covertype datasets demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with gold retrained model baselines compared to uniform-target unlearning methods.", "conclusion": "The conclusion is that target design and constraint-based formulations are crucial for effective quantum machine unlearning. The proposed framework enables more reliable and interpretable unlearning by better controlling the trade-off between forgetting and retention, as validated by improved performance in suppressing forgotten classes while maintaining model integrity."}}
{"id": "2601.04474", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04474", "abs": "https://arxiv.org/abs/2601.04474", "authors": ["Bill Marino", "Nicholas D. Lane"], "title": "Computational Compliance for AI Regulation: Blueprint for a New Research Domain", "comment": null, "summary": "The era of AI regulation (AIR) is upon us. But AI systems, we argue, will not be able to comply with these regulations at the necessary speed and scale by continuing to rely on traditional, analogue methods of compliance. Instead, we posit that compliance with these regulations will only realistically be achieved computationally: that is, with algorithms that run across the life cycle of an AI system, automatically steering it toward AIR compliance in the face of dynamic conditions. Yet despite their (we would argue) inevitability, the research community has yet to specify exactly how these algorithms for computational AIR compliance should behave - or how we should benchmark their performance. To fill these gaps, we specify a set of design goals for such algorithms. In addition, we specify a benchmark dataset that can be used to quantitatively measure whether individual algorithms satisfy these design goals. By delivering this blueprint, we hope to give shape to an important but uncrystallized new domain of research - and, in doing so, incite necessary investment in it.", "AI": {"tldr": "The paper argues AI regulation requires computational, algorithmic compliance across the AI lifecycle. It proposes design goals and a benchmark dataset for these algorithms to define and measure this new research field.", "motivation": "The motivation stems from the impending implementation of AI regulations globally and the recognition that these regulations will impose complex, dynamic compliance requirements that cannot be handled at speed and scale using traditional, manual compliance methods. There is a gap in research about how to achieve computational compliance with AI regulations.", "method": "The authors adopt a conceptual and normative approach, first arguing that traditional compliance methods will be insufficient and that computational methods are necessary. They then propose specific design goals for computational AI regulation compliance algorithms and create a benchmark dataset for quantitatively evaluating such algorithms.", "result": "The paper delivers a research blueprint that includes: 1) A set of design goals for computational AI regulation compliance algorithms, 2) A benchmark dataset for quantitatively measuring whether algorithms satisfy these design goals. This framework aims to shape an important but uncrystallized new domain of research.", "conclusion": "The authors argue that computational compliance algorithms are inevitable for scaling AI regulation and propose a specific research blueprint (design goals and benchmark dataset) to crystallize this emerging field and stimulate necessary investment in it."}}
{"id": "2601.04441", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04441", "abs": "https://arxiv.org/abs/2601.04441", "authors": ["Matthew Landers", "Taylor W. Killian", "Thomas Hartvigsen", "Afsaneh Doryab"], "title": "Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization", "comment": null, "summary": "Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\\times$.", "AI": {"tldr": "SPIN is a two-stage method for reinforcement learning with combinatorial action spaces, improving performance and training speed.", "motivation": "Existing methods for combinatorial action spaces are inefficient, either simplifying policies unsuitably or being slow/unstable in joint learning.", "method": "Pre-train an Action Structure Model (ASM) to learn valid action manifolds, then freeze it and train lightweight policy heads.", "result": "On DM Control benchmarks, SPIN increases average return by up to 39% and reduces convergence time by up to 12.8x compared to state of the art.", "conclusion": "SPIN effectively addresses combinatorial action challenges by decoupling structure learning from control, leading to superior efficiency and performance."}}
{"id": "2601.04447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04447", "abs": "https://arxiv.org/abs/2601.04447", "authors": ["Gal Fybish", "Teo Susnjak"], "title": "When Predictions Shape Reality: A Socio-Technical Synthesis of Performative Predictions in Machine Learning", "comment": null, "summary": "Machine learning models are increasingly used in high-stakes domains where their predictions can actively shape the environments in which they operate, a phenomenon known as performative prediction. This dynamic, in which the deployment of the model influences the very outcome it seeks to predict, can lead to unintended consequences, including feedback loops, performance issues, and significant societal risks. While the literature in the field has grown rapidly in recent years, a socio-technical synthesis that systemises the phenomenon concepts and provides practical guidance has been lacking. This Systematisation of Knowledge (SoK) addresses this gap by providing a comprehensive review of the literature on performative predictions. We provide an overview of the primary mechanisms through which performativity manifests, present a typology of associated risks, and survey the proposed solutions offered in the literature. Our primary contribution is the ``Performative Strength vs. Impact Matrix\" assessment framework. This practical tool is designed to help practitioners assess the potential influence and severity of performativity on their deployed predictive models and select the appropriate level of algorithmic or human intervention.", "AI": {"tldr": "This SoK paper provides a comprehensive review of performative prediction literature, introduces mechanisms and risks, and offers a practical \"Performative Strength vs. Impact Matrix\" framework to help practitioners assess and manage performativity in predictive models.", "motivation": "The motivation stems from the increasing use of machine learning models in high-stakes domains where predictions can shape their operating environments (performative prediction), leading to unintended consequences like feedback loops and societal risks. There was a lack of socio-technical synthesis and practical guidance in the rapidly growing literature on this topic.", "method": "The authors conducted a Systematisation of Knowledge (SoK) approach, providing a comprehensive review of literature on performative predictions. They identified primary mechanisms of performativity manifestation, developed a typology of associated risks, and surveyed proposed solutions from existing literature.", "result": "The primary result is the development of the \"Performative Strength vs. Impact Matrix\" assessment framework, which serves as a practical tool for practitioners to evaluate the influence and severity of performativity on deployed predictive models and determine appropriate levels of algorithmic or human intervention.", "conclusion": "This paper concludes with the introduction of the \"Performative Strength vs. Impact Matrix\" as a practical assessment framework to help practitioners evaluate performativity risks and select appropriate interventions for their predictive models."}}
{"id": "2601.04500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04500", "abs": "https://arxiv.org/abs/2601.04500", "authors": ["Yifei Gao", "Jiang Wu", "Xiaoyi Chen", "Yifan Yang", "Zhe Cui", "Tianyi Ma", "Jiaming Zhang", "Jitao Sang"], "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery", "comment": null, "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.", "AI": {"tldr": "GUITester is a multi-agent framework that decouples navigation from verification for autonomous exploratory GUI testing, significantly outperforming baselines on a new benchmark.", "motivation": "Exploratory GUI testing is costly and manual. MLLM agents can navigate but fail to autonomously discover defects due to Goal-Oriented Masking (prioritizing tasks over anomaly reporting) and Execution-Bias Attribution (misattributing system defects as agent errors).", "method": "Proposes GUITester, a multi-agent framework with two modules: (1) Planning-Execution Module (PEM) that embeds testing intents to proactively probe for defects, and (2) Hierarchical Reflection Module (HRM) that analyzes interaction history to resolve attribution ambiguity.", "result": "GUITester achieves an F1-score of 48.90% (Pass@3) on the GUITestBench benchmark, outperforming state-of-the-art baselines (33.35%). GUITestBench is introduced as the first interactive benchmark with 143 tasks across 26 defects.", "conclusion": "The paper concludes that autonomous exploratory GUI testing is feasible with GUITester\u2019s decoupled multi-agent design, achieving a 48.90% F1-score and significantly outperforming existing baselines. It establishes a robust benchmark and framework for future GUI quality assurance research."}}
