{"id": "2511.02997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02997", "abs": "https://arxiv.org/abs/2511.02997", "authors": ["Jon Kutasov", "Chloe Loughridge", "Yuqi Sun", "Henry Sleight", "Buck Shlegeris", "Tyler Tracy", "Joe Benton"], "title": "Evaluating Control Protocols for Untrusted AI Agents", "comment": null, "summary": "As AI systems become more capable and widely deployed as agents, ensuring\ntheir safe operation becomes critical. AI control offers one approach to\nmitigating the risk from untrusted AI agents by monitoring their actions and\nintervening or auditing when necessary. Evaluating the safety of these\nprotocols requires understanding both their effectiveness against current\nattacks and their robustness to adaptive adversaries. In this work, we\nsystematically evaluate a range of control protocols in SHADE-Arena, a dataset\nof diverse agentic environments. First, we evaluate blue team protocols,\nincluding deferral to trusted models, resampling, and deferring on critical\nactions, against a default attack policy. We find that resampling for\nincrimination and deferring on critical actions perform best, increasing safety\nfrom 50% to 96%. We then iterate on red team strategies against these protocols\nand find that attack policies with additional affordances, such as knowledge of\nwhen resampling occurs or the ability to simulate monitors, can substantially\nimprove attack success rates against our resampling strategy, decreasing safety\nto 17%. However, deferring on critical actions is highly robust to even our\nstrongest red team strategies, demonstrating the importance of denying attack\npolicies access to protocol internals.", "AI": {"tldr": "This paper systematically evaluates AI control protocols for safety against both default and adaptive attacks, finding that deferring on critical actions maintains high robustness while resampling is vulnerable to sophisticated adversaries.", "motivation": "As AI agents become more capable and widely deployed, ensuring their safe operation is critical. AI control protocols need evaluation against both current attacks and adaptive adversaries to understand their effectiveness and robustness.", "method": "The authors systematically evaluate control protocols using SHADE-Arena, a dataset of diverse agentic environments. They analyze blue team protocols (deferral to trusted models, resampling, deferring on critical actions) against default attacks, then iterate red team strategies against these protocols.", "result": "Resampling for incrimination and deferring on critical actions performed best against default attacks, increasing safety from 50% to 96%. However, adaptive red team strategies with additional affordances reduced safety for resampling to 17%, while deferring on critical actions remained highly robust.", "conclusion": "Deferring on critical actions demonstrates strong robustness against adaptive adversaries, highlighting the importance of denying attack policies access to protocol internals for maintaining AI safety."}}
{"id": "2511.03023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03023", "abs": "https://arxiv.org/abs/2511.03023", "authors": ["Sina Montazeri", "Yunhe Feng", "Kewei Sha"], "title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework", "comment": null, "summary": "Open data repositories hold potential for evidence-based decision-making, yet\nare inaccessible to non-experts lacking expertise in dataset discovery, schema\nmapping, and statistical analysis. Large language models show promise for\nindividual tasks, but end-to-end analytical workflows expose fundamental\nlimitations: attention dilutes across growing contexts, specialized reasoning\npatterns interfere, and errors propagate undetected. We present PublicAgent, a\nmulti-agent framework that addresses these limitations through decomposition\ninto specialized agents for intent clarification, dataset discovery, analysis,\nand reporting. This architecture maintains focused attention within agent\ncontexts and enables validation at each stage. Evaluation across five models\nand 50 queries derives five design principles for multi-agent LLM systems.\nFirst, specialization provides value independent of model strength--even the\nstrongest model shows 97.5% agent win rates, with benefits orthogonal to model\nscale. Second, agents divide into universal (discovery, analysis) and\nconditional (report, intent) categories. Universal agents show consistent\neffectiveness (std dev 12.4%) while conditional agents vary by model (std dev\n20.5%). Third, agents mitigate distinct failure modes--removing discovery or\nanalysis causes catastrophic failures (243-280 instances), while removing\nreport or intent causes quality degradation. Fourth, architectural benefits\npersist across task complexity with stable win rates (86-92% analysis, 84-94%\ndiscovery), indicating workflow management value rather than reasoning\nenhancement. Fifth, wide variance in agent effectiveness across models (42-96%\nfor analysis) requires model-aware architecture design. These principles guide\nwhen and why specialization is necessary for complex analytical workflows while\nenabling broader access to public data through natural language interfaces.", "AI": {"tldr": "PublicAgent is a multi-agent LLM framework that decomposes data analysis workflows into specialized agents (intent, discovery, analysis, reporting) to overcome limitations of single LLMs, enabling non-experts to access open data repositories through natural language interfaces.", "motivation": "Open data repositories are inaccessible to non-experts due to required expertise in dataset discovery, schema mapping, and statistical analysis. Single LLMs fail in end-to-end workflows due to attention dilution, interference of specialized reasoning patterns, and error propagation.", "method": "Multi-agent framework with specialized agents: intent clarification, dataset discovery, analysis, and reporting. This maintains focused attention within agent contexts and enables validation at each stage of the workflow.", "result": "Evaluation across 5 models and 50 queries showed: 97.5% agent win rates even for strongest models, universal agents (discovery, analysis) show consistent effectiveness (std dev 12.4%), conditional agents (report, intent) vary by model (std dev 20.5%), removing discovery/analysis causes catastrophic failures (243-280 instances), architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery).", "conclusion": "Five design principles guide when and why specialization is necessary for complex analytical workflows, enabling broader access to public data through natural language interfaces. Specialization provides value independent of model strength, and wide variance in agent effectiveness across models requires model-aware architecture design."}}
{"id": "2511.03051", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03051", "abs": "https://arxiv.org/abs/2511.03051", "authors": ["Tao Zhang", "Kehui Yao", "Luyi Ma", "Jiao Chen", "Reza Yousefi Maragheh", "Kai Zhao", "Jianpeng Xu", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation", "comment": "4 page, NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle", "summary": "Evaluating large language models (LLMs) as judges is increasingly critical\nfor building scalable and trustworthy evaluation pipelines. We present\nScalingEval, a large-scale benchmarking study that systematically compares 36\nLLMs, including GPT, Gemini, Claude, and Llama, across multiple product\ncategories using a consensus-driven evaluation protocol. Our multi-agent\nframework aggregates pattern audits and issue codes into ground-truth labels\nvia scalable majority voting, enabling reproducible comparison of LLM\nevaluators without human annotation. Applied to large-scale complementary-item\nrecommendation, the benchmark reports four key findings: (i) Anthropic Claude\n3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers\nthe best overall performance across categories; (iii) GPT-4o provides the most\nfavorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among\nopen-source models. Category-level analysis shows strong consensus in\nstructured domains (Electronics, Sports) but persistent disagreement in\nlifestyle categories (Clothing, Food). These results establish ScalingEval as a\nreproducible benchmark and evaluation protocol for LLMs as judges, with\nactionable guidance on scaling, reliability, and model family tradeoffs.", "AI": {"tldr": "ScalingEval is a large-scale benchmarking study comparing 36 LLMs as evaluators using a consensus-driven protocol, finding Claude 3.5 Sonnet has highest confidence, Gemini 1.5 Pro best overall performance, GPT-4o best tradeoff, and GPT-OSS 20B leads open-source models.", "motivation": "The need for scalable and trustworthy evaluation pipelines for large language models (LLMs) as judges is increasingly critical for reliable AI assessment.", "method": "Multi-agent framework using consensus-driven evaluation protocol with scalable majority voting to aggregate pattern audits and issue codes into ground-truth labels, applied to complementary-item recommendation.", "result": "Claude 3.5 Sonnet has highest decision confidence; Gemini 1.5 Pro best overall performance; GPT-4o best latency-accuracy-cost tradeoff; GPT-OSS 20B leads open-source models. Strong consensus in structured domains but disagreement in lifestyle categories.", "conclusion": "ScalingEval establishes a reproducible benchmark and evaluation protocol for LLMs as judges, providing actionable guidance on scaling, reliability, and model family tradeoffs."}}
{"id": "2511.03094", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03094", "abs": "https://arxiv.org/abs/2511.03094", "authors": ["Longling Geng", "Edward Y. Chang"], "title": "ALAS: Transactional and Dynamic Multi-Agent LLM Planning", "comment": null, "summary": "Large language models enable flexible multi-agent planning but remain fragile\nin practice: verification is often circular, state changes are not tracked for\nrepair, and small faults trigger costly global recomputation. We present ALAS,\na stateful, disruption-aware framework that separates planning from\nnon-circular validation, records a versioned execution log for grounded checks\nand restore points, and performs localized repair that preserves work in\nprogress. The validator operates independently of the planning LLM with fresh,\nbounded context, avoiding self-check loops and mid-context attrition. The\nrepair protocol edits only the minimal affected region under explicit policies\n(retry, catch, timeout, backoff, idempotency keys, compensation, loop guards)\ndefined in a canonical workflow IR that maps to Amazon States Language and Argo\nWorkflows. On job-shop scheduling suites (DMU, TA) across five classical\nbenchmarks, ALAS matches or exceeds strong single-LLM and multi-agent\nbaselines, achieving 83.7% success, reducing token usage by 60%, and running\n1.82times faster under comparable settings. A minimal reliability study shows\nthat the validator detects injected structural faults with low overhead, and\nthat localized repair contains runtime perturbations with a bounded edit radius\nand less makespan degradation than global recompute. Results indicate that the\ncombination of validator isolation, versioned execution logs, and localized\nrepair provides measurable efficiency, feasibility, and scalability for\nmulti-agent LLM planning. Code and seeds will be released.", "AI": {"tldr": "ALAS is a framework that improves multi-agent LLM planning by separating planning from validation, using versioned logs for tracking, and enabling localized repair to avoid costly global recomputation.", "motivation": "Current LLM-based multi-agent planning is fragile due to circular verification, lack of state change tracking, and minor faults triggering expensive global recomputation.", "method": "ALAS employs a stateful, disruption-aware design with a validator independent of the planning LLM, versioned execution logs for checks and restore points, and a repair protocol with policies like retry and compensation in a workflow IR.", "result": "On job-shop scheduling benchmarks, ALAS achieved 83.7% success, reduced token usage by 60%, ran 1.82x faster, and effectively detected faults with localized repair minimizing disruption.", "conclusion": "The integration of validator isolation, versioned logs, and localized repair enhances efficiency, feasibility, and scalability for multi-agent LLM planning systems."}}
{"id": "2511.03070", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.03070", "abs": "https://arxiv.org/abs/2511.03070", "authors": ["Drago Plecko", "Patrik Okanovic", "Torsten Hoefler", "Elias Bareinboim"], "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge", "comment": null, "summary": "Artificial intelligence (AI) systems hold great promise for advancing various\nscientific disciplines, and are increasingly used in real-world applications.\nDespite their remarkable progress, further capabilities are expected in order\nto achieve more general types of intelligence. A critical distinction in this\ncontext is between factual knowledge, which can be evaluated against true or\nfalse answers (e.g., \"what is the capital of England?\"), and probabilistic\nknowledge, reflecting probabilistic properties of the real world (e.g., \"what\nis the sex of a computer science graduate in the US?\"). In this paper, our goal\nis to build a benchmark for understanding the capabilities of LLMs in terms of\nknowledge of probability distributions describing the real world. Given that\nLLMs are trained on vast amounts of text, it may be plausible that they\ninternalize aspects of these distributions. Indeed, LLMs are touted as powerful\nuniversal approximators of real-world distributions. At the same time,\nclassical results in statistics, known as curse of dimensionality, highlight\nfundamental challenges in learning distributions in high dimensions,\nchallenging the notion of universal distributional learning. In this work, we\ndevelop the first benchmark to directly test this hypothesis, evaluating\nwhether LLMs have access to empirical distributions describing real-world\npopulations across domains such as economics, health, education, and social\nbehavior. Our results demonstrate that LLMs perform poorly overall, and do not\nseem to internalize real-world statistics naturally. When interpreted in the\ncontext of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that\nlanguage models do not contain knowledge on observational distributions (Layer\n1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional\n(Layer 2) and counterfactual (Layer 3) knowledge of these models is also\nlimited.", "AI": {"tldr": "This paper introduces a benchmark to test whether large language models (LLMs) internalize real-world probabilistic knowledge, finding they perform poorly and lack observational distribution knowledge.", "motivation": "Despite claims that LLMs are universal approximators of real-world distributions, statistical challenges like the curse of dimensionality question this ability. The paper aims to directly evaluate if LLMs capture empirical distributions from domains like economics and health.", "method": "The authors develop the first benchmark to test LLMs' knowledge of probability distributions describing real-world populations, assessing performance across various domains.", "result": "LLMs perform poorly overall and do not naturally internalize real-world statistics. They lack knowledge of observational distributions (Pearl's Causal Hierarchy Layer 1), implying limitations in interventional and counterfactual knowledge.", "conclusion": "The findings challenge the notion of LLMs as universal distributional learners, highlighting fundamental gaps in their probabilistic knowledge and implications for causal reasoning capabilities."}}
{"id": "2511.03348", "categories": ["cs.MA", "68T05"], "pdf": "https://arxiv.org/pdf/2511.03348", "abs": "https://arxiv.org/abs/2511.03348", "authors": ["Changxi Zhu", "Mehdi Dastani", "Shihan Wang"], "title": "Learning Communication Skills in Multi-task Multi-agent Deep Reinforcement Learning", "comment": "20 pages, 10 figures", "summary": "In multi-agent deep reinforcement learning (MADRL), agents can communicate\nwith one another to perform a task in a coordinated manner. When multiple tasks\nare involved, agents can also leverage knowledge from one task to improve\nlearning in other tasks. In this paper, we propose Multi-task Communication\nSkills (MCS), a MADRL with communication method that learns and performs\nmultiple tasks simultaneously, with agents interacting through learnable\ncommunication protocols. MCS employs a Transformer encoder to encode\ntask-specific observations into a shared message space, capturing shared\ncommunication skills among agents. To enhance coordination among agents, we\nintroduce a prediction network that correlates messages with the actions of\nsender agents in each task. We adapt three multi-agent benchmark environments\nto multi-task settings, where the number of agents as well as the observation\nand action spaces vary across tasks. Experimental results demonstrate that MCS\nachieves better performance than multi-task MADRL baselines without\ncommunication, as well as single-task MADRL baselines with and without\ncommunication.", "AI": {"tldr": "MCS is a multi-agent deep reinforcement learning method that enables agents to learn communication protocols across multiple tasks using Transformer encoders and prediction networks, outperforming both multi-task and single-task baselines.", "motivation": "In multi-agent deep reinforcement learning, agents need to coordinate through communication, and knowledge transfer across multiple tasks can improve learning efficiency and performance.", "method": "Uses Transformer encoders to encode observations into a shared message space and introduces a prediction network to correlate messages with sender agents' actions for better coordination in multi-task environments.", "result": "MCS achieves superior performance compared to multi-task MADRL without communication and single-task MADRL with/without communication across adapted multi-agent benchmark environments.", "conclusion": "The proposed MCS framework effectively leverages communication skills transfer across tasks, demonstrating significant improvements in multi-task multi-agent reinforcement learning scenarios."}}
{"id": "2511.02872", "categories": ["cs.LG", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.02872", "abs": "https://arxiv.org/abs/2511.02872", "authors": ["Jiedong Jiang", "Wanyi He", "Yuefeng Wang", "Guoxiong Gao", "Yongle Hu", "Jingting Wang", "Nailing Guan", "Peihao Wu", "Chunbo Dai", "Liang Xiao", "Bin Dong"], "title": "FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\ncapabilities in formal theorem proving, particularly on contest-based\nmathematical benchmarks like the IMO. However, these contests do not reflect\nthe depth, breadth, and abstraction of modern mathematical research. To bridge\nthis gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new\nbenchmark series in formal algebra designed to chart a course toward advanced\nmathematical reasoning. We present two new components, FATE-H and FATE-X, each\nwith 100 problems in abstract and commutative algebra. The FATE series spans a\ndifficulty spectrum from undergraduate exercises to problems exceeding PhD\nqualifying exams. Notably, FATE-X is the first formal benchmark to surpass both\nPhD-level exam difficulty and the coverage of the Mathlib library. Our\nevaluations of state-of-the-art LLM provers on this new benchmark reveal a\nstark performance gap compared to contest math: the best model achieves only 3%\n(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals\nthat models' natural-language reasoning is notably more accurate than their\nability to formalize this reasoning. We systematically classify the common\nerrors that arise during this formalization process. Furthermore, a comparative\nstudy shows that a specialized prover can exhibit less effective reflection\nthan general-purpose models, reducing its accuracy at the natural-language\nstage. We believe FATE provides a robust and challenging benchmark that\nestablishes essential checkpoints on the path toward research-level formal\nmathematical reasoning.", "AI": {"tldr": "FATE is a new benchmark for formal algebra that goes beyond contest math to evaluate research-level mathematical reasoning, revealing major gaps in LLMs' formalization capabilities.", "motivation": "Current LLM benchmarks focus on contest-based math problems, which don't reflect the depth and abstraction of modern mathematical research. There's a need for benchmarks that evaluate research-level formal reasoning.", "method": "Created FATE benchmark series with two components: FATE-H (100 problems in abstract algebra) and FATE-X (100 problems in commutative algebra), spanning from undergraduate to PhD+ difficulty levels. Evaluated state-of-the-art LLM provers using two-stage evaluation (natural language reasoning vs. formalization).", "result": "Best LLM achieved only 3% accuracy on FATE-H and 0% on FATE-X (pass@64). Models showed better natural-language reasoning than formalization ability. Specialized provers had worse reflection than general-purpose models.", "conclusion": "FATE establishes essential checkpoints for research-level formal mathematical reasoning and reveals significant challenges in LLMs' formalization capabilities that need to be addressed."}}
{"id": "2511.03092", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03092", "abs": "https://arxiv.org/abs/2511.03092", "authors": ["Jonathan Li", "Nasim Farahini", "Evgenii Iuliugin", "Magnus Vesterlund", "Christian Haggstrom", "Guangtao Wang", "Shubhangi Upasani", "Ayush Sachdeva", "Rui Li", "Faline Fu", "Chen Wu", "Ayesha Siddiqua", "John Long", "Tuowen Zhao", "Matheen Musaddiq", "Hakan Zeffer", "Yun Du", "Mingran Wang", "Qinghua Li", "Bo Li", "Urmish Thakker", "Raghu Prabhakar"], "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators", "comment": null, "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.", "AI": {"tldr": "SnapStream is a KV cache compression method that enables 4x improved on-chip memory usage with minimal accuracy degradation, deployed in production inference systems with static graphs and continuous batching.", "motivation": "Large LLMs with 100k+ context lengths require substantial on-chip memory for KV caches, but existing compression techniques like StreamingLLM and SnapKV are not widely adopted in industrial deployments due to framework constraints and unclear accuracy implications.", "method": "Developed SnapStream, a KV cache compression method that works with static graphs and continuous batching frameworks like vLLM and SGLang. Tested accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1.", "result": "4x improved on-chip memory usage with minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. Successfully deployed in 16-way tensor-parallel DeepSeek-671B on SambaNova SN40L accelerators at 128k context length achieving up to 1832 tokens per second.", "conclusion": "SnapStream is the first implementation of sparse KV attention techniques successfully deployed in production inference systems with static graphs and continuous batching, demonstrating practical KV cache compression at scale."}}
{"id": "2511.03100", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03100", "abs": "https://arxiv.org/abs/2511.03100", "authors": ["Hao Xiang Li", "Michael Amir", "Amanda Prorok"], "title": "Scaling Multi-Agent Environment Co-Design with Diffusion Models", "comment": null, "summary": "The agent-environment co-design paradigm jointly optimises agent policies and\nenvironment configurations in search of improved system performance. With\napplication domains ranging from warehouse logistics to windfarm management,\nco-design promises to fundamentally change how we deploy multi-agent systems.\nHowever, current co-design methods struggle to scale. They collapse under\nhigh-dimensional environment design spaces and suffer from sample inefficiency\nwhen addressing moving targets inherent to joint optimisation. We address these\nchallenges by developing Diffusion Co-Design (DiCoDe), a scalable and\nsample-efficient co-design framework pushing co-design towards practically\nrelevant settings. DiCoDe incorporates two core innovations. First, we\nintroduce Projected Universal Guidance (PUG), a sampling technique that enables\nDiCoDe to explore a distribution of reward-maximising environments while\nsatisfying hard constraints such as spatial separation between obstacles.\nSecond, we devise a critic distillation mechanism to share knowledge from the\nreinforcement learning critic, ensuring that the guided diffusion model adapts\nto evolving agent policies using a dense and up-to-date learning signal.\nTogether, these improvements lead to superior environment-policy pairs when\nvalidated on challenging multi-agent environment co-design benchmarks including\nwarehouse automation, multi-agent pathfinding and wind farm optimisation. Our\nmethod consistently exceeds the state-of-the-art, achieving, for example, 39%\nhigher rewards in the warehouse setting with 66% fewer simulation samples. This\nsets a new standard in agent-environment co-design, and is a stepping stone\ntowards reaping the rewards of co-design in real world domains.", "AI": {"tldr": "DiCoDe is a scalable co-design framework that uses diffusion models and critic distillation to jointly optimize agent policies and environment configurations, achieving state-of-the-art performance with significantly fewer samples.", "motivation": "Current co-design methods struggle with high-dimensional design spaces and sample inefficiency when dealing with the moving targets of joint optimization in multi-agent systems.", "method": "DiCoDe incorporates Projected Universal Guidance (PUG) for constraint-satisfying environment exploration and critic distillation to share knowledge from RL critics, enabling adaptation to evolving agent policies.", "result": "Achieves 39% higher rewards in warehouse settings with 66% fewer simulation samples, consistently exceeding state-of-the-art across multi-agent benchmarks including warehouse automation, pathfinding, and wind farm optimization.", "conclusion": "DiCoDe sets a new standard in agent-environment co-design and represents a stepping stone toward practical real-world applications of co-design principles."}}
{"id": "2511.02879", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02879", "abs": "https://arxiv.org/abs/2511.02879", "authors": ["Junhyung Park", "Hyungjin Kim", "Seokho Ahn", "Young-Duk Seo"], "title": "Stochastic Deep Graph Clustering for Practical Group Formation", "comment": null, "summary": "While prior work on group recommender systems (GRSs) has primarily focused on\nimproving recommendation accuracy, most approaches assume static or predefined\ngroups, making them unsuitable for dynamic, real-world scenarios. We reframe\ngroup formation as a core challenge in GRSs and propose DeepForm (Stochastic\nDeep Graph Clustering for Practical Group Formation), a framework designed to\nmeet three key operational requirements: (1) the incorporation of high-order\nuser information, (2) real-time group formation, and (3) dynamic adjustment of\nthe number of groups. DeepForm employs a lightweight GCN architecture that\neffectively captures high-order structural signals. Stochastic cluster learning\nenables adaptive group reconfiguration without retraining, while contrastive\nlearning refines groups under dynamic conditions. Experiments on multiple\ndatasets demonstrate that DeepForm achieves superior group formation quality,\nefficiency, and recommendation accuracy compared with various baselines.", "AI": {"tldr": "DeepForm is a stochastic deep graph clustering framework for dynamic group recommendation that handles real-time group formation with adaptive group numbers using efficient GCN architecture.", "motivation": "Traditional group recommender systems assume static groups and focus only on accuracy, making them unsuitable for dynamic real-world scenarios where group formation itself is a core challenge.", "method": "Uses lightweight GCN to capture high-order user information, stochastic cluster learning for adaptive group reconfiguration without retraining, and contrastive learning for dynamic refinement.", "result": "Experiments show superior group formation quality, efficiency, and recommendation accuracy across multiple datasets compared to baselines.", "conclusion": "DeepForm effectively addresses dynamic group formation challenges in recommender systems while maintaining high performance and adaptability."}}
{"id": "2511.03106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03106", "abs": "https://arxiv.org/abs/2511.03106", "authors": ["Katherine C. Kellogg", "Bingyang Ye", "Yifan Hu", "Guergana K. Savova", "Byron Wallace", "Danielle S. Bitterman"], "title": "Large language models require a new form of oversight: capability-based monitoring", "comment": "Under review", "summary": "The rapid adoption of large language models (LLMs) in healthcare has been\naccompanied by scrutiny of their oversight. Existing monitoring approaches,\ninherited from traditional machine learning (ML), are task-based and founded on\nassumed performance degradation arising from dataset drift. In contrast, with\nLLMs, inevitable model degradation due to changes in populations compared to\nthe training dataset cannot be assumed, because LLMs were not trained for any\nspecific task in any given population. We therefore propose a new organizing\nprinciple guiding generalist LLM monitoring that is scalable and grounded in\nhow these models are developed and used in practice: capability-based\nmonitoring. Capability-based monitoring is motivated by the fact that LLMs are\ngeneralist systems whose overlapping internal capabilities are reused across\nnumerous downstream tasks. Instead of evaluating each downstream task\nindependently, this approach organizes monitoring around shared model\ncapabilities, such as summarization, reasoning, translation, or safety\nguardrails, in order to enable cross-task detection of systemic weaknesses,\nlong-tail errors, and emergent behaviors that task-based monitoring may miss.\nWe describe considerations for developers, organizational leaders, and\nprofessional societies for implementing a capability-based monitoring approach.\nUltimately, capability-based monitoring will provide a scalable foundation for\nsafe, adaptive, and collaborative monitoring of LLMs and future generalist\nartificial intelligence models in healthcare.", "AI": {"tldr": "Proposes capability-based monitoring for LLMs in healthcare, shifting from task-based to capability-focused evaluation to detect systemic weaknesses and emergent behaviors across multiple tasks.", "motivation": "Traditional ML monitoring approaches are task-based and assume performance degradation from dataset drift, but LLMs are generalist systems not trained for specific tasks, requiring a new monitoring paradigm.", "method": "Organizes monitoring around shared model capabilities (summarization, reasoning, translation, safety guardrails) rather than individual downstream tasks, enabling cross-task detection of systemic issues.", "result": "A scalable framework for detecting systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss.", "conclusion": "Capability-based monitoring provides a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist AI models in healthcare."}}
{"id": "2511.03179", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03179", "abs": "https://arxiv.org/abs/2511.03179", "authors": ["Varun Kumar", "George Em Karniadakis"], "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework", "comment": null, "summary": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process.", "AI": {"tldr": "A multi-agent AI framework with specialized agents (Graph Ontologist, Design Engineer, Systems Engineer) collaborates to optimize NACA airfoil designs through iterative design-review loops using domain knowledge graphs.", "motivation": "Traditional engineering design processes are resource-intensive and inefficient due to complex multi-domain collaborations and iterative refinements.", "method": "Three-agent framework: Graph Ontologist builds domain knowledge graphs using LLMs, Systems Engineer formulates requirements, Design Engineer generates candidates, and iterative review loops with quantitative/qualitative feedback until validation.", "result": "Successfully applied to aerodynamic optimization of 4-digit NACA airfoils, demonstrating enhanced efficiency, consistency, and quality in engineering design.", "conclusion": "Collaborative AI agents with structured knowledge representations can significantly improve engineering design processes by enhancing efficiency, consistency, and quality."}}
{"id": "2511.02886", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02886", "abs": "https://arxiv.org/abs/2511.02886", "authors": ["Ronan Killian McGovern"], "title": "Test-time Adaptation of Tiny Recursive Models", "comment": null, "summary": "Prior to the close of the 2025 ARC Prize competition, the leading open source\napproach - known as TRM, or Tiny Recursive Models - involved training a 7M\nparameter recursive neural network on augmented variants of ARC tasks. That\napproach scored approximately 7.8% on the public ARC AGI II evaluation set, but\nrequired a level of compute far in excess of what is allowed during the\ncompetition. This paper shows that, by starting from a tiny recursive model\nthat has been pre-trained on public ARC tasks, one can efficiently fine-tune on\ncompetition tasks within the allowed compute limits. Specifically, a model was\npre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on\n4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model\nwas then post-trained in just 12,500 gradient steps during the competition to\nreach a score of 6.67% on semi-private evaluation tasks. Notably, such\npost-training performance is achieved by full-fine tuning of the tiny model,\nnot LoRA fine-tuning or fine-tuning of task embeddings alone.", "AI": {"tldr": "A 7M parameter recursive neural network pre-trained on public ARC tasks achieves 6.67% on semi-private evaluation after efficient fine-tuning within competition compute limits.", "motivation": "Previous open-source approaches like TRM scored well on ARC tasks but exceeded competition compute limits, requiring a more efficient method.", "method": "Pre-trained a tiny recursive model on 1,280 public tasks for 700k+ steps over 48 hours, then full fine-tuned it in just 12,500 gradient steps during competition.", "result": "Achieved 6.67% score on semi-private evaluation tasks, demonstrating efficient adaptation within allowed compute constraints.", "conclusion": "Starting from pre-trained tiny recursive models enables effective fine-tuning on competition tasks while staying within compute limits, outperforming previous approaches in efficiency."}}
{"id": "2511.03108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03108", "abs": "https://arxiv.org/abs/2511.03108", "authors": ["Azim Ospanov", "Farzan Farnia", "Roozbeh Yousefzadeh"], "title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward", "comment": null, "summary": "We perform a thorough analysis of the formal and informal statements in the\nminiF2F benchmark from the perspective of an AI system that is tasked to\nparticipate in a math Olympiad consisting of the problems in miniF2F. In such\nsetting, the model has to read and comprehend the problems in natural language,\nformalize them in Lean language, then proceed with proving the problems, and it\nwill get credit for each problem if the formal proof corresponds to the\noriginal informal statement presented to the model. Our evaluation results\nreveal that the best accuracy of such pipeline can be about 36% using the SoTA\nmodels in the literature, considerably lower than the individual SoTA\naccuracies, 97% and 69% reported in the autoformalization and theorem proving\nliterature. Analyzing the failure modes, we trace back a considerable portion\nof this drop to discrepancies between the formal and informal statements for\nmore than half of the problems in miniF2F. We proceed with correcting all the\nerrors, discrepancies and simplifications in formal and informal statements,\nand present the miniF2F-v2 with fully verified formal and informal statements\nand proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to\nthe best accuracy of 70%, a significant improvement from the 40% on the\noriginal miniF2F, yet indicating considerable misalignment between the\nautoformalization models and theorem provers. Our deep analysis suggests that a\nhigher quality benchmark can help the community better evaluate progress in the\nfield of formal reasoning and also better diagnose the failure and success\nmodes of autoformalization and theorem proving models. Our dataset is available\nat https://github.com/roozbeh-yz/miniF2F_v2.", "AI": {"tldr": "Analysis of formal/informal statement discrepancies in miniF2F benchmark reveals alignment issues between autoformalization and theorem proving models, leading to creation of improved miniF2F-v2 with 70% accuracy.", "motivation": "To analyze challenges in AI formal reasoning pipeline where models must understand natural language math problems, formalize them in Lean, and prove them, particularly focusing on discrepancies between formal and informal statements.", "method": "Thorough comparison of formal and informal statements in miniF2F benchmark, identification of discrepancies, corrections of errors, and evaluation using state-of-the-art autoformalization and theorem proving models.", "result": "Found significant statement discrepancies affecting more than half of miniF2F problems; after corrections in miniF2F-v2, pipeline accuracy improved from 40% to 70%, though still showing model misalignment.", "conclusion": "Higher quality benchmarks like miniF2F-v2 are crucial for proper evaluation of formal reasoning systems and better diagnosis of autoformalization/theorem proving model failures."}}
{"id": "2511.03724", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03724", "abs": "https://arxiv.org/abs/2511.03724", "authors": ["Richard Dewey", "Janos Botyanszki", "Ciamac C. Moallemi", "Andrew T. Zheng"], "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning", "comment": null, "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.", "AI": {"tldr": "Solly is the first AI to achieve elite human-level play in multi-player Liar's Poker, outperforming both humans and LLMs through self-play reinforcement learning.", "motivation": "Previous AI breakthroughs in poker-like games focused on two-player scenarios with subdued multi-player dynamics, while Liar's Poker features extensive multi-player engagement that presents a more challenging testbed.", "method": "Used self-play with a model-free, actor-critic, deep reinforcement learning algorithm to train the Solly agent.", "result": "Solly achieved elite human level performance (won over 50% of hands and positive equity) in both heads-up and multi-player Liar's Poker, outperformed LLMs, developed novel strategies, effectively randomized play, and was not easily exploitable by world-class players.", "conclusion": "The success of Solly demonstrates that deep reinforcement learning can achieve elite performance in complex multi-player games with extensive engagement, advancing beyond previous poker AI achievements."}}
{"id": "2511.02887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02887", "abs": "https://arxiv.org/abs/2511.02887", "authors": ["Chaitanya Rele", "Aditya Rathod", "Kaustubh Natu", "Saurabh Kulkarni", "Ajay Koli", "Swapnali Makdey"], "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets", "comment": null, "summary": "The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,\nrepresents a vital source of livelihood for coastal communities, yet fishermen\noften face uncertainty in locating productive fishing grounds. To address this\nchallenge, we present an AI-assisted framework for predicting Potential Fishing\nZones (PFZs) using oceanographic parameters such as sea surface temperature and\nchlorophyll concentration. The approach is designed to enhance the accuracy of\nPFZ identification and provide region-specific insights for sustainable fishing\npractices. Preliminary results indicate that the framework can support\nfishermen by reducing search time, lowering fuel consumption, and promoting\nefficient resource utilization.", "AI": {"tldr": "AI framework predicts fishing zones in the North Indian Ocean using ocean data to help fishermen save time and fuel.", "motivation": "Fishermen face uncertainty in locating productive fishing grounds, impacting their livelihood and resource efficiency.", "method": "Uses AI to analyze oceanographic parameters like sea surface temperature and chlorophyll concentration to identify Potential Fishing Zones.", "result": "Preliminary results show reduced search time, lower fuel consumption, and improved resource utilization for fishermen.", "conclusion": "The AI-assisted framework enhances sustainable fishing by providing accurate, region-specific PFZ predictions."}}
{"id": "2511.03137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03137", "abs": "https://arxiv.org/abs/2511.03137", "authors": ["Shipeng Cen", "Ying Tan"], "title": "Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks", "comment": null, "summary": "As optimization problems grow increasingly complex and diverse, advancements\nin optimization techniques and paradigm innovations hold significant\nimportance. The challenges posed by optimization problems are primarily\nmanifested in their non-convexity, high-dimensionality, black-box nature, and\nother unfavorable characteristics. Traditional zero-order or first-order\nmethods, which are often characterized by low efficiency, inaccurate gradient\ninformation, and insufficient utilization of optimization information, are\nill-equipped to address these challenges effectively. In recent years, the\nrapid development of large language models (LLM) has led to substantial\nimprovements in their language understanding and code generation capabilities.\nConsequently, the design of optimization algorithms leveraging large language\nmodels has garnered increasing attention from researchers. In this study, we\nchoose the fireworks algorithm(FWA) as the basic optimizer and propose a novel\napproach to assist the design of the FWA by incorporating multi-modal large\nlanguage model(MLLM). To put it simply, we propose the concept of Critical\nPart(CP), which extends FWA to complex high-dimensional tasks, and further\nutilizes the information in the optimization process with the help of the\nmulti-modal characteristics of large language models. We focus on two specific\ntasks: the \\textit{traveling salesman problem }(TSP) and \\textit{electronic\ndesign automation problem} (EDA). The experimental results show that FWAs\ngenerated under our new framework have achieved or surpassed SOTA results on\nmany problem instances.", "AI": {"tldr": "A novel framework that uses multi-modal large language models (MLLM) to enhance the fireworks algorithm (FWA) for solving complex optimization problems like TSP and EDA, achieving state-of-the-art results.", "motivation": "Traditional optimization methods struggle with non-convex, high-dimensional, black-box problems due to low efficiency and inaccurate gradient information. The advancement in LLMs' language understanding and code generation capabilities offers new opportunities for optimization algorithm design.", "method": "Proposed Critical Part (CP) concept to extend FWA for complex high-dimensional tasks, leveraging MLLMs' multi-modal characteristics to utilize optimization process information more effectively.", "result": "Experimental results show that FWAs generated under the new framework achieved or surpassed state-of-the-art results on many problem instances in traveling salesman problem (TSP) and electronic design automation (EDA) tasks.", "conclusion": "The integration of multi-modal large language models with optimization algorithms like FWA provides an effective approach to address complex optimization challenges, demonstrating significant performance improvements over traditional methods."}}
{"id": "2511.02894", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02894", "abs": "https://arxiv.org/abs/2511.02894", "authors": ["W. K. M Mithsara", "Ning Yang", "Ahmed Imteaj", "Hussein Zangoti", "Abdur R. Shahid"], "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models", "comment": null, "summary": "The widespread integration of wearable sensing devices in Internet of Things\n(IoT) ecosystems, particularly in healthcare, smart homes, and industrial\napplications, has required robust human activity recognition (HAR) techniques\nto improve functionality and user experience. Although machine learning models\nhave advanced HAR, they are increasingly susceptible to data poisoning attacks\nthat compromise the data integrity and reliability of these systems.\nConventional approaches to defending against such attacks often require\nextensive task-specific training with large, labeled datasets, which limits\nadaptability in dynamic IoT environments. This work proposes a novel framework\nthat uses large language models (LLMs) to perform poisoning detection and\nsanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot\nlearning paradigms. Our approach incorporates \\textit{role play} prompting,\nwhereby the LLM assumes the role of expert to contextualize and evaluate sensor\nanomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer\npoisoning indicators in the raw sensor data and plausible clean alternatives.\nThese strategies minimize reliance on curation of extensive datasets and enable\nrobust, adaptable defense mechanisms in real-time. We perform an extensive\nevaluation of the framework, quantifying detection accuracy, sanitization\nquality, latency, and communication cost, thus demonstrating the practicality\nand effectiveness of LLMs in improving the security and reliability of wearable\nIoT systems.", "AI": {"tldr": "A framework using large language models for detecting and cleaning data poisoning attacks in human activity recognition systems, enabling zero-shot to few-shot learning with role-play and step-by-step reasoning.", "motivation": "Wearable IoT systems need robust HAR but machine learning models are vulnerable to data poisoning attacks, while conventional defenses require extensive labeled data and lack adaptability.", "method": "Uses LLMs with role-play prompting (acting as expert) and think step-by-step reasoning to detect anomalies and generate clean sensor data alternatives in zero-shot, one-shot, and few-shot learning paradigms.", "result": "Evaluation shows effectiveness in detection accuracy, sanitization quality, latency, and communication cost, demonstrating practical security improvement.", "conclusion": "LLM-based framework provides adaptable, real-time defense against data poisoning in wearable IoT HAR systems, reducing reliance on large datasets."}}
{"id": "2511.03138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03138", "abs": "https://arxiv.org/abs/2511.03138", "authors": ["Qi Li", "Jianjun Xu", "Pingtao Wei", "Jiu Li", "Peiqiang Zhao", "Jiwei Shi", "Xuan Zhang", "Yanhui Yang", "Xiaodong Hui", "Peng Xu", "Wenqin Shao"], "title": "A Proprietary Model-Based Safety Response Framework for AI Agents", "comment": null, "summary": "With the widespread application of Large Language Models (LLMs), their\nassociated security issues have become increasingly prominent, severely\nconstraining their trustworthy deployment in critical domains. This paper\nproposes a novel safety response framework designed to systematically safeguard\nLLMs at both the input and output levels. At the input level, the framework\nemploys a supervised fine-tuning-based safety classification model. Through a\nfine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused\nAttention), it performs precise risk identification and differentiated handling\nof user queries, significantly enhancing risk coverage and business scenario\nadaptability, and achieving a risk recall rate of 99.3%. At the output level,\nthe framework integrates Retrieval-Augmented Generation (RAG) with a\nspecifically fine-tuned interpretation model, ensuring all responses are\ngrounded in a real-time, trustworthy knowledge base. This approach eliminates\ninformation fabrication and enables result traceability. Experimental results\ndemonstrate that our proposed safety control model achieves a significantly\nhigher safety score on public safety evaluation benchmarks compared to the\nbaseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk\ntest set, the framework's components attained a perfect 100% safety score,\nvalidating their exceptional protective capabilities in complex risk scenarios.\nThis research provides an effective engineering pathway for building\nhigh-security, high-trust LLM applications.", "AI": {"tldr": "A novel safety response framework for LLMs that uses input-level safety classification and output-level RAG with interpretation models to ensure secure responses and eliminate information fabrication.", "motivation": "Security issues in LLMs are limiting their trustworthy deployment in critical domains, requiring systematic safeguards at both input and output levels.", "method": "Input: supervised fine-tuning-based safety classification with 4-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention). Output: RAG integrated with fine-tuned interpretation model for real-time trustworthy knowledge grounding.", "result": "Achieved 99.3% risk recall rate, significantly higher safety scores on public benchmarks than baseline, and perfect 100% safety score on proprietary high-risk test set.", "conclusion": "The framework provides an effective engineering pathway for building high-security, high-trust LLM applications with exceptional protective capabilities in complex risk scenarios."}}
{"id": "2511.02936", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02936", "abs": "https://arxiv.org/abs/2511.02936", "authors": ["Neil Byers", "Ali Zaidi", "Valerie Skye", "Chris Beecroft", "Kjiersten Fagnan"], "title": "Zero-shot data citation function classification using transformer-based large language models (LLMs)", "comment": null, "summary": "Efforts have increased in recent years to identify associations between\nspecific datasets and the scientific literature that incorporates them. Knowing\nthat a given publication cites a given dataset, the next logical step is to\nexplore how or why that data was used. Advances in recent years with\npretrained, transformer-based large language models (LLMs) offer potential\nmeans for scaling the description of data use cases in the published\nliterature. This avoids expensive manual labeling and the development of\ntraining datasets for classical machine-learning (ML) systems. In this work we\napply an open-source LLM, Llama 3.1-405B, to generate structured data use case\nlabels for publications known to incorporate specific genomic datasets. We also\nintroduce a novel evaluation framework for determining the efficacy of our\nmethods. Our results demonstrate that the stock model can achieve an F1 score\nof .674 on a zero-shot data citation classification task with no previously\ndefined categories. While promising, our results are qualified by barriers\nrelated to data availability, prompt overfitting, computational infrastructure,\nand the expense required to conduct responsible performance evaluation.", "AI": {"tldr": "Using Llama 3.1-405B LLM to automatically generate structured data use case labels for genomic dataset citations in scientific literature, achieving promising F1 score of .674 in zero-shot classification.", "motivation": "Need to scale analysis of how specific datasets are used in scientific literature beyond just identifying citations, avoiding expensive manual labeling and training data development.", "method": "Applied open-source Llama 3.1-405B LLM for zero-shot classification of data use cases in publications citing genomic datasets, with novel evaluation framework.", "result": "Achieved F1 score of .674 on zero-shot data citation classification task without predefined categories, showing promise for automated analysis.", "conclusion": "While results are promising, barriers exist including data availability, prompt overfitting, computational infrastructure requirements, and evaluation costs that qualify the findings."}}
{"id": "2511.03169", "categories": ["cs.AI", "D.2.4; I.2.6; I.2.4; K.4.1; I.2.0"], "pdf": "https://arxiv.org/pdf/2511.03169", "abs": "https://arxiv.org/abs/2511.03169", "authors": ["Xuanxiang Huang", "Yacine Izza", "Alexey Ignatiev", "Joao Marques-Silva"], "title": "Uncovering Bugs in Formal Explainers: A Case Study with PyXAI", "comment": null, "summary": "Formal explainable artificial intelligence (XAI) offers unique theoretical\nguarantees of rigor when compared to other non-formal methods of\nexplainability. However, little attention has been given to the validation of\npractical implementations of formal explainers. This paper develops a novel\nmethodology for validating formal explainers and reports on the assessment of\nthe publicly available formal explainer PyXAI. The paper documents the\nexistence of incorrect explanations computed by PyXAI on most of the datasets\nanalyzed in the experiments, thereby confirming the importance of the proposed\nnovel methodology for the validation of formal explainers.", "AI": {"tldr": "Proposes a novel validation methodology for formal XAI explainers and identifies incorrect explanations in PyXAI across multiple datasets.", "motivation": "Formal XAI methods provide theoretical rigor but lack practical validation of their implementations, with little attention given to verifying real-world explainer tools.", "method": "Developed a novel validation methodology for formal explainers and applied it to assess PyXAI, a publicly available formal explainer.", "result": "Found incorrect explanations computed by PyXAI on most datasets analyzed, confirming the need for rigorous validation of formal explainer implementations.", "conclusion": "The proposed validation methodology is crucial for ensuring the reliability of formal XAI explainers, as demonstrated by the identification of flaws in PyXAI."}}
{"id": "2511.02944", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02944", "abs": "https://arxiv.org/abs/2511.02944", "authors": ["Fengxu Li", "Stephanie M. Carpenter", "Matthew P. Buman", "Yonatan Mintz"], "title": "Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics", "comment": null, "summary": "A common challenge for decision makers is selecting actions whose rewards are\nunknown and evolve over time based on prior policies. For instance, repeated\nuse may reduce an action's effectiveness (habituation), while inactivity may\nrestore it (recovery). These nonstationarities are captured by the Reducing or\nGaining Unknown Efficacy (ROGUE) bandit framework, which models real-world\nsettings such as behavioral health interventions. While existing algorithms can\ncompute sublinear regret policies to optimize these settings, they may not\nprovide sufficient exploration due to overemphasis on exploitation, limiting\nthe ability to estimate population-level effects. This is a challenge of\nparticular interest in micro-randomized trials (MRTs) that aid researchers in\ndeveloping just-in-time adaptive interventions that have population-level\neffects while still providing personalized recommendations to individuals. In\nthis paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored\nto the ROGUE framework, and provide theoretical guarantees of sublinear regret.\nWe then introduce a probability clipping procedure to balance personalization\nand population-level learning, with quantified trade-off that balances regret\nand minimum exploration probability. Validation on two MRT datasets concerning\nphysical activity promotion and bipolar disorder treatment shows that our\nmethods both achieve lower regret than existing approaches and maintain high\nstatistical power through the clipping procedure without significantly\nincreasing regret. This enables reliable detection of treatment effects while\naccounting for individual behavioral dynamics. For researchers designing MRTs,\nour framework offers practical guidance on balancing personalization with\nstatistical validity.", "AI": {"tldr": "Developed ROGUE-TS, a Thompson Sampling algorithm for nonstationary bandit problems with habituation and recovery effects, plus a probability clipping method to balance personalization and population-level learning in micro-randomized trials.", "motivation": "Existing bandit algorithms for nonstationary environments focus too much on exploitation, limiting exploration needed to estimate population-level treatment effects in micro-randomized trials for behavioral health interventions.", "method": "Created ROGUE-TS (Thompson Sampling for ROGUE framework) with theoretical guarantees, and introduced probability clipping to balance regret minimization with minimum exploration probability.", "result": "Validated on two MRT datasets (physical activity and bipolar disorder), achieving lower regret than existing approaches while maintaining high statistical power through clipping without significant regret increase.", "conclusion": "The framework provides practical guidance for MRT designers to balance personalization with statistical validity, enabling reliable treatment effect detection while accounting for individual behavioral dynamics."}}
{"id": "2511.02957", "categories": ["cs.LG", "cs.CE", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02957", "abs": "https://arxiv.org/abs/2511.02957", "authors": ["Mohsin Mahmud Topu", "Mahfuz Ahmed Anik", "Azmine Toushik Wasi", "Md Manjurul Ahsan"], "title": "Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks", "comment": null, "summary": "Pavement infrastructure monitoring is challenged by complex spatial\ndependencies, changing environmental conditions, and non-linear deterioration\nacross road networks. Traditional Pavement Management Systems (PMS) remain\nlargely reactive, lacking real-time intelligence for failure prevention and\noptimal maintenance planning. To address this, we propose a unified Digital\nTwin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven\npavement health monitoring and predictive maintenance. Pavement segments and\nspatial relations are modeled as graph nodes and edges, while real-time UAV,\nsensor, and LiDAR data stream into the DT. The inductive GNN learns\ndeterioration patterns from graph-structured inputs to forecast distress and\nenable proactive interventions. Trained on a real-world-inspired dataset with\nsegment attributes and dynamic connectivity, our model achieves an R2 of\n0.3798, outperforming baseline regressors and effectively capturing non-linear\ndegradation. We also develop an interactive dashboard and reinforcement\nlearning module for simulation, visualization, and adaptive maintenance\nplanning. This DT-GNN integration enhances forecasting precision and\nestablishes a closed feedback loop for continuous improvement, positioning the\napproach as a foundation for proactive, intelligent, and sustainable pavement\nmanagement, with future extensions toward real-world deployment, multi-agent\ncoordination, and smart-city integration.", "AI": {"tldr": "A Digital Twin-Graph Neural Network framework for proactive pavement management that models road networks as graphs and uses real-time data to predict deterioration and optimize maintenance.", "motivation": "Traditional Pavement Management Systems are reactive and lack real-time intelligence for failure prevention and optimal maintenance planning in complex road networks.", "method": "Model pavement segments as graph nodes and spatial relations as edges, stream real-time UAV, sensor, and LiDAR data into Digital Twin, and use inductive Graph Neural Networks to learn deterioration patterns from graph-structured inputs.", "result": "Achieved R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation patterns. Developed interactive dashboard and reinforcement learning module for simulation and adaptive maintenance planning.", "conclusion": "The DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, providing a foundation for proactive, intelligent pavement management with potential for real-world deployment and smart-city integration."}}
{"id": "2511.03186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03186", "abs": "https://arxiv.org/abs/2511.03186", "authors": ["Yiru Chen", "Sally Fang", "Sai Sree Harsha", "Dan Luo", "Vaishnavi Muppala", "Fei Wu", "Shun Jiang", "Kun Qian", "Yunyao Li"], "title": "Adobe Summit Concierge Evaluation with Human in the Loop", "comment": "Accepted by 6th Workshop on Data Science with Human in the Loop @\n  VLDB 2025", "summary": "Generative AI assistants offer significant potential to enhance productivity,\nstreamline information access, and improve user experience in enterprise\ncontexts. In this work, we present Summit Concierge, a domain-specific AI\nassistant developed for Adobe Summit. The assistant handles a wide range of\nevent-related queries and operates under real-world constraints such as data\nsparsity, quality assurance, and rapid deployment. To address these challenges,\nwe adopt a human-in-the-loop development workflow that combines prompt\nengineering, retrieval grounding, and lightweight human validation. We describe\nthe system architecture, development process, and real-world deployment\noutcomes. Our experience shows that agile, feedback-driven development enables\nscalable and reliable AI assistants, even in cold-start scenarios.", "AI": {"tldr": "Domain-specific AI assistant for Adobe Summit that handles event queries using human-in-the-loop development to overcome data sparsity and deployment challenges.", "motivation": "Enhance productivity, streamline information access, and improve user experience in enterprise contexts through generative AI assistants.", "method": "Human-in-the-loop workflow combining prompt engineering, retrieval grounding, and lightweight human validation to address data sparsity, quality assurance, and rapid deployment constraints.", "result": "Successful real-world deployment of Summit Concierge assistant capable of handling wide range of event-related queries under challenging constraints.", "conclusion": "Agile, feedback-driven development enables scalable and reliable AI assistants even in cold-start scenarios, demonstrating practical viability of the approach."}}
{"id": "2511.02966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02966", "abs": "https://arxiv.org/abs/2511.02966", "authors": ["Victor-Alexandru P\u0103durean", "Parameswaran Kamalaruban", "Nachiket Kotalwar", "Alkis Gotovos", "Adish Singla"], "title": "Inference-Time Personalized Alignment with a Few User Preference Queries", "comment": "NeurIPS'25 paper", "summary": "We study the problem of aligning a generative model's response with a user's\npreferences. Recent works have proposed several different formulations for\npersonalized alignment; however, they either require a large amount of user\npreference queries or require that the preference be explicitly specified as a\ntext input. In this paper, we propose a novel inference-time personalized\nalignment method, UserAlign, that elicits the user's preferences with a few\nqueries as pairwise response comparisons. In particular, UserAlign builds on\nthe theoretical framework of best-arm identification in logistic bandits and\nselects a personalized response from a fixed pool of the model's generated\nresponses. The key idea is to consider the user's feedback consistent and\nnoise-free, and incorporate it into the theoretical framework to identify the\nbest response quickly. Experimental results across several tasks, involving\npersonalized text and image generation, showcase the effectiveness of UserAlign\nin achieving personalized alignment.", "AI": {"tldr": "UserAlign uses pairwise response comparisons and logistic bandit theory for efficient personalized alignment of AI-generated content with minimal user queries.", "motivation": "Existing personalized alignment methods require many user preference queries or explicit text specifications, which are impractical or inefficient.", "method": "Proposes UserAlign, an inference-time method based on best-arm identification in logistic bandits, using few pairwise comparisons to select optimal responses from a fixed pool.", "result": "Experimental results demonstrate UserAlign's effectiveness in personalized text and image generation tasks.", "conclusion": "Personalized alignment can be achieved efficiently with minimal user interaction using the UserAlign framework."}}
{"id": "2511.03235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03235", "abs": "https://arxiv.org/abs/2511.03235", "authors": ["Yi-Fei Liu", "Yi-Long Lu", "Di He", "Hang Zhang"], "title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers", "comment": null, "summary": "Psychological constructs within individuals are widely believed to be\ninterconnected. We investigated whether and how Large Language Models (LLMs)\ncan model the correlational structure of human psychological traits from\nminimal quantitative inputs. We prompted various LLMs with Big Five Personality\nScale responses from 816 human individuals to role-play their responses on nine\nother psychological scales. LLMs demonstrated remarkable accuracy in capturing\nhuman psychological structure, with the inter-scale correlation patterns from\nLLM-generated responses strongly aligning with those from human data $(R^2 >\n0.89)$. This zero-shot performance substantially exceeded predictions based on\nsemantic similarity and approached the accuracy of machine learning algorithms\ntrained directly on the dataset. Analysis of reasoning traces revealed that\nLLMs use a systematic two-stage process: First, they transform raw Big Five\nresponses into natural language personality summaries through information\nselection and compression, analogous to generating sufficient statistics.\nSecond, they generate target scale responses based on reasoning from these\nsummaries. For information selection, LLMs identify the same key personality\nfactors as trained algorithms, though they fail to differentiate item\nimportance within factors. The resulting compressed summaries are not merely\nredundant representations but capture synergistic information--adding them to\noriginal scores enhances prediction alignment, suggesting they encode emergent,\nsecond-order patterns of trait interplay. Our findings demonstrate that LLMs\ncan precisely predict individual participants' psychological traits from\nminimal data through a process of abstraction and reasoning, offering both a\npowerful tool for psychological simulation and valuable insights into their\nemergent reasoning capabilities.", "AI": {"tldr": "LLMs can accurately model human psychological trait correlations by role-playing based on Big Five Personality Scale responses, achieving near-human-level accuracy through a two-stage process of personality summarization and reasoning.", "motivation": "To investigate whether LLMs can model the correlational structure of human psychological traits from minimal quantitative inputs and understand their reasoning process.", "method": "Prompted various LLMs with Big Five Personality Scale responses from 816 individuals to role-play responses on nine other psychological scales, analyzed reasoning traces and information selection patterns.", "result": "LLMs achieved remarkable accuracy (R\u00b2 > 0.89) in capturing human psychological structure, exceeding semantic similarity predictions and approaching trained ML algorithm performance. They use a systematic two-stage process: personality summarization followed by reasoning.", "conclusion": "LLMs can precisely predict individual psychological traits through abstraction and reasoning, offering powerful tools for psychological simulation and insights into emergent reasoning capabilities."}}
{"id": "2511.02969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02969", "abs": "https://arxiv.org/abs/2511.02969", "authors": ["Stergios Plataniotis", "Charilaos Akasiadis", "Georgios Chalkiadakis"], "title": "Value of Information-Enhanced Exploration in Bootstrapped DQN", "comment": null, "summary": "Efficient exploration in deep reinforcement learning remains a fundamental\nchallenge, especially in environments characterized by high-dimensional states\nand sparse rewards. Traditional exploration strategies that rely on random\nlocal policy noise, such as $\\epsilon$-greedy and Boltzmann exploration\nmethods, often struggle to efficiently balance exploration and exploitation. In\nthis paper, we integrate the notion of (expected) value of information (EVOI)\nwithin the well-known Bootstrapped DQN algorithmic framework, to enhance the\nalgorithm's deep exploration ability. Specifically, we develop two novel\nalgorithms that incorporate the expected gain from learning the value of\ninformation into Bootstrapped DQN. Our methods use value of information\nestimates to measure the discrepancies of opinions among distinct network\nheads, and drive exploration towards areas with the most potential. We evaluate\nour algorithms with respect to performance and their ability to exploit\ninherent uncertainty arising from random network initialization. Our\nexperiments in complex, sparse-reward Atari games demonstrate increased\nperformance, all the while making better use of uncertainty, and, importantly,\nwithout introducing extra hyperparameters.", "AI": {"tldr": "Integrating expected value of information (EVOI) into Bootstrapped DQN to enhance deep exploration in sparse-reward environments without extra hyperparameters.", "motivation": "Traditional exploration methods like \u03b5-greedy and Boltzmann struggle with efficient exploration-exploitation balance in high-dimensional, sparse-reward environments.", "method": "Developed two novel algorithms that incorporate EVOI into Bootstrapped DQN, using value of information estimates to measure network head discrepancies and drive exploration to high-potential areas.", "result": "Experiments in complex, sparse-reward Atari games show increased performance, better uncertainty utilization, and no additional hyperparameters needed.", "conclusion": "EVOI integration significantly improves Bootstrapped DQN's deep exploration capability in challenging sparse-reward environments while maintaining simplicity."}}
{"id": "2511.03471", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03471", "abs": "https://arxiv.org/abs/2511.03471", "authors": ["Ming Gu", "Ziwei Wang", "Sicen Lai", "Zirui Gao", "Sheng Zhou", "Jiajun Bu"], "title": "Towards Scalable Web Accessibility Audit with MLLMs as Copilots", "comment": "15 pages. Accepted by AAAI 2026 AISI", "summary": "Ensuring web accessibility is crucial for advancing social welfare, justice,\nand equality in digital spaces, yet the vast majority of website user\ninterfaces remain non-compliant, due in part to the resource-intensive and\nunscalable nature of current auditing practices. While WCAG-EM offers a\nstructured methodology for site-wise conformance evaluation, it involves great\nhuman efforts and lacks practical support for execution at scale. In this work,\nwe present an auditing framework, AAA, which operationalizes WCAG-EM through a\nhuman-AI partnership model. AAA is anchored by two key innovations: GRASP, a\ngraph-based multimodal sampling method that ensures representative page\ncoverage via learned embeddings of visual, textual, and relational cues; and\nMaC, a multimodal large language model-based copilot that supports auditors\nthrough cross-modal reasoning and intelligent assistance in high-effort tasks.\nTogether, these components enable scalable, end-to-end web accessibility\nauditing, empowering human auditors with AI-enhanced assistance for real-world\nimpact. We further contribute four novel datasets designed for benchmarking\ncore stages of the audit pipeline. Extensive experiments demonstrate the\neffectiveness of our methods, providing insights that small-scale language\nmodels can serve as capable experts when fine-tuned.", "AI": {"tldr": "The paper presents AAA, a scalable web accessibility auditing framework that combines GRASP (graph-based sampling) and MaC (multimodal AI copilot) to operationalize WCAG-EM evaluations with reduced human effort.", "motivation": "Current web accessibility auditing methods like WCAG-EM are resource-intensive and not scalable, leaving most websites non-compliant despite the importance of accessibility for digital equality.", "method": "The AAA framework uses GRASP for representative page sampling via multimodal embeddings and MaC, a fine-tuned multimodal LLM, to assist auditors with cross-modal reasoning and task automation.", "result": "Experiments show the framework enables effective, scalable auditing, with insights that small-scale fine-tuned models can perform well as AI copilots in accessibility tasks.", "conclusion": "AAA demonstrates a viable human-AI partnership for scalable web accessibility auditing, supported by novel datasets and efficient AI assistance, advancing practical compliance efforts."}}
{"id": "2511.03012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03012", "abs": "https://arxiv.org/abs/2511.03012", "authors": ["Hongrui Chen", "Liwei Wang", "Levent Burak Kara"], "title": "Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation", "comment": null, "summary": "Metamaterials are engineered materials composed of specially designed unit\ncells that exhibit extraordinary properties beyond those of natural materials.\nComplex engineering tasks often require heterogeneous unit cells to accommodate\nspatially varying property requirements. However, designing heterogeneous\nmetamaterials poses significant challenges due to the enormous design space and\nstrict compatibility requirements between neighboring cells. Traditional\nconcurrent multiscale design methods require solving an expensive optimization\nproblem for each unit cell and often suffer from discontinuities at cell\nboundaries. On the other hand, data-driven approaches that assemble structures\nfrom a fixed library of microstructures are limited by the dataset and require\nadditional post-processing to ensure seamless connections. In this work, we\npropose a neural network-based metamaterial design framework that learns a\ncontinuous two-scale representation of the structure, thereby jointly\naddressing these challenges. Central to our framework is a multiscale neural\nrepresentation in which the neural network takes both global (macroscale) and\nlocal (microscale) coordinates as inputs, outputting an implicit field that\nrepresents multiscale structures with compatible unit cell geometries across\nthe domain, without the need for a predefined dataset. We use a compatibility\nloss term during training to enforce connectivity between adjacent unit cells.\nOnce trained, the network can produce metamaterial designs at arbitrarily high\nresolution, hence enabling infinite upsampling for fabrication or simulation.\nWe demonstrate the effectiveness of the proposed approach on mechanical\nmetamaterial design, negative Poisson's ratio, and mechanical cloaking problems\nwith potential applications in robotics, bioengineering, and aerospace.", "AI": {"tldr": "A neural network-based framework for designing heterogeneous metamaterials that learns continuous two-scale representations, ensuring compatibility between unit cells without predefined datasets.", "motivation": "Traditional metamaterial design methods face challenges with enormous design spaces, compatibility issues between neighboring cells, and limitations of data-driven approaches that rely on fixed microstructure libraries.", "method": "Uses a multiscale neural representation where the network takes both global and local coordinates as inputs, outputs an implicit field representing multiscale structures, and employs a compatibility loss term to enforce connectivity between adjacent unit cells.", "result": "The framework can produce metamaterial designs at arbitrarily high resolution, enabling infinite upsampling for fabrication or simulation, and demonstrates effectiveness on mechanical metamaterial design, negative Poisson's ratio, and mechanical cloaking problems.", "conclusion": "The proposed neural network-based approach successfully addresses key challenges in heterogeneous metamaterial design by providing a continuous representation that ensures compatibility across unit cells without requiring predefined datasets."}}
{"id": "2511.03545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03545", "abs": "https://arxiv.org/abs/2511.03545", "authors": ["Sebastian Ordyniak", "Giacomo Paesani", "Mateusz Rychlicki", "Stefan Szeider"], "title": "Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)", "comment": "Part I of a greatly enhanced version of\n  https://doi.org/10.24963/kr.2024/53, whose full version is available on arXiv\n  under https://doi.org/10.48550/arXiv.2407.15780", "summary": "This paper presents a comprehensive theoretical investigation into the\nparameterized complexity of explanation problems in various machine learning\n(ML) models. Contrary to the prevalent black-box perception, our study focuses\non models with transparent internal mechanisms. We address two principal types\nof explanation problems: abductive and contrastive, both in their local and\nglobal variants. Our analysis encompasses diverse ML models, including Decision\nTrees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,\neach offering unique explanatory challenges. This research fills a significant\ngap in explainable AI (XAI) by providing a foundational understanding of the\ncomplexities of generating explanations for these models. This work provides\ninsights vital for further research in the domain of XAI, contributing to the\nbroader discourse on the necessity of transparency and accountability in AI\nsystems.", "AI": {"tldr": "Theoretical analysis of parameterized complexity for explanation problems in transparent ML models, covering abductive and contrastive explanations across various model types.", "motivation": "Address the gap in explainable AI by providing foundational understanding of explanation complexities for transparent ML models, contrary to black-box perceptions.", "method": "Comprehensive theoretical investigation using parameterized complexity analysis on diverse ML models including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles.", "result": "Provides insights into the computational complexities of generating explanations for various transparent ML models.", "conclusion": "This work contributes vital insights for XAI research and advances the discourse on transparency and accountability in AI systems."}}
{"id": "2511.03015", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.03015", "abs": "https://arxiv.org/abs/2511.03015", "authors": ["Ole Petersen", "Marcel Kollovieh", "Marten Lienen", "Stephan G\u00fcnnemann"], "title": "Discrete Bayesian Sample Inference for Graph Generation", "comment": null, "summary": "Generating graph-structured data is crucial in applications such as molecular\ngeneration, knowledge graphs, and network analysis. However, their discrete,\nunordered nature makes them difficult for traditional generative models,\nleading to the rise of discrete diffusion and flow matching models. In this\nwork, we introduce GraphBSI, a novel one-shot graph generative model based on\nBayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI\niteratively refines a belief over graphs in the continuous space of\ndistribution parameters, naturally handling discrete structures. Further, we\nstate BSI as a stochastic differential equation (SDE) and derive a\nnoise-controlled family of SDEs that preserves the marginal distributions via\nan approximation of the score function. Our theoretical analysis further\nreveals the connection to Bayesian Flow Networks and Diffusion models. Finally,\nin our empirical evaluation, we demonstrate state-of-the-art performance on\nmolecular and synthetic graph generation, outperforming existing one-shot graph\ngenerative models on the standard benchmarks Moses and GuacaMol.", "AI": {"tldr": "GraphBSI is a one-shot graph generative model using Bayesian Sample Inference that handles discrete structures via continuous belief refinement and achieves SOTA performance on molecular graph generation.", "motivation": "Graph-structured data is important for applications like molecular generation and network analysis, but their discrete and unordered nature makes them challenging for traditional generative models, creating a need for discrete diffusion and flow matching approaches.", "method": "GraphBSI uses Bayesian Sample Inference to iteratively refine a belief over graphs in continuous distribution parameter space. It formulates BSI as a stochastic differential equation and derives a noise-controlled SDE family that preserves marginal distributions via score function approximation.", "result": "GraphBSI achieves state-of-the-art performance on molecular and synthetic graph generation benchmarks (Moses and GuacaMol), outperforming existing one-shot graph generative models.", "conclusion": "GraphBSI is a novel and effective approach for graph generation that outperforms existing one-shot methods on standard benchmarks."}}
{"id": "2511.03022", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.03022", "abs": "https://arxiv.org/abs/2511.03022", "authors": ["Lingqing Shen", "Chi Heem Wong", "Misaki Mito", "Arnab Chakrabarti"], "title": "Adaptive-Sensorless Monitoring of Shipping Containers", "comment": "Published in 2025 IEEE Big Data", "summary": "Monitoring the internal temperature and humidity of shipping containers is\nessential to preventing quality degradation during cargo transportation.\nSensorless monitoring -- machine learning models that predict the internal\nconditions of the containers using exogenous factors -- shows promise as an\nalternative to monitoring using sensors. However, it does not incorporate\ntelemetry information and correct for systematic errors, causing the\npredictions to differ significantly from the live data and confusing the users.\nIn this paper, we introduce the residual correction method, a general framework\nfor correcting for systematic biases in sensorless models after observing live\ntelemetry data. We call this class of models ``adaptive-sensorless''\nmonitoring. We train and evaluate adaptive-sensorless models on the 3.48\nmillion data points -- the largest dataset of container sensor readings ever\nused in academic research -- and show that they produce consistent improvements\nover the baseline sensorless models. When evaluated on the holdout set of the\nsimulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\\sim$\n2.31$^\\circ$C (vs 2.43$^\\circ$C by sensorless) for temperature and 5.72 $\\sim$\n7.09% for relative humidity (vs 7.99% by sensorless) and average root\nmean-squared errors (RMSEs) of 3.19 $\\sim$ 3.26$^\\circ$C for temperature (vs\n3.38$^\\circ$C by sensorless) and 7.70 $\\sim$ 9.12% for relative humidity (vs\n10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo\nmonitoring, early risk detection, and less dependence on full connectivity in\nglobal shipping.", "AI": {"tldr": "Proposes adaptive-sensorless monitoring with residual correction to improve container temperature/humidity predictions by correcting systematic errors using live telemetry data.", "motivation": "Sensorless monitoring models have systematic errors and don't incorporate telemetry data, causing inaccurate predictions that confuse users during cargo transportation.", "method": "Introduces residual correction method - a framework to correct systematic biases in sensorless models after observing live telemetry data, creating adaptive-sensorless monitoring.", "result": "Achieved MAEs of 2.24-2.31\u00b0C (vs 2.43\u00b0C baseline) for temperature and 5.72-7.09% (vs 7.99% baseline) for humidity on 3.48 million data points - the largest container dataset used in research.", "conclusion": "Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and reduced dependence on full connectivity in global shipping."}}
{"id": "2511.03032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03032", "abs": "https://arxiv.org/abs/2511.03032", "authors": ["James C. Bowden", "Sergey Levine", "Jennifer Listgarten"], "title": "Leveraging Discrete Function Decomposability for Scientific Design", "comment": null, "summary": "In the era of AI-driven science and engineering, we often want to design\ndiscrete objects in silico according to user-specified properties. For example,\nwe may wish to design a protein to bind its target, arrange components within a\ncircuit to minimize latency, or find materials with certain properties. Given a\nproperty predictive model, in silico design typically involves training a\ngenerative model over the design space (e.g., protein sequence space) to\nconcentrate on designs with the desired properties. Distributional optimization\n-- which can be formalized as an estimation of distribution algorithm or as\nreinforcement learning policy optimization -- finds the generative model that\nmaximizes an objective function in expectation. Optimizing a distribution over\ndiscrete-valued designs is in general challenging because of the combinatorial\nnature of the design space. However, many property predictors in scientific\napplications are decomposable in the sense that they can be factorized over\ndesign variables in a way that could in principle enable more effective\noptimization. For example, amino acids at a catalytic site of a protein may\nonly loosely interact with amino acids of the rest of the protein to achieve\nmaximal catalytic activity. Current distributional optimization algorithms are\nunable to make use of such decomposability structure. Herein, we propose and\ndemonstrate use of a new distributional optimization algorithm,\nDecomposition-Aware Distributional Optimization (DADO), that can leverage any\ndecomposability defined by a junction tree on the design variables, to make\noptimization more efficient. At its core, DADO employs a soft-factorized\n\"search distribution\" -- a learned generative model -- for efficient navigation\nof the search space, invoking graph message-passing to coordinate optimization\nacross linked factors.", "AI": {"tldr": "Proposes DADO, a novel distributional optimization algorithm that leverages decomposability structure in design spaces for more efficient in silico design of discrete objects.", "motivation": "Current distributional optimization algorithms cannot utilize the decomposable structure of property predictors in scientific applications, making optimization over combinatorial design spaces challenging.", "method": "DADO employs a soft-factorized generative model with graph message-passing to coordinate optimization across linked factors based on junction tree decomposability.", "result": "The algorithm demonstrates improved efficiency in navigating complex design spaces by leveraging decomposable property predictors.", "conclusion": "DADO provides a general framework for decomposition-aware distributional optimization that can be applied to various scientific design problems like protein engineering and materials science."}}
{"id": "2511.03046", "categories": ["cs.LG", "cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2511.03046", "abs": "https://arxiv.org/abs/2511.03046", "authors": ["Emi Soroka", "Artem Arzyn"], "title": "Data-Efficient Realized Volatility Forecasting with Vision Transformers", "comment": "NeurIPS Generative AI in Finance", "summary": "Recent work in financial machine learning has shown the virtue of complexity:\nthe phenomenon by which deep learning methods capable of learning highly\nnonlinear relationships outperform simpler approaches in financial forecasting.\nWhile transformer architectures like Informer have shown promise for financial\ntime series forecasting, the application of transformer models for options data\nremains largely unexplored. We conduct preliminary studies towards the\ndevelopment of a transformer model for options data by training the Vision\nTransformer (ViT) architecture, typically used in modern image recognition and\nclassification systems, to predict the realized volatility of an asset over the\nnext 30 days from its implied volatility surface (augmented with date\ninformation) for a single day. We show that the ViT can learn seasonal patterns\nand nonlinear features from the IV surface, suggesting a promising direction\nfor model development.", "AI": {"tldr": "Vision Transformer (ViT) applied to options data can predict 30-day realized volatility from implied volatility surfaces, learning seasonal patterns and nonlinear features.", "motivation": "Transformers show promise in financial forecasting but haven't been explored for options data, despite the known benefits of complex models in financial machine learning.", "method": "Used Vision Transformer (ViT) architecture, typically for image recognition, to predict 30-day realized volatility from single-day implied volatility surfaces augmented with date information.", "result": "ViT successfully learned seasonal patterns and nonlinear features from the implied volatility surface.", "conclusion": "Transformer models show promising potential for options data analysis and volatility forecasting, suggesting a new direction for financial model development."}}
