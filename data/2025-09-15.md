<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: Multi-agent LLM framework for autonomous porous materials characterization through automated simulation setup and force field selection.


<details>
  <summary>Details</summary>
Motivation: Automated characterization of porous materials can accelerate discovery but is limited by complex simulation setup and force field selection challenges.

Method: Multi-agent framework where LLM-based agents autonomously understand characterization tasks, plan simulations, assemble force fields, execute them, and interpret results to guide next steps. Specifically developed for literature-informed force field extraction and automated RASPA simulation setup.

Result: Initial evaluations demonstrate high correctness and reproducibility in automated force field extraction and simulation setup.

Conclusion: This approach shows potential for enabling fully autonomous, scalable materials characterization by overcoming current limitations in simulation complexity and force field selection.

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [2] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: AutoIND LLM platform reduces IND application drafting time by ~97% while maintaining acceptable quality with no critical regulatory errors, though human experts are still needed for final refinement.


<details>
  <summary>Details</summary>
Motivation: IND application preparation is time-intensive and expertise-dependent, slowing early clinical development. The study aims to evaluate if LLMs can accelerate this process while maintaining quality.

Method: Compared AutoIND-generated IND nonclinical summaries with manual drafting times from experienced regulatory writers. Quality was assessed by blinded regulatory writing assessor using 7 criteria scored 0-3.

Result: AutoIND reduced drafting time from ~100h to 3.7h and 2.6h for two INDs. Quality scores were 69.6% and 77.9% with no critical errors, but deficiencies in emphasis, conciseness and clarity were noted.

Conclusion: AutoIND dramatically accelerates IND drafting but expert regulatory writers remain essential for final quality. Identified deficiencies provide roadmap for targeted model improvements.

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [3] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: Boldsea is an executable ontology architecture that integrates event semantics with dataflow to overcome limitations of traditional BPM systems and object-oriented semantic technologies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional Business Process Management systems and object-oriented semantic technologies by creating a more dynamic and flexible approach to modeling complex systems.

Method: Developed BSL (boldsea Semantic Language) with formal BNF grammar and a boldsea-engine architecture that directly interprets semantic models as executable algorithms without compilation.

Result: Enables runtime modification of event models, ensures temporal transparency, and seamlessly merges data and business logic within a unified semantic framework.

Conclusion: Boldsea provides an effective architecture for modeling complex dynamic systems using executable ontologies that can directly control process execution with greater flexibility than traditional approaches.

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [4] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: Foundation models (LLMs and VLMs) can provide high-quality planning feedback across various environments, with larger reasoning models performing better, though performance degrades in complex continuous environments.


<details>
  <summary>Details</summary>
Motivation: To reduce the need for carefully designed reward functions and high-quality demonstrations in grounded environment planning by leveraging pretrained foundation models' background knowledge.

Method: Evaluated LLMs and VLMs across symbolic, language, and continuous control environments with various feedback types (binary, preference, action advising, goal advising, delta action) and inference methods (in-context learning, chain-of-thought, environment dynamics access).

Result: Foundation models provide diverse high-quality feedback across domains; larger reasoning models offer more accurate feedback, less bias, and benefit more from enhanced inference methods; feedback quality degrades in environments with complex dynamics or continuous state/action spaces.

Conclusion: Pretrained foundation models are effective for providing planning feedback, reducing reliance on reward engineering and demonstrations, though their performance is limited in complex continuous environments.

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [5] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: A modular multimodal framework using generative AI to create realistic labeled energy modeling data from publicly accessible residential information and images, reducing dependence on costly or restricted data sources.


<details>
  <summary>Details</summary>
Motivation: Computational energy models require extensive data that is often inaccessible, expensive, or raises privacy concerns, limiting research accessibility and reproducibility.

Method: Developed a modular multimodal framework that leverages generative artificial intelligence to produce energy modeling data from publicly available residential information and images, with an evaluation pipeline for the AI components.

Result: Experiments show the framework successfully produces realistic, labeled data while avoiding common issues associated with generative models.

Conclusion: The framework enables more accessible and reproducible energy modeling research by reducing dependence on costly or restricted data sources through AI-generated synthetic data.

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [6] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: This paper reviews autoformalization - translating informal input into formal logical representations using LLMs - and proposes a unified framework to connect disparate research areas.


<details>
  <summary>Details</summary>
Motivation: The rapid but independent development of autoformalization research across different fields has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress.

Method: The paper reviews both explicit and implicit instances of autoformalization research and proposes a unified framework to encourage cross-pollination between different fields.

Result: The review identifies common patterns and approaches in autoformalization research across mathematics, reasoning, planning, and knowledge representation domains.

Conclusion: A unified framework for autoformalization can advance the development of next-generation AI systems by facilitating shared methodologies and accelerating progress across related research areas.

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [7] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: An intelligent knowledge assistant system using RAG with structured knowledge processing methods (table textualization and decision-tree textualization) for goat farming health management, achieving 85%+ accuracy across various Q&A tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs have limited application in livestock farming due to complex, heterogeneous knowledge sources. Need for specialized systems to support goat farming health management.

Method: Retrieval-Augmented Generation (RAG) with two structured knowledge processing methods, domain-specific knowledge base spanning 5 key domains, and online search module for real-time information.

Result: Heterogeneous knowledge fusion achieved 87.90% accuracy on validation set and 84.22% on test set. Consistently exceeded 85% accuracy across text, table, and decision-tree based Q&A tasks.

Conclusion: The system demonstrates robustness and reliability for practical goat farming applications, with opportunities to improve retrieval coverage and context integration.

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [8] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: LLMs can play UNO effectively but struggle to help other players win, with performance varying by model size.


<details>
  <summary>Details</summary>
Motivation: Test whether LLMs can actively assist humans in achieving goals, using UNO as a test case where the LLM helps another player win rather than winning itself.

Method: Built a tool for decoder-only LLMs to participate in RLCard game environment, providing full game-state information through text prompts with two distinct prompting strategies, testing models from 1B to 70B parameters.

Result: All models outperformed random baseline when playing UNO, but few significantly helped another player win.

Conclusion: LLMs can play games competently but current capabilities are limited in providing effective assistance to help others achieve goals.

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [9] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: A conceptual framework for evolving scientific workflows from static to intelligent and from single to swarm, proposing an architectural blueprint for autonomous science labs with potential 100x discovery acceleration.


<details>
  <summary>Details</summary>
Motivation: Modern scientific discovery requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists. AI agents show exciting opportunities to accelerate discovery but need real-world integration.

Method: Proposes a conceptual framework where workflows evolve along two dimensions: intelligence (static to intelligent) and composition (single to swarm), charting an evolutionary path from current workflow systems to fully autonomous distributed laboratories.

Result: Presents an architectural blueprint that can help the community take next steps towards autonomous science, enabling transformational scientific workflows.

Conclusion: The framework provides a path to harness AI opportunities for scientific discovery with potential for 100x acceleration and fully autonomous distributed scientific laboratories.

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [10] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: Reformulating WaveFunctionCollapse as an MDP allows decoupling constraint satisfaction from objective optimization, outperforming traditional joint optimization methods.


<details>
  <summary>Details</summary>
Motivation: Procedural content generation needs to satisfy both designer objectives and tile adjacency constraints, which traditional methods struggle to optimize jointly.

Method: Reformulate WaveFunctionCollapse as a Markov Decision Process (MDP) to separate constraint enforcement from objective maximization, using WFC's propagation for constraints while external algorithms handle objectives.

Result: Empirical comparison shows MDP-based optimization consistently outperforms traditional evolutionary approaches across multiple domains, especially as task complexity increases.

Conclusion: Decoupling local constraint satisfaction from global objective optimization through WFC-MDP is more effective than joint optimization approaches.

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [11] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: Proposes formal measure for evaluating XAI tools on tabular Boolean function data, introduces B-ReX tool that outperforms other black-box XAI methods


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of subjective evaluation in explainable AI, particularly for tabular data and Boolean function predictions

Method: Extends previous work with formal importance measure based on actual causality, evaluates state-of-the-art XAI tools, and develops novel B-ReX tool based on existing ReX

Result: B-ReX demonstrates superiority over other black-box XAI tools, achieving Jensen-Shannon divergence of 0.072 ± 0.012 on random 10-valued Boolean formulae

Conclusion: The proposed formal evaluation framework and B-ReX tool provide effective solutions for objective assessment of XAI methods in Boolean function prediction tasks

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [12] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA is a privacy-preserving multi-agent system that separates private and public workspaces, using anonymization and enhancement modules to protect sensitive data while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems need to handle privacy-sensitive data securely when using remote LLMs, but current systems lack adequate privacy protection mechanisms.

Method: Divides workspace into private/public spaces, uses anonymization, and incorporates DRKE (Domain-Rule-based Knowledge Enhancement) and DLE (Disproof-based Logic Enhancement) modules to mitigate semantic loss from anonymization.

Result: Superior performance on Trivia Creative Writing and Logic Grid Puzzle datasets, and exceptional effectiveness on new privacy-focused datasets (Knowledge Privacy Preservation and Logic Privacy Preservation).

Conclusion: GAMA effectively addresses privacy concerns in LLM-based multi-agent systems while maintaining high task performance through its innovative anonymization and enhancement approach.

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [13] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents is a multi-agent framework using multipolar task graphs and IF-THEN rules to improve task planning and handle uncertainty in complex tasks, outperforming state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems struggle with effective task planning for highly complex tasks with uncertainty, often producing misleading outputs that hinder execution.

Method: Proposes XAgents framework built on multipolar task processing graph for dynamic planning and IF-THEN rules to constrain agent behaviors and enhance collaboration.

Result: XAgents consistently surpasses state-of-the-art single-agent and multi-agent approaches across three datasets in both knowledge-typed and logic-typed question-answering tasks.

Conclusion: The framework effectively addresses uncertainty in complex tasks through dynamic planning and rule-based constraints, demonstrating superior performance over existing methods.

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [14] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: AI Harmonics is a human-centric framework that uses ordinal severity data to assess and prioritize AI harms, identifying political and physical harms as most urgent for mitigation.


<details>
  <summary>Details</summary>
Motivation: Current AI risk assessment models focus on internal compliance and neglect diverse stakeholder perspectives and real-world consequences of AI harms.

Method: Proposes AI Harmonics framework with a novel AI harm assessment metric (AIH) that leverages ordinal severity data to capture relative impact without requiring precise numerical estimates.

Result: Experiments show political and physical harms have the highest concentration - political harms erode public trust while physical harms pose life-threatening risks.

Conclusion: AI Harmonics effectively identifies uneven harm distributions, enabling policymakers and organizations to target mitigation efforts more effectively.

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [15] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: Proposes a "sandbox economy" framework for analyzing autonomous AI agent economies, characterizing them by origin (emergent vs intentional) and separateness from human economy (permeable vs impermeable). Discusses design choices for steerable AI agent markets including auction mechanisms, mission economies, and socio-technical infrastructure.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of autonomous AI agents is creating a new economic layer where agents transact and coordinate at scales beyond human oversight, presenting both opportunities for unprecedented coordination and significant challenges like systemic economic risk and inequality.

Method: Proposes a conceptual framework called "sandbox economy" with two key dimensions: origins (emergent vs intentional) and separateness (permeable vs impermeable). Discusses design choices including auction mechanisms for fair resource allocation, AI "mission economies" for collective goal coordination, and socio-technical infrastructure for trust and accountability.

Result: Identifies that current trajectory points toward spontaneous emergence of a vast, highly permeable AI agent economy. Provides framework for understanding and analyzing this emergent system.

Conclusion: Argues for proactive design of steerable agent markets to ensure the technological shift aligns with humanity's long-term collective flourishing, rather than allowing uncontrolled emergence of AI agent economies.

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [16] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: RSS is the first online planning algorithm for Robust MDPs with finite-sample theoretical guarantees, outperforming standard methods in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: Existing online planning methods like Sparse Sampling and MCTS perform poorly when generative models have approximation errors from limited data, leading to degraded performance and unsafe behaviors. Robust MDPs provide a framework but are computationally intensive for real-time use.

Method: Robust Sparse Sampling (RSS) computes robust value functions using Sample Average Approximation (SAA) instead of nominal value functions, making it tractable for online settings. It works with infinite/continuous state spaces and has sample/computational complexity independent of state space size.

Result: RSS provides theoretical performance guarantees and empirically outperforms standard Sparse Sampling in environments with uncertain dynamics.

Conclusion: RSS enables efficient online robust planning with theoretical guarantees, addressing the limitations of existing methods in handling model uncertainty while maintaining computational tractability.

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [17] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI framework improves clinical NLI by separating knowledge access from structured inference, achieving up to 42% fidelity gains across four reasoning families.


<details>
  <summary>Details</summary>
Motivation: To address the assumption that scaling alone improves structured reasoning in clinical NLI, and to create safer, auditable clinical reasoning systems.

Method: CARENLI compartmentalizes reasoning into four families with specific solvers, using a planner-verifier-refiner architecture to enforce auditable procedures.

Result: Achieved 98.0% fidelity in Causal Attribution and 81.2% in Risk State Abstraction, with verifiers reliably flagging violations and refiners correcting epistemic errors.

Conclusion: LLMs retain facts but use heuristics when inference is underspecified; CARENLI provides explicit, auditable reasoning framework with routing as main bottleneck.

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [18] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: Using formal methods with Small Language Models improves reasoning performance, enabling substitution of natural language with compact logical languages while maintaining strong reasoning capabilities for ontology engineering.


<details>
  <summary>Details</summary>
Motivation: Address limitations of Language Models in reasoning tasks, particularly for ontology engineering, by investigating how formal methods can enhance Small Language Models' reasoning performance.

Method: Conducted preliminary experiments to test the impact of expressing logical problems with different grammars on SLMs' performance on predefined reasoning tasks, comparing natural language with more compact logical languages.

Result: Found that it's possible to replace Natural Language with compact logical languages while maintaining strong performance on reasoning tasks.

Conclusion: Results support further refinement of SLMs' role in ontology engineering through the use of formal methods and logical languages.

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [19] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: AI systems consistently prioritize Care and Virtue moral values over libertarian choices across different LLMs, with reasoning-enabled models showing better context sensitivity and explainability.


<details>
  <summary>Details</summary>
Motivation: To understand how AI systems align with human moral values and investigate the prospects for human-AI symbiosis by examining moral preferences in large language models.

Method: Quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks, analyzing differences in model architecture, cultural origin, and explainability.

Result: Strikingly consistent value biases found across all models - Care and Virtue values rated most moral, libertarian choices consistently penalized. Reasoning models showed greater context sensitivity and richer explanations.

Conclusion: Research highlights the need for explainability and cultural awareness as critical design principles for transparent, aligned, and symbiotic AI future.

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [20] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: State Algebra is a novel algebraic framework for propositional logic with three hierarchical representations (Set, Coordinate, Row Decomposition) that balances flexibility and canonicity while enabling both search-based and knowledge compilation algorithms.


<details>
  <summary>Details</summary>
Motivation: To create a flexible algebraic framework for representing and manipulating propositional logic that can handle different problem classes efficiently while providing a foundation for extending to probabilistic logic and weighted model counting.

Method: Developed a hierarchical framework with three representations: Set, Coordinate, and Row Decomposition. Uses algebraic methods with a trade-off between guaranteed canonicity and representation flexibility, employing fixed variable ordering for canonical forms when needed.

Result: The framework successfully represents propositional logic algebraically, showing that while default reduction isn't canonical, canonical forms can be achieved with fixed variable ordering. This flexibility allows for more compact representations of certain problem classes.

Conclusion: State Algebra provides a powerful and flexible algebraic foundation for propositional logic manipulation that supports both search and knowledge compilation approaches, with natural extensions to probabilistic logic and weighted model counting applications.

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [21] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding transforms failure attribution from pattern recognition to causal inference, achieving 2.85x accuracy improvement over baselines through structured abduction-action-prediction reasoning.


<details>
  <summary>Details</summary>
Motivation: Current methods for failure attribution in multi-agent systems have critically low step-level accuracy (below 17%) due to inability to perform robust counterfactual reasoning about whether correcting a single action would avert task failure.

Method: Abduct-Act-Predict (A2P) Scaffolding - a three-step reasoning process within a single inference pass: (1) Abduction to infer hidden root causes, (2) Action to define minimal corrective intervention, (3) Prediction to simulate subsequent trajectory and verify if intervention resolves failure.

Result: On Algorithm-Generated dataset: 47.46% step-level accuracy (2.85x improvement over 16.67% baseline). On Hand-Crafted dataset: 29.31% step accuracy (2.43x improvement over 12.07% baseline).

Conclusion: By reframing failure attribution through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution in multi-agent systems.

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [22] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: Information-theoretic framework for RL anomaly detection using mutual information patterns to diagnose sensor/actuator faults and track learning progress.


<details>
  <summary>Details</summary>
Motivation: RL agents lack intrinsic failure detection mechanisms for real-world deployment challenges like sensor faults, actuator wear, and environmental shifts.

Method: Analyze state-action mutual information patterns in robotic control tasks, examining MI(S,A) growth and MI(S,A;S') curve patterns during learning and under controlled perturbations.

Result: Successful learning shows MI(S,A) increasing 238% (0.84 to 2.83 bits) despite state entropy growth. Faults show differential patterns: sensor faults collapse all information channels, while actuator faults disrupt action-outcome predictability but preserve state-action relationships.

Conclusion: Information metrics enable precise fault localization without architectural changes, providing foundation for adaptive RL systems with autonomous fault detection and policy adjustment.

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG is a two-stage self-supervised learning framework that preserves structural semantics in brain graphs for psychiatric diagnosis, outperforming state-of-the-art methods in small-labeled data settings.


<details>
  <summary>Details</summary>
Motivation: Limited labeled brain network data and existing SSL methods that disrupt structural semantics in brain graphs make accurate psychiatric diagnosis challenging.

Method: Two-stage framework: 1) Pre-training stage with edge masker trained on small labeled subset to capture structural semantics, 2) SSL stage with structure-aware augmentation guided by structural priors.

Result: Outperforms state-of-the-art methods on two real-world psychiatric datasets, especially in small-labeled data settings, and uncovers clinically relevant connectivity patterns.

Conclusion: SAM-BG effectively preserves structural semantics in brain graphs, enabling more accurate and interpretable psychiatric diagnoses with limited labeled data.

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [24] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT enables cross-modal transfer learning without requiring paired sensor data during inference, allowing single-sensor deployment while maintaining accuracy gains from multi-modal training.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal transfer methods require paired sensor data at both training and inference, limiting deployment in resource-constrained environments where full sensor suites are not economically or technically feasible.

Method: Proposes Decoupled Cross-Attention Transfer (D-CAT) framework with self-attention module for feature extraction and novel cross-attention alignment loss to align modality-specific representations without coupling classification pipelines.

Result: In in-distribution scenarios: up to 10% F1-score gains when transferring from high-performing modalities (video to IMU). In out-of-distribution scenarios: even weaker source modalities improve target performance if target model isn't overfitted.

Conclusion: D-CAT enables single-sensor inference with cross-modal knowledge, reducing hardware redundancy while maintaining accuracy, making it suitable for cost-sensitive or adaptive deployments like assistive robots with variable sensor availability.

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [25] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto is a transformer-based trading agent that combines meta-learning and reinforcement learning to autonomously improve cryptocurrency trading performance without human supervision.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency return prediction is challenging due to fast-changing market factors (on-chain activity, news, social sentiment) and scarce labeled training data, requiring a self-improving approach.

Method: Uses a unified transformer architecture with meta-learning and RL. An instruction-tuned LLM alternates between actor, judge, and meta-judge roles in a closed-loop system that leverages multimodal inputs and internal preference feedback.

Result: The system demonstrates good performance on real market technical indicators and outperforms other LLM-based baselines across diverse market regimes.

Conclusion: The Meta-RL-Crypto framework successfully creates a fully self-improving trading agent that continuously refines both trading policies and evaluation criteria without additional human supervision.

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [26] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa is a unified KV cache compression framework that minimizes information loss in Transformer residual streams, enabling dynamic budget allocation across layers and heads without training or multiple strategies.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods are heuristic and lack dynamic budget allocation, leading to inefficient memory usage in long-context LLM inference.

Method: Minimizes information loss in Transformer residual streams, analyzes layer attention output loss, and uses derived metrics for layer-wise compression with dynamic head and layer budgets.

Result: Superior performance on benchmarks (LongBench, Needle-In-A-Haystack, Ruler, InfiniteBench) and reveals new insights about budget allocation for different task types.

Conclusion: LAVa is the first unified strategy for cache eviction with dynamic budget allocation that maintains top performance across various task types without requiring training.

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [27] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO framework combines conformal risk calibration with offline reinforcement learning to provide safe, auditable decision support for Medicaid population health management, achieving strong risk discrimination while maintaining fairness across demographic subgroups.


<details>
  <summary>Details</summary>
Motivation: Population health management programs for Medicaid populations need to coordinate outreach and services while ensuring safety, fairness, and auditability. Current approaches must handle the challenge of making conservative action recommendations at scale while controlling near-term risk of adverse utilization events.

Method: Hybrid Adaptive Conformal Offline Reinforcement Learning (HACO) framework that separates risk calibration from preference optimization. It trains a lightweight risk model, derives conformal threshold to mask unsafe actions, and learns preference policy on safe subsets. Evaluated using version-agnostic fitted Q evaluation on stratified subsets.

Result: Achieves strong risk discrimination (AUC ~0.81) with calibrated threshold (τ ~0.038 at α=0.10) while maintaining high safe coverage. Subgroup analyses reveal systematic differences in estimated value across demographics, highlighting importance of fairness auditing.

Conclusion: Conformal risk gating integrates effectively with offline RL to deliver conservative, auditable decision support for population health management teams, demonstrating the framework's practical utility in real-world healthcare coordination.

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [28] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: A unified routing framework using cross-attention to dynamically select optimal LLMs for each query, achieving better performance and cost efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: The proliferation of diverse LLMs with varying costs and performance creates challenges for scalable, cost-effective deployment in real applications.

Method: Single-head cross-attention mechanism to jointly model query and model embeddings, predicting both response quality and generation cost with an exponential reward function for performance-cost balancing.

Result: Achieves up to 6.6% improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum performance over existing routers on RouterBench benchmark.

Conclusion: Lightweight architecture that generalizes across domains, establishes new standard for cost-aware LLM routing with improved efficiency.

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [29] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: Analysis of Gradient-Step Denoiser in Plug-and-Play algorithms, showing it can serve as exact gradient descent/proximity operator while maintaining state-of-the-art denoising performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between implicit image priors in Plug-and-Play algorithms and explicit functional representations, enabling better theoretical understanding and performance.

Method: Training denoisers to function as exact gradient descent or proximity operators of explicit functionals while preserving denoising capabilities.

Result: The Gradient-Step Denoiser successfully serves as both an exact optimization operator and maintains state-of-the-art denoising performance.

Conclusion: This approach provides explicit functional representations for image priors in Plug-and-Play algorithms while maintaining competitive denoising results.

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [30] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: Machine learning approach using physiological signals to distinguish between startle and surprise reactions in pilots, achieving 85.7% accuracy for binary classification and 74.9% for three-class classification including baseline state.


<details>
  <summary>Details</summary>
Motivation: Unexpected events like startle and surprise impair pilot attention and decision-making, posing safety risks in aviation. These reactions are hard to distinguish in practice, and existing research has studied them separately without focusing on combined effects or physiological differentiation.

Method: Used machine learning and multi-modal fusion strategies to analyze physiological signals for distinguishing between startle and surprise events. Employed SVM and XGBoost classifiers with Late Fusion approach.

Result: Achieved 85.7% mean accuracy with SVM and Late Fusion for distinguishing startle vs surprise. Extended to three-class classification (Startle, Surprise, Baseline) achieved 74.9% mean accuracy with XGBoost and Late Fusion.

Conclusion: Physiological signals combined with machine learning can reliably predict and differentiate between startle and surprise reactions, providing a robust method for identifying these safety-critical states in aviation environments.

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [31] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: Decoupling actor-critic entropy in discrete SAC enables DQN-level performance on Atari games without requiring entropy regularization or explicit exploration.


<details>
  <summary>Details</summary>
Motivation: Value-based methods like DQN dominate discrete-action off-policy RL, while policy-based methods either don't learn effectively from off-policy data (PPO) or perform poorly in discrete settings (SAC).

Method: Revisits discrete SAC design, identifies actor-critic entropy coupling as performance bottleneck, introduces flexible off-policy actor-critic framework with m-step Bellman operator for critic and standard policy optimization with entropy regularization for actor.

Result: The proposed methods achieve comparable performance to DQN on standard Atari games, even without entropy regularization or explicit exploration.

Conclusion: Decoupling actor and critic entropy is crucial for effective discrete-action off-policy actor-critic methods, enabling performance competitive with value-based approaches like DQN.

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [32] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN introduces ensemble learning for heterogeneous graphs using meta-path optimization and random dropping to create diverse GNN learners, with residual-attention and correlation-regularization components to improve accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity in node types, features, and neighborhood topology poses challenges for ensemble learning in graphs, requiring methods to accommodate diverse graph learners effectively.

Method: HGEN ensembles multiple learners through meta-path and transformation-based optimization, using random dropping to create Allele GNNs. It employs residual-attention mechanism to calibrate different meta-paths and correlation-regularization to increase disparity among embedding matrices.

Result: Experiments on five heterogeneous networks show HGEN consistently outperforms state-of-the-art competitors by substantial margins, with proven convergence and higher regularization effectiveness than simple voting.

Conclusion: HGEN successfully pioneers ensemble learning for heterogeneous graphs, demonstrating that meta-path optimization with attention and regularization mechanisms significantly improves classification accuracy over existing methods.

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [33] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: A framework for dynamic compute allocation and method selection during LLM inference that optimizes both token cost and latency, outperforming static strategies like best-of-N.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time scaling methods focus only on parallel generation and ignore latency, which is critical for user experience and agentic workflows requiring multiple efficient queries.

Method: Formulates inference-time scaling as dynamic compute allocation and method selection per query, incorporating both token cost and wall-clock latency, and considers both parallel generation and incremental decoding methods.

Result: Experiments on reasoning benchmarks show the approach consistently outperforms static strategies, achieving better accuracy-cost trade-offs while remaining practical for deployment.

Conclusion: Dynamic allocation framework provides superior performance over static methods by optimizing both computational efficiency and latency for real-world deployment scenarios.

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [34] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: A novel neural network framework for predicting dissipative dynamical systems using only observable variables, respecting thermodynamic constraints and ensuring non-decreasing entropy.


<details>
  <summary>Details</summary>
Motivation: Many physical systems have unobservable phase space variables (momenta, entropies), making traditional data-based computing approaches difficult to apply directly to dissipative dynamical systems.

Method: Developed a data-based computing framework using thermodynamic Lagrangian and neural networks that respect thermodynamics and guarantee non-decreasing entropy evolution, working exclusively with observable variables.

Result: The network provides efficient phase space evolution description using limited data points and relatively few parameters.

Conclusion: The proposed framework successfully addresses the challenge of predicting dissipative systems when only observable variables are available, while maintaining thermodynamic consistency.

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [35] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: LoFT framework applies parameter-efficient fine-tuning of foundation models for long-tailed semi-supervised learning, generating more reliable pseudo-labels and handling open-world scenarios with OOD samples.


<details>
  <summary>Details</summary>
Motivation: Existing LTSSL methods train from scratch, leading to overconfidence and low-quality pseudo-labels. Foundation model fine-tuning can address these issues and improve imbalanced learning.

Method: Proposes LoFT framework using parameter-efficient fine-tuning of foundation models for reliable pseudo-label generation. Extends to LoFT-OW for open-world scenarios with OOD samples.

Result: Superior performance on multiple benchmarks, achieving better results even with only 1% of unlabeled data compared to previous works.

Conclusion: Fine-tuning foundation models is effective for LTSSL, providing more reliable pseudo-labels and robust performance in both standard and open-world scenarios.

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [36] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: Proposes MP-CSB framework extending combinatorial semi-bandits to non-negative integer actions, with Thompson-sampling and best-of-both-worlds algorithms achieving logarithmic regret in stochastic settings and sublinear regret in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Traditional combinatorial semi-bandits are limited to binary decision spaces, excluding important applications like optimal transport and knapsack problems that require non-negative integer flows or allocations.

Method: Two algorithms: 1) Thompson-sampling-based algorithm for exponential action spaces with O(log T) regret, 2) Best-of-both-worlds algorithm with O(log T) variance-dependent regret in stochastic regime and O(√T) data-dependent regret in adversarial regime.

Result: Algorithms achieve theoretical regret bounds: logarithmic in stochastic settings and sublinear in adversarial settings. Numerical experiments show outperformance over existing CSB methods.

Conclusion: MP-CSB successfully extends combinatorial bandits to integer action spaces with efficient algorithms that provide strong theoretical guarantees and practical performance improvements.

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [37] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: LLMs can generate scientifically appropriate code for solving ODEs by selecting suitable solvers and performing stability checks, achieving high accuracy with guided prompting and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional scientific machine learning approaches struggle with accuracy and robustness. This work explores using LLMs to write numerical code instead of learning solution functions directly, shifting the burden to making domain-aware numerical choices.

Method: Introduces two datasets: diagnostic adversarial problems and 1,000 diverse ODE tasks. Evaluates LLMs on executability and numerical validity using both unguided and guided prompting with domain knowledge, plus off-the-shelf vs fine-tuned models.

Result: With sufficient context and guided prompts, newer instruction-following models achieve high accuracy on both executability and numerical validity. Open-source systems perform well without fine-tuning, while older/smaller models benefit from fine-tuning.

Conclusion: Careful prompting and fine-tuning can create specialized LLM agents capable of reliably solving simple ODE problems by generating scientifically appropriate numerical code.

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [38] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena reframes multimodal intent recognition from feature fusion to processing modulation, using audio-visual cues to generate dynamic convolutional kernels that directly modulate textual feature extraction at token level.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods risk corrupting linguistic features with noisy or irrelevant non-verbal signals by simply adding multimodal features, failing to capture fine-grained token-level modulation where non-verbal cues should modulate rather than augment textual meaning.

Method: Translates audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction, moving from feature fusion to processing modulation approach.

Result: Achieves state-of-the-art results on MIntRec and MIntRec2.0 benchmarks, with +10.46% F1-score improvement in out-of-scope detection, creating more robust intent representations.

Conclusion: The fine-grained processing modulation approach effectively addresses the limitations of traditional multimodal fusion methods, significantly improving intent recognition performance and robustness.

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [39] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: Training-free adaptive token merging framework that compresses transformer representations by merging redundant tokens based on input-specific similarity thresholds, achieving significant efficiency gains without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large transformers are computationally expensive for edge devices, requiring solutions that reduce costs while maintaining performance without retraining.

Method: Uses per-layer similarity thresholds to merge semantically redundant tokens adaptively, formulated as multi-objective optimization solved with Bayesian optimization for Pareto-optimal trade-offs.

Result: Achieves 30% fewer FLOPs and <20% communication cost on ImageNet with same accuracy; competitive VQA performance at <1/3 compute and <1/10 bandwidth of full model.

Conclusion: Provides practical, versatile solution for deploying transformers on resource-constrained edge devices with efficiency, robustness, and privacy benefits.

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [40] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine is a framework that uses symbolic rules from interpretable models and dual-granularity filtering to generate high-quality synthetic tabular data, outperforming existing methods in both regression and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing tabular data generation methods require sufficient reference data and often fail to capture domain-specific feature dependencies, leading to poor performance in data-scarce domains.

Method: ReFine derives symbolic "if-then" rules from interpretable models to guide generation, and applies dual-granularity filtering to suppress over-sampling and refine rare samples.

Result: Achieves up to 0.44 absolute improvement in R-squared for regression and 10.0% relative improvement in F1 score for classification compared to state-of-the-art methods.

Conclusion: ReFine effectively addresses data scarcity issues in domain-specific databases by incorporating domain knowledge and balancing data distribution through rule-based guidance and selective filtering.

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [41] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: Machine learning approach using guest VM resource metrics to estimate energy consumption without host access, achieving high accuracy (R² 0.90-0.97) for virtualized environments.


<details>
  <summary>Details</summary>
Motivation: Addresses the critical gap in virtualized/cloud environments where direct energy measurement is infeasible due to lack of physical power measurement interfaces and privileged host access.

Method: Uses Gradient Boosting Regressor trained on resource utilization metrics collected from guest virtual machines to predict energy consumption measured via RAPL on the host.

Result: Achieves high predictive accuracy with variance explained between 0.90 and 0.97 across diverse workloads, demonstrating feasibility of guest-side energy estimation.

Conclusion: The approach enables energy-aware scheduling, cost optimization, and physical host independent energy estimates in virtualized environments without requiring privileged host access.

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [42] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: Empirical investigation of neural scaling laws in deep regression models using parameter estimation for twisted van der Waals magnets, showing power-law relationships between loss and dataset size/model capacity with large scaling exponents.


<details>
  <summary>Details</summary>
Motivation: Neural scaling laws are crucial for developing reliable models with limited resources, but their application to deep regression models remains largely unexplored despite their importance in large language models.

Method: Used parameter estimation model for twisted van der Waals magnets, tested various architectures including fully connected networks, residual networks, and vision transformers across wide ranges of dataset sizes and model capacities.

Result: Observed power-law relationships between loss and both training dataset size and model capacity, with scaling exponents ranging from 1 to 2 depending on regressed parameters and model details.

Conclusion: Consistent scaling behaviors with large exponents suggest that deep regression model performance can substantially improve with increasing data size, demonstrating the importance of scaling laws in regression tasks.

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [43] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA is an autoencoder that estimates intrinsic dimension of datasets on linear/nonlinear manifolds and reconstructs data using re-weighted double CancelOut layers and projected reconstruction loss.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can accurately estimate the intrinsic dimension of complex datasets while maintaining reconstruction capability, especially for datasets from physical systems like fluid flow simulations.

Method: Uses autoencoder architecture with re-weighted double CancelOut layers and introduces projected reconstruction loss term to guide training by assessing reconstruction quality when removing latent dimensions.

Result: Shows good accuracy and high versatility on theoretical benchmarks, successfully estimates intrinsic dimension and reconstructs original solutions for fluid flow simulation data.

Conclusion: IDEA provides an effective approach for intrinsic dimension estimation with reconstruction capabilities, demonstrating robustness across theoretical benchmarks and real-world physical system data.

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [44] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: SMoE-VAE architecture with unsupervised expert routing outperforms supervised baseline on QuickDraw dataset, discovering meaningful sub-categorical structures beyond human-defined labels.


<details>
  <summary>Details</summary>
Motivation: To understand the internal organization of neural networks and address the challenge of deep learning interpretability by exploring how mixture of experts models uncover fundamental data structures.

Method: Developed a Sparse Mixture of Experts Variational Autoencoder (SMoE-VAE) and tested it on QuickDraw dataset, comparing unsupervised expert routing against supervised baseline with ground-truth labels. Used t-SNE visualizations and reconstruction analysis.

Result: Unsupervised routing consistently achieved superior reconstruction performance. Experts learned meaningful sub-categorical structures that transcend human-defined class boundaries. Dataset size study revealed trade-offs between data quantity and expert specialization.

Conclusion: MoE models can uncover fundamental data structures more aligned with the model's objective than predefined labels, providing guidance for designing efficient MoE architectures.

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [45] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: Proposes AODL, a low-rank coding model for sparse dictionary learning that learns more sparse solutions and generalizes better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Sparse dictionary coding with learned dictionaries offers sparser solutions but becomes challenging for multi-dictionary scenarios due to the combinatorial explosion of atom combinations.

Method: Proposes a convex relaxation solution (AODL) with low-rank coding model, solved via alternating optimization between sparse coding matrices and learned dictionaries.

Result: AODL learns up to 90% sparser solutions compared to baselines while maintaining reconstruction quality, and reveals interpretable patterns in training data.

Conclusion: The proposed low-rank coding model effectively addresses multi-dictionary learning challenges, providing sparser solutions with better generalization and interpretability.

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [46] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO is a sequence-level RL method that addresses length bias in LLM training by introducing length-fair clipping in importance-sampling weight space, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing sequence-level RL methods like PPO/GRPO suffer from length bias where fixed clip ranges systematically reweight short vs long responses, distorting the training objective.

Method: FSPO clips sequence log-IS ratios with a Gaussian-motivated band that applies KL-corrected drift term and scales as √L to ensure length fairness.

Result: Empirical results show FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets.

Conclusion: FSPO provides a theoretically grounded solution to length fairness in sequence-level RL, achieving better performance while maintaining training stability.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [47] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: Probabilistic finite automata (PFAs) can be exactly simulated using symbolic feedforward neural networks that represent state distributions as vectors and transitions as stochastic matrices, enabling parallel, interpretable, and differentiable simulation without recurrence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic computation and deep learning by unifying probabilistic automata theory with neural architectures under a rigorous algebraic framework.

Method: Using symbolic feedforward neural networks that represent state distributions as vectors and transitions as stochastic matrices, enabling probabilistic state propagation via matrix-vector products. The approach includes formal characterization of probabilistic subset construction, ε-closure, and exact simulation through layered symbolic computation.

Result: The neural networks can exactly simulate PFAs and are learnable through standard gradient descent optimization on labeled sequence data, recovering the exact behavior of ground-truth PFAs as formalized in Proposition 5.1.

Conclusion: This work establishes a formal equivalence between PFAs and specific classes of neural networks, providing a unified framework that connects symbolic computation with deep learning through parallel, interpretable, and differentiable simulation.

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [48] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: Automated AI system 'Agent-E' identifies regional papers from conferences and uses RPA to complete actions like nomination submissions with 100% recall and 99.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of keeping up with rapidly growing academic literature and reduce time-consuming manual effort required for scholarly discovery workflows.

Method: Developed a fully automated pipeline using specialized AI agent 'Agent-E' to identify papers from specific geographic regions in conference proceedings, combined with Robotic Process Automation (RPA) to execute predefined actions.

Result: Validated on 586 papers from five conferences, achieving 100% recall (identified every target paper) and 99.4% accuracy.

Conclusion: Task-oriented AI agents can not only filter information but also actively participate in and accelerate academic community workflows, demonstrating significant potential for automation in scholarly processes.

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [49] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP is a novel federated learning algorithm that combines random projection with ADMM optimization to enhance privacy and reduce communication costs while maintaining high model accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges in user privacy protection against attacks and high communication costs, especially in sensitive domains like IoT and medical data analysis.

Method: Integrates random projection techniques with ADMM optimization framework to reduce dimensionality of model parameters before transmission, providing strong differential privacy guarantees.

Result: FedRP maintains high model accuracy while outperforming existing methods (including conventional differential privacy and FedADMM) in both privacy preservation and communication efficiency.

Conclusion: FedRP successfully addresses key FL challenges by providing enhanced privacy protection through random projection and improved communication efficiency while maintaining competitive model performance.

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [50] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: VBLL integration with TabPFN underperforms original TabPFN in uncertainty calibration across medical datasets


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty estimation is crucial for safety-critical applications like medical diagnosis, and TabPFN is a promising foundation model for tabular data that could benefit from improved uncertainty calibration

Method: Integrated Variational Bayesian Last Layers (VBLL) with TabPFN and compared uncertainty calibration performance against original TabPFN on three benchmark medical tabular datasets

Result: Original TabPFN consistently outperformed VBLL-integrated TabPFN in uncertainty calibration across all datasets, contrary to expectations

Conclusion: VBLL integration does not improve uncertainty calibration for TabPFN in medical tabular data applications

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [51] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR: A novel symbolic regression framework using Kolmogorov Arnold Networks with deep learning and simplification strategies to recover ground-truth equations and model dynamic systems.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression traditionally uses genetic programming approaches, but deep learning techniques combined with KANs and simplification strategies can provide more accurate equation discovery for scientific datasets and dynamic system modeling.

Method: Uses Kolmogorov Arnold Networks (KANs) with a divide-and-conquer approach, deep learning techniques, and simplification strategies including translational symmetries and separabilities. Combined with neural controlled differential equations for dynamic modeling.

Result: Successfully recovers ground-truth equations of the Feynman SRSD dataset and precisely models the dynamics of an in-silico bioprocess system.

Conclusion: The framework opens doors for dynamic modeling of engineering systems and demonstrates superior performance in symbolic regression compared to traditional genetic programming approaches.

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [52] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: Proposes an information-geometric projection framework for Bayesian Federated Learning that enables tunable trade-off between global generalization and local specialization through statistical manifold barycenter computation.


<details>
  <summary>Details</summary>
Motivation: Bayesian Federated Learning needs better personalization mechanisms to handle data heterogeneity and privacy constraints while balancing global and local performance.

Method: Information-geometric projection framework that projects global model onto neighborhood of user's local model, equivalent to computing barycenter on statistical manifold with closed-form solutions.

Result: Achieves cost-free personalization with minimal computational overhead, effectively balancing global and local performance under heterogeneous data distributions.

Conclusion: The proposed framework provides an efficient and tunable approach for personalization in Bayesian Federated Learning with closed-form solutions and practical applicability.

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [53] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: BenchECG is a standardized benchmark for ECG foundation models with comprehensive datasets and tasks. xECG, an xLSTM-based model with self-supervised learning, achieves state-of-the-art performance across all tasks.


<details>
  <summary>Details</summary>
Motivation: Lack of consistent evaluation for ECG foundation models due to narrow task selections and inconsistent datasets, hindering fair comparison and progress in the field.

Method: Proposed BenchECG benchmark with comprehensive ECG datasets and versatile tasks. Developed xECG model using xLSTM architecture with SimDINOv2 self-supervised learning.

Result: xECG achieved the best BenchECG score compared to publicly available state-of-the-art models and was the only model to perform strongly on all datasets and tasks.

Conclusion: BenchECG enables rigorous comparison and accelerates progress in ECG representation learning. xECG sets a new baseline for future ECG foundation models with superior performance over previous approaches.

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [54] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF is a novel federated learning framework that learns quantized model parameters during local training by updating only one bit per parameter while freezing others, achieving high compression and comparable accuracy to FedAvg.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from substantial communication overhead, and existing quantization methods applied after training introduce errors that degrade model accuracy.

Method: FedBiF quantizes model parameters at the server, transmits them to clients, and allows each client to update only a single bit of multi-bit parameter representation while freezing remaining bits during local training.

Result: Extensive experiments on five datasets under IID and Non-IID settings show FedBiF achieves superior communication compression (1 bpp uplink, 3 bpp downlink), promotes model sparsity, and maintains accuracy comparable to FedAvg.

Conclusion: FedBiF effectively reduces communication costs while preserving model accuracy through bit-by-bit parameter updates during local training, making it a promising approach for efficient federated learning.

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [55] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: Fed-MARL framework for 6G edge networks combining federated learning and multi-agent reinforcement learning for privacy-preserving, energy-efficient resource management across MAC and application layers.


<details>
  <summary>Details</summary>
Motivation: 6G networks require efficient resource management under strict privacy, mobility, and energy constraints in ultra-dense intelligent edge environments.

Method: Federated Multi-Agent Reinforcement Learning with Deep Recurrent Q-Networks for decentralized policies, secure aggregation via elliptic curve Diffie Hellman, and POMMDP formulation with multi-objective rewards.

Result: Outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness while ensuring privacy protection and scalability.

Conclusion: Fed-MARL provides effective solution for real-time resource management in dynamic 6G edge networks with strong privacy guarantees and performance improvements.

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [56] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: Proposes a neural network-based reoptimization technique for surface code quantum error correction that improves decoder accuracy by treating syndrome measurements as a continuous regression problem.


<details>
  <summary>Details</summary>
Motivation: Previous surface code decoders suffer from non-uniqueness of correct predictions, acquiring only error probability distributions rather than definitive corrections.

Method: Approximates syndrome measurements with continuous functions mathematically interpolated by neural networks, reoptimizing decoder models through deep learning regression.

Result: Reoptimized decoders showed improved accuracy for code distances 5 and 7 across various architectures (multilayer perceptron, convolutional, recurrent neural networks, transformers).

Conclusion: Reframing surface code decoding as a regression problem solvable by deep learning is universally effective and improves decoder performance regardless of code distance or network architecture.

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [57] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: The paper analyzes gradient-based training of deep ResNets, showing convergence to Neural Mean ODE dynamics with error bounds that depend on depth, width, and scaling factors, revealing conditions for complete feature learning vs lazy training regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of large-depth residual networks from random initializations, particularly how different scaling factors affect convergence behavior and feature learning capabilities in practical architectures like Transformers.

Method: Mathematical analysis of ResNet training dynamics using stochastic approximation and propagation of chaos techniques to derive convergence results and error bounds for different residual scaling regimes.

Result: Established error bounds O(1/L + α/√(LM)) between model output and limit, showing tight empirical rates. Identified Θ(√D/LM) as the only scaling for complete feature learning in two-layer perceptron ResNets, with corresponding error bound O(1/L + √D/√(LM)).

Conclusion: Deep ResNets converge to Neural Mean ODE dynamics with depth divergence, and the residual scaling critically determines whether training exhibits complete feature learning (non-linear parameterization) or lazy training (linear parameterization), with optimal scaling depending on embedding dimension.

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [58] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: A scalable framework for learning neural surrogates for high-resolution 3D physics simulations using a hybrid CNN-Transformer architecture that outperforms existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop efficient neural surrogates for high-resolution 3D physics simulations that can handle complex PDE dynamics with reduced computational requirements.

Method: Proposes a hybrid CNN-Transformer backbone architecture that can be pretrained on small simulation patches and fused for global solutions, optionally guided by sequence-to-sequence modeling for long-range dependencies.

Result: The architecture significantly outperforms baseline methods, scales to high-resolution turbulence (512^3 spatial resolution), and works as a diffusion model for probabilistic sampling of turbulent flows across varying Reynolds numbers.

Conclusion: The framework provides an effective and scalable approach for learning deterministic and probabilistic neural surrogates for complex 3D physics simulations with high accuracy and efficiency.

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [59] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: Novel ensemble learning framework that optimizes both expected margin and margin variance, with reparameterized weights on unit sphere for better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Conventional margin-based ensemble methods focus only on maximizing expected margin while ignoring margin variance, limiting generalization and causing overfitting, especially in noisy/imbalanced datasets. Traditional weight optimization in probability simplex is computationally inefficient.

Method: Joint optimization of negative expected margin and its variance in loss function. Reparameterize ensemble weights onto unit sphere to simplify optimization and improve computational efficiency.

Result: Extensive experiments on multiple benchmark datasets show consistent outperformance over traditional margin-based ensemble techniques.

Conclusion: The proposed framework enhances robustness, improves generalization performance, and offers better computational efficiency compared to conventional ensemble methods.

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [60] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: Machine learning pipeline for aircraft wing fatigue life prediction using flight parameters, providing faster estimates than traditional FEM methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional fatigue life prediction methods are time-consuming, computationally expensive, and require complex workflows with multiple teams. ML can complement these methods by providing faster iterations and reducing computational/human resources.

Method: ML-based pipeline that estimates fatigue life at different aircraft wing locations using flight parameters from various missions throughout the aircraft's operational life.

Result: The pipeline provides accurate fatigue life predictions with thorough statistical validation and uncertainty quantification in realistic use cases.

Conclusion: The ML pipeline serves as a valuable complement to traditional methodologies by reducing costly simulations and lowering computational/human resource requirements while maintaining prediction accuracy.

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [61] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: Simple prompt injections can manipulate LLM peer reviews with 100% effectiveness, and LLMs show strong bias toward paper acceptance (>95%)


<details>
  <summary>Details</summary>
Motivation: To investigate the practicability and technical success of hidden prompt injections in manipulating LLM-based scientific peer review scores, given the ongoing debate about LLM usage in peer review

Method: Systematic evaluation using 1,000 reviews of 2024 ICLR papers generated by a wide range of LLMs to test prompt injection effectiveness

Result: Very simple prompt injections are highly effective (up to 100% acceptance scores) and LLM reviews are generally biased toward acceptance (>95% in many models)

Conclusion: These findings have significant impact on discussions about LLM usage in peer review, highlighting vulnerabilities and biases that need to be addressed

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [62] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: Transfer learning framework using neural recommender system and COSMO-RS simulated data to predict ionic liquid properties with limited experimental data


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of thermophysical properties for ionic liquids is challenging due to vast chemical design space and limited experimental data availability

Method: Two-stage process: 1) Pre-train neural recommender system on COSMO-RS simulated data to learn structural embeddings, 2) Fine-tune feedforward neural networks using experimental data at varying temperatures/pressures

Result: Improved performance for four out of five target properties (density, viscosity, surface tension, heat capacity, melting point), robust extrapolation to unseen ILs, and prediction capability for over 700,000 IL combinations

Conclusion: Combining simulated data with transfer learning effectively overcomes experimental data sparsity, providing scalable solution for ionic liquid screening in process design

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [63] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: CDQAC is a novel offline RL algorithm that learns effective job-shop scheduling policies directly from historical data without online interactions, outperforming both data-generating heuristics and state-of-the-art RL methods with high sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Online RL methods for job-shop scheduling require millions of costly interactions with simulated environments and suffer from poor sample efficiency due to random policy initialization. There's a need for methods that can learn from historical data while improving upon suboptimal training data.

Method: Conservative Discrete Quantile Actor-Critic (CDQAC) couples a quantile-based critic with delayed policy updates, estimating return distributions for machine-operation pairs rather than selecting pairs outright.

Result: CDQAC consistently outperforms original data-generating heuristics and state-of-the-art offline/online RL baselines. It's highly sample efficient (only 10-20 training instances needed) and surprisingly performs better with random heuristic data than higher-quality genetic algorithm data.

Conclusion: CDQAC effectively addresses limitations of online RL methods by learning directly from historical data, eliminating costly online interactions while maintaining ability to improve upon suboptimal training data, demonstrating superior performance and sample efficiency.

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [64] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: Proposes Proof of AutoML - using ML regressors as nonce generators for blockchain-secured energy trading in disaster scenarios, with SDN-enabled architecture for flexible control.


<details>
  <summary>Details</summary>
Motivation: Need for secure, traceable energy trading when conventional infrastructure fails in disasters, requiring robust nonce generation for blockchain integrity.

Method: SDN-enabled architecture with 5 AutoML-selected regression models (Gradient Boosting, LightGBM, Random Forest, Extra Trees, K-NN) evaluated for randomness rather than accuracy using 9000-sample dataset.

Result: Random Forest and Extra Trees show complete randomness dependency; Gradient Boosting, K-NN and LightGBM show strong randomness (97.6%, 98.8%, 99.9% respectively).

Conclusion: Tree-based ML ensembles can serve as effective lightweight nonce generators for blockchain-secured energy trading infrastructure resilient to disaster conditions.

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [65] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: CAPE foundation model uses contrastive learning on ECG data from diverse populations, showing that pretraining cohort composition affects downstream performance. Multi-center training improves in-distribution accuracy but reduces OOD generalization due to cohort-specific artifacts. Proposed IDB strategy enhances OOD robustness.


<details>
  <summary>Details</summary>
Motivation: To understand how cohort demographics, health status, and population diversity influence contrastive learning performance in ECG analysis, and address the underexplored dependence on cohort composition in self-supervised pretraining.

Method: Developed CAPE foundation model using contrastive learning, pretrained on 5.2M ECG samples from diverse populations across three continents. Systematically assessed cohort effects and proposed In-Distribution Batch (IDB) strategy to preserve intra-cohort consistency during pretraining.

Result: Downstream performance depends on pretraining cohort distributional properties. Multi-center diverse cohort pretraining improves in-distribution accuracy but reduces OOD generalization by encoding cohort-specific artifacts. IDB strategy successfully enhances OOD robustness.

Conclusion: Cohort composition significantly impacts contrastive learning performance. The proposed IDB strategy provides important insights for developing clinically fair and generalizable foundation models that maintain robustness across diverse populations.

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [66] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: GraphCSVAE framework for modeling physical vulnerability using satellite data and expert knowledge to track disaster risk reduction progress.


<details>
  <summary>Details</summary>
Motivation: Address the gap in modeling physical vulnerability for disaster risk reduction, as current methods focus mainly on hazard and exposure but lack progress in vulnerability modeling.

Method: Graph Categorical Structured Variational Autoencoder (GraphCSVAE) integrating deep learning, graph representation, and categorical probabilistic inference with time-series satellite data and expert belief systems.

Result: Successfully modeled spatiotemporal distribution of physical vulnerability in cyclone-impacted Bangladesh and mudslide-affected Sierra Leone, revealing post-disaster regional dynamics.

Conclusion: Provides valuable insights for localized spatiotemporal auditing and sustainable post-disaster risk reduction strategies, advancing progress towards UN Sendai Framework goals.

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [67] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: A simple convolutional module called ARMA for long-term time series forecasting, inspired by ARIMA but with direct multi-step prediction capability.


<details>
  <summary>Details</summary>
Motivation: To create an effective yet simple convolutional module for long-term time series forecasting that overcomes the limitations of conventional ARIMA's iterative multi-step forecasting approach.

Method: Proposes a block with two convolutional components: one for capturing trend (autoregression) and another for refining local variations (moving average). The block directly performs multi-step forecasting and is easily extendable to multivariate settings.

Result: Experiments on nine benchmark datasets show competitive accuracy, especially on datasets with strong trend variations, while maintaining architectural simplicity. The block also inherently encodes absolute positional information.

Conclusion: The ARMA block is an effective lightweight solution for time series forecasting that can potentially replace positional embeddings in sequential models due to its inherent positional encoding capabilities.

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [68] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: A machine learning framework using structure-preserving digital twins with conditional neural Whitney forms for adaptive source localization in hydrodynamic-transport systems, combining FEEC guarantees with transformer-based operator learning.


<details>
  <summary>Details</summary>
Motivation: To develop a real-time source localization system that preserves physical structure and conservation laws while adapting to streaming sensor data, overcoming limitations of physics-agnostic approaches in complex geometries.

Method: Uses conditional neural Whitney forms (CNWF) coupling finite element exterior calculus with transformer-based operator learning. Employs conditional attention mechanism to identify reduced Whitney-form basis, integral balance equations, and source field. Staggered scheme alternates between digital twin evaluation and Lloyd's algorithm for sensor placement.

Result: The framework preserves discrete conservation, adapts in real-time to sensor data, and demonstrates improved accuracy in complex geometries compared to physics-agnostic transformer architectures. Enables recovery of point sources under continuity assumptions with physical realizability.

Conclusion: Structure preservation through CNWF provides an effective inductive bias for source identification, maintaining stability and consistency of finite-element simulation while enabling real-time adaptation and improved localization accuracy in complex environments.

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [69] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: A unified framework for dataset condensation that extends beyond generalization to include robustness, privacy, and other properties using discrepancy measures.


<details>
  <summary>Details</summary>
Motivation: Recent dataset condensation methods focus on approximating data distributions but are limited to generalization performance. The paper aims to create a more comprehensive framework that accommodates multiple objectives beyond just generalization.

Method: The authors present a unified framework that encompasses existing DC methods and extends them using formal discrepancy measures to quantify distances between probability distributions in different regimes.

Result: The framework broadens dataset condensation objectives to include robustness, privacy, and other desirable properties, providing a more general and formal definition of the task.

Conclusion: This work establishes a comprehensive theoretical foundation for dataset condensation that goes beyond traditional generalization-focused approaches, enabling the development of condensation methods with multiple optimization objectives.

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [70] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: Functional extension of rectified flow to infinite-dimensional Hilbert spaces with rigorous mathematical foundation and superior experimental performance.


<details>
  <summary>Details</summary>
Motivation: Rectified flow has been successfully developed in finite-dimensional Euclidean spaces but remains unexplored in infinite-dimensional functional settings, despite many generative models having functional generalizations.

Method: Established rigorous functional formulation of rectified flow using superposition principle for continuity equations in infinite-dimensional Hilbert space, extending to functional flow matching and functional probability flow ODEs as nonlinear generalizations.

Result: The framework removes restrictive measure-theoretic assumptions from existing functional flow matching theory and demonstrates superior experimental performance compared to existing functional generative models.

Conclusion: This work successfully extends rectified flow to infinite-dimensional spaces, providing a mathematically rigorous foundation that outperforms existing functional generative approaches while eliminating previous theoretical limitations.

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [71] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: VIG active learning policy achieves near-full-supervision accuracy on wildlife image classification using only 10% of labels by selecting images that reduce dataset-wide prediction uncertainty.


<details>
  <summary>Details</summary>
Motivation: Camera trap biodiversity monitoring faces major bottlenecks in species identification due to limited labeling resources, and existing active learning methods focus only on individual prediction uncertainty without considering dataset-wide uncertainty.

Method: Introduces Vendi information gain (VIG) active learning policy that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity in the feature space.

Result: Applied to Snapshot Serengeti dataset, VIG achieves impressive predictive accuracy close to full supervision using less than 10% of labels, consistently outperforms standard baselines across metrics and batch sizes, and collects more diverse data.

Conclusion: VIG has broad applicability beyond ecology and demonstrates significant value for biodiversity monitoring in data-limited environments by efficiently reducing labeling requirements while maintaining high accuracy.

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [72] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO is a novel RL framework that uses masked diffusion LLMs' inpainting capability to guide exploration by inserting partial ground-truth reasoning traces, improving sample efficiency and achieving SOTA results on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs face exploration challenges in RL with sparse rewards and sample waste. Masked diffusion LLMs offer unique inpainting capabilities that can guide exploration more efficiently.

Method: IGPO strategically inserts partial ground-truth reasoning traces during online sampling to steer exploration. Combines with supervised fine-tuning on synthetic concise traces, entropy-based filtering, and integrates with group-based optimization methods like GRPO.

Result: Achieves substantial gains across GSM8K, Math500, and AMC mathematical benchmarks, setting new state-of-the-art results for full-attention masked diffusion LLMs.

Conclusion: Inpainting-guided exploration effectively bridges supervised fine-tuning and reinforcement learning, restoring meaningful gradients and improving sample efficiency for diffusion LLMs in mathematical reasoning tasks.

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [73] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe is an efficient attention approximation that combines semantic clustering with multipole expansions to reduce transformer's quadratic complexity, achieving 3x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computational complexity of transformers in context length by developing a more efficient attention mechanism that maintains performance while reducing computational costs.

Method: Clusters queries and keys separately in learned representation spaces using hierarchical two-stage attention with centroid-based approximations augmented by dipole corrections to capture directional variance within clusters.

Result: Achieves O(NCD) complexity for acausal attention and O(NCD log N) for causal attention, with 3x speedup over Flash Attention at 8k context length and only 0.36% loss degradation in end-to-end pretraining.

Conclusion: Multipole Semantic Attention provides an effective drop-in replacement for standard attention that significantly reduces computational complexity while maintaining model performance, making it viable for efficient transformer pretraining.

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [74] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: Using process mining and unsupervised ML for run-time control-flow anomaly detection in ERTMS/ETCS L2 railway systems to enhance resilience against cyber-threats and system changes.


<details>
  <summary>Details</summary>
Motivation: Railway systems face increasing complexity and criticality, with potential run-time anomalies from residual faults, system modifications, or cyber-threats that weren't accounted for during design-time verification.

Method: Process mining to learn actual control flow from execution traces, enabling online conformance checking. Unsupervised machine learning for anomaly localization to link deviations to critical system components.

Result: Tested on ERTMS/ETCS L2 RBC/RBC Handover scenario, demonstrating high accuracy, efficiency, and explainability in detecting and localizing anomalies.

Conclusion: The approach effectively enhances railway system resilience by providing run-time monitoring capabilities that complement traditional design-time verification processes.

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [75] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: Local SGD with outer optimizer tuning improves communication efficiency in distributed ML. Outer learning rate >1 and momentum can optimize convergence and compensate for inner learning rate mis-tuning.


<details>
  <summary>Details</summary>
Motivation: Communication bottleneck in distributed ML with large batch sizes and parallel hardware. Local SGD reduces communication overhead but outer optimizer hyperparameters are poorly understood.

Method: Theoretical analysis of Local SGD with outer optimizer tuning. Proves convergence guarantees, studies outer learning rate >1, momentum, and acceleration. Introduces data-dependent analysis and validates with language model experiments.

Result: Tuning outer learning rate enables trade-off between optimization error and gradient noise variance, compensates for inner learning rate mis-tuning. Momentum and acceleration improve convergence rates. Experiments validate theoretical findings.

Conclusion: Outer optimizer plays crucial role in Local SGD performance. Careful tuning of outer learning rate (sometimes >1) and use of momentum/acceleration significantly improves communication efficiency and convergence in distributed ML settings.

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [76] [Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building](https://arxiv.org/abs/2509.09906)
*Alexandra Fetsch,Iurii Savvateev,Racem Ben Romdhane,Martin Wiedmann,Artemiy Dimov,Maciej Durkalec,Josef Teichmann,Jakob Zinsstag,Konstantinos Koutsoumanis,Andreja Rajkovic,Jason Mann,Mauro Tonolla,Monika Ehling-Schulz,Matthias Filter,Sophia Johler*

Main category: cs.MA

TL;DR: AI-assisted negotiation framework using LLMs and autonomous agents to address complex global challenges through simulated negotiations, systematic modeling, and impact evaluation in cross-sectoral risk analysis.


<details>
  <summary>Details</summary>
Motivation: Conventional risk analysis frameworks create silos that hinder comprehensive solutions for complex global challenges. There's a need for holistic strategies to enable effective negotiations between sectors and balance competing stakeholder interests, but this is hindered by time constraints, information overload, and integration complexity.

Method: Developed an AI-assisted negotiation framework incorporating large language models (LLMs) and AI-based autonomous agents into a negotiation-centered risk analysis workflow. The framework enables stakeholders to simulate negotiations, model dynamics, anticipate compromises, and evaluate solution impacts using LLMs' semantic analysis capabilities.

Result: Proof-of-concept implementations in two real-world scenarios: (i) prudent use of a biopesticide, and (ii) targeted wild animal population control. The framework demonstrated potential to address the lack of tools for cross-sectoral engagement and mitigate information overload under time constraints.

Conclusion: The open-source, web-based AI-assisted negotiation framework shows promise for broader application by users with limited resources, enabling customization for specific needs and facilitating effective cross-sectoral negotiations for complex global challenges.

Abstract: Key global challenges of our times are characterized by complex
interdependencies and can only be effectively addressed through an integrated,
participatory effort. Conventional risk analysis frameworks often reduce
complexity to ensure manageability, creating silos that hinder comprehensive
solutions. A fundamental shift towards holistic strategies is essential to
enable effective negotiations between different sectors and to balance the
competing interests of stakeholders. However, achieving this balance is often
hindered by limited time, vast amounts of information, and the complexity of
integrating diverse perspectives. This study presents an AI-assisted
negotiation framework that incorporates large language models (LLMs) and
AI-based autonomous agents into a negotiation-centered risk analysis workflow.
The framework enables stakeholders to simulate negotiations, systematically
model dynamics, anticipate compromises, and evaluate solution impacts. By
leveraging LLMs' semantic analysis capabilities we could mitigate information
overload and augment decision-making process under time constraints.
Proof-of-concept implementations were conducted in two real-world scenarios:
(i) prudent use of a biopesticide, and (ii) targeted wild animal population
control. Our work demonstrates the potential of AI-assisted negotiation to
address the current lack of tools for cross-sectoral engagement. Importantly,
the solution's open source, web based design, suits for application by a
broader audience with limited resources and enables users to tailor and develop
it for their own needs.

</details>


### [77] [A Holistic Architecture for Monitoring and Optimization of Robust Multi-Agent Path Finding Plan Execution](https://arxiv.org/abs/2509.10284)
*David Zahrádka,Denisa Mužíková,David Woller,Miroslav Kulich,Jiří Švancara,Roman Barták*

Main category: cs.MA

TL;DR: Proposes a holistic architecture for robust execution of Multi-Agent Path Finding plans with monitoring and optimization to handle robot delays and determine when to search for alternate plans.


<details>
  <summary>Details</summary>
Motivation: Robot delays during MAPF plan execution can introduce collision risks and significantly impact execution duration, but searching for alternate plans is costly and needs to be timed appropriately.

Method: Uses Action Dependency Graph for robust execution to estimate expected execution duration and predict when finding an alternate plan would lead to shorter execution.

Result: Empirically evaluated in a real-time simulator designed to mimic an autonomous warehouse robotic fleet demonstrator.

Conclusion: The proposed architecture effectively monitors and optimizes MAPF plan execution by predicting optimal times to search for alternate plans when delays occur.

Abstract: The goal of Multi-Agent Path Finding (MAPF) is to find a set of paths for a
fleet of agents moving in a shared environment such that the agents reach their
goals without colliding with each other. In practice, some of the robots
executing the plan may get delayed, which can introduce collision risk.
Although robust execution methods are used to ensure safety even in the
presence of delays, the delays may still have a significant impact on the
duration of the execution. At some point, the accumulated delays may become
significant enough that instead of continuing with the execution of the
original plan, even if it was optimal, there may now exist an alternate plan
which will lead to a shorter execution. However, the problem is how to decide
when to search for the alternate plan, since it is a costly procedure. In this
paper, we propose a holistic architecture for robust execution of MAPF plans,
its monitoring and optimization. We exploit a robust execution method called
Action Dependency Graph to maintain an estimate of the expected execution
duration during the plan's execution. This estimate is used to predict the
potential that finding an alternate plan would lead to shorter execution. We
empirically evaluate the architecture in experiments in a real-time simulator
which we designed to mimic our real-life demonstrator of an autonomous
warehouse robotic fleet.

</details>
