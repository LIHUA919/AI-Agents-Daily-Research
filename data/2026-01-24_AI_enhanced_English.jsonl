{"id": "2601.16091", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16091", "abs": "https://arxiv.org/abs/2601.16091", "authors": ["Saar Cohen"], "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals", "comment": "To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026", "summary": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.", "AI": {"tldr": "Online clustering with delayed assignments in stochastic arrival model achieves constant competitive ratio against optimal offline clustering.", "motivation": "Traditional online clustering requires immediate assignment decisions upon point arrival, which can lead to poor cluster quality. Allowing delayed assignments with delay costs provides flexibility but creates the challenge of balancing clustering quality against delay costs.", "method": "A new framework for online non-centroid clustering with delays where points arrive sequentially in a metric space. Decisions can be postponed with delay costs. The paper focuses on stochastic arrival model where points are drawn independently from an unknown fixed distribution over the metric space.", "result": "In worst-case arrival model, no algorithm achieves better than sublogarithmic competitive ratio. However, in stochastic arrival model, the authors devise an algorithm that achieves constant competitive ratio - the expected overall costs (clustering + delay) are bounded by a constant factor of optimal offline clustering as number of points grows.", "conclusion": "By moving beyond worst-case analysis to stochastic arrivals, it's possible to achieve constant competitive online clustering with delayed assignments, overcoming strong impossibility results in adversarial settings and providing hope for practical online clustering algorithms."}}
{"id": "2601.16187", "categories": ["cs.MA", "cs.GT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16187", "abs": "https://arxiv.org/abs/2601.16187", "authors": ["Pan-Yang Su", "Arwa Alanqary", "Bryce L. Ferguson", "Manxi Wu", "Alexandre M. Bayen", "Shankar Sastry"], "title": "Average Unfairness in Routing Games", "comment": "14 pages, 5 figures, 1 table. Accepted for publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.", "AI": {"tldr": "The paper introduces average unfairness as a new fairness measure in routing games, compares it with existing measures, and analyzes constrained system optimization with fairness constraints.", "motivation": "Existing unfairness measures in routing games (loaded unfairness and UE unfairness) don't capture average user experience. There's a need for a more comprehensive fairness measure that considers the ratio between average latency and minimum latency.", "method": "Proposes average unfairness measure, analyzes relationships between three unfairness measures, studies worst-case values characterized by latency function steepness, and examines constrained system optimum problem with unfairness constraints.", "result": "Shows worst-case values of all three unfairness measures coincide, average unfairness \u2264 loaded unfairness, and optimal flows under average unfairness constraints achieve lower total latency than those under loaded unfairness constraints.", "conclusion": "Average unfairness provides valuable theoretical insights for fairness-efficiency tradeoffs in network routing, with practical advantages over existing fairness measures in constrained optimization problems."}}
{"id": "2601.15487", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15487", "abs": "https://arxiv.org/abs/2601.15487", "authors": ["Chandan Kumar Sahu", "Premith Kumar Chilukuri", "Matthew Hetrich"], "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation", "comment": "12 pages, 2 figures, Submitted to ACL", "summary": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.", "AI": {"tldr": "MiRAGE is a multiagent framework that automatically generates domain-specific, multimodal, multi-hop QA datasets for evaluating RAG systems, addressing the lack of specialized benchmarks for complex technical documents.", "motivation": "Current RAG evaluation benchmarks are inadequate for multimodal, high-stakes enterprise applications as they rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is multimodal and requires synthesizing disjoint evidence.", "method": "MiRAGE uses a collaborative swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize expert personas and domains to mimic expert cognitive workflows.", "result": "Evaluation across four domains (regulations, finance, quantitative biology, journalism) shows MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Ablation studies indicate it works with LLMs when image descriptions are available, though visual grounding remains challenging.", "conclusion": "MiRAGE automates creation of gold standard evaluation datasets that reflect proprietary corpora's latent thematic structure, providing necessary infrastructure to rigorously benchmark next-generation information retrieval systems."}}
{"id": "2601.15551", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15551", "abs": "https://arxiv.org/abs/2601.15551", "authors": ["Bismack Tokoli", "Luis Jaimes", "Ayesha S. Dina"], "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance", "comment": "35 pages", "summary": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "AI": {"tldr": "ALIGNAgent is a multi-agent educational framework that integrates knowledge estimation, skill-gap identification, and personalized resource recommendation to create a cohesive adaptive learning system.", "motivation": "Existing personalized learning systems are fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle.", "method": "ALIGNAgent uses a multi-agent framework with a Skill Gap Agent that processes student data (quiz performance, gradebook data, learner preferences) to generate topic-level proficiency estimates and identify specific misconceptions. The Recommender Agent then retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop.", "result": "Extensive evaluation on authentic datasets from two undergraduate computer science courses shows ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "conclusion": "ALIGNAgent successfully integrates previously fragmented components of personalized learning systems into a cohesive adaptive framework, demonstrating strong performance in knowledge proficiency estimation and providing a foundation for more comprehensive personalized education."}}
