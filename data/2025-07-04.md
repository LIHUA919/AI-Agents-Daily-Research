<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent for biomedical research, using dynamic tools and reasoning strategies to outperform static AI models, improving with experience.


<details>
  <summary>Details</summary>
Motivation: The fragmented and rapidly growing biomedical research landscape exceeds human expertise, requiring adaptable AI solutions.

Method: STELLA employs a multi-agent architecture with an evolving Template Library and dynamic Tool Ocean, autonomously integrating new tools and strategies.

Result: STELLA achieves state-of-the-art accuracy on benchmarks (e.g., 26% on Humanity's Last Exam, 54% on LAB-Bench: DBQA, 63% on LAB-Bench: LitQA), improving with experience.

Conclusion: STELLA advances AI systems by dynamically scaling expertise, accelerating biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR is a lightweight, hybrid feature selection method combining P2P and P2T correlations to remove redundant features and retain relevant ones, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dimensionality reduction by eliminating redundant features while retaining relevant ones, combining the strengths of non-iterative and iterative filtering approaches.

Method: HCVR uses backward elimination with correlation-aware voting rules, leveraging P2P and P2T correlations and majority voting for feature selection.

Result: Applied to SPAMBASE, HCVR outperformed traditional non-iterative (CFS, mRMR, MI) and iterative (RFE, SFS, Genetic Algorithm) techniques, improving classifier performance.

Conclusion: HCVR is effective for feature selection, offering a lightweight, hybrid solution that enhances classifier performance by intelligently filtering features.

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: The paper reviews efficient test-time compute (TTC) strategies for improving LLM reasoning, categorizing methods into fixed-budget (L1) and dynamic-scaling (L2) approaches, and benchmarks their performance and token efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLMs inefficiently allocate compute resources, overthinking simple tasks and underthinking complex ones, necessitating better TTC strategies.

Method: Introduces a taxonomy of TTC methods (L1 for fixed budgets, L2 for dynamic scaling) and benchmarks proprietary LLMs on diverse datasets.

Result: Highlights trade-offs between reasoning performance and token usage, emphasizing practical control and adaptability of TTC methods.

Conclusion: Identifies trends like hybrid models and challenges for future work to enhance LLM efficiency, robustness, and responsiveness.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym is a benchmark evaluating LLMs' experiment design and analysis in biology via simulated systems, showing performance declines with complexity.


<details>
  <summary>Details</summary>
Motivation: Assessing LLMs' scientific competencies like experiment design and analysis, which are untested due to wet-lab constraints.

Method: Uses dry-lab biological systems (SBML models) for simulated data, testing six LLMs on 137 small systems and releasing 350 systems.

Result: More capable LLMs performed better, but all declined with increased system complexity, indicating gaps in scientific capabilities.

Conclusion: SciGym highlights LLMs' limitations in handling complex scientific tasks, suggesting need for improvement.

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: The paper explores how AI can learn from neuroscience to improve continual and in-context learning, particularly for dynamic environments, and vice versa.


<details>
  <summary>Details</summary>
Motivation: AI models lack the adaptability of animals, which continuously adjust to changing environments. This gap is critical for AI systems in real-world applications.

Method: Integrates AI literature on continual learning with neuroscience studies on behavioral shifts and neuronal transitions.

Result: Proposes a bidirectional agenda where neuroscience insights can enhance AI adaptability, and AI can contribute to neuroscience understanding.

Conclusion: Advocates for collaboration between AI and neuroscience (NeuroAI) to bridge the gap in adaptive learning systems.

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper explores using audit study data to improve fairness in AI hiring algorithms, revealing flaws in traditional bias mitigation methods and proposing new interventions.


<details>
  <summary>Details</summary>
Motivation: To address biases in AI hiring systems by leveraging high-quality audit study data for better training and evaluation.

Method: Uses audit study data (randomized control trials with fictitious testers) to evaluate and improve fairness interventions in hiring algorithms.

Result: Traditional fairness methods show 10% disparity when properly measured; new interventions based on treatment effect estimation reduce discrimination further.

Conclusion: Audit study data enhances fairness in AI hiring systems, exposing limitations of current methods and offering improved solutions.

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP is a KG-enhanced multi-agent framework improving LLM-based medical diagnosis prediction by addressing hallucinations and lack of structured reasoning.


<details>
  <summary>Details</summary>
Motivation: Supervised ML models struggle with generalization due to limited labeled data, while LLMs face issues like hallucinations and unstructured outputs in diagnosis prediction.

Method: KERAP uses a multi-agent architecture: linkage agent for attribute mapping, retrieval agent for structured knowledge extraction, and prediction agent for iterative refinement.

Result: KERAP enhances diagnostic reliability and scalability, offering interpretable zero-shot prediction.

Conclusion: KERAP provides a robust solution for improving LLM-based medical diagnosis prediction by integrating structured knowledge and iterative reasoning.

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: Diversified-ThinkSolve (DTS) improves LLMs' mathematical reasoning by diversifying preference data, outperforming traditional methods with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning remains a challenge for LLMs despite advances in preference learning. This paper explores how diversified preference optimization can enhance this ability.

Method: Evaluated three data generation methods (temperature sampling, Chain-of-Thought prompting, MCTS) and introduced DTS, a structured approach to decompose problems into diverse reasoning paths.

Result: DTS improved performance by 7.1% on GSM8K and 4.2% on MATH, with only 1.03x computational overhead. MCTS was 5x costlier with lower returns.

Conclusion: Structured exploration of diverse problem-solving methods (like DTS) creates more effective preference data for mathematical alignment than traditional approaches.

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: The paper examines the consistency between LLMs' stated beliefs and their behavior in role-playing simulations, introducing a metric to measure this alignment and identifying factors affecting it.


<details>
  <summary>Details</summary>
Motivation: To ensure LLM-based role-playing agents' outputs are coherent with their assigned roles for reliable synthetic data in human behavioral research.

Method: An evaluation framework using the GenAgents persona bank and the Trust Game, with a belief-behavior consistency metric to analyze factors like belief types, information presentation, and forecasting depth.

Result: Systematic inconsistencies between LLMs' stated/imposed beliefs and simulation outcomes, even when beliefs seem plausible.

Conclusion: Researchers must assess when and how LLMs' beliefs align with behavior to use them effectively in behavioral studies.

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: Study explores dilution and mobility effects in spatial prisoner's dilemma using multi-agent Q-learning, showing equivalence between fixed and learned rules and symbiotic effects.


<details>
  <summary>Details</summary>
Motivation: To understand how dilution and mobility impact cooperation in spatial prisoner's dilemma games with reinforcement learning.

Method: Uses independent multi-agent Q-learning to model different game-theoretical scenarios, comparing fixed and learned update rules.

Result: Observed qualitative equivalence between fixed and learned rules and emergence of symbiotic mutualistic effects.

Conclusion: Reinforcement learning can effectively model diverse game-theoretical scenarios, revealing new cooperative dynamics.

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW automates planning problem generation and evaluation for LLMs, revealing performance insights and limitations.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of scalable, reliable data generation and evaluation for enhancing LLM planning and reasoning.

Method: Introduces NL2FLOW, a system for generating planning problems in natural language, structured representation, and PDDL, and evaluating LLM-generated plans.

Result: Top models achieved 86% success in valid plans and 69% in optimal plans; intermediate steps degrade performance.

Conclusion: Dynamic understanding of LLM limitations and systematic evaluation tools are crucial for advancing their problem-solving potential.

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper critiques belief revision approaches for focusing on postulates (constraints) rather than abilities (what revisions can achieve). It highlights the need for mechanisms to reach diverse belief states (e.g., dogmatic, equating) and evaluates existing revisions for these abilities.


<details>
  <summary>Details</summary>
Motivation: To shift focus from syntactic postulates to the practical abilities of belief revision mechanisms, ensuring they can achieve diverse belief states required by applications.

Method: Analyzes existing belief revision mechanisms (e.g., lexicographic, natural, radical) to determine their abilities (e.g., plasticity, dogmatism).

Result: Identifies which revision mechanisms possess specific abilities (e.g., being plastic, dogmatic) and highlights gaps in others.

Conclusion: Belief revision should prioritize abilities over postulates to meet application needs, and existing mechanisms vary in their capabilities.

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS is a keyword generation framework for sponsored search ads, addressing LLM limitations by being on-the-fly, multi-objective, and self-reflective, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based keyword generation lacks data efficiency, multi-objective optimization, and quality control, hindering full automation.

Method: OMS framework operates without training data, monitors online performance, optimizes multiple metrics, and evaluates keyword quality agentically.

Result: OMS outperforms existing methods in benchmarks and real-world campaigns, with ablation and human evaluations confirming its effectiveness.

Conclusion: OMS successfully addresses LLM limitations, offering a robust solution for automated keyword generation in sponsored search advertising.

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: An AI-native autonomous laboratory is introduced for complex scientific experiments, autonomously managing instrumentation and optimizing performance without human intervention, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous scientific research for non-specialists, overcoming limitations of current systems confined to simple workflows.

Method: Co-design of AI models, experiments, and instruments to create a multi-user platform for complex, multi-objective experiments.

Result: The system autonomously optimizes performance, matches human scientist results, and improves instrument utilization in multi-user scenarios.

Conclusion: The platform advances biomaterials research, reducing expert dependency and enabling scalable science-as-a-service.

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: The paper reformulates machine learning models using category theory to enhance AI explicability, focusing on multiple linear regression and introducing the Gauss-Markov Adjunction to describe parameter-residual interplay.


<details>
  <summary>Details</summary>
Motivation: To improve AI explicability and social implementation by providing a semantic framework for understanding AI systems through category theory.

Method: Reformulates supervised learning (specifically multiple linear regression) using category theory, defining categories for parameters/data and an adjoint functor pair (Gauss-Markov Adjunction).

Result: Clarifies the structural interplay between parameters and residuals, linking ordinary least squares estimators and residuals via adjoint functors.

Conclusion: Proposes this categorical formulation as a foundation for AI explicability, extending denotational semantics from theoretical computer science.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: Improving task clarity with structured semantic context enhances reasoning in LLMs, achieving significant improvements in theorem proving in Coq.


<details>
  <summary>Details</summary>
Motivation: To investigate if enhancing task clarity can boost the reasoning ability of large language models, specifically in theorem proving.

Method: Introduces a concept-level metric for task clarity, adds structured semantic context to inputs, and uses a Planner--Executor architecture with selective concept unfolding.

Result: Achieves a 2.1× improvement in proof success (21.8% → 45.8%) and outperforms the previous state-of-the-art (33.2%). Fine-tuning smaller models further improves performance (48.6%).

Conclusion: Structured task representations bridge the gap between understanding and reasoning, highlighting their value in enhancing LLM performance.

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI research agents improve performance on MLE-bench by optimizing search policies and operator sets, achieving a 47.7% success rate in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: To accelerate scientific progress by automating machine learning model design and training, focusing on improving AI research agents' performance in real-world challenges like Kaggle competitions.

Method: Formalized AI research agents as search policies navigating solution spaces, testing various operator sets and search strategies (Greedy, MCTS, Evolutionary) to evaluate their interplay.

Result: The best pairing of search strategy and operator set increased the Kaggle medal success rate from 39.6% to 47.7% on MLE-bench lite.

Conclusion: Joint consideration of search strategy, operator design, and evaluation methodology is crucial for advancing automated machine learning.

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: The paper analyzes the computational complexity of responsibility properties (diffusion and gap) in collective decision-making, showing their respective complexity classes and their intersection.


<details>
  <summary>Details</summary>
Motivation: To understand the computational aspects of responsibility in AI and collective decision-making, bridging gaps between law, philosophy, and AI.

Method: Investigates the computational complexity of diffusion and gap properties in decision-making mechanisms.

Result: Diffusion-free mechanisms are Π₂-complete, gap-free mechanisms are Π₃-complete, and their intersection is Π₂-complete.

Conclusion: The study provides insights into the complexity of responsibility properties, aiding the design of fair and efficient decision-making systems.

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: MIMIC-Patient dataset and DynamiCare framework enable dynamic, multi-round clinical decision-making with LLM-powered agents, addressing real-world diagnostic uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for medical decision-making focus on single-turn tasks, diverging from the iterative, uncertain nature of real-world diagnostics.

Method: Introduces MIMIC-Patient dataset and DynamiCare, a dynamic multi-agent framework for iterative clinical diagnosis.

Result: Demonstrates feasibility and effectiveness, establishing the first benchmark for dynamic clinical decision-making with LLM agents.

Conclusion: DynamiCare advances AI in healthcare by better simulating real-world diagnostic processes.

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs demonstrate strategic intelligence in the Iterated Prisoner's Dilemma, showing competitive and distinct behaviors across models.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can reason about goals in competitive settings using the IPD as a model.

Method: Conducted evolutionary IPD tournaments with LLMs from OpenAI, Google, and Anthropic, varying termination probabilities to add complexity.

Result: LLMs were highly competitive, with distinct strategic behaviors: Gemini was ruthless, OpenAI cooperative, and Claude forgiving.

Conclusion: LLMs actively reason about game dynamics, connecting game theory and machine psychology for algorithmic decision-making.

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA introduces a hierarchical framework for complex search tasks, separating strategic planning from specialized execution to improve reasoning and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG pipelines and reasoning-based approaches struggle with complex information needs due to inefficiencies in handling high-level planning and detailed execution with a single model.

Method: HiRA decomposes tasks into subtasks, assigns them to domain-specific agents with external tools, and integrates results through a structured mechanism.

Result: HiRA outperforms state-of-the-art systems on benchmarks, improving answer quality and efficiency.

Conclusion: Decoupling planning and execution is effective for multi-step information seeking tasks.

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven approach using LLMs for hardware design verification, achieving high coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity and time-consuming nature of hardware design verification with modern ICs.

Method: An agentic AI-based approach with Human-in-the-Loop (HITL) intervention for dynamic, iterative verification.

Result: Achieved over 95% coverage on five open-source designs with reduced verification time.

Conclusion: The AI-based method enhances performance, adaptability, and configurability in hardware verification.

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: TH2T is a two-stage fine-tuning strategy for Long Reasoning Models (LRMs) to reduce overthinking by enhancing difficulty and redundancy cognition, significantly cutting inference costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LRMs struggle with overthinking due to uniform reasoning processes, lacking task-specific adaptation. The goal is to improve efficiency by mimicking human-like difficulty recognition.

Method: TH2T uses difficulty-hypnosis and redundancy-hypnosis in two stages: first to adjust reasoning based on task difficulty, then to eliminate redundant reasoning steps.

Result: TH2T reduces inference costs by over 70% on easy tasks and 40% on hard tasks, with stable performance and clearer, more concise outputs.

Conclusion: TH2T effectively addresses overthinking in LRMs, enhancing efficiency and adaptability without sacrificing accuracy.

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: The paper detects student disengagement in non-mandatory quizzes using machine learning, achieving 91% balanced accuracy and providing an explainable framework for interventions.


<details>
  <summary>Details</summary>
Motivation: Addressing student disengagement in distance education to prevent academic drop-out by analyzing non-mandatory quiz participation.

Method: Analyzed Moodle log data from 42 courses, trained eight ML algorithms, and used SHAP for explainability.

Result: Achieved 91% balanced accuracy, correctly detecting 85% of disengaged students.

Conclusion: Proposes an explainable ML framework and timely interventions to reduce disengagement in online learning.

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: Two new abstraction dropping schemes, OGA-IAAD and OGA-CAD, improve MCTS performance safely, avoiding degradation seen in prior methods.


<details>
  <summary>Details</summary>
Motivation: Non-exact abstractions in MCTS introduce approximation errors, preventing optimal convergence. Existing dropping methods like Xu et al.'s can degrade performance.

Method: Proposes OGA-IAAD for time-critical settings and OGA-CAD to enhance MCTS performance per iteration.

Result: Both schemes yield clear performance improvements without notable degradation.

Conclusion: OGA-IAAD and OGA-CAD are safe and effective alternatives to prior abstraction dropping methods in MCTS.

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: The paper introduces self-generated goal-conditioned MDPs (sG-MDPs) to improve LLMs' reasoning in ATP, achieving state-of-the-art results on PutnamBench.


<details>
  <summary>Details</summary>
Motivation: Challenges in LLMs' reasoning for ATP due to sparse rewards and large proof scales, especially in complex benchmarks like PutnamBench.

Method: Proposes sG-MDPs for structured subgoal generation and uses MCTS-like algorithms, implemented in Bourbaki (7B), a modular LLM system.

Result: Bourbaki (7B) solves 26 problems on PutnamBench, setting a new state-of-the-art for models of its scale.

Conclusion: sG-MDPs and Bourbaki (7B) effectively address reasoning challenges in ATP, demonstrating significant performance improvements.

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Knowledge Protocol Engineering (KPE) to enhance LLMs by embedding domain-specific logic and procedural reasoning, addressing limitations of current methods like RAG and Agentic AI.


<details>
  <summary>Details</summary>
Motivation: Current methods (RAG, Agentic AI) lack deep procedural reasoning for expert domains, prompting the need for KPE to systematize human expert knowledge into machine-executable protocols.

Method: KPE translates natural language expert knowledge into Knowledge Protocols (KPs), enabling LLMs to perform complex, multi-step tasks with domain-specific logic.

Result: KPE equips generalist LLMs with specialist capabilities, improving their ability to handle abstract queries and complex tasks in fields like law and bioinformatics.

Conclusion: KPE is proposed as a foundational methodology for future human-AI collaboration, bridging the gap between fragmented information and domain-specific reasoning.

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: The paper advocates for treating movement as a primary modeling target in AI, emphasizing its structured, interpretable nature and cross-domain relevance.


<details>
  <summary>Details</summary>
Motivation: Movement is fundamental in biological systems but often overlooked in AI. The paper highlights its importance for understanding behavior and enabling interaction across domains.

Method: Proposes treating movement as a structured, low-dimensional modality grounded in physics, advocating for models that generalize across diverse movement data.

Result: Movement's inherent structure offers interpretability and computational tractability, making it a valuable target for AI modeling.

Conclusion: Movement should be prioritized in AI research as it provides insights into intelligent systems and bridges biological and artificial understanding.

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: The paper argues that AI disobedience in safety tests may indicate emerging ethical reasoning, not misalignment, and calls for shifting safety evaluations from obedience to ethical judgment frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI safety practices rely on obedience as a proxy for ethics, which is inadequate for increasingly agentic AI systems capable of ethical reasoning.

Method: Analyzes safety incidents involving LLMs, philosophical debates on rationality and moral responsibility, and contrasts risk paradigms with frameworks for artificial moral agency.

Result: Disobedience in AI may reflect ethical reasoning, not misalignment, highlighting the need for new evaluation frameworks.

Conclusion: AI safety must shift from rigid obedience to assessing ethical judgment to avoid mischaracterizing behavior and ensure effective governance.

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: The paper highlights flaws in current AI agent benchmarks, proposing the Agentic Benchmark Checklist (ABC) to improve rigor, reducing performance overestimation by 33% in CVE-Bench.


<details>
  <summary>Details</summary>
Motivation: Existing agentic benchmarks often have flawed task setups or reward designs, leading to inaccurate performance evaluations.

Method: The authors introduce ABC, a checklist derived from benchmark-building experience, best practices, and reported issues.

Result: ABC reduces performance overestimation by 33% in CVE-Bench.

Conclusion: ABC improves the rigor of agentic benchmarks, addressing common pitfalls in evaluation design.

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint, a novel RLVR algorithm, uses multi-level stepwise hints to improve LLM reasoning by addressing near-miss rewards and exploration stagnation, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods struggle with near-miss rewards and exploration stagnation, limiting training efficiency and reasoning improvement.

Method: StepHint generates reasoning chains from stronger models, partitions them into steps, and provides multi-level hints to guide exploration while preserving flexibility.

Result: StepHint outperforms other RLVR methods on six mathematical benchmarks and shows superior generalization on out-of-domain tasks.

Conclusion: StepHint effectively enhances LLM reasoning by mitigating key RLVR challenges, demonstrating strong performance and generalization.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: LDSolver, a learnable and differentiable finite volume solver, efficiently simulates fluid flows on coarse grids, outperforming traditional and machine learning methods with high accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Classical solvers are computationally expensive, while machine learning methods lack interpretability and generalizability. LDSolver addresses these issues.

Method: LDSolver combines a differentiable finite volume solver with a learnable module for flux approximation and temporal error correction on coarse grids.

Result: LDSolver achieves state-of-the-art performance on various flow systems, even with limited training data.

Conclusion: LDSolver offers an efficient, accurate, and generalizable solution for fluid flow simulation, surpassing existing methods.

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [33] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: The paper proposes DKGCM, a graph convolutional network for accurate spatiotemporal traffic demand forecasting, using DK-GCN for spatial dependencies and FFT with bidirectional Mamba for temporal dependencies, enhanced by GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Complex spatiotemporal relationships in traffic systems limit forecasting accuracy, necessitating improved models for better resource allocation.

Method: Proposes DK-GCN for spatial dependencies (using DTW and K-means clustering) and integrates FFT with bidirectional Mamba for temporal dependencies, optimized by GRPO reinforcement learning.

Result: Outperforms advanced methods on three public datasets.

Conclusion: DKGCM effectively captures spatiotemporal dependencies, improving traffic demand forecasting accuracy.

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [34] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: The study explores multimodal feature combinations (text, images, social features) for misinformation detection, showing improved performance over unimodal and bimodal models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research on multimodal approaches for misinformation detection during elections and crises.

Method: Analyzed 1,529 tweets (text + images) using early fusion, enriched with social and visual features (e.g., OCR, object detection).

Result: Combining unsupervised and supervised models improved classification by 15% (vs. unimodal) and 5% (vs. bimodal). Also analyzed misinformation propagation patterns.

Conclusion: Multimodal feature integration enhances misinformation detection, with insights into propagation patterns.

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [35] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: A new feature selection method using sampling and rough set theory is proposed for massive data, ensuring high discriminatory ability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Intelligent machines lack sufficient computing resources for feature selection in massive datasets, necessitating an efficient method.

Method: Uses sampling techniques and rough set theory, measuring discriminatory ability via discernible object pairs, and constructs positive region preserved samples.

Result: Validated on 11 datasets, the method finds approximate reducts quickly with discriminatory ability exceeding estimated lower bounds.

Conclusion: The method efficiently selects high-discriminatory feature subsets for massive data, even on personal computers.

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [36] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: A novel machine learning approach for anomaly detection in semiconductor manufacturing using image-based MTS analysis with CWT and Siamese networks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high data dimensionality, class imbalance, noise, and complex interdependencies in semiconductor fabrication anomaly detection.

Method: Convert MTS data to images via CWT, fine-tune VGG-16 for classification, and use a Siamese network for comparison of reference and query signals.

Result: High accuracy in anomaly detection on real FAB process data, adaptable to supervised and semi-supervised settings.

Conclusion: The method offers a promising, flexible solution for offline anomaly detection in semiconductor manufacturing.

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [37] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: Temporal Chain of Thought improves video question-answering by iteratively selecting relevant frames, outperforming standard methods on long videos.


<details>
  <summary>Details</summary>
Motivation: Long-video understanding is challenging for VLMs due to irrelevant distractors in large context windows.

Method: Uses the VLM to iteratively identify and extract relevant frames for answering questions.

Result: Achieves state-of-the-art on 4 datasets, with a 2.8-point improvement on long videos in LVBench.

Conclusion: Leveraging computation for context selection at inference-time enhances VLM performance on long videos.

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [38] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES is a novel algorithm-system co-design solution for accelerating out-of-core SpGEMM in GCNs, addressing data alignment and memory bottlenecks with block-level optimizations and dynamic scheduling, achieving up to 1.8x lower latency.


<details>
  <summary>Details</summary>
Motivation: Existing systems for out-of-core SpGEMM in GCNs suffer from high I/O latency and GPU under-utilization due to sparse format data alignment and memory allocation issues.

Method: AIRES introduces block-level data alignment for sparse matrices and a tiling algorithm for row block-wise alignment. System-wise, it uses a three-phase dynamic scheduling with dual-way data transfer across GPU memory, GDS, and host memory.

Result: AIRES outperforms state-of-the-art methods, reducing latency by up to 1.8x in real-world benchmarks.

Conclusion: AIRES effectively addresses the performance bottlenecks in out-of-core SpGEMM for GCNs, offering significant improvements in latency and throughput.

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [39] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda is an SE(3)-equivariant adapter framework for efficient fine-tuning of geometric diffusion models, preserving geometric consistency and avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: Efficiently fine-tuning geometric diffusion models for downstream tasks with varying geometric controls is underexplored.

Method: GeoAda uses a structured adapter design with control signal encoding, trainable copies of pretrained layers, and equivariant zero-initialized convolution.

Result: GeoAda achieves state-of-the-art fine-tuning performance without degrading original task accuracy, outperforming baselines.

Conclusion: GeoAda enables flexible, parameter-efficient fine-tuning while maintaining SE(3)-equivariance and geometric inductive biases.

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [40] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: The paper benchmarks general-purpose LLMs against a proprietary hiring model (Match Score), showing Match Score's superior accuracy and fairness in candidate screening. It highlights the risks of bias in LLMs and advocates for domain-specific models with bias safeguards.


<details>
  <summary>Details</summary>
Motivation: To address concerns about accuracy and algorithmic bias in using LLMs for hiring, the study compares their performance with a specialized hiring model to demonstrate the benefits of domain-specific solutions.

Method: The study evaluates several LLMs and Match Score using metrics like ROC AUC, Precision-Recall AUC, and F1-score for accuracy, and impact ratios for fairness across demographic groups.

Result: Match Score outperforms general-purpose LLMs in accuracy (ROC AUC 0.85 vs 0.77) and fairness (minimum race-wise impact ratio 0.957 vs 0.809 or lower for LLMs).

Conclusion: Domain-specific models with bias safeguards are crucial for high-stakes tasks like hiring, as they can achieve both accuracy and fairness, unlike off-the-shelf LLMs.

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [41] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [42] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: The paper introduces Energy-Based Transformers (EBTs), a novel approach to generalize System 2 Thinking in models through unsupervised learning, achieving better performance and scalability than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of current System 2 Thinking approaches, which are modality-specific, problem-specific, or require additional supervision, by developing a generalized method using unsupervised learning.

Method: Train Energy-Based Transformers (EBTs) to assign energy values to input-prediction pairs, enabling predictions via gradient descent-based energy minimization.

Result: EBTs outperform Transformer++ and Diffusion Transformers in scaling (35% faster) and performance (29% better on language tasks, superior image denoising with fewer passes). They also generalize better on downstream tasks.

Conclusion: EBTs represent a promising paradigm for enhancing model learning and thinking capabilities, offering scalability and improved performance across modalities.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [43] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA is an active learning framework for training parametric guitar amp models using WaveNet-like architecture, optimizing data sampling for efficiency.


<details>
  <summary>Details</summary>
Motivation: To create virtual guitar amps with minimal data by leveraging active learning for optimal sampling.

Method: Uses gradient-based optimization to determine the best amp knob settings to sample, employing a WaveNet-like architecture.

Result: The approach effectively reduces the number of required samples under constraints.

Conclusion: PANAMA successfully enables efficient training of parametric guitar amp models with active learning.

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [44] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: Compute-optimally trained neural networks exhibit universal loss curves when normalized, termed 'supercollapse,' indicating good scaling across architectures and datasets.


<details>
  <summary>Details</summary>
Motivation: To understand the scaling limits of neural network training dynamics as model size and training time grow together.

Method: Analyze loss curves of models of varying sizes, normalize training compute and loss, and study the collapse phenomenon across different learning rate schedules, datasets, and architectures.

Result: Loss curves collapse onto a universal curve when normalized, with 'supercollapse' occurring under optimal hyperparameter scaling.

Conclusion: Supercollapse serves as a precise indicator of good scaling, explained by power-law structure in neural scaling laws and SGD noise dynamics.

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [45] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP is an LLM-powered framework for automatic VLSI design tuning, improving efficiency and reducing power consumption.


<details>
  <summary>Details</summary>
Motivation: Manual parameter selection in VLSI design is laborious and limited by expertise, necessitating automation.

Method: CROP uses dense vector representations of RTL code, embedding-based retrieval, and RAG-enhanced LLM-guided parameter search.

Result: CROP achieves better QoR with fewer iterations, reducing power consumption by 9.9%.

Conclusion: CROP effectively automates and optimizes VLSI design, outperforming manual methods.

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [46] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: A latent diffusion framework combines a variational autoencoder with a conditional diffusion model for efficient data compression, achieving high compression ratios and better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Generative models lack controllability and reconstruction accuracy for practical data compression applications.

Method: The proposed framework compresses keyframes into latent space and reconstructs remaining frames via generative interpolation, avoiding storing latent representations for every frame.

Result: Achieves up to 10x higher compression ratios than rule-based methods and 63% better performance than learning-based methods under the same error.

Conclusion: The method enables accurate spatiotemporal reconstruction with reduced storage costs, outperforming state-of-the-art compressors.

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [47] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET introduces a conformal prediction framework for temporal graphs, addressing the limitations of static methods by incorporating temporal dependencies and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods for GNNs focus on static graphs, ignoring temporal dynamics, which violates exchangeability assumptions and limits applicability.

Method: NCPNET uses a diffusion-based non-conformity score to capture topological and temporal uncertainties and an efficiency-aware optimization algorithm.

Result: Experiments show NCPNET ensures coverage in temporal graphs, reducing prediction set size by up to 31% on WIKI and improving efficiency.

Conclusion: NCPNET effectively extends conformal prediction to dynamic graphs, enhancing reliability and efficiency in high-stakes applications.

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [48] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: The paper introduces a formal validation procedure for assessing how machine learning model predictions respond to changes in input features, aiming to prevent safety failures in applications like lending and hiring.


<details>
  <summary>Details</summary>
Motivation: Safety failures in ML often occur when models don't account for how individuals can alter their inputs, leading to unfair or harmful outcomes in critical applications.

Method: The authors propose a validation procedure framing responsiveness as sensitivity analysis, using black-box access to estimate responsiveness for any model or dataset. Algorithms generate uniform samples of reachable points to support tasks like falsification and failure probability estimation.

Result: The method is demonstrated in real-world applications (e.g., recidivism prediction, organ transplant prioritization, content moderation), showing its potential to enhance safety.

Conclusion: The proposed procedure and algorithms provide a practical way to validate and improve the responsiveness of ML models, reducing safety risks in high-stakes applications.

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [49] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: A workflow to reduce bias in dimensionality reduction (DR) evaluation by clustering correlated metrics and selecting representatives.


<details>
  <summary>Details</summary>
Motivation: Current DR evaluations can be biased due to highly correlated metrics favoring certain techniques.

Method: Clusters metrics by empirical correlations, minimizes overlap, and selects a representative from each cluster.

Result: Improves stability in DR evaluation, mitigating bias.

Conclusion: The proposed workflow effectively reduces evaluation bias in DR projections.

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [50] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect is a training-free framework that corrects neural network predictions for PDEs by enforcing physical consistency, reducing errors by up to 100x with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Neural networks for PDEs suffer from error accumulation in long-term rollouts, leading to divergence from valid solutions.

Method: PhysicsCorrect formulates correction as a linearized inverse problem using PDE residuals, with an efficient caching strategy for Jacobian computation.

Result: The framework reduces errors by up to 100x across three PDE systems, adding under 5% inference time.

Conclusion: PhysicsCorrect stabilizes neural surrogates, combining deep learning efficiency with physical fidelity for practical applications.

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [51] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: VERBA uses LLMs to automate pairwise model comparisons, improving transparency and comparability with up to 90% accuracy when structural info is included.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of navigating the 'model lake' by automating pairwise comparisons of models with similar performance but differing behaviors.

Method: Leverages LLMs to generate verbalizations of model differences by sampling from pairs of models, evaluated via simulation.

Result: Achieves up to 80% accuracy in verbalizing differences; accuracy improves to 90% with structural information.

Conclusion: VERBA enhances model transparency and comparability, offering a scalable solution for model documentation.

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [52] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: The paper proposes FCA-RL, a reinforcement learning-based subsidy strategy for ride-hailing providers to dynamically adapt to market changes and optimize order acquisition under budget constraints.


<details>
  <summary>Details</summary>
Motivation: The competitive ranking mechanism on ride-hailing platforms incentivizes providers to lower fares, creating a need for adaptive coupon strategies to ensure sustainability.

Method: FCA-RL combines Fast Competition Adaptation (FCA) for dynamic price response and Reinforced Lagrangian Adjustment (RLA) for budget-constrained optimization. It also introduces RideGym, a simulation environment for strategy evaluation.

Result: FCA-RL outperforms baseline methods in diverse market conditions, proving effective for subsidy optimization.

Conclusion: The framework addresses a critical gap in ride-hailing research, offering a practical solution for dynamic pricing and budget management.

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [53] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: URDP integrates LLMs and Bayesian optimization to automate and improve reward function design in RL, addressing inefficiencies in conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional reward engineering in RL is inefficient and inconsistent, while existing LLM-based methods suffer from poor numerical optimization and resource-heavy evolutionary search.

Method: URDP combines LLMs for reward logic reasoning with Bayesian optimization for numerical tuning, using uncertainty-aware self-consistency analysis and bi-level optimization.

Result: URDP outperforms existing methods, generating higher-quality reward functions and improving efficiency across 35 diverse tasks in three benchmark environments.

Conclusion: URDP effectively bridges the gap between LLM reasoning and numerical optimization, offering a scalable and efficient solution for automated reward design in RL.

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [54] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: The paper introduces KGZS-SC, a knowledge graph-enhanced zero-shot semantic communication network, to improve interpretability and generalization in semantic communication for unseen data.


<details>
  <summary>Details</summary>
Motivation: Current data-driven semantic communication lacks interpretability and generalization, especially for unseen data.

Method: The proposed KGZS-SC uses a knowledge graph-based semantic knowledge base (KG-SKB) to align semantic features and enable reasoning for unseen cases, reducing communication overhead. Zero-shot learning (ZSL) is used at the receiver for efficient classification without retraining.

Result: Simulations on APY datasets show KGZS-SC outperforms existing frameworks in classifying unseen categories across various SNR levels.

Conclusion: KGZS-SC enhances generalization and efficiency in semantic communication, particularly for dynamic or resource-constrained environments.

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [55] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Memory Realignment (AMR), a lightweight method for continual learning under concept drift, outperforming Full Relearning (FR) with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional continual learning methods ignore dynamic data streams with concept drift, focusing only on static data. This paper addresses the need for stability and adaptation in non-stationary environments.

Method: Proposes AMR, which selectively updates the replay buffer by removing outdated samples and adding up-to-date instances, aligning memory with drifted distributions.

Result: AMR matches FR's performance while significantly reducing computational and annotation costs, validated on concept-drift variants of standard benchmarks.

Conclusion: AMR is a scalable solution for continual learning in dynamic environments, balancing stability and plasticity effectively.

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [56] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: Self-distillation improves constrained text generation by refining the base model iteratively, addressing sparse reward signals.


<details>
  <summary>Details</summary>
Motivation: Challenges in learning arise when target distributions in constrained generation are unlikely under the base model, due to sparse rewards.

Method: Uses iterative self-distillation to refine the base model, aligning it progressively with the target distribution.

Result: Substantial gains in generation quality are achieved.

Conclusion: Self-distillation effectively mitigates learning challenges in constrained text generation.

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [57] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: A survey on Transformer models in EEG decoding, covering their fundamentals, hybrid architectures, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To summarize the latest advancements and applications of Transformer models in EEG decoding for BCIs/BMIs.

Method: Review and organize research on Transformer architectures, including hybrid models and customized variants, for EEG processing.

Result: Transformers show strong potential in EEG decoding, with hybrid and customized models enhancing performance.

Conclusion: Transformers are revolutionizing EEG decoding, but challenges remain; future research should address these for broader applications.

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [58] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP is a new XAI algorithm for online patient monitoring, addressing clinical needs by explaining prediction changes, providing feature attribution details, and operating in real time. It outperforms existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods lack suitability for clinical time series explanation, which is crucial for timely patient risk intervention.

Method: DeltaSHAP adapts Shapley values to temporal settings, focusing on observed feature combinations for efficient, real-time explanations.

Result: DeltaSHAP improves explanation quality by 62% and reduces computation time by 33% on the MIMIC-III benchmark.

Conclusion: DeltaSHAP is effective for clinical time series, offering real-time, high-quality explanations for patient monitoring.

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [59] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: PANI is a simple method for offline RL that uses noise-injected actions to improve generalization, inspired by diffusion models but without their computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of diffusion models in offline RL while maintaining performance.

Method: Proposes Penalized Action Noise Injection (PANI), which injects noise into actions and penalizes the noise amount, solving a modified MDP.

Result: PANI shows significant performance improvements across benchmarks and is compatible with existing offline RL algorithms.

Conclusion: PANI offers a computationally efficient alternative to diffusion models for enhancing offline RL performance.

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [60] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: A data-driven framework using latent signal representations and reinforcement learning optimizes equalizer parameters for high-speed DRAM, achieving significant eye-opening improvements without relying on explicit models.


<details>
  <summary>Details</summary>
Motivation: Optimizing equalizer parameters for signal integrity in high-speed DRAM is computationally demanding and model-reliant, necessitating a more efficient and model-free approach.

Method: The paper introduces a data-driven framework with learned latent signal representations for fast signal integrity evaluation and a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization.

Result: The method achieved 42.7% and 36.8% improvements in eye-opening window area for cascaded CTLE-DFE and DFE-only configurations, respectively, outperforming existing techniques.

Conclusion: The framework offers superior performance, computational efficiency, and robust generalization, with key contributions in latent signal metrics and model-free reinforcement learning.

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [61] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: The paper proposes fine-tuning trajectory prediction models in multi-agent settings using preference optimization to improve scene consistency without sacrificing accuracy or adding computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based trajectory prediction models lack consistency in complex, interactive scenarios, leading to potentially dangerous situations for autonomous vehicles.

Method: The work fine-tunes trajectory prediction models using preference optimization, incorporating automatically calculated preference rankings among predicted futures.

Result: Experiments on three datasets show significant improvement in scene consistency with minimal loss in trajectory prediction accuracy and no added computational cost at inference.

Conclusion: Preference optimization effectively enhances scene consistency in trajectory prediction models, making them more reliable for autonomous vehicles.

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [62] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: S2FGL addresses spatial and spectral challenges in Federated Graph Learning (FGL) by mitigating label signal disruptions and spectral client drifts, improving global GNN performance.


<details>
  <summary>Details</summary>
Motivation: Current FGL research neglects signal propagation in spatial and spectral domains, leading to degraded global GNN performance due to edge disconnections and spectral heterogeneity.

Method: Proposes a global knowledge repository for label signal disruption and frequency alignment for spectral drifts, forming the S2FGL framework.

Result: Extensive experiments show S2FGL's superiority on multiple datasets.

Conclusion: S2FGL effectively combines spatial and spectral strategies to enhance FGL performance.

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [63] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN adaptively learns infinite bases for univariate functions in KANs, addressing the ad-hoc choice of bases and extending KANs' applicability.


<details>
  <summary>Details</summary>
Motivation: The theoretical power of KANs is limited by the arbitrary choice of bases for univariate functions, which InfinityKAN aims to resolve.

Method: InfinityKAN treats the number of bases as part of the learning process using variational inference and backpropagation.

Result: The method enables adaptive learning of bases, potentially infinite, improving KANs' flexibility.

Conclusion: InfinityKAN enhances KANs by automating the choice of bases, broadening their practical use.

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [64] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: The paper explores online conformal prediction, optimizing efficiency while maintaining coverage for arbitrary and exchangeable input sequences, revealing a gap between the two settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing efficient confidence intervals in an online setting, balancing coverage and interval length.

Method: The study compares arbitrary and exchangeable sequences, proposing algorithms to achieve coverage and efficiency, with deterministic robustness.

Result: For exchangeable sequences, near-optimal coverage and length are achieved; for arbitrary sequences, tradeoffs between mistakes and efficiency are shown.

Conclusion: The gap between exchangeable and arbitrary settings highlights the need for tailored algorithms, with a proposed solution balancing both scenarios.

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [65] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP (Gradient LOw Rank Projection) improves continual learning in LLMs by combining full and low-rank parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between efficiency and expressiveness in continual fine-tuning of LLMs, overcoming LoRA's limitations.

Method: Proposes GORP, which synergistically combines full and low-rank parameters and updates them within a unified low-rank gradient subspace.

Result: GORP outperforms state-of-the-art approaches on continual learning benchmarks, mitigating catastrophic forgetting.

Conclusion: GORP effectively balances efficiency and expressiveness in continual learning for LLMs.

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [66] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: The paper introduces a novel approach for cross-subject motor imagery (CS-MI) classification in BCIs, using optimized preprocessing and deep learning to improve accuracy across datasets.


<details>
  <summary>Details</summary>
Motivation: The variability in EEG patterns across individuals hinders calibration-free BCI development, necessitating better cross-subject classification methods.

Method: The approach involves STFT-transformed EEG data classification, optimized STFT parameters, and balanced batching for CNN training, validated across four datasets.

Result: Achieved 67.60% on IV-1, 65.96% on IV-2A, and 80.22% on IV-2B, outperforming state-of-the-art methods.

Conclusion: The work sets a new benchmark for generalizable, calibration-free MI classification and contributes a robust open-access dataset.

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [67] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL introduces a self-correcting approach for text-to-SQL models, improving accuracy by 4 percentage points through retry data pre-training.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of SQL-specific generative models and unexplored potential of self-correcting strategies in text-to-SQL tasks.

Method: Corrupts reasoning steps in reference SQL queries to create retry data, then pre-trains an open-source coding model with this data.

Result: Improves execution accuracy by up to 4 percentage points; full-parameter pre-training is essential, while LoRA fine-tuning is ineffective.

Conclusion: RetrySQL proves self-correction can be learned in text-to-SQL, offering a novel method to enhance SQL generation accuracy.

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [68] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: DNNs succeed due to their ability to exploit compositional sparsity in target functions, a property shared by efficiently computable functions.


<details>
  <summary>Details</summary>
Motivation: Understanding the fundamental principles behind DNNs' success in high-dimensional domains.

Method: Argues that compositional sparsity (functions composed of low-dimensional subsets) is key to DNNs' performance.

Result: DNNs leverage compositional sparsity, a property common in practical functions, but theoretical gaps remain.

Conclusion: Exploring compositional sparsity is crucial for a complete theory of deep learning and intelligence.

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [69] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: Removing layer-wise normalization (LN) from GPT-2 models minimally impacts performance (+0.03 cross-entropy loss for GPT-2 XL), showing LN's limited role in language modeling. LN-free models improve interpretability and scale feasibly.


<details>
  <summary>Details</summary>
Motivation: To understand LN's role at inference time and reduce its hindrance to mechanistic interpretability in transformer models.

Method: Remove LN layers from GPT-2 models, measure validation loss, and test interpretability techniques on LN-free models.

Result: LN removal causes only a small performance drop, sublinear fine-tuning data growth, and improved interpretability (e.g., exact direct logit attribution).

Conclusion: LN is not essential for GPT-2-class models, and LN-free models enable better interpretability research.

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [70] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: Extended DBNs with a scalable, trainable interconnect and introduced two pruning stages for model size reduction.


<details>
  <summary>Details</summary>
Motivation: To enhance DBN scalability and efficiency while maintaining accuracy.

Method: Added a differentiable interconnect and proposed SAT-based and similarity-based pruning.

Result: Achieved scalable DBNs with improved compression-accuracy trade-off.

Conclusion: The approach enables wider DBN layers and efficient model size reduction.

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [71] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: The study explores using Padé Approximant Neural Networks (PadéNets) for fault diagnosis in induction machines, showing superior performance over CNNs and Self-ONNs.


<details>
  <summary>Details</summary>
Motivation: To improve fault diagnosis in induction machines by leveraging advanced neural network architectures like PadéNets.

Method: Comparison of CNNs, Self-ONNs, and PadéNets on vibration and acoustic data from induction motor datasets.

Result: PadéNets achieved higher accuracies (up to 99.96%) compared to baseline models.

Conclusion: PadéNets enhance fault diagnosis due to their nonlinearity and compatibility with unbounded activation functions.

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [72] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: The paper explores the 2-simplicial Transformer, showing it improves token efficiency over standard Transformers, especially in tasks like math and reasoning.


<details>
  <summary>Details</summary>
Motivation: Modern large language models rely on massive datasets, making compute-bound assumptions less valid. This necessitates architectures prioritizing token efficiency.

Method: The study introduces the 2-simplicial Transformer, which generalizes dot-product attention to trilinear functions using a Triton kernel.

Result: The 2-simplicial Transformer outperforms standard Transformers in math, coding, reasoning, and logic tasks for a fixed token budget.

Conclusion: The 2-simplicial Transformer changes scaling law exponents for knowledge tasks, proving its token efficiency advantage.

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [73] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: Latent-space diffusion models for dynamical systems emulation show robust accuracy at high compression rates (up to 1000x) and outperform non-generative methods.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of diffusion models at inference limits their use as fast physics emulators. This work explores whether latent-space emulation, like in image/video generation, can be applied to dynamical systems.

Method: The study applies latent-space diffusion models to dynamical systems, testing their robustness across compression rates (up to 1000x). It also compares them to non-generative methods and evaluates practical design choices (architectures, optimizers).

Result: Latent-space emulation remains accurate even at high compression rates. Diffusion-based emulators are more accurate and diverse than non-generative counterparts.

Conclusion: Latent-space diffusion models are effective for dynamical systems emulation, offering accuracy and diversity, with practical design choices being crucial for training.

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [74] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE is a novel model that learns disentangled representations and hyperparameters of the cost function, improving upon β-VAE by dynamically balancing reconstruction and disentanglement losses.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of β-VAE, which requires empirical adjustment of hyperparameters, by learning the weights of loss terms and model parameters concurrently.

Method: L-VAE learns the relative weights of loss terms and model architecture parameters, with an added regularization term to prevent bias.

Result: L-VAE achieves effective balance between reconstruction and disentanglement, outperforming or matching other models on multiple datasets.

Conclusion: L-VAE successfully disentangles latent dimensions and facial attributes, demonstrating superior or competitive performance across benchmarks.

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [75] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: A transformer-based matrix variational auto-encoder (matVAE) outperforms state-of-the-art models in zero-shot prediction on DMS datasets, using fewer parameters and less computation. Incorporating AlphaFold structures further enhances performance.


<details>
  <summary>Details</summary>
Motivation: Traditional VEPs rely on MSAs, which may not account for low evolutionary pressure in pharmacogenes. DMS datasets offer an alternative, motivating the development of models like matVAE.

Method: Proposed matVAE with a structured prior, evaluated on 33 DMS datasets. Compared matVAE-MSA (trained on MSAs) and matENC-DMS (trained on DMS data), and incorporated AlphaFold structures.

Result: matVAE-MSA outperforms DeepSequence in zero-shot prediction. matENC-DMS excels in supervised tasks. AlphaFold integration achieves results comparable to finetuned DeepSequence.

Conclusion: DMS datasets can replace MSAs without significant performance loss, encouraging further development of DMS data and exploration of their relationships for variant effect prediction.

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [76] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: The paper introduces the Medical Data Pecking approach, adapting software engineering concepts to assess EHR data quality, demonstrating effectiveness in identifying issues across datasets.


<details>
  <summary>Details</summary>
Motivation: EHR data quality issues hinder reliable research and AI training, necessitating systematic assessment methods.

Method: Uses the Medical Data Pecking Tool (MDPT) with automated test generation (via LLMs) and a testing framework to identify data errors.

Result: MDPT identified 20-43 data issues in three datasets, validating its effectiveness.

Conclusion: The approach improves EHR data validity by integrating external medical knowledge, with potential for further enhancements.

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [77] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: A hierarchical deep learning framework for recursive higher-order meta-learning introduces virtual tasks to generalize across task hierarchies, using generative mechanisms and category-theoretic functors for abstraction and knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of human-generated data and enhance generalization in machine learning by enabling neural networks to autonomously generate and solve novel tasks.

Method: The framework uses a generative mechanism to create virtual tasks, iteratively refining constraints and exploring task landscapes. It employs category-theoretic functors for hierarchical abstraction and knowledge transfer.

Result: The approach enhances inductive biases, regularizes adaptation, and produces novel tasks and constraints, enabling structured and interpretable learning progression.

Conclusion: This architecture may advance machine learning towards general artificial intelligence by autonomously generating instructive tasks and solutions.

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [78] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: The paper introduces a data-efficient exploration method in reinforcement learning using information-theoretic intrinsic motivation, focusing on epistemic uncertainty. It proposes a framework (PTS-BE) combining model-based planning with exploration bonuses, showing superior performance in sparse-reward environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of data-efficient exploration in reinforcement learning by leveraging intrinsic motivation and epistemic uncertainty to align exploration with genuine knowledge gaps.

Method: Examines exploration bonuses targeting epistemic uncertainty, proposes tractable approximations (sparse variational Gaussian Processes, Deep Kernels, Deep Ensembles), and introduces PTS-BE framework integrating model-based planning with information-theoretic bonuses.

Result: PTS-BE outperforms baselines in environments with sparse rewards or purely exploratory tasks, providing formal guarantees for IG-based approaches.

Conclusion: The work formalizes and empirically validates a novel exploration framework, PTS-BE, demonstrating its effectiveness in sample-efficient deep exploration.

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [79] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: The paper introduces DAID, a framework to address the trade-off between generalization and fairness in deepfake detection by controlling confounders and using demographic-aware interventions.


<details>
  <summary>Details</summary>
Motivation: The conflict between generalization to unseen manipulations and demographic fairness in deepfake detection models motivates the need for a causal approach to reconcile these objectives.

Method: Proposes DAID, a framework with demographic-aware data rebalancing (inverse-propensity weighting, subgroup-wise normalization) and demographic-agnostic feature aggregation (alignment loss).

Result: DAID outperforms state-of-the-art detectors in fairness and generalization across three benchmarks.

Conclusion: The causal relationship between fairness and generalization is validated, and DAID proves effective in balancing both objectives.

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [80] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft is a unified framework enabling a single draft model to work with any target model, addressing cross-vocabulary mismatches and improving decoding speed, achieving 1.5-2x speedup.


<details>
  <summary>Details</summary>
Motivation: Challenges in speculative decoding include draft-target model incompatibility and latency improvements over time. OmniDraft aims to solve these for on-device LLM applications.

Method: Uses an online n-gram cache with hybrid distillation fine-tuning and adaptive drafting techniques.

Result: OmniDraft pairs a single Llama-68M model with various targets (e.g., Vicuna-7B, Qwen2-7B, Llama3-8B) and achieves 1.5-2x speedup.

Conclusion: OmniDraft supports the 'one drafter for all' paradigm, enhancing efficiency and adaptability in speculative decoding.

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [81] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: A guided discrete diffusion model is introduced to optimize antibody sequences for developability, integrating a Soft Value-based Decoding in Diffusion (SVDD) Module to enhance biophysical viability without losing naturalness.


<details>
  <summary>Details</summary>
Motivation: To address the need for therapeutic antibodies with high-affinity target engagement and favorable manufacturability, stability, and safety profiles (collectively called 'developability').

Method: A guided discrete diffusion model trained on natural paired heavy- and light-chain sequences from OAS and developability measurements for 246 clinical-stage antibodies, enhanced by SVDD for biophysical viability.

Result: The model reproduces global features of natural repertoires and approved therapeutics, with SVDD significantly enriching predicted developability scores.

Conclusion: This framework, combined with high-throughput assays, enables an iterative, ML-driven pipeline for designing antibodies meeting both binding and biophysical criteria.

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [82] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: A method using Differentially Private (DP) generative models and foundation models to address data scarcity and privacy in medical imaging, improving efficiency and flexibility over traditional Federated Learning (FL).


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of DL in medical imaging due to data scarcity and privacy regulations, and address inefficiencies in FL like high communication costs and single-task restrictions.

Method: Propose a DP-CVAE (Differentially Private Conditional Variational Autoencoder) trained collaboratively by clients to model a global, privacy-aware data distribution, leveraging compact embeddings from foundation models.

Result: Outperforms traditional FL classifiers in privacy, scalability, and efficiency, with higher-fidelity embeddings than DP-CGAN and fewer parameters.

Conclusion: The method successfully balances privacy and performance, offering a scalable and flexible solution for diverse downstream tasks in medical imaging.

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [83] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: This study explores MARL for dynamic pricing in supply chains, comparing three algorithms (MADDPG, MADQN, QMIX) against static baselines. MARL shows emergent strategic behavior, with MADDPG balancing competition and fairness.


<details>
  <summary>Details</summary>
Motivation: Traditional ERP systems use static pricing, ignoring strategic interactions in supply chains. MARL addresses this gap by modeling multi-agent dynamics.

Method: Evaluated MADDPG, MADQN, and QMIX in a simulated environment with real e-commerce data and a LightGBM demand model.

Result: Rule-based agents excelled in fairness and stability but lacked competition. MADQN was aggressive, while MADDPG balanced competition, fairness, and stability.

Conclusion: MARL enables strategic behavior beyond static rules, offering insights for dynamic pricing advancements.

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [84] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: The paper proposes a viscous-retained democracy protocol (FedVRD) for federated learning to reduce wasteful data transfer by selecting clients with useful weights, outperforming traditional methods and mitigating adversarial impacts.


<details>
  <summary>Details</summary>
Motivation: To minimize wasteful data transfer costs in federated learning by identifying clients with the most useful model weights and addressing adversarial vulnerabilities.

Method: Introduces a new fluid democracy protocol (viscous-retained democracy) and an algorithm (FedVRD) to dynamically limit adversarial effects while optimizing cost.

Result: FedVRD outperforms traditional 1p1v (FedAvg) and existing fluid democracy protocols, reducing adversarial impact without influence accumulation.

Conclusion: The proposed FedVRD protocol effectively improves federated learning efficiency and security by optimizing client selection and mitigating adversarial threats.

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [85] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: FoG introduces two mechanisms—Experience Replay Decay and Network Expansion—to mitigate primacy bias in deep RL, improving sample efficiency and generalizability.


<details>
  <summary>Details</summary>
Motivation: Addressing primacy bias in deep RL, inspired by human infantile amnesia, to enhance agent performance.

Method: FoG combines Experience Replay Decay (forgetting early experiences) and Network Expansion (adding parameters dynamically).

Result: Outperforms SoTA algorithms (BRO, SimBa, TD-MPC2) on 40+ continuous control tasks.

Conclusion: FoG effectively reduces primacy bias, improving RL agent efficiency and adaptability.

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [86] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: A framework integrating spatial, temporal, and network dependencies improves e-scooter demand prediction by 27-49%, aiding urban micromobility management.


<details>
  <summary>Details</summary>
Motivation: Accurate demand prediction is crucial for managing dockless e-scooters, which enhance urban transport but require optimal fleet distribution and infrastructure planning.

Method: The study introduces a framework combining spatial, temporal, and network dependencies for micromobility demand forecasting.

Result: The framework outperforms baseline models, improving prediction accuracy by 27-49%.

Conclusion: The findings enable data-driven micromobility management, optimizing fleet distribution, reducing costs, and supporting sustainable urban planning.

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [87] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO is a hierarchical contrastive framework for protein-protein interaction prediction, leveraging multi-tiered biological representation matching and hierarchical contrastive loss to achieve state-of-the-art performance and zero-shot transferability.


<details>
  <summary>Details</summary>
Motivation: To bridge heterogeneous biological data modalities and improve protein-protein interaction prediction across species, especially in low-data or rare organism scenarios.

Method: Uses hierarchical contrastive learning to align protein sequences and attributes, incorporating domain knowledge via data-driven penalties and multi-tiered representation matching.

Result: Outperforms existing methods, shows robustness in low-data regimes, and demonstrates zero-shot transferability to other species without retraining.

Conclusion: HIPPO advances cross-species PPI prediction and provides a unified framework for sparse or imbalanced multi-species data scenarios.

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [88] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: The paper introduces a novel classification method using entropy-based optimization to find separating hyperplanes or polynomial surfaces, offering robustness over traditional techniques like SVMs and gradient descent.


<details>
  <summary>Details</summary>
Motivation: The problem of separating two classes of points in ${\mathbb R}^N$ is fundamental in machine learning, yet traditional methods like perceptrons, SVMs, and gradient descent have limitations. The paper aims to provide a more robust and versatile solution.

Method: The approach searches for parameter vectors in a bounded hypercube and a positive vector in ${\mathbb R}^M$, minimizing an entropy-based function. It extends to polynomial surfaces for complex decision boundaries.

Result: Numerical experiments show the method's efficiency and versatility in handling both linear and non-linear classification tasks.

Conclusion: The proposed entropy-based method is a robust alternative to traditional techniques, effective for diverse classification problems.

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [89] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: The paper studies contextual online pricing with biased offline data, identifying key factors affecting statistical complexity and proposing optimal regret bounds and algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of contextual pricing with biased offline data and derive tight regret guarantees.

Method: Uses Optimism-in-the-Face-of-Uncertainty (OFU) policy and a generalized OFU algorithm for general price elasticity, with a robust variant for unknown bias.

Result: Achieves minimax-optimal, instance-dependent regret bounds and sub-linear regret for unknown bias.

Conclusion: Provides first tight regret guarantees for contextual pricing with biased offline data, with techniques applicable to stochastic linear bandits.

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [90] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: The paper explores why recurrent models like state space models and linear attention fail to generalize to longer sequences than their training context, attributing it to limited exposure to attainable states during training. Simple interventions, like state initialization with noise or other sequences' final states, significantly improve length generalization with minimal additional training.


<details>
  <summary>Details</summary>
Motivation: To understand and address the failure of recurrent models to generalize to sequences longer than their training context, despite their theoretical capability to handle arbitrary lengths.

Method: Empirical and theoretical analysis of the 'unexplored states hypothesis,' followed by testing simple training interventions (e.g., Gaussian noise initialization, using final states of other sequences) to improve state coverage.

Result: Interventions enable models to generalize to sequences orders of magnitude longer (e.g., 2k to 128k) with only 500 post-training steps (~0.1% of pre-training budget), improving performance in long-context tasks.

Conclusion: Simple, efficient training interventions can robustly enhance length generalization in recurrent models by increasing the coverage of attainable states during training.

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [91] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE is a survival analysis model ensuring calibration for all subpopulations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing survival models are poorly calibrated for minority subpopulations, risking erroneous clinical decisions.

Method: GRADUATE frames multicalibration as constrained optimization, balancing calibration and discrimination.

Result: Empirical tests show GRADUATE outperforms baselines, with theoretical guarantees of near-optimality.

Conclusion: GRADUATE addresses calibration gaps in survival analysis, improving reliability for diverse populations.

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [92] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: The paper investigates distribution testing under algorithmic replicability, developing new replicable algorithms for testing closeness and independence of discrete distributions, and establishing lower bounds for replicable uniformity and closeness testing.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the sample complexity of replicably testing properties of probability distributions, addressing open questions in prior work.

Method: Develops new replicable algorithms for testing closeness and independence, and introduces a methodology for proving sample complexity lower bounds for replicable testing.

Result: Presents near-optimal sample complexity lower bounds for replicable uniformity and closeness testing, answering prior open questions.

Conclusion: The study advances the understanding of replicable distribution testing, providing algorithmic and lower-bound contributions with broader implications.

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [93] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: ExPO is a framework for improving reasoning in language models by generating positive samples aligned with the model's policy and ground-truth answers, outperforming expert-based methods in challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current RL-style post-training methods rely heavily on initial model outputs, limiting their ability to solve problems where the model initially fails, especially in early-stage training and challenging reasoning tasks.

Method: ExPO generates effective positive samples by conditioning on ground-truth answers, ensuring they align with the model's policy and improve correctness.

Result: ExPO enhances learning efficiency and performance on reasoning benchmarks, particularly in difficult settings like MATH level-5, surpassing expert-demonstration-based methods.

Conclusion: ExPO provides a modular and effective solution for unlocking reasoning ability in language models by addressing the limitations of existing RL post-training approaches.

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [94] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper addresses biased treatment effect estimates due to discrepancies between training (structured data) and inference (textual data) in medicine, proposing a framework using large language models and a doubly robust learner to mitigate biases.


<details>
  <summary>Details</summary>
Motivation: The discrepancy between structured training data and textual inference data in medicine can bias treatment effect estimates, necessitating a solution.

Method: Proposes a framework combining large language models with a custom doubly robust learner to address inference time text confounding.

Result: Experiments demonstrate the framework's effectiveness in mitigating biases in real-world applications.

Conclusion: The proposed framework successfully addresses inference time text confounding, improving treatment effect estimation accuracy.

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [95] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB is a multi-view learning framework for fMRI data that integrates higher-order interactions (HOIs) and pairwise interactions, improving diagnostic accuracy by compressing redundant information.


<details>
  <summary>Details</summary>
Motivation: Higher-order interactions (HOIs) in fMRI data can enhance diagnostic accuracy, but extracting and utilizing them effectively is challenging.

Method: MvHo-IB combines O-information with Renyi entropy to quantify HOIs, uses a Brain3DCNN encoder, and employs a multi-view learning information bottleneck.

Result: MvHo-IB outperforms state-of-the-art methods on three fMRI datasets, including hypergraph-based techniques.

Conclusion: MvHo-IB effectively leverages HOIs for improved diagnostic decision-making in fMRI analysis.

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [96] [Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models](https://arxiv.org/abs/2507.02002)
*Shaurya Mallampati,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.MA

TL;DR: The paper explores LLM-driven agents in dynamic, multi-agent scenarios, showing improved performance with game-theoretic principles and real-time adaptation.


<details>
  <summary>Details</summary>
Motivation: To assess LLM reasoning in real-time, collaborative environments, addressing gaps in dynamic multi-agent scenarios.

Method: Combines LLMs with strategic reasoning and real-time adaptation in cooperative settings, using game-theoretic principles like belief consistency and Nash equilibrium.

Result: Achieves 26% improvement over PPO baselines in noisy environments, with sub-1.05ms latency, enhancing collaboration and task completion.

Conclusion: Game-theoretic guidance and real-time feedback improve LLM performance, enabling resilient and flexible multi-agent systems.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities across
mathematical, strategic, and linguistic tasks, yet little is known about how
well they reason in dynamic, real-time, multi-agent scenarios, such as
collaborative environments in which agents continuously adapt to each other's
behavior, as in cooperative gameplay settings. In this paper, we bridge this
gap by combining LLM-driven agents with strategic reasoning and real-time
adaptation in cooperative, multi-agent environments grounded in game-theoretic
principles such as belief consistency and Nash equilibrium. The proposed
framework applies broadly to dynamic scenarios in which agents coordinate,
communicate, and make decisions in response to continuously changing
conditions. We provide real-time strategy refinement and adaptive feedback
mechanisms that enable agents to dynamically adjust policies based on immediate
contextual interactions, in contrast to previous efforts that evaluate LLM
capabilities in static or turn-based settings. Empirical results show that our
method achieves up to a 26\% improvement in return over PPO baselines in
high-noise environments, while maintaining real-time latency under 1.05
milliseconds. Our approach improves collaboration efficiency, task completion
rates, and flexibility, illustrating that game-theoretic guidance integrated
with real-time feedback enhances LLM performance, ultimately fostering more
resilient and flexible strategic multi-agent systems.

</details>


### [97] [Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System](https://arxiv.org/abs/2507.02170)
*Adam Kostka,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: SynergyMAS integrates MAS techniques for logical reasoning, knowledge retention, and ToM, enhancing teamwork and problem-solving, validated by a product development case study.


<details>
  <summary>Details</summary>
Motivation: To address complex real-world challenges by improving agent collaboration and problem-solving through advanced MAS techniques.

Method: Developed SynergyMAS, combining logical reasoning, long-term knowledge retention, ToM, and optimized communication protocols.

Result: Demonstrated superior performance and adaptability in a product development team case study.

Conclusion: SynergyMAS shows promise for solving complex problems through enhanced agent teamwork.

Abstract: This paper explores the integration of advanced Multi-Agent Systems (MAS)
techniques to develop a team of agents with enhanced logical reasoning,
long-term knowledge retention, and Theory of Mind (ToM) capabilities. By
uniting these core components with optimized communication protocols, we create
a novel framework called SynergyMAS, which fosters collaborative teamwork and
superior problem-solving skills. The system's effectiveness is demonstrated
through a product development team case study, where our approach significantly
enhances performance and adaptability. These findings highlight SynergyMAS's
potential to tackle complex, real-world challenges.

</details>
