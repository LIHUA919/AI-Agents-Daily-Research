<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 62]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO is a new CTDE framework for MARL that improves centralized training with theoretical guarantees and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CTDE methods underutilize centralized training or lack theoretical guarantees, limiting their effectiveness in cooperative MARL.

Method: MAGPO integrates centralized guidance with decentralized execution, using an auto-regressive joint policy for scalable exploration and aligning it with decentralized policies.

Result: MAGPO outperforms CTDE baselines and matches/surpasses centralized approaches in 43 tasks across 6 environments.

Conclusion: MAGPO provides a principled and practical solution for decentralized multi-agent learning with theoretical guarantees.

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [2] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: Symbolic Regression (SR) is used to model 3D incompressible flow in a rectangular channel, yielding interpretable equations that match analytical solutions. A hybrid SR/ASP framework ensures physical plausibility.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between accurate prediction and understanding of flow physics in fluid mechanics, leveraging SR for interpretable models.

Method: Applied SR via PySR to derive symbolic equations from simulation data, then integrated with Answer Set Programming (ASP) for physical plausibility.

Result: Derived equations matched analytical solutions (parabolic velocity profile, pressure drop). Hybrid SR/ASP ensured domain-aligned results.

Conclusion: SR simplifies complex flows into interpretable equations; hybrid SR/ASP enhances reliability and alignment with domain principles.

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [3] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA introduces a structured reasoning agent for data analysis, outperforming prior systems by formalizing cognitive workflows.


<details>
  <summary>Details</summary>
Motivation: Existing agentic systems for data analysis lack structured reasoning, despite its importance for real-world analytical tasks.

Method: I2I-STRADA models analytical reasoning via modular sub-tasks, formalizing steps like goal interpretation, contextual grounding, and adaptive execution.

Result: I2I-STRADA outperforms prior systems in planning coherence and insight alignment on DABstep and DABench benchmarks.

Conclusion: Structured cognitive workflows are crucial for effective agent design in data analysis, as demonstrated by I2I-STRADA's success.

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [4] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: SmartAPS is a conversational system using a tool-augmented LLM to make advanced planning systems (APS) more accessible via natural language interaction.


<details>
  <summary>Details</summary>
Motivation: Traditional APS are costly due to consultant fees, limiting accessibility for supply chain planners.

Method: Built on a tool-augmented LLM, SmartAPS offers a chat interface for queries, counterfactual reasoning, recommendations, and scenario analysis.

Result: The system provides an intuitive way for planners to manage operations without high consultant costs.

Conclusion: SmartAPS demonstrates the potential of LLMs to democratize access to advanced planning tools.

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [5] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: The paper identifies a deterministic fragment of qualitative timeline-based planning, enabling direct mapping to deterministic finite automata for strategy synthesis, and identifies a maximal subset of Allen's relations fitting this fragment.


<details>
  <summary>Details</summary>
Motivation: The plan-existence problem in qualitative timeline-based planning is PSPACE-complete, and while nondeterministic automata can prove PSPACE-membership, they require costly determinization for strategy synthesis. This work aims to bypass this limitation.

Method: The authors identify a fragment of qualitative timeline-based planning that can be directly mapped to the nonemptiness problem of deterministic finite automata (DFA), allowing strategy synthesis without determinization. They also identify a maximal subset of Allen's relations compatible with this fragment.

Result: The deterministic fragment enables direct use of DFA for planning strategy synthesis, avoiding the need for determinization. A maximal subset of Allen's relations is identified as fitting this deterministic framework.

Conclusion: The deterministic fragment simplifies strategy synthesis in qualitative timeline-based planning by leveraging DFA, and the identified subset of Allen's relations ensures compatibility with this approach.

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [6] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: The paper introduces the E.A.R.T.H. framework, a five-stage pipeline that leverages model-generated errors to enhance AI creativity, showing significant improvements in novelty, relevance, and human evaluation scores.


<details>
  <summary>Details</summary>
Motivation: To move AI beyond imitation by exploring how errors can be transformed into creative assets, inspired by the idea that 'creative potential hides in failure.'

Method: The E.A.R.T.H. framework (Error generation, Amplification, Refine selection, Transform, Harness feedback) uses structured prompts, semantic scoring, and human-in-the-loop evaluation with tools like LLaMA-2-7B-Chat, SBERT, and Stable Diffusion.

Result: Creativity scores increased by 52.5%, with refined outputs showing 70.4% improvement. Slogans were shorter, more novel, and maintained relevance. Human evaluations rated 60% of outputs highly, with metaphorical slogans outperforming literal ones.

Conclusion: Error-centered, feedback-driven generation enhances AI creativity, providing a scalable approach for self-evolving, human-aligned creative AI.

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [7] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: AI systems (GPT 4.1 and Claude 3.5) perform better in data analysis tasks when raw data is accompanied by scatterplots, especially for complex datasets.


<details>
  <summary>Details</summary>
Motivation: To explore whether charts and graphs, which aid human data analysis, can also enhance AI system performance.

Method: Experiments with GPT 4.1 and Claude 3.5 on three analysis tasks, comparing performance with raw data alone, blank charts, and mismatched charts.

Result: AI systems describe synthetic datasets more precisely and accurately with scatterplots, with performance improvements attributed to chart content.

Conclusion: Visualizations benefit AI systems, similar to humans, in data analysis tasks.

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [8] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch is an autonomous AI system for neural architecture discovery, surpassing human limitations by conducting end-to-end research, discovering 106 SOTA architectures, and establishing a scaling law for scientific discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the linear bottleneck of human cognitive capacity in AI research by enabling AI to autonomously innovate and discover new architectures.

Method: ASI-Arch moves beyond traditional NAS by autonomously hypothesizing, implementing, training, and validating novel architectures through 1,773 experiments over 20,000 GPU hours.

Result: The system discovered 106 innovative SOTA linear attention architectures, revealing emergent design principles surpassing human baselines.

Conclusion: ASI-Arch transforms AI research into a computation-scalable process, providing a blueprint for self-accelerating AI systems.

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [9] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: An Agentic AI framework automates clinical data pipelines, handling structured and unstructured data for feature extraction, model selection, and preprocessing, reducing manual intervention in healthcare ML.


<details>
  <summary>Details</summary>
Motivation: High costs and labor-intensive processes in healthcare ML due to fragmented workflows, model compatibility issues, and privacy constraints.

Method: Modular, task-specific agents automate data ingestion, anonymization, feature extraction, model selection, preprocessing, and inference. Evaluated on geriatrics, palliative care, and colonoscopy datasets.

Result: Automated pipeline reduces expert intervention, offering scalable, cost-efficient AI deployment in clinical settings.

Conclusion: The framework streamlines ML workflows in healthcare, enhancing efficiency and scalability while ensuring privacy compliance.

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [10] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: The study uses NLP and Active Learning to create a classifier for detecting vaccine safety issues from ED triage notes, improving surveillance efficiency.


<details>
  <summary>Details</summary>
Motivation: Limited safety data from clinical trials and early vaccine implementation necessitates better post-licensure surveillance systems.

Method: Combines NLP techniques, Active Learning, and data augmentation to develop a classifier for ED triage notes.

Result: A classifier is developed to enhance vaccine safety surveillance, reducing reliance on keyword-based methods.

Conclusion: The approach offers a more accurate and efficient alternative for vaccine safety monitoring, leveraging NLP and Active Learning.

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [11] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: Mean GNNs match ratio modal logic in non-uniform settings, with expressive power between max and sum GNNs. In uniform settings, they are less expressive than sum and max GNNs under certain assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand and compare the expressive power of GNNs using mean aggregation against other aggregation functions (max, sum) in both non-uniform and uniform settings.

Method: Analyze mean GNNs' expressive power by comparing them to modal logic variants (ratio, alternation-free) and MSO, under specific assumptions.

Result: Mean GNNs align with ratio modal logic in non-uniform settings but are less expressive than sum and max GNNs in uniform settings under given assumptions.

Conclusion: Mean GNNs have intermediate expressive power, influenced by aggregation choice and setting (uniform/non-uniform), with assumptions playing a critical role.

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [12] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: The paper proposes a framework to decouple knowledge and reasoning in LLMs, analyzing their contributions through fast and slow thinking modes. Results show domain-specific reasoning benefits, parameter scaling effects, and layer-wise distribution of knowledge and reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand and distinguish the roles of knowledge and reasoning in LLMs for better model analysis, interpretability, and development.

Method: A cognition attribution framework decomposes LLM cognition into knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2), tested via fast and slow thinking modes across 15 LLMs and 3 datasets.

Result: (1) Reasoning adjustment is domain-specific. (2) Parameter scaling improves knowledge and reasoning, with knowledge gains more significant. (3) Knowledge resides in lower layers, reasoning in higher layers.

Conclusion: The framework provides insights into LLM behavior, scaling laws, knowledge editing, and small-model reasoning limitations.

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [13] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: The paper compares four non-minimal semantics for disjunction in Answer Set Programming, showing three coincide and are stronger than the fourth.


<details>
  <summary>Details</summary>
Motivation: To explore alternative semantics for disjunction in Answer Set Programming that do not rely on model minimality.

Method: Comparison of four approaches: Justified Models, Strongly Supported Models, Forks, and Determining Inference (DI) semantics.

Result: Three approaches (Forks, Justified Models, relaxed DI) coincide and are stronger than Strongly Supported Models, which treats disjunctions classically.

Conclusion: The common semantics of the three approaches provides a superset of stable models and is more robust than the fourth.

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [14] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: A conceptual framework for qualitative AI risk assessment under the EU AI Act, integrating legal compliance and rights protection via definitional balancing and defeasible reasoning.


<details>
  <summary>Details</summary>
Motivation: Address complexities of AI risk assessment in legal contexts, ensuring compliance and fundamental rights protection.

Method: Combines definitional balancing (proportionality analysis) and defeasible reasoning to analyze AI deployment scenarios and their impacts.

Result: Provides philosophical foundations for AI risk analysis, enabling operative models for high-risk and General Purpose AI systems.

Conclusion: Future work will develop formal models and algorithms to enhance practical AI risk assessment and governance.

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [15] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: A method for automatically marking Physics exams using a combination of computer algebra, SMT solving, and term rewriting, enhanced by a Large Language Model for error correction.


<details>
  <summary>Details</summary>
Motivation: To automate the challenging task of assessing typed student answers for correctness against ground truth solutions in Physics exams.

Method: Combines a computer algebra system, SMT solver, and term rewriting system, with a Large Language Model for preprocessing student responses. Uses automated reasoning (SMT solving and term rewriting) for correctness assessment.

Result: Evaluated on 1500+ real-world student exam responses from the 2023 Australian Physics Olympiad.

Conclusion: The method effectively automates Physics exam marking, leveraging advanced computational tools and reasoning techniques.

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [16] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: ConDiFi is a benchmark evaluating divergent and convergent thinking in LLMs for finance, revealing performance gaps in models like GPT-4o compared to others like DeepSeek-R1.


<details>
  <summary>Details</summary>
Motivation: Current reasoning benchmarks lack focus on creative, plausible futures under uncertainty, crucial for financial decision-making.

Method: ConDiFi includes 607 prompts for divergent reasoning and 990 multi-hop MCQs for convergent reasoning, tested on 14 models.

Result: GPT-4o underperforms in Novelty and Actionability, while models like DeepSeek-R1 excel in actionable insights.

Conclusion: ConDiFi offers a new way to assess reasoning for safe and strategic LLM deployment in finance.

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [17] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces IB-aware reasoning optimization (IBRO), a theoretical framework grounded in the information bottleneck principle, to improve LLM reasoning by ensuring informative and generalizable reasoning trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for LLM reasoning are heuristic and lack principled methodologies, limiting their development.

Method: The authors propose IBRO, deriving a token-level surrogate objective and a lightweight IB regularization method that integrates into existing RL frameworks with minimal overhead.

Result: Empirical validation across mathematical reasoning benchmarks shows consistent improvements in LLM reasoning performance.

Conclusion: IBRO provides a principled and efficient way to enhance LLM reasoning, requiring minimal implementation effort.

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [18] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: The paper compares model-based (Value Iteration) and model-free (Proximal Policy Optimization) RL methods for call routing optimization, finding PPO superior in reducing client waiting time and staff idle time.


<details>
  <summary>Details</summary>
Motivation: To optimize call routing in call centers by minimizing client waiting time and staff idle time using RL.

Method: Compares model-based (Value Iteration) and model-free (PPO) approaches, both framed as MDPs in a Skills-Based Routing framework, using Poisson arrivals and exponential service times.

Result: PPO outperforms Value Iteration and random policies, achieving the highest rewards and lowest waiting/idle times after 1,000 test episodes.

Conclusion: PPO is more effective for call routing optimization despite longer training times.

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [19] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: The paper explores enhancing the Compact-Table (CT) algorithm for table constraints in constraint programming by leveraging GPU acceleration to handle large-scale problems efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional CPU-based methods struggle with real-world problems involving hundreds or thousands of cases in table constraints, necessitating more powerful computational approaches.

Method: The authors enhance the CT algorithm using GPU acceleration, detailing its design, implementation, and integration into an existing constraint solver.

Result: Experimental validation on a significant set of instances demonstrates the effectiveness of the GPU-accelerated CT approach.

Conclusion: GPU acceleration significantly improves the handling of large table constraints, making the enhanced CT algorithm a practical solution for complex real-world problems.

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [20] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: The paper investigates the impact of training data on concept probing models in image classification, addressing a gap in current research, and provides concept labels for two datasets.


<details>
  <summary>Details</summary>
Motivation: Current concept probing research overlooks the importance of training data for probing models, focusing instead on the models themselves.

Method: The study examines the effect of training data on probing model performance in image classification tasks.

Result: The paper highlights the significance of training data in concept probing and releases concept labels for two popular datasets.

Conclusion: Training data plays a crucial role in concept probing, and the provided labels can aid future research.

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [21] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1 is a multimodal reasoning model developed using the SafeLadder framework, achieving significant safety improvements without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a model where safety and capabilities co-evolve, addressing limitations of previous alignment methods like RLHF.

Method: Uses the SafeLadder framework with progressive, safety-oriented reinforcement learning and multi-principled verifiers. Includes inference-time interventions and deliberative search.

Result: Achieves 46.54% improvement over base model on safety benchmarks, outperforming GPT-4.1 and Claude Opus 4.

Conclusion: Demonstrates that safety and capability can synergistically co-evolve, proving the generalizability of the SafeLadder framework.

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC is a QAT framework for edge devices that uses coreset selection and cascaded layer correction to reduce quantization errors and improve model performance with small datasets.


<details>
  <summary>Details</summary>
Motivation: The need for efficient low-bit quantized models on edge devices, coupled with privacy concerns and high computational costs of traditional QAT, drives the development of QuaRC.

Method: QuaRC employs Relative Entropy Score for coreset selection and Cascaded Layer Correction to align intermediate layer outputs between quantized and full-precision models.

Result: QuaRC improves Top-1 accuracy by 5.72% on ResNet-18 quantized to 2-bit using only 1% of the ImageNet-1K dataset.

Conclusion: QuaRC effectively addresses quantization errors and computational costs, making it a viable solution for edge device deployment.

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [23] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: The paper critiques current MARL methods for relying on brittle conventions instead of effective reasoning, advocating for better-designed environments to test genuine skills.


<details>
  <summary>Details</summary>
Motivation: To highlight the limitations of current MARL algorithms in addressing Dec-POMDP challenges and propose improvements for benchmarking.

Method: Analyzes empirical success of MARL methods through a case study, comparing brittle conventions with grounded policies.

Result: Shows that current MARL benchmarks may not test core Dec-POMDP assumptions, leading to fragile solutions.

Conclusion: Advocates for new cooperative environments emphasizing observation-grounded behaviors and memory-based reasoning.

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [24] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: A low-complexity AI model using GANs with causality-invariant learning improves semantic communication by extracting causal and non-causal representations, ensuring consistency across domains and robust data reconstruction.


<details>
  <summary>Details</summary>
Motivation: To enhance semantic communication by capturing invariant knowledge for reliable data reconstruction across diverse domains and evolving user data.

Method: Proposes a generative adversarial network (GAN) with causality-invariant learning to extract causal (invariant) and non-causal representations, coupled with sparse update protocols for knowledge consistency.

Result: Ensures consistency across devices, improves classification performance, and achieves superior PSNR in data reconstruction compared to state-of-the-art methods.

Conclusion: The model effectively leverages invariant knowledge for robust semantic communication, outperforming existing techniques in reconstruction quality and domain adaptability.

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [25] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: The paper introduces 'Moving Out,' a benchmark for human-AI collaboration in physical environments, and proposes BASS, a method to improve agent adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of continuous state-action spaces and physical constraints in human-AI collaboration.

Method: Proposes BASS (Behavior Augmentation, Simulation, and Selection) to enhance agent diversity and action understanding.

Result: BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration tasks.

Conclusion: The Moving Out benchmark and BASS method advance physically grounded human-AI collaboration.

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


### [26] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: The paper explores hierarchical self-similarity in deep neural networks, proposing a method to analyze and adjust self-similarity in feature networks to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Lack of quantitative analysis on how self-similarity in hidden space geometry affects weight optimization and unclear dynamics of internal neurons.

Method: Complex network modeling based on hidden-layer neuron output features to study self-similarity and its impact on classification performance.

Result: Self-similarity varies by architecture; embedding constraints during training improves performance by up to 6% in certain models.

Conclusion: Adjusting self-similarity in feature networks can enhance deep neural network performance, with architecture-specific effects.

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [27] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: A reinforcement learning (RL) algorithm for aerodynamic shape optimization reduces dimensionality by using a surrogate-based, actor-critic MCMC approach with parameter 'freezing' to minimize computational effort and interpret optimization results.


<details>
  <summary>Details</summary>
Motivation: To streamline aerodynamic shape optimization by reducing computational costs and interpreting optimization results for better understanding of flow-field dynamics.

Method: Uses RL with a surrogate-based, actor-critic MCMC approach, freezing parameters temporarily, and relies on local optimized parameter changes around CFD simulations for global optimization.

Result: Demonstrates faster global optimization when local parameter neighborhoods are large and reward/cost estimates are accurate, with an example showing interpretable feature importance.

Conclusion: The method effectively balances computational efficiency and interpretability in aerodynamic shape optimization.

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [28] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: The paper reviews hyperbolic neural networks as an alternative to Euclidean geometry for foundation models, addressing limitations like representational capacity and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current foundation models (LLMs, VLMs, etc.) face limitations in representational capacity, adaptability, and scalability, prompting exploration of non-Euclidean geometries like hyperbolic spaces.

Method: The paper reviews hyperbolic neural networks, leveraging their properties for low-distortion embeddings of hierarchical structures and power-law distributions.

Result: Hyperbolic spaces enhance foundation models, improving reasoning, zero-shot generalization, and cross-modal alignment while maintaining parameter efficiency.

Conclusion: The paper highlights the potential of hyperbolic spaces for foundation models and outlines future research directions.

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [29] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: The paper addresses position bias and low repetition consistency in LLMs when ranking or evaluating items, proposing a dynamic early-stopping method to reduce computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit position bias and inconsistency in rankings, requiring costly repetition strategies. The paper aims to mitigate these issues efficiently.

Method: Introduces a dynamic early-stopping method to adaptively determine repetitions per instance, reducing LLM calls. Also proposes a confidence-based adaptation.

Result: Dynamic repetition reduces LLM calls by 81% (87% with confidence-based adaptation) while preserving accuracy.

Conclusion: The dynamic approach effectively balances computational efficiency and accuracy, addressing LLM inconsistencies.

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [30] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix is a Python-based framework for reproducible and interpretable ML workflows for tabular data, focusing on transparency and usability.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for transparent, reproducible, and interpretable ML workflows, especially for stakeholders without data science expertise.

Method: Provides modules for data preprocessing, visualization, model training, evaluation, interpretation, and prediction, with a user-friendly interface.

Result: Helix enables accessible, documented, and FAIR-compliant ML workflows, supporting community-driven development.

Conclusion: Helix is a versatile tool for reproducible ML, emphasizing transparency and usability for diverse users.

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [31] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: CICME is a three-step method for inferring causal mechanisms from heterogeneous data across domains, using Causal Transfer Learning to identify domain-invariant mechanisms and guide individual domain estimations. It outperforms baselines in linear Gaussian models.


<details>
  <summary>Details</summary>
Motivation: To understand complex sensor systems through causality by identifying common and individual causal mechanisms from heterogeneous multi-domain data.

Method: CICME uses Causal Transfer Learning (CTL) to detect domain-invariant causal mechanisms and then estimates individual domain mechanisms. Evaluated on linear Gaussian models inspired by manufacturing processes.

Result: CICME reliably identifies common causal mechanisms and outperforms baseline methods in certain scenarios.

Conclusion: CICME effectively combines pooled and individual domain data for causal discovery, demonstrating superior performance in specific cases.

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [32] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: Proposes LSDM, a model combining diffusion models and LLMs for accurate mobile traffic prediction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods lack adaptability and accuracy due to uncertain traffic patterns and missing environmental context.

Method: Integrates diffusion models and transformers (LLMs) to capture dynamic traffic and environmental features.

Result: Improves prediction accuracy by 2.83% (R²) and reduces RMSE by 8.29% compared to similar models.

Conclusion: LSDM effectively enhances service-level traffic prediction with better generalization and adaptability.

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [33] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: A novel framework, CoCAI, combines generative AI and copula-based modeling for accurate predictions and robust anomaly detection in multivariate time-series.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multivariate time-series analysis, such as accurate predictions and anomaly detection.

Method: Uses a diffusion-based model for dependencies, conformal prediction for calibration, and copula-based modeling for anomaly detection.

Result: Effective in forecasting and anomaly detection, validated on real water and sewerage system data.

Conclusion: CoCAI provides statistically valid predictions and robust anomaly detection with minimal deployment overhead.

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [34] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: GenSelect leverages LLMs' comparative reasoning to efficiently select the best solution among multiple candidates, outperforming pointwise and pairwise methods in math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods (pointwise or pairwise) underutilize LLMs' comparative abilities or scale poorly with larger sampling budgets.

Method: GenSelect uses long reasoning by LLMs to select the best solution from N candidates, scaling efficiently with parallel sampling.

Result: GenSelect outperforms existing scoring methods, demonstrating effectiveness with models like QwQ and DeepSeek-R1-0528.

Conclusion: GenSelect efficiently combines LLMs' comparative strengths with scalable parallel sampling, improving performance in reasoning tasks.

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [35] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: The paper proposes using WGAN for high-resolution precipitation downscaling, achieving visually realistic results despite slightly lower conventional metric performance. The critic scores help identify unrealistic outputs and data artifacts.


<details>
  <summary>Details</summary>
Motivation: High-resolution precipitation prediction is crucial for mitigating damage from localized heavy rainfall, but traditional methods face challenges.

Method: The study employs Wasserstein Generative Adversarial Network (WGAN) with optimal transport cost for precipitation downscaling, comparing it to conventional neural networks.

Result: WGAN produces visually realistic precipitation fields with fine-scale structures, though it performs slightly worse on conventional metrics. The critic scores correlate with human perceptual realism and help detect unrealistic outputs and data artifacts.

Conclusion: The WGAN framework enhances perceptual realism in precipitation downscaling and provides a novel approach for evaluating and quality-controlling precipitation datasets.

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [36] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GraphEXT is a novel framework for explaining GNN predictions by leveraging cooperative game theory and social externalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The 'black-box' nature of GNNs and the lack of effective methods to capture node interactions motivated the development of GraphEXT.

Method: GraphEXT partitions nodes into coalitions, decomposing the graph into subgraphs, and uses Shapley value under externalities to quantify node importance.

Result: GraphEXT outperforms baseline methods in fidelity across diverse GNN architectures, enhancing explainability.

Conclusion: GraphEXT effectively improves GNN explainability by focusing on node interactions and structural changes.

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [37] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: Proposes molecular task arithmetic to generate positive molecules by training on negative examples and reversing property directions, outperforming models trained on positive data.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of positive molecules in generative design by leveraging abundant negative examples.

Method: Trains models on negative examples to learn property directions, then reverses these directions to generate positive molecules.

Result: Outperforms models trained on positive data in diversity and success, especially in zero-shot, dual-objective, and few-shot tasks.

Conclusion: Molecular task arithmetic is a simple, efficient, and effective transfer learning strategy for molecule design.

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [38] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: The paper introduces MFNO, a neural operator for stochastic systems, extending FNO with mirror padding for non-periodic inputs, proving its accuracy and showing superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning dynamics in stochastic systems, especially with non-periodic inputs, where standard architectures like LSTMs or TCNs fall short.

Method: MFNO extends FNO by incorporating mirror padding, enabling non-periodic input handling. Theoretical proofs use Wong-Zakai theorems and approximation techniques.

Result: MFNO achieves high accuracy in approximating stochastic dynamics, outperforms baselines (LSTMs, TCNs, DeepONet), and offers faster sample path generation.

Conclusion: MFNO is a robust, efficient solution for stochastic system dynamics, with strong theoretical backing and empirical performance.

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [39] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: The paper extends lower bounds for public-private learning to cases with significant distribution shift, showing that public data is either redundant or useless depending on the shift's magnitude.


<details>
  <summary>Details</summary>
Motivation: To understand the complementary value of public and private data in differentially private machine learning, especially under distribution shift.

Method: Extends known lower bounds to scenarios with distribution shift, focusing on Gaussian mean estimation and linear regression.

Result: For small shifts, abundant public or private data is needed; for large shifts, public data offers no benefit.

Conclusion: Public data's utility in private learning depends on the magnitude of distribution shift, with diminishing returns as shift increases.

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [40] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: The paper discusses Federated Learning (FL) as a distributed machine learning paradigm for cloud robotic manipulation, highlighting its advantages, challenges, and opportunities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of classical ML and individual robotic capabilities by leveraging FL for scalable and efficient cloud robotic manipulation.

Method: The paper explores FL's application in cloud robotics, proposing centralized or decentralized FL model designs for robotic manipulation tasks.

Result: FL offers potential benefits for cloud robotic manipulation but also introduces challenges like efficiency and reliability at scale.

Conclusion: The paper concludes by emphasizing the opportunities and challenges of using FL in cloud robotic manipulation, suggesting further research in centralized and decentralized FL models.

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [41] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: The study introduces a deep learning-based generative framework (pVAE) for inverse design of porous metamaterials, combining VAE with a regressor to tailor hydraulic properties like porosity and permeability. It uses LBM for data generation and CNN for property prediction, reducing computational costs. The framework is validated on synthetic and real datasets, enabling efficient structure-property exploration and inverse design.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges and limitations in designing porous metamaterials with tailored hydraulic properties, the study aims to leverage deep learning for efficient inverse design.

Method: Develops a property-variational autoencoder (pVAE) combining VAE and regressor, uses LBM for permeability data, and trains a CNN for property prediction. Validated on synthetic and real datasets.

Result: The pVAE framework successfully generates metamaterials with desired properties, reduces computational costs, and provides interpretable latent space for structure-property mapping.

Conclusion: The approach enables efficient inverse design of porous metamaterials, with open-access datasets and codes to support further research.

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [42] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: SETOL explains SOTA NN performance using heavy-tailed metrics and introduces a new metric, ERG, validated on MLPs and SOTA NNs.


<details>
  <summary>Details</summary>
Motivation: To formally explain the origin of heavy-tailed metrics in NN performance and predict test accuracies without training/testing data.

Method: Uses statistical mechanics, random matrix theory, and quantum chemistry to derive SETOL, introducing ERG as a new metric.

Result: Excellent agreement with theoretical assumptions on MLPs; SETOL and HTSR metrics align well on SOTA NNs.

Conclusion: SETOL provides a robust theoretical framework for understanding NN performance and introduces practical metrics for layer quality assessment.

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [43] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: Seed2Harvest combines human creativity and machine scalability to generate diverse adversarial prompts for robust T2I model evaluation.


<details>
  <summary>Details</summary>
Motivation: Current adversarial prompt datasets are either small and imbalanced (human-crafted) or lack realism (synthetic). A hybrid approach is needed for comprehensive testing.

Method: Seed2Harvest expands human-crafted adversarial prompts using machine guidance, preserving human-like attack patterns while increasing diversity.

Result: The method achieves high diversity (535 locations, 7.48 entropy) and maintains strong attack success rates (e.g., 0.31 NudeNet).

Conclusion: Human-machine collaboration is key for scalable, realistic adversarial testing of T2I models.

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [44] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse is a scalable deep learning framework for ultra-fine-grained city-wide OD flow predictions, addressing limitations of traditional and deep learning models by treating each POI as a node and using a three-stage transfer learning strategy.


<details>
  <summary>Details</summary>
Motivation: Accurate population flow prediction is crucial for urban planning, transportation, and public health, but existing methods lack generalization, computational efficiency, and fine-grained resolution.

Method: UrbanPulse combines a temporal graph convolutional encoder with a transformer-based decoder and employs a three-stage transfer learning strategy (pretraining, cold-start adaptation, reinforcement learning fine-tuning).

Result: Evaluated on 103 million GPS records from three California metropolitan areas, UrbanPulse achieves state-of-the-art accuracy and scalability.

Conclusion: UrbanPulse advances high-resolution urban forecasting, making it practical for diverse cities through efficient transfer learning.

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [45] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: The paper proposes MFTRR, a framework for post-quality assessment using multimodal data and relational reasoning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to leverage multimodal cues, introduce noise, and lack semantic relationship capture, limiting post-quality assessment.

Method: MFTRR reframes the task as ranking, using two modules: Local-Global Semantic Correlation Reasoning and Multi-Level Evidential Relational Reasoning.

Result: MFTRR achieves up to 9.52% NDCG@3 improvement over unimodal methods on the Art History dataset.

Conclusion: MFTRR effectively addresses limitations of existing methods, enhancing post-quality assessment through multimodal relational reasoning.

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [46] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE is a two-stage Transformer that fuses video, audio, and text features to predict fMRI activity, achieving high correlations in both in-distribution and out-of-distribution tests.


<details>
  <summary>Details</summary>
Motivation: To improve fMRI activity prediction by leveraging multi-modal features (video, audio, text) and advanced Transformer architectures.

Method: Uses a two-stage Transformer: modality-fusion for merging features (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) and temporal decoding with rotary embeddings. Trained on 65 hours of movie data (CNeuroMod) and ensembled across 20 seeds.

Result: Achieved mean parcel-wise Pearson correlations of 32.25 (in-distribution) and 21.25 (out-of-distribution). Earlier iteration won Phase-1 and placed second in Algonauts 2025 Challenge.

Conclusion: VIBE demonstrates strong performance in fMRI prediction, validating the effectiveness of multi-modal fusion and Transformer-based decoding.

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [47] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: The paper improves GeoAggregator (GA), a transformer-based model for geospatial tabular data, by optimizing its pipeline for efficiency and adding explainability features. The enhanced GA shows better accuracy, speed, and spatial effect capture.


<details>
  <summary>Details</summary>
Motivation: To enhance the computational efficiency and explainability of GeoAggregator (GA) for geospatial tabular data modeling.

Method: 1) Optimized pipeline for faster dataloading and streamlined forward pass. 2) Added model ensembling and GeoShapley-based explanation.

Result: Improved GA achieves higher prediction accuracy, faster inference, and effectively captures spatial effects in synthetic datasets.

Conclusion: The enhanced GA model is more efficient and interpretable, with its pipeline made publicly available.

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [48] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL is a privacy-conscious method for identifying data shifts in tabular datasets, outperforming baselines with high F1 scores even under noise.


<details>
  <summary>Details</summary>
Motivation: Privacy rules and noise complicate analysis of data shifts in healthcare datasets.

Method: SIFOTL uses privacy-compliant summary statistics, twin XGBoost models with LLM assistance, and a Pareto-weighted decision tree to identify interpretable segments.

Result: Achieves F1 scores of 0.85-0.96, outperforming baselines (F1=0.19-0.67).

Conclusion: SIFOTL offers a robust, interpretable, and privacy-safe solution for data shift analysis.

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [49] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: A novel learning paradigm, Machine Unlearning TSEP, is introduced to allow trained traffic state estimation and prediction models to selectively forget sensitive, outdated, or poisoned data, enhancing trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Address privacy, cybersecurity, and data freshness concerns in data-driven TSEP, which relies on sensitive data and must comply with regulations like the 'right to be forgotten.'

Method: Proposes Machine Unlearning TSEP, enabling models to selectively forget problematic data without retraining from scratch.

Result: Enhances the reliability and trustworthiness of TSEP models by ensuring compliance with privacy regulations and improving data quality.

Conclusion: Machine Unlearning TSEP is a promising solution to privacy and data quality challenges in traffic state estimation and prediction.

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [50] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: A predictive framework optimizes resource usage for fine-tuning LLMs with GRPO, identifying three training phases and suggesting early stopping to save compute.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs with GRPO is computationally expensive, prompting the need for a resource-efficient method.

Method: Proposes a predictive framework modeling training dynamics, tested on Llama and Qwen models (3B, 8B) to derive an empirical scaling law.

Result: Identifies three training phases (slow start, rapid improvement, plateau) and shows early stopping reduces compute without performance loss.

Conclusion: The framework generalizes across models, offering a practical guide for efficient GRPO-based fine-tuning.

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [51] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: A deep learning framework using neural operators is introduced to solve PDEs and downscale ocean current data, providing high-resolution solutions for applications like coastal management.


<details>
  <summary>Details</summary>
Motivation: High-resolution ocean current data is crucial for coastal management, environmental monitoring, and maritime safety, but existing satellite products and models lack sufficient spatial granularity.

Method: A supervised deep learning framework based on neural operators is developed to solve PDEs and downscale data, including Copernicus ocean current data. The method also models surrogate PDEs for arbitrary resolution predictions.

Result: The model was tested on real-world Copernicus data and synthetic Navier-Stokes datasets, demonstrating its ability to provide high-resolution solutions.

Conclusion: The proposed framework effectively addresses the resolution limitations of current oceanographic data, offering scalable and accurate solutions for PDE-based modeling.

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [52] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO is a reinforcement learning algorithm for training large language models, outperforming GRPO in efficiency and stability, especially for MoE RL training.


<details>
  <summary>Details</summary>
Motivation: To improve training efficiency and stability in reinforcement learning for large language models by moving from token-level to sequence-level optimization.

Method: GSPO uses sequence likelihood for importance ratios, performing sequence-level clipping, rewarding, and optimization.

Result: GSPO outperforms GRPO, stabilizes MoE RL training, and simplifies RL infrastructure design, enhancing Qwen3 models.

Conclusion: GSPO is a superior, stable, and efficient algorithm for RL in large language models, with demonstrated success in Qwen3.

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [53] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: C-AAE combines an Anonymizing AutoEncoder (AAE) with ADPCM to protect user privacy in wearable sensor data while maintaining activity recognition accuracy and reducing data volume.


<details>
  <summary>Details</summary>
Motivation: Wearable sensors capture behavioral data that can re-identify users, posing privacy risks for healthcare applications.

Method: C-AAE uses an AAE to suppress identity cues in sensor data and ADPCM to further anonymize and compress the data.

Result: C-AAE reduces re-identification F1 scores by 10-15 points, maintains activity recognition accuracy, and cuts data volume by 75%.

Conclusion: C-AAE effectively balances privacy and utility for sensor-based healthcare applications.

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [54] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM is a staged mixed-precision PTQ framework that reduces LLM weights by 10x, achieving 1.6 bits per weight with innovations like PBAR and FIAS, improving accuracy from 43% to 56%.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face deployment challenges due to high computational costs and massive parameters. Ultra low-bit quantization often degrades performance, necessitating a better solution.

Method: Proposes Squeeze10-LLM, a staged mixed-precision PTQ framework, using PBAR (Post-Binarization Activation Robustness) and FIAS (Full Information Activation Supervision) to optimize quantization.

Result: Achieves 1.6 bits per weight, improving accuracy from 43% to 56% on six zero-shot tasks, outperforming existing PTQ methods.

Conclusion: Squeeze10-LLM offers an efficient solution for extreme LLM compression, balancing performance and computational savings.

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [55] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for improving classification models with limited training data by combining hard labels and additional supervision into soft labels. It identifies the distribution over non-hard-labeled classes as the key beneficial component of additional supervision.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited training data and explore how additional supervision (e.g., label confidences) can enhance model accuracy.

Method: Theoretical framework treating hard labels and additional supervision as probability distributions, constructing soft labels via affine combination. Analyzes the roles of additional supervision and mixing coefficient.

Result: Additional supervision's distribution over non-hard-labeled classes is crucial. The mixing coefficient and supervision refine soft labels complementarily, improving generalization.

Conclusion: The framework theoretically and experimentally shows that additional supervision, when designed properly, enhances classification accuracy.

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [56] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: The paper addresses RAN slicing in O-RAN, proposing PDA-DRL for delay-aware resource allocation, reducing average delay by 38% and introducing a reward-based personalization method for MVNOs.


<details>
  <summary>Details</summary>
Motivation: To optimize PRB utilization while meeting probabilistic delay constraints for MVNOs in O-RAN.

Method: Derives a reward function using LLN, adapts it for real-world use, and introduces PDA-DRL for delay-aware resource allocation. Also proposes reward-based model weight personalization.

Result: PDA-DRL reduces average delay by 38% compared to baselines. Reward-based personalization outperforms traditional methods.

Conclusion: PDA-DRL and reward-based personalization effectively optimize RAN slicing, improving delay performance and resource utilization.

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [57] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: GraphALP is a novel framework using LLMs and pseudo-labeling to address class-imbalanced graph node classification with noisy labels, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs often have class imbalance and noisy labels, but existing methods assume clean labels, creating a gap in robust classification.

Method: GraphALP combines LLM-based oversampling for minority nodes, dynamically weighted pseudo-labeling for noise reduction, and secondary LLM-guided oversampling to balance class distribution.

Result: GraphALP outperforms state-of-the-art methods on class-imbalanced graphs with noisy labels.

Conclusion: GraphALP effectively addresses class imbalance and label noise, offering a robust solution for real-world graph node classification.

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [58] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: Proposes an adversarial attack method for RL systems using LLMs to generate tailored adversarial rewards and a critical state identification algorithm, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of adversarial attacks in RL without modifying the environment or policy, enhancing practicality.

Method: Uses LLMs for reward iteration optimization and a critical state identification algorithm to exploit agent vulnerabilities.

Result: Demonstrates superiority in inducing suboptimal behavior in diverse environments.

Conclusion: The method effectively enhances adversarial attacks on RL systems without environmental modifications.

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [59] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: FedSA-GCL is a semi-asynchronous federated framework for graph learning, addressing inefficiencies of synchronous methods and limitations of AFL in graph data. It outperforms baselines by 2.92-3.4%.


<details>
  <summary>Details</summary>
Motivation: Existing FGL methods rely on synchronous communication, which is inefficient, while AFL methods ignore graph topology, risking semantic drift.

Method: Proposes FedSA-GCL, a semi-asynchronous framework using ClusterCast to leverage label divergence and graph topology.

Result: Outperforms 9 baselines by 2.92% (Louvain) and 3.4% (Metis) in robustness and efficiency.

Conclusion: FedSA-GCL effectively addresses inefficiencies and graph-specific challenges, demonstrating superior performance.

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [60] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: Language models self-improve using their own confidence for mathematical reasoning, outperforming majority voting and being less biased than BoN.


<details>
  <summary>Details</summary>
Motivation: To explore how language models can scale their performance at test-time for mathematical reasoning tasks by leveraging their own confidence.

Method: Uses prefix-confidence scaling to select the most promising attempts, evaluated on five datasets (GSM8K, MATH500, AMC23, AIME24, AIME25).

Result: Prefix-confidence scaling with 32-token prefixes achieves better accuracy-compute trade-off than majority voting and is less biased than BoN. Test-time training improves over the base model but not over prefix-confidence scaling.

Conclusion: Prefix-confidence scaling is effective for improving language model performance in mathematical reasoning, offering advantages over existing methods.

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [61] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: GLANCE is a novel GNN framework designed for heterophilous graphs, combining logic-guided reasoning, dynamic refinement, and clustering to improve performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with heterophilous graphs due to indiscriminate neighbor aggregation and lack of higher-order structural patterns.

Method: GLANCE integrates logic layers, multi-head attention for edge pruning, and clustering mechanisms to refine graph structures and capture global patterns.

Result: GLANCE achieves competitive performance on benchmark datasets (Cornell, Texas, Wisconsin) for heterophilous graphs.

Conclusion: GLANCE is a lightweight, adaptable, and interpretable solution for heterophilous graph challenges.

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [62] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: A survey on neuromorphic computing for autonomous systems, covering algorithms, hardware, and optimization, with a focus on event-based vision sensors and spiking neural networks.


<details>
  <summary>Details</summary>
Motivation: The need for intelligent, adaptive, and energy-efficient autonomous systems in robotics, UAVs, and self-driving vehicles drives interest in neuromorphic computing.

Method: Surveys recent progress in neuromorphic algorithms, hardware, and cross-layer optimization, emphasizing event-based dynamic vision sensors and spiking neural networks.

Result: Highlights improved energy efficiency, robustness, adaptability, and reliability in autonomous systems through neuromorphic approaches.

Conclusion: Identifies emerging trends and open challenges in real-time decision-making, continual learning, and secure autonomous systems.

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [63] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: C2G-KD is a data-free knowledge distillation method using a class-conditional generator guided by a teacher model and PCA-derived constraints, achieving effective synthetic training with minimal real data.


<details>
  <summary>Details</summary>
Motivation: To enable knowledge distillation without real training data by leveraging synthetic samples generated under geometric and semantic constraints.

Method: Uses a class-conditional generator trained with semantic and structural losses, constrained by PCA subspaces from few real examples.

Result: Demonstrates effectiveness on MNIST, showing minimal class structure suffices for useful synthetic training.

Conclusion: C2G-KD successfully bootstraps synthetic training pipelines with limited real data, preserving consistency and diversity.

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [64] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: The paper addresses the challenge of testing AI fairness without complete demographic data by proposing synthetic data generation from overlapping datasets, validated for accuracy and consistency with real data.


<details>
  <summary>Details</summary>
Motivation: The need for fairness testing in AI is urgent due to regulations, but data scarcity and privacy concerns hinder access to demographic data required for bias audits.

Method: Leveraging separate overlapping datasets to construct synthetic data with demographics, ensuring it reflects real-world relationships and validating its fidelity.

Result: Fairness metrics from synthetic data align with those from real data, proving its effectiveness for fairness testing.

Conclusion: Synthetic data offers a practical solution to data scarcity, enabling independent and model-agnostic fairness evaluation.

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [65] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: ChronoSelect is a novel framework for learning with noisy labels, using a four-stage memory architecture and temporal dynamics to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for noisy labels suffer from static evaluations and ignore learning evolution dynamics.

Method: Proposes ChronoSelect with a four-stage memory architecture, sliding updates, and decay to analyze temporal trajectories for sample partitioning.

Result: Achieves state-of-the-art performance on synthetic and real-world benchmarks with theoretical convergence guarantees.

Conclusion: ChronoSelect effectively leverages temporal dynamics for robust learning under noisy labels.

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [66] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: A new GNN model improves generalization for autonomous driving by using a heterogeneous graph with traffic participants and road networks to classify goals in multi-staged trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with generalization to new/unseen areas, leading to performance drops.

Method: Proposes a GNN with a heterogeneous graph (traffic participants + vectorized road network) for multi-staged goal classification in trajectory prediction.

Result: Demonstrates effectiveness via cross-dataset evaluation (trained on Argoverse2, tested on NuScenes).

Conclusion: The goal selection process enhances generalization in unseen scenarios.

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [67] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: SINDy-LOM combines sparse regression and library optimization to improve dynamical system modeling by optimizing basis functions for long-term prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional SINDy struggles with library design for complex systems, requiring manual effort and lacking long-term reliability.

Method: SINDy-LOM uses a two-layer optimization: inner-layer for sparse regression and outer-layer for optimizing parametrized basis functions based on recursive long-term prediction accuracy.

Result: The approach yields interpretable, parsimonious models and reduces user burden while improving reliability over traditional SINDy.

Conclusion: SINDy-LOM is validated on a diesel engine airpath system, demonstrating effectiveness for complex industrial applications.

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [68] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: A large-scale study of six LP-based boosting methods, including two new ones (NM-Boost and QRLP-Boost), shows they can match or outperform XGBoost and LightGBM with shallow trees while being sparser.


<details>
  <summary>Details</summary>
Motivation: To empirically evaluate the effectiveness of totally corrective boosting methods based on linear programming, which have been theoretically appealing but underexplored.

Method: Conducted experiments on 20 diverse datasets using six LP-based boosting formulations, including heuristic and optimal base learners. Analyzed accuracy, sparsity, margin distribution, anytime performance, and hyperparameter sensitivity.

Result: Totally corrective methods outperform or match XGBoost/LightGBM with shallow trees and produce sparser ensembles. They can also thin pre-trained ensembles without performance loss.

Conclusion: LP-based boosting methods are viable alternatives to heuristics, offering sparsity and performance benefits, though optimal decision trees have limitations.

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [69] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM is a self-supervised learning framework for Predictive Process Monitoring, using Siamese learning and Statistical Augmentation to improve data variability and performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning PPM approaches struggle with low variability and small size of real-world event logs.

Method: Combines Siamese learning with Statistical Augmentation, using three novel transformation methods to generate realistic trace variants.

Result: Outperforms SOTA in next activity and final outcome prediction, with statistical augmentation proving superior to random transformations.

Conclusion: SiamSA-PPM is a promising approach for data enrichment and improved performance in process prediction.

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [70] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: An algorithm for coarsening unstructured grids using differentiable physics, k-means clustering, autodifferentiation, and stochastic minimization, reducing grid points by up to 10x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational load in numerical simulations necessitates methods to reduce problem size without sacrificing accuracy.

Method: Uses k-means clustering, autodifferentiation, and stochastic minimization to coarsen grids. Tested on linear parabolic and wave equations.

Result: Reduced grid points by up to 10 times while preserving variable dynamics in key points.

Conclusion: The approach is effective for systems described by evolutionary PDEs and can be generalized.

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [71] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: The paper addresses security regression in continual learning (CL)-based malware detectors, proposing a regression-aware penalty to mitigate harmful prediction changes while maintaining detection performance.


<details>
  <summary>Details</summary>
Motivation: Malware evolves rapidly, making full retraining impractical. CL offers scalability but introduces security regression risks, where previously detected malware evades detection after updates, undermining trust.

Method: The authors formalize and quantify security regression, adapting Positive Congruent Training (PCT) to CL to preserve prior predictive behavior model-agnostically.

Result: Experiments on ELSA, Tesseract, and AZ-Class datasets show the method effectively reduces regression across CL scenarios while maintaining strong detection performance.

Conclusion: The proposed regression-aware penalty mitigates security regression in CL-based malware detectors, ensuring reliable updates without compromising detection accuracy.

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [72] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: A novel architecture, TIDSIT, is proposed for accurate battery health monitoring, addressing challenges of irregular data and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Battery health monitoring is critical for safety and efficiency, but existing models struggle with irregular real-world data.

Method: TIDSIT uses continuous time embeddings and temporal attention to handle irregular and variable-length discharge cycle data without information loss.

Result: TIDSIT reduces prediction error by over 50% and achieves an SoH prediction error below 0.58%.

Conclusion: TIDSIT is effective for battery health monitoring and has potential for broader applications in irregular time-series tasks.

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [73] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets reduce complexity and improve performance in operator learning by using low-rank adaptation (LoRA) to decompose hypernetwork weights.


<details>
  <summary>Details</summary>
Motivation: Address the high memory and computational costs of HyperDeepONets while improving expressivity.

Method: Introduce PI-LoRA-HyperDeepONets, leveraging LoRA to decompose hypernetwork output weights into low-rank matrices, reducing parameters and adding regularization.

Result: Achieves up to 70% parameter reduction and outperforms HyperDeepONets in predictive accuracy and generalization.

Conclusion: PI-LoRA-HyperDeepONets offer a more efficient and effective alternative for operator learning in physics-informed settings.

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [74] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: The paper introduces a method for efficient uncertainty estimation in LLMs by distilling uncertainty-aware teacher models into compact student models using LoRA, achieving comparable performance with fewer computational costs.


<details>
  <summary>Details</summary>
Motivation: Standard LLMs struggle with accurate uncertainty quantification, and existing Bayesian/ensemble methods are computationally expensive.

Method: Distill uncertainty-aware teacher models into student models using LoRA, comparing softmax-based and Dirichlet-distributed outputs for uncertainty modeling.

Result: Student models achieve comparable or better predictive and uncertainty performance than teachers, with only one forward pass.

Conclusion: Evidential distillation enables efficient and robust uncertainty quantification in LLMs, a novel advancement.

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [75] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: The paper reviews the application of diffusion models in agriculture, highlighting their advantages over GANs in data augmentation, image generation, and denoising, while acknowledging challenges like computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of arable land and the need for smart agriculture, the paper explores AI's role, particularly diffusion models, in enhancing agricultural tasks like pest detection and remote sensing.

Method: The study reviews diffusion models' applications in agriculture, focusing on crop pest detection, remote sensing enhancement, and growth prediction, comparing them to GANs.

Result: Diffusion models improve accuracy and robustness in tasks like data augmentation and image generation, outperforming GANs in complex environments.

Conclusion: Despite computational challenges, diffusion models hold significant promise for smart and precision agriculture, supporting global agricultural sustainability.

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [76] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER combines multi-model ensemble and reservoir computing for efficient, accurate flood prediction in data-scarce regions, outperforming benchmarks like LSTM in computational efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Many regions lack river discharge data, limiting flood prediction accuracy. Existing models struggle with accuracy, interpretability, and efficiency under data scarcity.

Method: HYPER uses Bayesian model averaging (BMA) on 43 uncalibrated hydrological models, then corrects errors via reservoir computing (RC) for efficiency. Weights are inferred from gauged basins for ungauged ones.

Result: HYPER matched LSTM performance (KGE 0.56 vs. 0.55) in data-rich cases but was 20x faster. In data-scarce cases, HYPER maintained KGE 0.55, while LSTM dropped to -0.04.

Conclusion: HYPER offers a robust, efficient solution for discharge prediction, especially in ungauged basins, without requiring individual model calibration.

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [77] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: The paper identifies flaws in the conventional bisimulation metric, proposes a revised version with adaptive coefficients, and validates its effectiveness through theory and experiments.


<details>
  <summary>Details</summary>
Motivation: The conventional bisimulation metric fails to represent certain scenarios and relies on fixed weights, limiting its adaptability and precision.

Method: Introduces a state-action pair measure, a refined reward gap definition, and adaptive update operators.

Result: Theoretical convergence guarantees and improved representation distinctiveness are proven. Experiments on DeepMind Control and Meta-World benchmarks confirm effectiveness.

Conclusion: The revised bisimulation metric addresses prior limitations, offering better adaptability and precision in reinforcement learning tasks.

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [78] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: The paper introduces a universal FMB law (force-metric-bias) using the Price equation to unify diverse learning algorithms, optimization methods, and natural selection under a common mathematical framework.


<details>
  <summary>Details</summary>
Motivation: To reveal the shared mathematical structure among seemingly disparate learning and optimization processes, enabling a unified understanding and design of algorithms.

Method: The Price equation is used to partition change into force, metric, bias, and noise components, forming the FMB law.

Result: The FMB law unifies natural selection, Bayesian updating, Newton's method, stochastic gradient descent, and other algorithms as special cases.

Conclusion: The FMB law provides a principled foundation for understanding, comparing, and designing learning algorithms across disciplines.

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [79] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: GPTQ, a method for quantizing LLM weights, is mathematically equivalent to Babai's nearest plane algorithm, providing geometric insights and error bounds.


<details>
  <summary>Details</summary>
Motivation: To clarify the theoretical foundations of GPTQ and connect it to established lattice algorithms for better quantization methods.

Method: Mathematical equivalence of GPTQ to Babai's algorithm for the closest vector problem (CVP) on a lattice defined by the Hessian matrix.

Result: GPTQ's error propagation gains a geometric interpretation and inherits an error upper bound from Babai's algorithm.

Conclusion: GPTQ is theoretically grounded, enabling future advancements in quantization by leveraging lattice algorithm research.

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [80] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: Analysis of Fisher information matrices and NTK for 2-layer ReLU networks, focusing on their linear transformation relation, spectral decomposition, and function approximation.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between Fisher information matrices and NTK in 2-layer ReLU networks with random hidden weights.

Method: Discuss the linear transformation relation, derive spectral decomposition of NTK with eigenfunctions, and develop an approximation formula for network functions.

Result: Concrete forms of eigenfunctions with major eigenvalues and an approximation formula for 2-layer neural network functions.

Conclusion: The study provides insights into the relationship between Fisher information and NTK, along with practical tools for analyzing neural networks.

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [81] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: Proposes a linear-memory SE(2) invariant transformer for spatial data processing in autonomous driving tasks, outperforming non-invariant methods.


<details>
  <summary>Details</summary>
Motivation: Spatial data processing is crucial for autonomous driving tasks, but existing SE(2) invariant methods require quadratic memory, limiting scalability.

Method: Introduces an SE(2) invariant scaled dot-product attention mechanism with linear memory usage, leveraging transformer architecture.

Result: The approach is practical to implement and improves performance over non-invariant architectures.

Conclusion: The proposed SE(2) invariant transformer offers scalable and efficient spatial data processing for autonomous driving applications.

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [82] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: A multi-level conditional diffusion model integrates sequence and structure data for functional protein design, with a new evaluation metric, Protein-MMD, to assess quality.


<details>
  <summary>Details</summary>
Motivation: Reliable generation of functional proteins remains challenging, especially with conditional diffusion models, due to the multi-level nature of protein structures.

Method: Proposes a multi-level conditional diffusion model combining sequence and structure information for end-to-end protein design, and introduces Protein-MMD for evaluation.

Result: The framework effectively models hierarchical protein relations, and Protein-MMD captures distributional and functional similarities between real and generated proteins.

Conclusion: The proposed model and metric demonstrate efficacy in conditional protein generation tasks, advancing de novo protein design.

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


### [83] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: A gait recognition system using Tiny ML and IMU sensors achieves over 80% accuracy in classifying four activities, with low-power operation for battery-powered devices.


<details>
  <summary>Details</summary>
Motivation: To develop a real-time, low-power gait recognition system for activity classification using Tiny ML and IMU sensors.

Method: Uses XIAO-nRF52840 Sense microcontroller and LSM6DS3 IMU sensor to capture motion data, processed via Edge Impulse for training a DNN classifier with sliding windows and normalization.

Result: Over 80% accuracy in classifying walking, stationary, upstairs, and downstairs activities, with anomaly detection for robustness.

Conclusion: The system effectively classifies activities with high accuracy and low power consumption, suitable for battery-powered applications.

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [84] [Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation](https://arxiv.org/abs/2507.17852)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.MA

TL;DR: This paper details Tippy's multi-agent system for drug discovery lab automation, featuring a distributed microservices architecture with five specialized agents, OpenAI Agents SDK orchestration, and Kubernetes deployment.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how specialized AI agents can coordinate complex lab workflows while ensuring security, scalability, and integration with existing infrastructure.

Method: Uses a distributed microservices architecture with five agents, OpenAI Agents SDK for orchestration, Model Context Protocol (MCP) for lab tool access, and Kubernetes for deployment.

Result: The system effectively coordinates lab workflows, integrates with existing infrastructure, and ensures security and scalability.

Conclusion: Specialized AI agents can successfully automate and coordinate complex drug discovery lab workflows using standardized protocols and modern deployment strategies.

Abstract: Building on the conceptual framework presented in our previous work on
agentic AI for pharmaceutical research, this paper provides a comprehensive
technical analysis of Tippy's multi-agent system implementation for drug
discovery laboratory automation. We present a distributed microservices
architecture featuring five specialized agents (Supervisor, Molecule, Lab,
Analysis, and Report) that coordinate through OpenAI Agents SDK orchestration
and access laboratory tools via the Model Context Protocol (MCP). The system
architecture encompasses agent-specific tool integration, asynchronous
communication patterns, and comprehensive configuration management through
Git-based tracking. Our production deployment strategy utilizes Kubernetes
container orchestration with Helm charts, Docker containerization, and CI/CD
pipelines for automated testing and deployment. The implementation integrates
vector databases for RAG functionality and employs an Envoy reverse proxy for
secure external access. This work demonstrates how specialized AI agents can
effectively coordinate complex laboratory workflows while maintaining security,
scalability, reliability, and integration with existing laboratory
infrastructure through standardized protocols.

</details>


### [85] [Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation](https://arxiv.org/abs/2507.18224)
*Shiyuan Li,Yixin Liu,Qingsong Wen,Chengqi Zhang,Shirui Pan*

Main category: cs.MA

TL;DR: ARG-Designer reframes multi-agent system (MAS) design as a conditional autoregressive graph generation task, dynamically creating collaboration graphs tailored to task-specific needs, outperforming existing methods in performance, efficiency, and extensibility.


<details>
  <summary>Details</summary>
Motivation: Existing MAS designs are limited by rigid templates and predefined structures, hindering adaptability to diverse tasks.

Method: ARG-Designer uses an autoregressive model to generate collaboration graphs from scratch, dynamically determining agent roles and communication links based on task queries.

Result: ARG-Designer achieves state-of-the-art performance, token efficiency, and extensibility across six benchmarks.

Conclusion: ARG-Designer offers a flexible, extensible solution for MAS design, addressing limitations of traditional approaches.

Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged
as a powerful solution for dealing with complex problems across diverse
domains. The effectiveness of MAS is critically dependent on its collaboration
topology, which has become a focal point for automated design research.
However, existing approaches are fundamentally constrained by their reliance on
a template graph modification paradigm with a predefined set of agents and
hard-coded interaction structures, significantly limiting their adaptability to
task-specific requirements. To address these limitations, we reframe MAS design
as a conditional autoregressive graph generation task, where both the system
composition and structure are designed jointly. We propose ARG-Designer, a
novel autoregressive model that operationalizes this paradigm by constructing
the collaboration graph from scratch. Conditioned on a natural language task
query, ARG-Designer sequentially and dynamically determines the required number
of agents, selects their appropriate roles from an extensible pool, and
establishes the optimal communication links between them. This generative
approach creates a customized topology in a flexible and extensible manner,
precisely tailored to the unique demands of different tasks. Extensive
experiments across six diverse benchmarks demonstrate that ARG-Designer not
only achieves state-of-the-art performance but also enjoys significantly
greater token efficiency and enhanced extensibility. The source code of
ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.

</details>


### [86] [Designing Value-Aligned Traffic Agents through Conflict Sensitivity](https://arxiv.org/abs/2507.18284)
*Astrid Rakow,Joe Collenette,Maike Schwammberger,Marija Slavkovik,Gleifer Vs Alves*

Main category: cs.MA

TL;DR: The paper proposes using epistemic game theory to model value conflicts in autonomous traffic agents (ATAs), focusing on aligning behavior with stakeholder values during design rather than runtime.


<details>
  <summary>Details</summary>
Motivation: To ensure ATAs act safely and align with legal, social, and moral values, addressing value conflicts proactively.

Method: Adopts a formal model from epistemic game theory for conflict analysis, applied to value elicitation, capability specification, explanation, and adaptive refinement. Introduces Value-Aligned Operational Design Domains (VODDs).

Result: Shifts focus from runtime moral dilemmas to structured, value-sensitive behavior during development.

Conclusion: The approach enhances ATA design by embedding value alignment early, reducing runtime conflicts.

Abstract: Autonomous traffic agents (ATAs) are expected to act in ways tat are not only
safe, but also aligned with stakeholder values across legal, social, and moral
dimensions. In this paper, we adopt an established formal model of conflict
from epistemic game theory to support the development of such agents. We focus
on value conflicts-situations in which agents face competing goals rooted in
value-laden situations and show how conflict analysis can inform key phases of
the design process. This includes value elicitation, capability specification,
explanation, and adaptive system refinement. We elaborate and apply the concept
of Value-Aligned Operational Design Domains (VODDs) to structure autonomy in
accordance with contextual value priorities. Our approach shifts the emphasis
from solving moral dilemmas at runtime to anticipating and structuring
value-sensitive behaviour during development.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [87] [Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search](https://arxiv.org/abs/2507.17953)
*Chang Eun Song,Weihong Xu,Keming Fan,Soumil Jain,Gopabandhu Hota,Haichao Yang,Leo Liu,Kerem Akarvardar,Meng-Fan Chang,Carlos H. Diaz,Gert Cauwenberghs,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: Clo-HDnn is an on-device learning accelerator for continual learning, combining hyperdimensional computing and efficient feature extraction to boost accuracy and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and accurate continual learning on devices, leveraging hyperdimensional computing and gradient-free updates.

Method: Integrates Kronecker HD Encoder and weight clustering feature extraction, uses gradient-free CL, dual-mode operation, and progressive search for reduced complexity.

Result: Achieves 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier), with 7.77x and 4.85x higher energy efficiency than SOTA ODL accelerators.

Conclusion: Clo-HDnn is a highly efficient and accurate solution for on-device continual learning tasks.

Abstract: Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging
continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing
(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature
extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts
gradient-free CL to efficiently update and store the learned knowledge in the
form of class hypervectors. Its dual-mode operation enables bypassing costly
feature extraction for simpler datasets, while progressive search reduces
complexity by up to 61% by encoding and comparing only partial query
hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),
Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL
accelerators.

</details>
