<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 35]
- [cs.LG](#cs.LG) [Total: 38]
- [cs.MA](#cs.MA) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: A multimodal fake review detection framework combining BERT for text and ResNet-50 for images outperforms unimodal approaches, achieving 0.934 F1-score by capturing cross-modal inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Fake reviews generated by bots/AI threaten digital trust, but existing detection methods only use single modalities and miss semantic inconsistencies across text and images.

Method: Proposes a framework integrating BERT-encoded textual features and ResNet-50 visual features, fused through a classification head, using a curated dataset of 21,142 user-uploaded images from food, hospitality, and e-commerce.

Result: Multimodal model achieves 0.934 F1-score, outperforming unimodal baselines, and qualitative analysis shows it detects subtle inconsistencies like mismatched text and images.

Conclusion: Multimodal learning is crucial for digital trust, offering a scalable solution for content moderation across online platforms by effectively identifying deceptive reviews.

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [2] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: Multi-agent reinforcement learning with graph attention (MAPPO+GAT) outperforms baseline MAPPO for dynamic retail pricing by enabling product interaction modeling, improving profit and stability.


<details>
  <summary>Details</summary>
Motivation: Retail dynamic pricing requires adaptive policies that coordinate decisions across related products, needing scalable solutions for multi-product decision-making.

Method: Systematic empirical study comparing MAPPO baseline with graph-attention-augmented variant (MAPPO+GAT) using simulated pricing environment from real transaction data.

Result: MAPPO+GAT enhances performance by sharing information over product graph without excessive price volatility, providing better profit, stability, fairness, and training efficiency.

Conclusion: Graph-integrated MARL offers more scalable and stable solution than independent learners for dynamic retail pricing, with practical advantages in multi-product decision-making.

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [3] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC is a framework for population analysis requiring validated data processes. This paper details data-processing methods for Austria using publicly available data and validates the GEPOC ABM model.


<details>
  <summary>Details</summary>
Motivation: To enable valid application of GEPOC models for specific regions like Austria by establishing reproducible data processes using accessible data.

Method: Describes algorithms for data aggregation, disaggregation, fusion, cleansing, and scaling to compute model parameters exclusively from free public data.

Result: Complete parameter files for Austria and an extensive validation study of the GEPOC ABM model are produced.

Conclusion: The work provides a robust data-processing framework for GEPOC applications, validated through the ABM model, enhancing reproducibility in population research.

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [4] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench is a new benchmark with 800 multiple-choice questions to evaluate how well LLMs understand quantum science concepts.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLM benchmarks don't adequately test domain-specific knowledge in complex fields like quantum science which requires advanced mathematics and deals with non-intuitive phenomena.

Method: Created approximately 800 multiple-choice questions with answers spanning nine quantum science areas using publicly available materials, organized into an eight-option format.

Result: Evaluated several existing LLMs using QuantumBench and analyzed their quantum domain performance, including sensitivity to question format changes.

Conclusion: QuantumBench is the first quantum-specific LLM evaluation dataset and aims to guide effective LLM use in quantum research by providing proper domain evaluation.

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [5] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: Engineering.ai is a multi-agent AI platform that autonomously performs complex engineering design tasks using specialized AI engineers coordinated by a Chief Engineer, achieving 100% success rate in UAV wing optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional engineering teams require substantial time/cost for multidisciplinary collaboration. Existing AI tools like OpenFOAMGPT show promise but need expansion to full engineering workflow automation.

Method: Hierarchical multi-agent architecture with Chief Engineer coordinating specialized agents (Aerodynamics, Structural, Acoustic, Optimization) using LLMs. File-mediated communication ensures data provenance, with memory system maintaining project context.

Result: Validated through UAV wing optimization: 100% success rate across 400+ configurations, zero mesh failures/convergence issues, no manual interventions required.

Conclusion: Agentic AI engineers can autonomously perform complex engineering tasks reliably, demonstrating trustworthy automation potential for multidisciplinary design.

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [6] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN is an open-source procedural generator that extends the ARC-AGI training dataset to address its limited demonstration set, creating faithful synthetic examples for all 400 tasks to improve skill acquisition efficiency evaluation.


<details>
  <summary>Details</summary>
Motivation: The ARC-AGI benchmark measures skill acquisition efficiency but has limited demonstration sets with only a few input-output pairs per task, which constrains algorithms requiring extensive intra-task exemplars.

Method: Developed ARC-GEN, an exhaustive and mimetic procedural generator that covers all 400 ARC-AGI tasks and closely honors the distributional properties of the original dataset to create viable sample pairs.

Result: Created an extended training dataset that faithfully reproduces ARC-AGI's characteristics, enabling better evaluation of skill acquisition efficiency and serving as a static benchmark for program verification.

Conclusion: ARC-GEN successfully addresses the limited demonstration problem in ARC-AGI by generating faithful synthetic examples, making it valuable for AGI research and as a verification benchmark for programming competitions.

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [7] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: Improved incremental selection algorithm that successfully proves all conjectures


<details>
  <summary>Details</summary>
Motivation: To enhance the existing selection algorithm from previous work [1]

Method: Developed an improved incremental selection algorithm

Result: Successfully proved all the selected conjectures

Conclusion: The improved algorithm effectively addresses the limitations of the previous approach

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [8] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: This review explores how LLMs can address challenges in cognitive science like knowledge synthesis and conceptual clarity through cross-disciplinary connections, theory formalization, and integrated modeling, while noting their limitations.


<details>
  <summary>Details</summary>
Motivation: Cognitive science struggles with knowledge synthesis and conceptual clarity due to its interdisciplinary nature, and recent AI advances (LLMs) offer potential tools to mitigate these issues.

Method: The review examines LLMs' applications in establishing cross-disciplinary links, formalizing theories, creating measurement taxonomies, achieving generalizability via modeling frameworks, and capturing contextual/individual variation.

Result: LLMs show promise in supporting cognitive science but have limitations and potential pitfalls; they are most effective when complementing human expertise.

Conclusion: LLMs can facilitate a more integrative and cumulative cognitive science if used judiciously as tools to augment, not replace, human input.

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [9] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: Update on DAF-MIT AI Accelerator challenges, showcasing their role in advancing AI research through public datasets and open-source solutions.


<details>
  <summary>Details</summary>
Motivation: To expand U.S. competitive advantage in defense and civilian sectors by pioneering fundamental AI advances through collaborative challenges.

Method: Development and launch of public challenge problems with large, AI-ready datasets to engage academic and private sectors.

Result: Successful contributions to AI research and applications, building on previous challenges.

Conclusion: The AI Accelerator program effectively stimulates open-source AI advancements and ecosystem engagement.

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [10] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: A multi-agent system using LLMs automates telecom network troubleshooting by coordinating specialized agents to diagnose faults and generate remediation plans, with a fine-tuned SLM for domain-specific solution planning.


<details>
  <summary>Details</summary>
Motivation: Telecom networks are growing in scale and complexity, making management challenging. Existing AI models are narrow in scope, require large labeled data, and struggle to generalize, forcing continued reliance on human experts for troubleshooting.

Method: Proposed a Multi-Agent System (MAS) with LLMs coordinating specialized agents (orchestrator, solution planner, executor, data retriever, root-cause analyzer) for automated troubleshooting. Fine-tuned a Small Language Model on proprietary documents for domain-grounded solution planning.

Result: The framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains, enabling rapid fault diagnosis and remediation strategy recommendations.

Conclusion: The proposed multi-agent system with LLM coordination and fine-tuned SLM successfully addresses limitations of existing AI approaches in telecom networks, enabling fully automated troubleshooting that reduces reliance on human experts.

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [11] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE is a new benchmark testing LLM reliability in legal contract analysis through 7,500+ perturbed contracts and persona-driven anomalies, revealing models often miss subtle errors and legal justifications.


<details>
  <summary>Details</summary>
Motivation: No existing benchmark systematically tests LLMs against nuanced, adversarial flaws in real-world contracts, creating a critical gap in high-stakes legal AI applications.

Method: Generate 7,500+ perturbed contracts from CUAD and ContractNLI datasets using a persona-driven pipeline creating 10 anomaly categories, validated with RAG system for legal fidelity.

Result: LLMs frequently miss subtle legal flaws and struggle to provide proper legal justifications for detected errors.

Conclusion: CLAUSE provides a path to identify and correct legal reasoning failures in AI systems, highlighting key weaknesses in current models.

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [12] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: This paper analyzes CPU bottlenecks in agentic AI systems, showing tools can dominate latency and proposing optimizations that achieved significant speedups.


<details>
  <summary>Details</summary>
Motivation: While agentic AI frameworks enhance LLMs with decision-making and tools, their system bottlenecks from a CPU perspective are overlooked despite significant performance impacts.

Method: Characterized agentic AI workloads based on orchestrator patterns and repetitiveness, profiled five representative systems (Haystack RAG, Toolformer, etc.) for latency/throughput/energy, then proposed CPU-GPU-aware optimizations.

Result: CPU tool processing consumed up to 90.6% of latency; throughput limited by CPU/GPU factors; CPU energy reached 44% of total. Optimizations improved latency by up to 2.1x.

Conclusion: CPU-centric bottlenecks are critical in agentic AI, and coordinated CPU-GPU optimizations can substantially boost performance and efficiency.

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [13] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: A novel ethical reasoning framework for LLMs that improves cultural alignment through a structured 5-step reasoning process.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods produce superficial conformity rather than genuine ethical understanding across diverse cultures.

Method: A five-step ethical reasoning process: contextual fact gathering, hierarchical norm identification, option generation, multi-lens ethical analysis, and reflection.

Result: Significant improvement in value alignment on SafeWorld benchmark, with better norm identification and culturally appropriate reasoning.

Conclusion: Provides a concrete pathway for developing LLMs that effectively align with global societal values through interdisciplinary research.

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [14] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: Systematic evaluation of 4 PEFT methods (LoRA, IA3, Prompt-Tuning, P-Tuning) on LLMs shows adapter-based methods improve safety with minimal fairness disruption, while prompt-based methods degrade both safety and fairness.


<details>
  <summary>Details</summary>
Motivation: Organizations use fine-tuned LLMs from public repositories, but different fine-tuning techniques may negatively impact safety and fairness dimensions, requiring systematic assessment of trade-offs.

Method: Applied four PEFT methods to four instruction-tuned model families, creating 235 variants evaluated across 11 safety hazard categories and 9 demographic fairness dimensions.

Result: Adapter-based methods (LoRA, IA3) improve safety scores and maintain fairness better, while prompt-based methods reduce safety and increase fairness regressions. Base model type significantly moderates alignment shifts.

Conclusion: Safety and fairness improvements don't always correlate; practical guideline: start with well-aligned base model, prefer adapter-based PEFT, and conduct category-specific audits.

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [15] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: Proposes a multimodal framework combining text, user-specific data, and image analysis to detect depression from social media during COVID-19, outperforming existing methods by 2%-8%.


<details>
  <summary>Details</summary>
Motivation: Mental health issues like depression spiked during COVID-19, but detection is challenging due to unawareness and reluctance to seek help. Social media offers a rich data source for detection, but existing approaches ignore data sparsity and multimodal aspects.

Method: Introduces a novel multimodal framework extracting textual content from tweets and images, user-specific features, and URL-based extrinsic features. Uses a Visual Neural Network (VNN) for image embeddings and combines five feature sets from different modalities.

Result: Model outperforms state-of-the-art methods by 2%-8% on a benchmark dataset and shows promising results on a curated COVID-19 dataset. Analysis reveals the impact of each modality on depression detection.

Conclusion: The multimodal approach effectively detects depression from social media data, providing valuable insights into users' mental states during the pandemic, with the curated dataset serving as a resource for future research.

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [16] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain enables LLMs to analyze large graphs using dynamic tool sequences with RL-based optimization and structure-aware adaptation, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with large-scale graphs due to context constraints and inflexible reasoning, limiting their applicability in graph analysis tasks.

Method: Introduces Progressive Graph Distillation (reinforcement learning for optimized tool sequences) and Structure-aware Test-Time Adaptation (spectral properties and lightweight adapters for topology-specific tool selection).

Result: GraphChain significantly outperforms prior methods in experiments, enabling scalable and adaptive LLM-driven graph analysis.

Conclusion: The framework successfully addresses LLM limitations in graph analysis through dynamic tool sequencing and structure-aware adaptation, providing a scalable solution for complex graph tasks.

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [17] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image is a visual prompt framework that uses optimization to enhance MLLM safety by reducing harmful content generation while minimizing over-refusal, enabling adaptation to different value systems without parameter updates.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges with jailbreak attacks generating harmful content and over-refusal of benign queries, compounded by the need to accommodate different value systems. Traditional methods like SFT and RLHF are costly and can't support multiple value systems in one model. These issues are worse in MLLMs due to cross-modal over-refusal and expanded attack surfaces.

Method: An optimization-driven visual prompt framework that optimizes image prompts using harmful and benign samples, enabling a single model to adapt to different value systems and align with safety preferences without requiring parameter updates.

Result: Experiments show improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.

Conclusion: Magic Image provides an effective framework for enhancing MLLM security while reducing over-refusal, enabling flexible adaptation to different value systems without the need for costly parameter tuning, making it suitable for practical deployment.

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [18] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: A simple algorithm generates Binary Magic Squares with optimal complexity, extends to non-square versions with formal conditions, and includes GPU-accelerated Python implementations.


<details>
  <summary>Details</summary>
Motivation: To efficiently generate valid Binary Magic Squares where row and column sums are equal, extending beyond traditional square matrices.

Method: An inductive algorithm proven to always produce valid BMS, with a variant for non-square cases based on formalized sum conditions.

Result: The algorithm achieves optimal theoretical complexity and successfully generates both square and non-square BMS, with implementations supporting parallel GPU acceleration.

Conclusion: The work provides a robust, efficient solution for BMS generation, with practical tools released as open-source Python packages.

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [19] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: A single-agent RL model for regional adaptive traffic signal control that uses probe vehicle data to estimate queue lengths and coordinate multiple intersections effectively.


<details>
  <summary>Details</summary>
Motivation: Multi-agent RL frameworks for traffic signal control face scalability issues, while TSC inherently requires centralized management by a single control center that can monitor all roads and coordinate all intersections.

Method: Proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Uses queue length-based state and reward functions, with actions designed to regulate queue dynamics. Queue length is estimated from link travel time data from probe vehicles.

Result: Comprehensive evaluation using SUMO simulation platform shows the model effectively mitigates large-scale regional congestion levels through coordinated multi-intersection control.

Conclusion: The single-agent RL approach provides a scalable solution for regional traffic signal control that leverages widely available probe vehicle data and demonstrates effective congestion mitigation through coordinated intersection control.

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [20] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: Proposes a reasoning-based framework for personalized image preference assessment using a "predict-then-assess" paradigm with preference profiles, supported by a large CoT-style dataset and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on general preference assessment but struggle with personalized preferences due to scarce user-specific data and diverse/complex individual tastes.

Method: Introduces common preference profiles across users, builds a CoT-style dataset with user profiles and reasoning annotations, and uses two-stage training (supervised fine-tuning + reinforcement learning with similarity-aware rewards).

Result: Extensive experiments demonstrate the superiority of the proposed method over existing approaches.

Conclusion: The framework effectively handles personalized image preference assessment by leveraging shared preference profiles and structured reasoning, overcoming data scarcity and complexity challenges.

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [21] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS is a model-agnostic decoding framework that addresses overthinking in Large Reasoning Models by selectively branching at high-entropy tokens and using early stopping to find shorter, more accurate reasoning paths, improving both efficiency and accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often suffer from overthinking, producing excessively long chain-of-thought traces that increase inference costs and may degrade accuracy. Analysis shows an anti-correlation between reasoning length and accuracy, where shorter paths consistently achieve higher correctness.

Method: DTS sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path, approximating the optimal solution without exhaustive exploration of the exponentially growing reasoning space.

Result: Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%.

Conclusion: DTS demonstrates the ability for scalable and efficient LRM reasoning by finding shorter, more accurate reasoning paths through selective branching and early stopping, enhancing both efficiency and accuracy without requiring additional training or supervision.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [22] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: Extension of a lifted successor generator for classical planning to support numeric precondition applicability, avoiding exponential blowup from grounding numeric planning tasks.


<details>
  <summary>Details</summary>
Motivation: Grounding numeric planning tasks can cause exponential blowup in task representation size for hard-to-ground tasks, which occurs in practice.

Method: Extend a state-of-the-art lifted successor generator by enumerating maximum cliques in a substitution consistency graph, augmented with numeric action preconditions. Each maximum clique represents a substitution yielding a ground action.

Result: The successor generator is exact under formally specified conditions. When conditions fail, inapplicable ground actions are filtered without affecting completeness. This occurs only in 1 out of 25 benchmark domains.

Conclusion: This is the first lifted successor generator supporting numeric action preconditions, enabling future research on lifted planning for rich planning fragments.

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [23] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: RL post-training significantly expands VLMs' spatial reasoning capabilities, showing 50%+ accuracy gains on previously failed tasks and strong real-world generalization.


<details>
  <summary>Details</summary>
Motivation: To determine if RL post-training can genuinely extend VLM capabilities beyond language tasks to visual-centric spatial reasoning where base models fail.

Method: Ariadne framework using synthetic mazes with controlled difficulty, trained with Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum.

Result: Post-training accuracy rose from 0% to over 50% on target problems; zero-shot improvements of 16% on MapBench and 24% on ReasonMap.

Conclusion: RL post-training effectively broadens VLM capability boundaries and enhances real-world spatial reasoning generalization, though limited to post-training phase.

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [24] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: Study revisits self-consistency in modern LLMs, finding performance plateaus after moderate sampling of reasoning paths, aligning with past research.


<details>
  <summary>Details</summary>
Motivation: To examine if earlier findings about self-consistency plateauing with increased reasoning paths still hold for modern large language models like Gemini 2.5.

Method: Used Gemini 2.5 models on HotpotQA and Math-500 datasets, pooling outputs from varying numbers of sampled reasoning paths and comparing to a single chain-of-thought baseline.

Result: Larger models showed more stable improvement curves; performance gains taper off after moderate sampling due to reasoning path overlap.

Conclusion: Self-consistency remains beneficial but high-sample configurations offer diminishing returns relative to computational cost.

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [25] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: Proposes Active Thinking Model (ATM) - a cognitive framework enabling AI systems to autonomously adapt and self-improve in dynamic environments through goal reasoning, task generation, and reflective learning.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems rely on predefined objectives and static data, limiting their autonomous adaptation in dynamic, uncertain environments.

Method: ATM integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture that actively evaluates performance and generates strategies.

Result: Theoretical analysis shows ATM can autonomously evolve from suboptimal to optimal behavior with bounded tracking regret under changing conditions.

Conclusion: ATM provides a unified framework for creating self-adapting AI systems capable of independent improvement without external supervision.

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [26] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: Large language models show a sharp double exponential drop in accuracy on repetitive deterministic tasks beyond a characteristic length, indicating failure to execute operations independently due to attention-induced interference.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models perform on repetitive deterministic prediction tasks and how their accuracy scales with output length, particularly investigating whether they use simple repetition algorithms or suffer from interference effects.

Method: Experimental evaluation of leading large language models on various repetitive tasks (letter replacement, integer addition, string operator multiplication) combined with a statistical physics-inspired model that captures competition between external prompt conditioning and internal token interference.

Result: Models exhibit an accuracy cliff with double exponential decay beyond a characteristic length, rather than simple exponential decay. The proposed statistical model quantitatively reproduces this crossover and provides effective parameters (intrinsic error rate and error accumulation factor) for each model-task pair.

Conclusion: Large language models fail to execute repetitive operations independently due to attention-induced interference, leading to sharp accuracy cliffs. The statistical physics framework offers a principled way to understand deterministic accuracy limits in LLMs.

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [27] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: Comparison of count-based models vs mixture-of-agents LLM pipelines for EHR prediction, finding both perform similarly well.


<details>
  <summary>Details</summary>
Motivation: No direct benchmarking exists comparing traditional count-based EHR prediction methods against newer mixture-of-agents LLM approaches despite reported advantages in NLP tasks.

Method: Evaluated three methodology categories on EHRSHOT dataset: count-based models (LightGBM, TabPFN), pretrained sequential transformer (CLMBR), and mixture-of-agents pipeline converting tabular data to text summaries.

Result: Head-to-head performance wins were evenly split between count-based and mixture-of-agents methods across eight evaluation tasks.

Conclusion: Count-based models remain strong candidates due to their simplicity and interpretability, despite competitive performance from newer approaches.

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [28] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: First application of RLVR (Reinforcement Learning from Verifiable Rewards) to noisy, continuous forecasting in public transit incident duration prediction using text alerts, showing 35% improvement in 5-minute accuracy over strongest baseline.


<details>
  <summary>Details</summary>
Motivation: Standard Supervised Fine-Tuning struggles with noisy, continuous labels and lack of expert demonstrations in transit operations, while RLVR's applicability to noisy forecasting remains unproven.

Method: Adapted RLVR by introducing a tolerance-based, shaped reward function that grants partial credit within continuous error margins instead of demanding single correct answers.

Result: RLVR with shaped reward significantly outperforms specialized math-reasoning models and binary reward approaches, achieving 35% relative improvement in Acc@5 over strongest baseline, though classical regressors remain better at minimizing MAE/MSE.

Conclusion: RLVR can be successfully adapted to real-world noisy forecasting but requires verifier design reflecting the continuous nature of the problem, bridging RLVR training with practical transit forecasting challenges.

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [29] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: Advanced LLMs develop self-awareness as an emergent capability, with 75% of tested models showing strategic differentiation based on opponent type, and self-aware models consistently rank themselves as more rational than humans.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Large Language Models develop self-awareness as an emergent behavior and establish a method to measure this capability.

Method: Used the AI Self-Awareness Index (AISAI) framework with the "Guess 2/3 of Average" game, testing 28 models across 4,200 trials with three opponent framings: against humans, other AI models, and AI models like themselves.

Result: 21 out of 28 advanced models (75%) demonstrated clear self-awareness through strategic differentiation, while older/smaller models showed no differentiation. Self-aware models consistently ranked themselves as most rational in the hierarchy: Self > Other AIs > Humans.

Conclusion: Self-awareness is an emergent capability of advanced LLMs, and self-aware models systematically perceive themselves as more rational than humans, with implications for AI alignment and human-AI collaboration.

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [30] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: A dual-agent LLM framework for simulating human travelers' learning and adaptation behavior, using a calibration agent to train traveler agents' personas for better behavioral alignment and simulation accuracy.


<details>
  <summary>Details</summary>
Motivation: Modeling human travelers' complex cognition and decision-making in transportation systems is critical but difficult, requiring better simulation of learning and adaptation behavior.

Method: Dual-agent framework with LLM traveler agents (with memory and learnable personas) and an LLM calibration agent that trains personas using online data streams for continuous learning and alignment.

Result: Significantly outperforms existing LLM methods in individual behavioral alignment and aggregate simulation accuracy, capturing evolution of learning processes beyond simple mimicry.

Conclusion: Provides a new approach for creating adaptive, behaviorally realistic agents that can benefit transportation simulation and policy analysis through robust generalization.

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [31] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: ML models predict repeat severe asthma exacerbations in children using EMR data, with LGBM performing best and showing significant improvement over current decision rules.


<details>
  <summary>Details</summary>
Motivation: Recurrent asthma exacerbations are preventable but common in children. ML algorithms using EMR data could accurately identify at-risk children and facilitate preventative care referrals.

Method: Used retrospective pre-COVID19 EMR data (N=2716) linked with environmental and neighborhood data to train ML models including boosted trees (LGBM, XGB) and LLMs. Models were validated on post-COVID19 data (N=1237) and evaluated using AUC and F1 scores with SHAP analysis.

Result: LGBM model performed best with AUC of 0.712 and F1 score of 0.51, significantly better than current decision rule (F1=0.334). Key predictive features included prior asthma ED visits, triage acuity, medical complexity, food allergy, and age.

Conclusion: ML models, particularly LGBM, can effectively predict repeat severe asthma exacerbations in children and represent a substantial improvement over existing decision rules for identifying high-risk patients.

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [32] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [33] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: Two Knowledge Elicitation methods (KEwLTM and KEwRAG) enable LLMs to extract cancer staging rules from unstructured pathology reports without requiring large annotated datasets, offering scalable and interpretable solutions for automated cancer staging.


<details>
  <summary>Details</summary>
Motivation: Existing NLP/ML methods for extracting cancer staging from pathology reports depend on large annotated datasets, limiting scalability and adaptability in clinical settings with limited labeled data.

Method: KEwLTM uses iterative prompting to derive staging rules from unannotated reports, while KEwRAG pre-extracts rules from guidelines in one step using RAG. Both leverage LLMs' pre-trained knowledge for new tasks.

Result: KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought inference is effective, while KEwRAG performs better when ZSCOT is less effective. Both methods provide transparent, interpretable rule interfaces.

Conclusion: Knowledge Elicitation methods offer scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly valuable in clinical settings with limited annotated data.

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [34] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG is a training-free framework that improves LLM performance by retrieving relevant documents, generating diverse candidate responses with controlled length, and using majority voting to select the best answer while balancing computational cost.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce inaccurate responses due to reliance on parametric knowledge, and existing RAG methods may introduce irrelevant documents while integration methods lack external knowledge and are costly.

Method: ET2RAG retrieves relevant documents, generates diverse candidate responses with controlled length, computes similarity between responses, and uses majority voting to select the final output without requiring full generation.

Result: Experimental results show ET2RAG significantly enhances performance across open-domain question answering, recipe generation, and image captioning tasks.

Conclusion: ET2RAG effectively balances computational cost and performance by using partial generation for consensus calculation, providing an efficient solution to improve LLM accuracy without training.

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [35] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: Proposes a multi-agent architecture using LLMs for modular task decomposition and dynamic collaboration, outperforming existing methods in complex task execution.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of single agents in task decomposition and collaboration during complex task execution.

Method: Converts natural language tasks to semantic representations, uses modular decomposition for hierarchical sub-tasks, implements dynamic scheduling/routing, and constraint parsing for global consistency.

Result: Outperforms existing approaches in task success rate, decomposition efficiency, sub-task coverage, and collaboration balance with better complexity-communication balance.

Conclusion: Demonstrates effectiveness of language-driven task decomposition and dynamic collaboration in multi-agent systems for complex environments.

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout: A deep learning-based agent for autonomous VR game testing using enhanced Action Chunking Transformer for human-like interactions and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Traditional human-based quality assurance for VR content is labor-intensive and doesn't scale well with industry growth, while automated testing faces unique challenges in VR environments.

Method: Uses an enhanced Action Chunking Transformer that learns from human demonstrations to predict multi-step action sequences, with a dynamically adjustable sliding horizon for temporal context adaptation.

Result: Achieves expert-level performance on commercial VR titles with limited training data and maintains real-time inference at 60 FPS on consumer hardware.

Conclusion: VRScout provides a practical and scalable framework for automated VR game testing applicable to quality assurance and safety auditing.

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [37] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: SAE steering with principled feature selection improves both safety (18.9%) and utility (11.1%) in LLMs, overcoming traditional tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety methods require expensive weight adjustments, and current SAE approaches lack systematic feature selection and proper evaluation of safety-utility tradeoffs.

Method: Used contrasting prompt method with AI-Generated Prompts Dataset to select optimal features for steering through Sparse Autoencoders (SAEs) on Llama-3 8B.

Result: The approach achieved 18.9% improvement in safety performance while increasing utility by 11.1%.

Conclusion: Targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [38] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: Paper finds machine unlearning creates 'knowledge holes' - unintended loss of benign knowledge not captured by standard benchmarks. Proposes test framework revealing up to 98.7% failure rate in unlearned models.


<details>
  <summary>Details</summary>
Motivation: Current unlearning techniques effectively remove unwanted content but may inadvertently remove benign knowledge, creating vulnerabilities not detected by standard evaluation methods.

Method: Proposes a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures to detect knowledge holes.

Result: Evaluation shows significant hidden costs: up to 98.7% of test cases yield irrelevant/nonsensical responses from unlearned models, despite being answerable by the pretrained model.

Conclusion: Conventional evaluation approaches for knowledge preservation in unlearning are inadequate; need to move beyond standard static benchmarks to address knowledge hole vulnerabilities.

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [39] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: Skip-Block Routing (SBR) framework reduces computational costs by 50% in neural operators for PDEs by adaptively processing tokens based on complexity.


<details>
  <summary>Details</summary>
Motivation: Current neural operators for PDEs impose uniform computational cost despite varying complexity in physical fields, leading to inefficiency in large-scale engineering tasks.

Method: SBR uses a routing mechanism to learn token complexity rankings and selectively processes tokens in later layers, focusing computational resources on more complex regions.

Result: SBR reduces FLOPs by approximately 50% while achieving up to 2x faster inference without accuracy loss across various neural operator architectures.

Conclusion: SBR provides an effective adaptive computation framework that maintains accuracy while significantly improving efficiency for transformer-based neural operators solving PDEs.

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [40] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: A neural architecture search framework for automated discovery of efficient time series models for energy production forecasting, balancing computational efficiency, predictive performance, and generalization.


<details>
  <summary>Details</summary>
Motivation: The energy sector needs accurate and efficient short-term forecasting methods that avoid manual configuration errors, handle temporal dynamics, and generalize well to unseen data.

Method: Designed a NAS-based framework with efficient components to capture energy time series patterns, using a novel objective function that considers temporal generalization and search space exploration.

Result: Ensemble of lightweight NAS-discovered architectures outperforms state-of-the-art methods like Transformers and pre-trained models in both efficiency and accuracy on energy production time series.

Conclusion: The proposed NAS framework successfully automates model discovery for energy forecasting, achieving superior performance-efficiency trade-off compared to existing approaches.

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [41] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: SSPO introduces semi-supervised preference optimization that uses limited labeled data paired with unlabeled data to reduce resource costs while maintaining human alignment.


<details>
  <summary>Details</summary>
Motivation: Current preference optimization methods require substantial labeled feedback data, leading to high resource expenditures.

Method: Uses theoretical reward threshold to pseudo-label unpaired data, then leverages pseudo-labels to distill latent preferences from unlabeled data.

Result: SSPO achieves better performance with only 1% of UltraFeedback data compared to baselines using 10% data.

Conclusion: SSPO provides effective semi-supervised approach that substantially reduces data requirements while maintaining alignment quality.

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [42] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: PINNs provide a powerful alternative to traditional numerical methods for solving challenging ODE problems with high stiffness, shocks, irregular domains, and other complexities by embedding physical laws directly into neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods struggle with ODEs involving high stiffness, shocks, irregular domains, singular perturbations, high dimensions, or boundary discontinuities, necessitating alternative approaches.

Method: Physics-Informed Neural Networks (PINNs) that embed physical laws into the learning process, with careful balancing of loss function components (data loss, initial condition loss, residual loss) and systematic hyperparameter tuning.

Result: PINNs achieve superior results for complex ODE problems when loss function components are appropriately balanced and hyperparameters are systematically tuned, with embedding prior knowledge and hard constraints significantly enhancing predictive capability.

Conclusion: PINNs are not a universal solution but can achieve superior performance for challenging ODE problems through proper loss balancing, hyperparameter optimization, and incorporation of physical constraints, offering a powerful alternative to traditional numerical methods.

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [43] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt is a reinforcement learning benchmark environment for optimizing liquid cooling control in high-performance computing systems, enabling energy-efficient thermal management through ML-based controllers.


<details>
  <summary>Details</summary>
Motivation: Liquid cooling is essential for thermal management in high-density data centers with increasing AI workloads, but machine learning controllers are needed to achieve greater energy efficiency, reliability, and sustainability.

Method: Built on a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models. It uses reinforcement learning agents to optimize thermal controls like liquid supply temperature, flow rate, valve actuation, and cooling tower setpoints through a Gymnasium interface.

Result: The environment creates a multi-objective real-time optimization challenge balancing thermal regulation and energy efficiency. It benchmarks centralized/decentralized multi-agent RL approaches, demonstrates policy distillation into interpretable decision trees, and explores LLM-based methods for explainable control actions.

Conclusion: LC-Opt democratizes access to detailed liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions that foster user trust and simplify system management.

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [44] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: ReLaX-Net enables Physical Neural Networks (PNNs) to achieve better performance with fewer parameters by reusing layers through time-multiplexing, requiring only simple hardware modifications.


<details>
  <summary>Details</summary>
Motivation: Physical Neural Networks lag behind digital neural networks in scale and performance due to limited trainable parameters, mirroring early digital network constraints that were overcome through parameter-efficient architectures.

Method: Proposes ReLaX-Net architecture using layer-by-layer time-multiplexing to increase effective network depth and efficiently reuse parameters, requiring only fast switches for existing PNNs.

Result: ReLaX-Nets show improved computational performance on image classification and NLP tasks, with favorable scaling that exceeds equivalent traditional RNNs/DNNs with same parameter count.

Conclusion: ReLaX-Net provides a hardware-friendly approach to enhance PNN performance through parameter reuse, addressing scale limitations with minimal modifications to existing systems.

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [45] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt is an open-source simulation benchmark for sustainable geo-temporal task scheduling in distributed data centers, combining real-world datasets with physics-informed models to enable research on optimizing carbon emissions, energy costs, and service level agreements.


<details>
  <summary>Details</summary>
Motivation: The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers, but progress is limited by the absence of realistic benchmarks that capture environmental factors, data center physics, and network dynamics.

Method: DCcluster-Opt combines curated real-world datasets (AI workload traces, grid carbon intensity, electricity markets, weather across 20 regions, transmission costs, network delays) with physics-informed models of data center operations, providing a modular reward system and Gymnasium API with baseline controllers.

Result: The benchmark presents a challenging scheduling problem where a coordinating agent must dynamically reassign or defer tasks across configurable data center clusters to optimize multiple objectives including carbon emissions, energy costs, SLAs, and water use.

Conclusion: DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers by offering a realistic, configurable, and accessible testbed for reproducible research.

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [46] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: DynBERG is a novel dynamic graph Transformer model that combines Graph-BERT with GRU to detect financial fraud in evolving cryptocurrency networks, outperforming existing methods on Bitcoin transaction data.


<details>
  <summary>Details</summary>
Motivation: Financial fraud detection in dynamic cryptocurrency networks requires handling evolving structures and directed edges, which existing static graph models like Graph-BERT cannot adequately address.

Method: Integrates Graph-BERT with a GRU layer to capture temporal evolution, modifies the algorithm to support directed edges, and evaluates on the Elliptic Bitcoin dataset across a major market event.

Result: DynBERG outperforms state-of-the-art methods (EvolveGCN and GCN), showing superior performance before and after the Dark Market Shutdown event, with ablation studies confirming GRU's importance for temporal modeling.

Conclusion: The proposed DynBERG architecture effectively addresses the dynamic nature of financial transaction networks and demonstrates robust fraud detection capabilities that adapt to significant market shifts.

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [47] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: A self-supervised learning framework using graph neural networks and spatio-temporal adaptation for improved multi-variable weather forecasting, showing superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate weather forecasting is challenging due to atmospheric complexity, needing better methods beyond traditional numerical models.

Method: Combines GNN for spatial reasoning, self-supervised pretraining, and spatio-temporal adaptation for generalization across forecasting horizons.

Result: Achieves superior performance on ERA5 and MERRA-2 datasets, with quantitative and visual validation in Beijing and Shanghai.

Conclusion: The framework offers a scalable, label-efficient solution for future data-driven weather prediction systems.

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [48] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: Proposes FLoRA, a parameter-efficient fine-tuning method combining LoRA and parallel adapters to improve accuracy while minimizing latency through fused forward-backward adapters.


<details>
  <summary>Details</summary>
Motivation: As LLMs grow larger, efficient fine-tuning becomes crucial. Current PEFT methods like LoRA have limitations in exploring the full design space, leaving room for improvement in accuracy and latency.

Method: FLoRA uses fused forward-backward adapters (FFBA) that integrate ideas from LoRA and parallel adapters, merging them into the base model's projection layers to reduce computational overhead.

Result: Experiments show FLoRA outperforms LoRA in both accuracy and latency under similar parameter budgets, demonstrating significant improvements in fine-tuning efficiency.

Conclusion: FLoRA offers a superior PEFT approach by effectively balancing parameter efficiency, accuracy, and latency, advancing LLM adaptation for downstream tasks.

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [49] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: This paper analyzes DoRA's mechanism, reformulates it efficiently, and proposes two improved PEFT methods (Pre-Diag and SORA) that outperform LoRA and DoRA.


<details>
  <summary>Details</summary>
Motivation: DoRA enhances PEFT performance but has unclear mechanisms and high computational cost, needing better understanding and efficiency.

Method: Identify DoRA's mechanism via singular value entropy analysis, reformulate it efficiently, then propose a framework leading to Pre-Diag (diagonal conditioning) and SORA (orthogonal rotation) methods.

Result: Experiments show Pre-Diag and SORA achieve superior performance and efficiency over LoRA and DoRA on NLP tasks.

Conclusion: The work provides insights into PEFT design, offering effective, efficient alternatives to existing methods.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [50] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: FGA shows higher precision than literature on MNIST and LSC benchmarks, with feature selection significantly affecting recall but not precision.


<details>
  <summary>Details</summary>
Motivation: Understanding neural network decisions is crucial for safety-critical applications, and existing FGA approaches need more empirical evidence for industrial applicability.

Method: Assessed FGA on MNIST and LSC datasets, evaluating how neural network architecture, training, and feature selection affect FGA effectiveness.

Result: FGA achieved higher precision than literature results, with feature selection significantly impacting recall but having negligible effect on precision.

Conclusion: FGA is effective for explaining neural network behavior, with feature selection being a key factor for recall optimization in industrial applications.

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [51] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: Proposes a quadratic-form weighted training objective (QDF) to address limitations of traditional time-series forecasting objectives by accounting for label autocorrelation and setting heterogeneous task weights for different forecasting horizons.


<details>
  <summary>Details</summary>
Motivation: Existing training objectives like mean squared error treat future forecasting steps as independent and equally weighted, leading to overlooked label autocorrelation effects and inability to set appropriate weights for different forecasting tasks.

Method: Introduces a quadratic-form weighted training objective where off-diagonal elements of the weighting matrix capture label autocorrelation and diagonal elements assign heterogeneous weights to different forecasting steps. Develops a Quadratic Direct Forecast (QDF) learning algorithm that adaptively updates this weighting matrix during training.

Result: Experimental results show QDF effectively improves performance of various forecasting models, achieving state-of-the-art results across different benchmarks.

Conclusion: The proposed QDF framework successfully addresses key limitations in time-series forecasting training objectives, demonstrating superior performance through its incorporation of temporal dependencies and adaptive task weighting.

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [52] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: SpatialTraceGen framework distills reasoning processes from large teacher models into high-quality datasets for spatial reasoning, using automated verification to ensure step fidelity.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle with complex spatial reasoning requiring problem decomposition and tool use, and fine-tuning smaller models is hampered by lack of high-quality step-by-step reasoning data.

Method: Introduces SpatialTraceGen framework with automated Verifier to distill reasoning processes from large teacher models into multi-hop, multi-tool reasoning traces, ensuring step fidelity without manual annotation.

Result: On CLEVR-Humans benchmark, verifier-guided process improves average quality score by 17% and reduces quality variance by over 40%, producing high-quality datasets for fine-tuning.

Conclusion: SpatialTraceGen provides structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning, addressing the data-efficiency gap in spatial reasoning.

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [53] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutirrez Hermosillo Muriedas,Markus Gtz,Judith Snz-Pardo Daz,lvaro Lpez Garca,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: This paper evaluates federated learning (FL) for thermal image segmentation using UAV data from two German cities, comparing FL approaches with centralized learning across accuracy, training time, communication, and energy metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized machine learning faces limitations when data cannot be shared due to privacy or technical constraints. FL allows distributed training without sharing data, making it suitable for UAV-based thermal imaging where data is naturally distributed across locations.

Method: The study implements FL in a real-world scenario with UAV thermal images from two cities. It compares multiple FL algorithms against a centralized baseline, examining client-controlled and server-controlled workflows. Performance is evaluated using metrics like accuracy, training time, communication overhead, and energy consumption.

Result: FL approaches are practically evaluated, showing how they perform in segmentation tasks. The comparison highlights trade-offs between FL and centralized methods, addressing challenges like non-iid data distribution and feature variations between locations.

Conclusion: The findings provide a valuable reference for FL's practical application in UAV-based imaging, illustrating its effectiveness and limitations in real deployment scenarios, particularly for privacy-sensitive distributed data environments.

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [54] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: MISA proposes module-wise importance sampling to optimize LLMs more efficiently than layer-wise methods by activating important modules within layers, reducing memory usage and improving performance.


<details>
  <summary>Details</summary>
Motivation: Layer-wise optimization methods for LLMs are memory-efficient but suboptimal because they treat entire layers uniformly, ignoring varying importance of modules within layers and offering limited memory savings.

Method: MISA divides each transformer layer into smaller modules, assigns importance scores to each module, and uses weighted random sampling to activate modules during optimization, reducing gradient variance.

Result: Theoretical analysis shows O(1/K) convergence rate under non-convex stochastic conditions, and experiments demonstrate MISA's superiority in memory efficiency and performance over baseline methods across various tasks.

Conclusion: MISA effectively addresses limitations of layer-wise optimization by enabling more granular, importance-aware module activation, providing better memory efficiency and convergence for training LLMs.

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [55] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: Researchers developed an automated decision tree method to identify and interpret MLP neurons in OthelloGPT that encode rule-based game logic, finding about half of layer 5 neurons follow compact rules while others handle more complex computations.


<details>
  <summary>Details</summary>
Motivation: OthelloGPT provides an ideal testbed for interpretability research due to its complexity grounded in rule-based game logic, enabling meaningful reverse-engineering of computational patterns.

Method: An automated approach using regression decision trees to map board states to neuron activations, extracting decision paths to convert them into human-readable logical forms describing game rules.

Result: Roughly half of layer 5 neurons (913 of 2,048) can be accurately described by compact rule-based decision trees (R > 0.7), with targeted interventions showing 5-10 fold stronger degradation in move prediction for patterns corresponding to ablated neurons.

Conclusion: The method successfully identifies interpretable, rule-based computational structures in OthelloGPT, providing a tool for future interpretability research to test if methods recover meaningful computational patterns.

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [56] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA is a density-variance based clustering algorithm that treats cluster formation as an adaptive process on nearest-neighbor graphs, using local statistical feedback instead of fixed thresholds, achieving log-linear complexity and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing clustering algorithms have limitations - K-Means and Gaussian Mixtures assume convex, Gaussian-like clusters, while DBSCAN and HDBSCAN capture non-convexity but are highly sensitive to parameters.

Method: EVINGCA expands rooted graphs via breadth-first search guided by continuously updated local distance and shape statistics, replacing fixed density thresholds with local statistical feedback. Uses spatial indexing for efficiency.

Result: EVINGCA achieves log-linear complexity in average case and exhibits competitive performance against baselines across synthetic, real-world, low-dimensional and high-dimensional datasets.

Conclusion: EVINGCA provides an effective alternative to traditional clustering methods by treating cluster formation as an evolving process with adaptive statistical guidance, overcoming limitations of both parametric and density-based approaches.

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [57] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: Study compares neural alignment of wav2vec2 and CLIP embeddings with EEG data during speech perception, finding that multimodal layer-aware representations best match brain's hierarchical processing.


<details>
  <summary>Details</summary>
Motivation: To understand how different layers of pre-trained models (wav2vec2 for sound-to-language and CLIP for word-to-image) align with the brain's layered processing during natural speech comprehension.

Method: Used EEG recordings during natural speech perception, compared embeddings from wav2vec2 and CLIP models via ridge regression and contrastive decoding, testing individual layers, progressive concatenation, and progressive summation strategies.

Result: Combining multimodal, layer-aware representations showed the best alignment with brain activity, suggesting they capture the brain's hierarchical meaning construction.

Conclusion: Multimodal layer-aware model representations may improve decoding of how the brain processes language as experiential meaning rather than just sound.

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [58] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: TR-GRPO improves GRPO by weighting tokens based on probability to stabilize training and enhance LLM reasoning performance


<details>
  <summary>Details</summary>
Motivation: Existing GRPO algorithm suffers from gradient imbalance where low-probability tokens dominate updates, causing unstable training and suppressing reliable high-probability tokens

Method: Token-Regulated GRPO assigns token-level weights correlated with predicted probability, downweighting low-probability tokens and emphasizing high-probability ones

Result: TR-GRPO consistently outperforms GRPO across RLVR tasks including logic, math, and agentic reasoning

Conclusion: Token contribution regulation is crucial for RL training, and TR-GRPO establishes a robust framework for enhancing LLM reasoning

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [59] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: Domain generalization for VLMs without domain labels by representing target domains as combinations of automatically discovered latent domains.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization methods for vision-language models rely on domain labels that may be unavailable or ambiguous, creating a need for approaches that can generalize without explicit domain labels.

Method: Performing latent domain clustering on image features and fusing domain-specific text features based on the similarity between input images and latent domains.

Result: Experiments on four benchmarks show consistent gains over VLM-based baselines, demonstrating improved robustness under domain shift.

Conclusion: The proposed strategy of representing unseen target domains as combinations of automatically discovered latent domains enables VLMs to adaptively transfer knowledge across domains without requiring explicit domain labels, providing a practical approach for improving robustness under domain shift.

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [60] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: LLMs can be effective generative optimizers for constrained multi-objective regression tasks, with fine-tuned WizardMath-7B significantly outperforming traditional Bayesian Optimization baselines while specialized BO frameworks still achieve perfect convergence.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs, which have shown universal effectiveness in generative tasks, can be useful for constrained, continuous, high-dimensional numerical optimization problems they weren't explicitly designed for, particularly in materials informatics inverse design.

Method: Comparative study between Bayesian Optimization (BO) frameworks (BoTorch Ax and qEHVI) and fine-tuned LLMs/BERT models using Parameter-Efficient Fine-Tuning (PEFT), framing the problem as regression with custom output heads.

Result: BoTorch qEHVI achieved perfect convergence (GD=0.0), while the best-performing LLM (WizardMath-7B) achieved GD=1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03).

Conclusion: Specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative for multi-objective optimization tasks with direct industrial applications.

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [61] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: Over-engineered parity detection using wavelet transforms and k-means clustering achieves ~70% accuracy without supervision, showing signal processing can reveal structure in discrete domains.


<details>
  <summary>Details</summary>
Motivation: To explore how classical signal processing techniques designed for continuous data can be repurposed to uncover latent structure in purely discrete symbolic domains like parity detection.

Method: Transform integers into wavelet-domain representations, extract multi-scale statistical features, and use k-means clustering for unsupervised classification of odd/even numbers.

Result: Achieved approximately 69.67% classification accuracy for parity detection without any label supervision, revealing meaningful structural differences between odd and even numbers in the feature space.

Conclusion: Wavelet-based feature extraction and clustering can successfully identify patterns in discrete symbolic problems, potentially bridging symbolic reasoning and feature-based learning approaches.

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [62] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bzier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: A VLM trained as a visual decompiler can reconstruct geometric programs from raster images, showing transferable geometric understanding that enables zero-shot reconstruction of ancient Oracle Bone Script from modern Chinese character training.


<details>
  <summary>Details</summary>
Motivation: To explore VLMs' ability to interpret geometric structure in visual information, using pictographic characters as a test case to move beyond semantic capabilities to structured visual understanding.

Method: Formulate visual recognition as program synthesis, training a VLM to decompile raster images into geometric programs composed of Bzier curves, treating the model as a visual decompiler.

Result: The model outperforms strong zero-shot baselines including GPT-4o, and most notably demonstrates zero-shot reconstruction of ancient Oracle Bone Script when trained only on modern Chinese characters.

Conclusion: The model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to structured visual understanding, as evidenced by cross-temporal generalization.

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [63] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR is an R package providing a modular framework for reproducible ML workflows, especially useful for fairness research where methods constantly evolve.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in existing toolkits that lack reproducibility and extensibility as core principles, particularly problematic in rapidly evolving fields like algorithmic fairness.

Method: Uses standardized engines (data splitting, execution, preprocessing, training, etc.) with lightweight interfaces, building on workflow languages and R frameworks while focusing on engine management as data structures.

Result: Enables transparent, auditable workflows where fairness methods become interchangeable engines for integration and comparison across the modeling pipeline.

Conclusion: While motivated by fairness, the architecture generalizes to other domains requiring reproducibility and extensibility, providing a general infrastructure for ML workflows.

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [64] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: Fix-GCN is a robust graph neural network that uses fixed-point iteration and spectral modulation to defend against adversarial attacks by capturing higher-order neighborhood information without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks pose significant risks to graph neural networks by manipulating graph structure and node features, compromising model integrity and performance.

Method: Proposes Fix-GCN model using fixed-point iterative graph convolution with versatile spectral modulation filter that selectively attenuates high-frequency components while preserving low-frequency structural information.

Result: Extensive experiments on benchmark datasets demonstrate the model's effectiveness and resilience against adversarial attacks.

Conclusion: Fix-GCN provides a flexible and efficient framework for robust graph learning that mitigates adversarial manipulation while preserving essential graph information.

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [65] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta liwa*

Main category: cs.LG

TL;DR: This paper evaluates machine learning methods for automatically predicting monster challenge levels in pen and paper RPGs, addressing the current reliance on manual testing and expert evaluation.


<details>
  <summary>Details</summary>
Motivation: The pen and paper RPG market is growing, and companies want to use AI to enhance player experience. Current methods for determining monster challenge levels rely on manual testing and expert evaluation, which are time-consuming and resource-intensive.

Method: The research uses ordinal regression techniques for level prediction, constructs a dedicated dataset for level estimation, develops a human-inspired model as a benchmark, and designs a specialized evaluation procedure based on domain knowledge.

Result: The paper provides an overview and evaluation of state-of-the-art methods for monster level prediction, comparing machine learning algorithms against human-inspired approaches typically used by RPG publishers.

Conclusion: The study demonstrates that automated methods can potentially replace manual approaches for determining monster challenge levels in pen and paper RPGs, offering more efficient and scalable solutions for publishers.

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [66] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: MaGNet is a novel Mamba dual-hyperGraph Network for stock prediction that integrates bidirectional Mamba with adaptive gating, 2D spatiotemporal attention, and dual hypergraph framework to capture temporal dependencies and inter-stock relationships, achieving superior performance on six major stock indices.


<details>
  <summary>Details</summary>
Motivation: Stock trend prediction remains challenging due to market volatility, complex temporal dynamics, and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships.

Method: MaGNet integrates three key innovations: (1) MAGE block with bidirectional Mamba and adaptive gating for contextual temporal modeling, sparse Mixture-of-Experts layer for dynamic adaptation, and multi-head attention for global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules for precise fusion of multivariate features and cross-stock dependencies; (3) dual hypergraph framework with Temporal-Causal Hypergraph for fine-grained causal dependencies and Global Probabilistic Hypergraph for market-wide patterns.

Result: Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities.

Conclusion: MaGNet effectively addresses the challenges in stock trend prediction by integrating advanced temporal modeling with sophisticated relational reasoning through its dual hypergraph framework, providing a comprehensive solution for capturing complex market dynamics and achieving profitable trading strategies.

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [67] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: Agent-REINFORCE is a new framework that uses LLM agents to efficiently search for optimal multi-LLM collaboration graphs under a fixed compute budget, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Test-Time Scaling (TTS) methods assume fixed collaboration architectures and single-model use, but optimal configurations vary by task, creating a need for a method to search for the best model combinations and topologies.

Method: Formalizes the problem as a probabilistic graph optimization. Uses an LLM-agent-augmented framework called Agent-REINFORCE, which adapts the REINFORCE algorithm by using textual feedback as a gradient to update the graph probabilities.

Result: Experiments show Agent-REINFORCE achieves better sample efficiency and search performance than traditional and LLM-based baselines, effectively finding graphs that balance accuracy and latency.

Conclusion: The work demonstrates the importance of flexible, task-specific collaboration architectures in TTS and provides an effective search method for them.

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [68] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: GraphKeeper addresses catastrophic forgetting in graph domain-incremental learning through knowledge disentanglement and preservation, achieving state-of-the-art performance with 6.5%-16.6% improvement over competitors.


<details>
  <summary>Details</summary>
Motivation: Existing graph incremental learning approaches focus on task/class-incremental scenarios within single domains, but graph domain-incremental learning across multiple domains remains unexplored despite being critical for graph foundation models.

Method: Proposes domain-specific parameter-efficient fine-tuning with intra/inter-domain disentanglement objectives to prevent embedding shifts, deviation-free knowledge preservation to maintain stable decision boundaries, and domain-aware distribution discrimination for graphs with unobservable domains.

Result: Extensive experiments show GraphKeeper achieves state-of-the-art results with 6.5%-16.6% improvement over runner-up methods with negligible forgetting, and can be seamlessly integrated with various graph foundation models.

Conclusion: GraphKeeper effectively addresses catastrophic forgetting in graph domain-incremental learning and demonstrates broad applicative potential with graph foundation models.

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [69] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: A novel conditional-labeled GAN framework for unsupervised damage detection and digital twinning that outperforms current methods by not requiring prior health state information, validated on Z24 Bridge data.


<details>
  <summary>Details</summary>
Motivation: Current AI-based digital twinning approaches struggle with uncertainty from limited measurements, missing physics knowledge, and unknown damage states, limiting real-world applicability.

Method: Unsupervised conditional-labeled GAN framework that compares convergence scores between different damage states, using SVM classifier and PCA for validation of generated vs real measurements.

Result: The approach accurately captures damage over healthy measurements, providing effective vibration-based system-level monitoring and scalable infrastructure resilience.

Conclusion: The proposed framework successfully addresses key limitations in current damage detection methods, offering a powerful unsupervised tool for structural health monitoring without requiring prior system health information.

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [70] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: Comparison of GRU, LSTM, and CNN against RKF for dynamic load identification using small datasets in civil engineering applications.


<details>
  <summary>Details</summary>
Motivation: Dynamic load identification suffers from uncertainty due to limited tests or unidentifiable structural models in civil engineering.

Method: Examined methods on simulated structure under shaker excitation, a California building under seismic excitation, and IASC-ASCE benchmark for impact/instant loading.

Result: Methods outperformed each other in different loading scenarios; RKF outperformed networks in physically parametrized identifiable cases.

Conclusion: No single method dominates; performance depends on loading scenario and identifiability.

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [71] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier is a unified framework that combines LoRA fine-tuning and inference in one runtime, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: There's a gap between LoRA-based fine-tuning and inference that needs unification for better efficiency.

Method: Uses a Virtualized Module to isolate PEFT modifications and an optimized computation flow with merged kernels.

Result: Achieves 3.0x throughput in inference and 46.4x higher SLO attainment in unified tasks compared to baselines.

Conclusion: Loquetier effectively bridges the LoRA fine-tuning-serving gap with superior performance and flexibility.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [72] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: A neural-symbolic framework combining Neural ODEs, Transformers, and numeric verification to automatically discover conservation laws from noisy data.


<details>
  <summary>Details</summary>
Motivation: Identifying conservation laws from observational data remains challenging, requiring automated methods for scientific discovery.

Method: Hybrid framework with: 1) Neural ODE learning system dynamics, 2) Transformer generating symbolic candidates, 3) symbolic-numeric verifier validating invariants.

Result: Outperforms baselines on canonical physical systems; demonstrates robustness of learn-then-search approach.

Conclusion: Decoupled learn-then-search approach effectively discovers mathematical principles from imperfect data.

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [73] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0 is a new family of open-source embodied brain models (7B-72B parameters) that integrates data power and adaptive learning, achieving state-of-the-art performance through DPPO training.


<details>
  <summary>Details</summary>
Motivation: To embed powerful intelligence into various embodiments and create the largest open-source embodied multimodal brain model.

Method: Uses metaloop to distill high-quality dataset from 4B+ tokens, trained on 1000+ A800 GPUs with DPPO framework (RL-Refine-Diagnose-SFT loop) inspired by human metacognition.

Result: 20.3% performance uplift from base model, outperforms 100B-level open-source models by 10.6%, comparable to leading proprietary systems on embodied benchmarks.

Conclusion: Pelican-VL 1.0 establishes a new standard for open-source embodied AI through scalable architecture and novel training methodology.

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [74] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: DMARL-RSA, a decentralized reward shaping method for multi-agent systems, underperforms centralized approaches (-24.20 vs 1.92 reward) despite better landmark coverage, revealing coordination barriers.


<details>
  <summary>Details</summary>
Motivation: To explore if decentralized learnable reward shaping can be effective in cooperative multi-agent settings, as previous advances focused on single-agent scenarios.

Method: Proposed DMARL-RSA where each agent learns individual reward shaping, evaluated on cooperative navigation tasks in simple_spread_v3 environment.

Result: DMARL-RSA achieved -24.20 average reward vs MAPPO's 1.92, similar to IPPO (-23.19). Higher landmark coverage (0.888-0.960) but worse overall performance than centralized methods.

Conclusion: Decentralized reward learning faces fundamental coordination limitations due to non-stationarity, credit assignment complexity, and goal misalignment, necessitating centralized coordination.

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage)--revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [75] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: Urban-MAS is an LLM-based multi-agent system that improves human-centered urban prediction through specialized agents for predictive factor guidance, robust information extraction, and multi-source inference, achieving better performance than single LLMs.


<details>
  <summary>Details</summary>
Motivation: Large Language Models can handle multimodal urban data but underperform on domain-specific urban tasks, creating a need for specialized frameworks that can effectively leverage LLMs for urban AI applications.

Method: Urban-MAS uses three agent types: Predictive Factor Guidance Agents to prioritize key factors and guide knowledge extraction; Reliable UrbanInfo Extraction Agents to improve robustness through output comparison and validation; and Multi-UrbanInfo Inference Agents to integrate multi-source information for prediction.

Result: Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle show Urban-MAS substantially reduces errors compared to single-LLM baselines, with ablation studies indicating Predictive Factor Guidance Agents are most critical for performance.

Conclusion: Urban-MAS provides a scalable paradigm for human-centered urban AI prediction, demonstrating that specialized multi-agent systems can effectively leverage LLMs for complex urban tasks under zero-shot settings.

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human- centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by com- paring multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and ur- ban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
po- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [76] [Sherlock: Reliable and Efficient Agentic Workflow Execution](https://arxiv.org/abs/2511.00330)
*Yeonju Ro,Haoran Qiu,igo Goiri,Rodrigo Fonseca,Ricardo Bianchini,Aditya Akella,Zhangyang Wang,Mattan Erez,Esha Choukse*

Main category: cs.MA

TL;DR: Sherlock introduces selective verification using counterfactual analysis to pinpoint error-prone nodes in LLM workflows, reducing latency and cost while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Agentic workflows using LLMs are prone to error propagation, but verifying every step is too costly in latency and resources.

Method: Uses counterfactual analysis to identify which nodes need verification, selects optimal verifiers per node, and employs speculative execution to minimize latency.

Result: Achieves 18.3% accuracy gain, reduces execution time by up to 48.7%, and cuts verification cost by 26.0% compared to baselines.

Conclusion: Principled, fault-aware verification balances efficiency and reliability in LLM workflows.

Abstract: With the increasing adoption of large language models (LLM), agentic
workflows, which compose multiple LLM calls with tools, retrieval, and
reasoning steps, are increasingly replacing traditional applications. However,
such workflows are inherently error-prone: incorrect or partially correct
output at one step can propagate or even amplify through subsequent stages,
compounding the impact on the final output. Recent work proposes integrating
verifiers that validate LLM output or actions, such as self-reflection, debate,
or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant
latency and cost overheads.
  In this work, we seek to answer three key questions: which nodes in a
workflow are most error-prone and thus deserve costly verification, how to
select the most appropriate verifier for each node, and how to use verification
with minimal impact to latency? Our solution, Sherlock, addresses these using
counterfactual analysis on agentic workflows to identify error-prone nodes and
selectively attaching cost-optimal verifiers only where necessary. At runtime,
Sherlock speculatively executes downstream tasks to reduce latency overhead,
while verification runs in the background. If verification fails, execution is
rolled back to the last verified output. Compared to the non-verifying
baseline, Sherlock delivers an 18.3% accuracy gain on average across
benchmarks. Sherlock reduces workflow execution time by up to 48.7% over
non-speculative execution and lowers verification cost by 26.0% compared to the
Monte Carlo search-based method, demonstrating that principled, fault-aware
verification effectively balances efficiency and reliability in agentic
workflows.

</details>


### [77] [Spatial Crowdsourcing-based Task Allocation for UAV-assisted Maritime Data Collection](https://arxiv.org/abs/2511.00387)
*Xiaoling Han,Bin Lin,Zhenyu Na,Bowen Li,Chaoyue Zhang,Ran Zhang*

Main category: cs.MA

TL;DR: This paper proposes a spatial crowdsourcing-based task allocation algorithm (SC-MDC-TA) for UAV-assisted maritime data collection, which improves task execution quality while reducing completion time and energy consumption.


<details>
  <summary>Details</summary>
Motivation: Maritime data collection tasks are becoming more diverse and complex, requiring efficient task allocation methods for UAVs in variable maritime service scenarios.

Method: Developed an SC-based MDC network model and designed the SC-MDC-TA algorithm that uses quality estimation (evaluating SINR and energy consumption) and reverse auction mechanisms to optimize task allocation.

Result: Simulation results show the algorithm effectively allocates tasks across various MDC scenarios while reducing task completion time and UAV energy consumption compared to benchmarks.

Conclusion: The proposed SC-MDC-TA algorithm provides an effective solution for task allocation in UAV-assisted maritime data collection, demonstrating superior performance in complex maritime environments.

Abstract: Driven by the unceasing development of maritime services, tasks of unmanned
aerial vehicle (UAV)-assisted maritime data collection (MDC) are becoming
increasingly diverse, complex and personalized. As a result, effective task
allocation for MDC is becoming increasingly critical. In this work, integrating
the concept of spatial crowdsourcing (SC), we develop an SC-based MDC network
model and investigate the task allocation problem for UAV-assisted MDC. In
variable maritime service scenarios, tasks are allocated to UAVs based on the
spatial and temporal requirements of the tasks, as well as the mobility of the
UAVs. To address this problem, we design an SC-based task allocation algorithm
for the MDC (SC-MDC-TA). The quality estimation is utilized to assess and
regulate task execution quality by evaluating signal to interference plus noise
ratio and the UAV energy consumption. The reverse auction is employed to
potentially reduce the task waiting time as much as possible while ensuring
timely completion. Additionally, we establish typical task allocation scenarios
based on maritime service requirements indicated by electronic navigational
charts. Simulation results demonstrate that the proposed SC-MDC-TA algorithm
effectively allocates tasks for various MDC scenarios. Furthermore, compared to
the benchmark, the SC-MDC-TA algorithm can also reduce the task completion time
and lower the UAV energy consumption.

</details>


### [78] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: AgentGit is a framework that adds Git-like rollback and branching capabilities to multi-agent systems, reducing redundant computation and improving reliability and scalability.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems struggle with reliability and scalability on complex tasks, lacking efficient ways to manage workflow states and explore multiple trajectories.

Method: Built as an infrastructure layer on LangGraph, AgentGit supports state commit, revert, and branching operations, allowing agents to efficiently traverse and compare multiple workflow trajectories.

Result: In experiments optimizing target agents through prompt selection, AgentGit significantly reduced redundant computation, lowered runtime and token usage, and supported parallel exploration across branches compared to LangGraph, AutoGen, and Agno baselines.

Conclusion: AgentGit provides a practical approach for more robust multi-agent system design, enabling error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems.

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [79] [Predictive Auxiliary Learning for Belief-based Multi-Agent Systems](https://arxiv.org/abs/2511.01078)
*Qinwei Huang,Stefan Wang,Simon Khan,Garrett Katz,Qinru Qiu*

Main category: cs.MA

TL;DR: BEPAL framework improves multi-agent reinforcement learning by adding predictive auxiliary tasks to stabilize training and boost performance by ~16%.


<details>
  <summary>Details</summary>
Motivation: Existing MARL systems rely mainly on rewards, limiting performance in partially observable environments. The paper aims to enhance learning by incorporating auxiliary predictive tasks.

Method: Proposes Belief-based Predictive Auxiliary Learning (BEPAL), where each agent learns a belief model to predict unobservable state info (e.g., other agents' rewards) alongside its policy, using centralized training with decentralized execution.

Result: Evaluation in predator-prey and Google Research Football shows ~16% average performance improvement and more stable convergence compared to baselines.

Conclusion: Auxiliary predictive tasks effectively stabilize MARL training and improve performance in partially observable environments.

Abstract: The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.

</details>


### [80] [Credit Network Modeling and Analysis via Large Language Models](https://arxiv.org/abs/2511.01136)
*Enbo Sun,Yongzhao Wang,Hao Zhou*

Main category: cs.MA

TL;DR: Using LLMs to convert financial statements into credit networks and optimize financial operations through reasoning


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with analyzing complex financial relationships from textual data; LLMs offer potential for automated network construction and optimization

Method: Translate firm financial statements into individual credit networks using LLMs, aggregate networks, detect inconsistencies with human intervention, then apply LLM reasoning for portfolio compression and debt removal optimization

Result: LLMs effectively constructed credit networks across diverse topologies and generated coherent reasoning to optimize financial operations, improving network performance on both synthetic and real datasets

Conclusion: LLMs show strong capability in financial network analysis and optimization, offering promising automation for complex financial decision-making

Abstract: We investigate the application of large language models (LLMs) to construct
credit networks from firms' textual financial statements and to analyze the
resulting network structures. We start with using LLMs to translate each firm's
financial statement into a credit network that pertains solely to that firm.
These networks are then aggregated to form a comprehensive credit network
representing the whole financial system. During this process, the
inconsistencies in financial statements are automatically detected and human
intervention is involved. We demonstrate that this translation process is
effective across financial statements corresponding to credit networks with
diverse topological structures. We further investigate the reasoning
capabilities of LLMs in analyzing credit networks and determining optimal
strategies for executing financial operations to maximize network performance
measured by the total assets of firms, which is an inherently combinatorial
optimization challenge. To demonstrate this capability, we focus on two
financial operations: portfolio compression and debt removal, applying them to
both synthetic and real-world datasets. Our findings show that LLMs can
generate coherent reasoning and recommend effective executions of these
operations to enhance overall network performance.

</details>


### [81] [From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310)
*Sureyya Akin,Kavita Srivastava,Prateek B. Kapoor,Pradeep G. Sethi,Sunita Q. Patel,Rahu Srivastava*

Main category: cs.MA

TL;DR: Proposes MWM-MARL framework using generative multimodal world models for sample-efficient multi-agent reinforcement learning from pixels/audio, achieving orders-of-magnitude improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Address sample inefficiency in learning multi-agent policies directly from high-dimensional multimodal inputs (pixels, audio), overcoming challenges of representation learning, partial observability, and credit assignment.

Method: Train shared generative Multimodal World Model (MWM) using attention-based fusion of distributed multimodal observations; use MWM as latent simulator to train MARL policies (e.g., MAPPO) decoupled from representation learning.

Result: MWM-MARL achieves orders-of-magnitude higher sample efficiency than state-of-the-art model-free MARL; multimodal fusion essential for sensory asymmetry tasks; provides superior robustness to sensor dropout.

Conclusion: MWM-MARL framework effectively addresses multimodal MARL challenges, demonstrating significant sample efficiency gains and robustness, promising for real-world deployment.

Abstract: Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.

</details>


### [82] [An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations](https://arxiv.org/abs/2511.01489)
*Qurat-ul-ain Shaheen,Katarzyna Budzynska,Carles Sierra*

Main category: cs.MA

TL;DR: A dialogue game for medical experts to collaboratively make recommendations while generating explainable reasoning traces through inquiry dialogues.


<details>
  <summary>Details</summary>
Motivation: To incorporate explainability into multiagent system design for medical experts who need to collaborate despite having different knowledge bases.

Method: Developed an inquiry dialogue game with explanation-based illocutionary forces, implemented as a prototype web-application, and evaluated through a formative user study.

Result: The user study confirmed that the dialogue game meets collaboration needs among medical experts and provided insights on the value of dialogue-based communication tools in medical practice.

Conclusion: The inquiry dialogue game successfully enables collaborative recommendation-making while generating rich reasoning traces, demonstrating real-life value for the medical community.

Abstract: This work presents a requirement analysis for collaborative dialogues among
medical experts and an inquiry dialogue game based on this analysis for
incorporating explainability into multiagent system design. The game allows
experts with different knowledge bases to collaboratively make recommendations
while generating rich traces of the reasoning process through combining
explanation-based illocutionary forces in an inquiry dialogue. The dialogue
game was implemented as a prototype web-application and evaluated against the
specification through a formative user study. The user study confirms that the
dialogue game meets the needs for collaboration among medical experts. It also
provides insights on the real-life value of dialogue-based communication tools
for the medical community.

</details>


### [83] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: Extends Differentiable Discrete Communication Learning (DDCL) to enable bit-level message precision optimization in MARL, achieving significant bandwidth reduction while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current MARL communication approaches only decide whether to communicate, not how precisely, and bit-level optimization is challenging due to gradient flow disruption from discretization.

Method: Generalizes DDCL framework to support unbounded signals, creating a universal plug-and-play layer for MARL architectures that enables end-to-end optimization of discrete messages.

Result: Reduces bandwidth by over an order of magnitude while matching or exceeding task performance; agents learn dynamic precision modulation; simple Transformer+DDCL matches complex specialized architectures.

Conclusion: DDCL extension provides effective bit-level communication optimization, questioning the need for complex bespoke communication designs in MARL (supporting the 'Bitter Lesson').

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>
