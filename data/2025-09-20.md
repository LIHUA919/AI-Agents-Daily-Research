<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: A unified optimization framework for multi-line metro crew planning and replanning with heterogeneous workforce, using hierarchical time-space network modeling and efficient algorithms that outperform benchmarks in cost reduction and task completion.


<details>
  <summary>Details</summary>
Motivation: Metro crew planning is crucial for smart city development and operational efficiency, but current research focuses on individual lines with insufficient attention to cross-line coordination and rapid replanning during disruptions in rapidly expanding metro networks.

Method: Proposed a hierarchical time-space network model to represent unified crew action space, with computationally efficient constraints for heterogeneous qualifications and preferences. Developed solution algorithms based on column generation and shortest path adjustment.

Result: Experiments with real data from Shanghai and Beijing Metro show the methods outperform benchmark heuristics in cost reduction and task completion, achieving notable efficiency gains through cross-line operations, especially for urgent tasks during disruptions.

Conclusion: This work demonstrates the importance of global optimization and cross-line coordination in multi-line metro system operations, providing insights for efficient and reliable public transportation in smart cities.

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI is an automated framework for health informatics research that integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language capabilities for medical visualizations and quality control.


<details>
  <summary>Details</summary>
Motivation: Health informatics research involves diverse data modalities and requires integration across biomedical science, data analytics, and clinical practice. Existing LLM-based agent systems lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements.

Method: The framework integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility.

Result: OpenLens AI automates the entire research pipeline and produces publication-ready LaTeX manuscripts with transparent and traceable workflows.

Conclusion: The framework offers a domain-adapted solution for advancing health informatics research by addressing the specific gaps in existing automated research systems.

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [3] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: Evaluation of LLM-based agents for penetration testing shows targeted functional augmentations significantly improve performance in complex multi-step attack scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness and reliability of LLM-based agents in penetration testing across different attack phases and identify key functional capabilities needed for success.

Method: Comprehensive evaluation of multiple LLM agent architectures (single-agent to modular designs) across realistic penetration testing scenarios, with targeted testing of five core functional capabilities: Global Context Memory, Inter-Agent Messaging, Context-Conditioned Invocation, Adaptive Planning, and Real-Time Monitoring.

Result: Targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks, though some architectures natively exhibit subsets of these properties.

Conclusion: Functional augmentations are crucial for enhancing LLM-based penetration testing agents, with specific capabilities like context management, coordination, and real-time responsiveness being particularly important for success in complex attack scenarios.

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [4] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: Novel dual-layered security framework for multi-agent systems using Sentinel Agents for continuous monitoring and Coordinator Agents for governance, successfully tested against 162 synthetic attacks.


<details>
  <summary>Details</summary>
Motivation: To enhance security and reliability in multi-agent systems against emerging threats like prompt injection, LLM hallucinations, privacy breaches, and coordinated attacks.

Method: Architectural framework with distributed Sentinel Agents using semantic analysis (LLMs), behavioral analytics, retrieval-augmented verification, and anomaly detection, plus Coordinator Agent for policy management and threat response.

Result: Successfully detected 162 synthetic attacks (prompt injection, hallucination, data exfiltration) in simulation study, confirming practical feasibility of the monitoring approach.

Conclusion: The dual-layered security approach provides dynamic, adaptive defense mechanisms while enhancing system observability, regulatory compliance, and enabling policy evolution over time.

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [5] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: Proposes a modular evaluation framework for web agents that decomposes agent pipelines into interpretable stages to enable detailed error analysis, addressing the limitation of current evaluations that focus only on overall success.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of web agents powered by LLMs mostly focus on overall success while overlooking intermediate errors, which limits insight into failure modes and hinders systematic improvement.

Method: Developed a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis, using the SeeAct framework and Mind2Web dataset as a case study.

Result: The approach reveals actionable weaknesses missed by standard metrics, providing more granular diagnostic capabilities for web agent performance.

Conclusion: The proposed framework paves the way for more robust and generalizable web agents by enabling fine-grained error analysis and systematic improvement.

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [6] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench is the first benchmark for predicting founder success in VC, featuring 9,000 anonymized founder profiles with strong privacy protection. LLMs like DeepSeek-V3 and GPT-4o significantly outperform human benchmarks and market indices.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like SWE-bench and ARC-AGI accelerate AGI progress, but there's no standardized benchmark for VC founder success prediction where signals are sparse and outcomes uncertain.

Method: Created VCBench with 9,000 anonymized founder profiles standardized to preserve predictive features while resisting identity leakage. Evaluated nine state-of-the-art LLMs against human and market benchmarks.

Result: DeepSeek-V3 delivered over 6x baseline precision, GPT-4o achieved highest F0.5 score, and most models surpassed human benchmarks. Market index precision was 1.9%, Y Combinator 1.7x better, tier-1 firms 2.9x better.

Conclusion: VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in venture forecasting, available as a public evolving resource at vcbench.com.

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [7] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: The paper proposes a new paradigm for defining True Intelligence (TI) with 6 core components, creating a 5-level AGI taxonomy based on measurable components, arguing that Level-5 AGI is functionally equivalent to TI.


<details>
  <summary>Details</summary>
Motivation: Current performance-based AGI definitions are inadequate as they lack mechanism-focused research roadmaps and fail to define the qualitative nature of genuine intelligence, necessitating a shift from external mimicry to cognitive architecture development.

Method: Drawing from human brain inspiration, the authors define True Intelligence with six core components and propose a five-level taxonomy of AGI based on the number of measurable components (embodied sensory fusion, core directives, dynamic schemata creation, multi-expert architecture, orchestration layer) a system exhibits.

Result: The framework provides clear developmental milestones and a practical path for building genuinely intelligent systems, with Level-5 AGI (implementing all five measurable components) considered functionally equivalent to True Intelligence.

Conclusion: This work offers the first holistic, mechanism-based definition of AGI that synthesizes insights from psychology, schema theory, metacognition, brain architectures, and AI, providing an actionable research path where consciousness emerges as a byproduct of integrated higher-order cognition.

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [8] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: Bayesian Measurement Layouts analyze Melting Pot contest agents, revealing that top performers may exploit evaluation limitations rather than demonstrating true cooperation skills, with some lower-scoring agents showing stronger prosocial abilities.


<details>
  <summary>Details</summary>
Motivation: To develop better methods for evaluating social AI capabilities, particularly cooperation, in complex multi-agent environments where current evaluation frameworks may be exploited by agents optimized for specific conditions rather than genuine cooperation.

Method: Applied Bayesian Measurement Layouts to infer capability profiles of multi-agent systems in the Melting Pot contest, analyzing performance data to reveal underlying prosocial abilities and predict future performance.

Result: Found that higher prosocial capabilities don't always correlate with better performance, top-performing submissions excel in scenarios where cooperation isn't required, and contest winner used hard-coded solutions tailored to specific environments rather than genuine cooperation.

Conclusion: Measurement Layouts provide accurate predictions and actionable insights for AI evaluation, but current frameworks need improved cooperation demand annotation and bias accounting to prevent exploitation and ensure genuine social capability assessment.

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [9] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: DeKeyNLU dataset improves NL2SQL accuracy by enhancing task decomposition and keyword extraction in RAG pipelines, achieving significant performance gains on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current NL2SQL systems where inaccurate task decomposition and keyword extraction by LLMs lead to SQL generation errors, and existing datasets suffer from over-fragmentation and lack domain-specific annotations.

Method: Created DeKeyNLU dataset with 1,500 annotated QA pairs, then developed DeKeySQL - a RAG-based pipeline with three modules for question understanding, entity retrieval, and SQL generation, fine-tuned with the new dataset.

Result: Fine-tuning with DeKeyNLU significantly improved SQL generation accuracy: from 62.31% to 69.10% on BIRD dev dataset and from 84.2% to 88.7% on Spider dev dataset.

Conclusion: The DeKeyNLU dataset effectively addresses key bottlenecks in NL2SQL systems, demonstrating that targeted annotation for task decomposition and keyword extraction can substantially improve RAG pipeline performance.

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [10] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: First benchmark for evaluating omnibus rationality of LLMs across theoretical and practical domains, with toolkit and analysis showing where LLMs converge/diverge from human rationality.


<details>
  <summary>Details</summary>
Motivation: Concern about whether and under what circumstances LLMs think and behave like real human agents, given their human-like capabilities and increasing use as AI assistants.

Method: Proposed benchmark covering wide range of domains and LLMs, including easy-to-use toolkit and extensive experimental evaluation.

Result: Analysis illuminates where LLMs converge and diverge from idealized human rationality across different domains.

Conclusion: Benchmark serves as foundational tool for both developers and users of LLMs to assess their rationality.

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [11] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: Proposes a dynamic framework for automated LLM workflow construction that combines Q-table learning with a priori decision-making to optimize task-specific workflows, improving performance while reducing costs.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous workflow construction methods rely too heavily on historical experience, lacking efficiency and adaptability to unique task characteristics.

Method: Uses Q-table learning to optimize decision space and guide agent decisions, with a priori dynamic framework that evaluates task progress to proactively select optimal workflow structures. Includes cold-start initialization, early stopping, and pruning mechanisms.

Result: Achieves 4.05% average improvement over state-of-the-art baselines while reducing workflow construction and inference costs to 30.68%-48.31% of existing methods.

Conclusion: The proposed framework effectively combines historical experience with task-specific adaptability, demonstrating significant performance gains and cost efficiency in automated workflow construction.

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [12] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: This paper addresses privacy challenges in high-stakes domains by developing a comprehensive evaluation framework for differentially private text generation, benchmarking state-of-the-art methods, and revealing privacy risks from public data contamination.


<details>
  <summary>Details</summary>
Motivation: Data sharing in healthcare and finance faces regulatory and privacy barriers, with existing anonymization methods inadequate for unstructured text. Differential privacy offers formal privacy assurances but lacks proper evaluation frameworks for domain-specific synthetic data generation.

Method: Three key contributions: 1) Comprehensive evaluation framework with standardized utility/fidelity metrics across 9 curated domain-specific datasets, 2) Large-scale empirical study benchmarking DP text generation methods and LLMs of varying sizes, 3) Development of membership inference attack methodology for synthetic text.

Result: High-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Empirical evidence shows public datasets in pre-training corpora can invalidate claimed privacy guarantees.

Conclusion: Urgent need for rigorous privacy auditing and persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings.

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [13] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass is the first evaluation framework for post-deployment monitoring and debugging of LLM-based multi-agent workflows, featuring a structured analytical pipeline and dual memory system that achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods fail to capture risks from errors, emergent behaviors, and systemic failures in LLM-based multi-agent workflows, creating a need for robust post-deployment monitoring tools.

Method: A structured multi-stage analytical pipeline modeling expert debuggers: error identification/categorization, thematic clustering, quantitative scoring, and strategic summarization, enhanced with dual episodic/semantic memory for continual learning.

Result: Achieves state-of-the-art results on TRAIL benchmark, uncovers critical issues missed in human annotations, and demonstrates practical utility through real-world deployments with design partners.

Conclusion: AgentCompass serves as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production environments.

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [14] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: Applying Schoenfeld's Episode Theory to analyze Large Reasoning Models' thought structures through cognitive labeling of math problem solutions.


<details>
  <summary>Details</summary>
Motivation: Lack of principled framework to understand how Large Reasoning Models structure their chain-of-thought reasoning, despite generating extensive reasoning traces.

Method: Annotated thousands of sentences/paragraphs from model-generated math solutions using seven cognitive labels (Plan, Implement, Verify, etc.) based on Schoenfeld's Episode Theory.

Result: Created first publicly available benchmark for fine-grained analysis of machine reasoning, including large annotated corpus and annotation guides. Preliminary analysis reveals distinct patterns in LRM reasoning and transition dynamics between cognitive states.

Conclusion: Provides theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [15] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly is a novel framework that combines Chain-of-Thought fine-tuning with reinforcement learning to improve log anomaly detection, addressing interpretability and reliability issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing log anomaly detection approaches face limitations: traditional deep learning models lack interpretability and generalization, while LLM-based methods suffer from unreliability and factual inaccuracies.

Method: The framework uses CoT-guided supervised fine-tuning with expert-corrected data to instill reasoning patterns, followed by reinforcement learning with a multi-faceted reward function to optimize accuracy and logical consistency while mitigating hallucinations.

Result: RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs.

Conclusion: The proposed framework successfully addresses the limitations of existing methods by combining CoT fine-tuning with reinforcement learning, resulting in improved accuracy, reliability, and interpretability for log anomaly detection.

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [16] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo is a Japanese riddle-based benchmark for testing insight-based reasoning in LLMs, showing most models underperform humans except GPT-5, with reasoning models outperforming non-reasoning ones regardless of size.


<details>
  <summary>Details</summary>
Motivation: Address benchmark saturation and contamination issues in LLM evaluation by creating a cost-effective, extensible benchmark that tests insight-based reasoning.

Method: Built benchmark from Japanese children's riddles - short items requiring no specialized knowledge, scalable generation. Evaluated 38 frontier models and 126 humans on 120 riddles, with extended analysis on 201 items including thought log analysis.

Result: No model except GPT-5 comparable to human performance (52.9% mean accuracy). Reasoning models significantly outperform non-reasoning peers, model size shows no reliable association with accuracy. Many cases of verification failure observed where models produce correct solution but fail to select it.

Conclusion: Nazonazo provides cost-effective, scalable, renewable benchmark format addressing evaluation crisis while revealing meta-cognitive weaknesses in models, offering clear targets for future control and calibration methods.

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [17] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: AC-RAG framework uses adversarial collaboration between two agents (Detector and Resolver) to address retrieval hallucinations in RAG systems, significantly improving performance across domains.


<details>
  <summary>Details</summary>
Motivation: Address retrieval hallucinations in RAG systems where models fail to recognize and act upon poor-quality retrieved documents, which undermines performance.

Method: Propose AC-RAG framework with two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides solutions. They engage in adversarial collaboration guided by a moderator, with iterative questioning and problem dissection.

Result: Extensive experiments show AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.

Conclusion: The adversarial collaboration approach effectively addresses retrieval hallucinations and enhances RAG system performance through iterative problem dissection and refined knowledge retrieval.

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [18] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: Transformer-based AI framework predicts CPE infection risks and patient outcomes from EMR data, outperforming traditional models with explainable insights on key risk factors.


<details>
  <summary>Details</summary>
Motivation: Carbapenemase-Producing Enterobacteriace (CPE) poses critical infection control challenges in hospitals, but predictive modeling of CPE-associated risks like readmission, mortality, and extended length of stay remains underexplored with modern deep learning approaches.

Method: Developed an eXplainable AI framework using Transformer-based architectures benchmarked against traditional ML models. Analyzed inpatient EMR data including diagnostic codes, ward transitions, demographics, infection variables, and contact network features. Applied XAI techniques to interpret model decisions.

Result: TabTransformer consistently outperformed baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). Infection-related features, historical hospital exposure, admission context, and network centrality measures were highly influential. Key risk factors identified included Area of Residence, Admission Ward, prior admissions, and Ward PageRank.

Conclusion: The study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes, demonstrating superior performance of Transformer models and highlighting the importance of diverse clinical and network features.

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [19] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: Generalization of single-argument contribution functions to set-based functions in quantitative bipolar argumentation graphs, with new principles for set interactions and application to recommendation systems.


<details>
  <summary>Details</summary>
Motivation: To extend existing quantitative bipolar argumentation frameworks by developing functions that measure the collective contribution of argument sets (rather than individual arguments) to determine the final strength of a topic argument.

Method: Generalize existing single-argument contribution functions to set-based functions, establish new principles specific to set interactions, and perform principle-based analysis across different set contribution functions.

Result: Developed generalized set contribution functions that quantify how argument sets collectively influence topic strength, with new principles addressing set-specific properties and interactions between arguments within sets.

Conclusion: The proposed set contribution functions provide a more comprehensive framework for analyzing collective argument influence in bipolar argumentation, with practical applications demonstrated in recommendation systems.

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [20] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC is a dynamic multi-agent framework that enables LLM agents to form and expand expert teams based on diagnostic context, outperforming existing methods in complex medical scenarios.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent collaboration frameworks use static, pre-assigned roles that limit adaptability and dynamic knowledge integration needed for complex medical decision-making.

Method: KAMAC starts with expert agents and conducts knowledge-driven discussions to identify gaps, then recruits additional specialists as needed, with decisions finalized through reviewing updated agent comments.

Result: Experiments on real-world medical benchmarks show KAMAC significantly outperforms both single-agent and advanced multi-agent methods, especially in complex clinical scenarios like cancer prognosis.

Conclusion: The framework enables flexible, scalable collaboration for complex clinical scenarios requiring dynamic, cross-specialty expertise integration.

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [21] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: Study shows generative AI can provide effective formative assessment feedback in online graduate courses by approximating human feedback qualities like directive clarity, supportive stance, and balanced critique.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI can support formative assessment through machine-generated reviews of peer reviews in online graduate education.

Method: Analyzed 120 metareviews using Systemic Functional Linguistics and Appraisal Theory to examine how AI feedback constructs meaning across ideational, interpersonal, and textual dimensions.

Result: Generative AI feedback demonstrated key features of effective human feedback - balanced praise and constructive critique, alignment with rubric expectations, structured staging that foregrounded student agency, and maintained supportive stance with directive clarity.

Conclusion: AI metafeedback has potential to scaffold feedback literacy and enhance learner engagement with peer review by modeling effective feedback qualities.

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [22] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: Survey on maritime professionals' perceptions of XAI for trust and usability in autonomous maritime operations.


<details>
  <summary>Details</summary>
Motivation: As AI systems become integral to maritime operations, trust depends on transparency and interpretability, not just performance. Effective human-machine teaming requires explainable AI to enable informed oversight and shared understanding in complex maritime environments.

Method: Propose a domain-specific survey designed to capture maritime professionals' perceptions of trust, usability, and explainability of AI systems.

Result: The paper presents a survey framework to assess XAI needs in maritime domain, but does not report specific survey results or findings.

Conclusion: The survey aims to foster awareness and guide development of user-centric XAI systems tailored to seafarers' needs, supporting effective integration of explainable AI in maritime operations.

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [23] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA is a reinforcement learning framework that uses multi-agent debate to train language models to be more self-consistent reasoners by aligning with internal consensus from deliberative exchanges.


<details>
  <summary>Details</summary>
Motivation: Language models are inconsistent reasoners that generate contradictory responses to identical prompts, and current inference-time methods don't address the core problem of unreliable reasoning pathway selection.

Method: Multi-Agent Consensus Alignment (MACA) - a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with internal consensus using majority/minority outcomes from multi-agent debate with deliberative exchanges.

Result: Substantial improvements across multiple benchmarks: +27.6% on GSM8K, +23.7% on MATH, +22.4% Pass@20 on MATH, +42.7% on MathQA, with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA).

Conclusion: MACA enables robust self-alignment that more reliably unlocks the latent reasoning potential of language models through multi-agent consensus learning without external supervision.

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [24] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: RLVR method enhances multimodal LLMs' geometric reasoning by generating high-quality training data with verifiable rewards, improving performance on both geometric and non-geometric tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs struggle with complex geometric problems due to lack of high-quality image-text datasets and limited generalization of template-based data synthesis methods.

Method: Introduces Reinforcement Learning with Verifiable Rewards (RLVR) to refine captions for geometric images synthesized from 50 basic relations, using reward signals from math problem-solving tasks.

Result: Achieves 2.8%-4.8% accuracy improvements on MathVista/MathVerse non-geometric tasks and 2.4%-3.9% improvements on MMMU Art/Design/Tech/Engineering tasks, demonstrating better generalization.

Conclusion: RLVR pipeline effectively captures geometry problem-solving features, enhances multimodal LLM reasoning capabilities, and improves performance across diverse tasks including out-of-distribution scenarios.

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>


### [25] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: Proposes Local State Attention module using self-attention to improve autonomous vehicle coordination in mixed-traffic environments, showing significant merging efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles need to adapt to human-controlled vehicles and unusual driving situations in mixed-traffic environments. Current MARL methods often fail to resolve local conflicts between agents and cannot generalize to stochastic events.

Method: Introduces a Local State Attention module that uses self-attention operators to compress essential information from nearby agents, helping resolve traffic conflicts. Tested in simulated highway merging scenarios with priority vehicles as unexpected events.

Result: The approach demonstrates significant improvements in merging efficiency compared to popular baselines, particularly in high-density traffic settings. It effectively prioritizes other vehicles' information to manage merging processes.

Conclusion: The Local State Attention module successfully enhances autonomous vehicle coordination in mixed-traffic environments by better handling local conflicts and stochastic events through improved state representation.

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Discovering New Theorems via LLMs with In-Context Proof Learning in Lean](https://arxiv.org/abs/2509.14274)
*Kazumi Kasaura,Naoto Onda,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.LG

TL;DR: LLMs can automatically generate and prove novel mathematical theorems using a Conjecturing-Proving Loop that leverages in-context learning with previously generated theorems and proofs.


<details>
  <summary>Details</summary>
Motivation: Previous works focused on solving existing problems, but this paper explores LLMs' ability to discover novel theorems automatically.

Method: Proposed Conjecturing-Proving Loop pipeline that generates mathematical conjectures and proves them in Lean 4 format, using context from previously generated theorems and proofs for in-context learning.

Result: The framework rediscovered theorems published in past mathematical papers that hadn't been formalized, and demonstrated that in-context learning was essential for proving at least one theorem that couldn't be proved without it.

Conclusion: The approach enables generation of more difficult proofs through in-context learning of proof strategies without changing LLM parameters, showing effectiveness for neural theorem proving.

Abstract: Large Language Models have demonstrated significant promise in formal theorem
proving. However, previous works mainly focus on solving existing problems. In
this paper, we focus on the ability of LLMs to find novel theorems. We propose
Conjecturing-Proving Loop pipeline for automatically generating mathematical
conjectures and proving them in Lean 4 format. A feature of our approach is
that we generate and prove further conjectures with context including
previously generated theorems and their proofs, which enables the generation of
more difficult proofs by in-context learning of proof strategies without
changing parameters of LLMs. We demonstrated that our framework rediscovered
theorems with verification, which were published in past mathematical papers
and have not yet formalized. Moreover, at least one of these theorems could not
be proved by the LLM without in-context learning, even in natural language,
which means that in-context learning was effective for neural theorem proving.
The source code is available at
https://github.com/auto-res/ConjecturingProvingLoop.

</details>


### [27] [A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation](https://arxiv.org/abs/2509.14384)
*Nishantak Panigrahi,Mayank Patwal*

Main category: cs.LG

TL;DR: DNNs for nonlocal conservation law approximation from Kuramoto model, showing tanh activation provides stable convergence while sine can achieve lower errors but may produce artifacts. DNNs offer competitive accuracy vs traditional methods but have limitations with singular/piecewise solutions due to oversmoothing.


<details>
  <summary>Details</summary>
Motivation: Investigate efficiency of Deep Neural Networks to approximate solutions of nonlocal conservation laws derived from identical-oscillator Kuramoto model, focusing on architectural choices and their impact on accuracy and computation time.

Method: Systematic experimentation with different network configurations: activation functions (tanh, sin, ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count). Comparative analysis with traditional numerical methods.

Result: Tanh activation yields stable convergence across configurations. Sine activation can achieve marginally lower errors and training times in some cases but occasionally produces nonphysical artifacts. Optimally configured DNNs offer competitive accuracy with different computational trade-offs compared to traditional methods.

Conclusion: Standard feed-forward architectures have fundamental limitations when handling singular or piecewise-constant solutions, inherently oversmoothing sharp features due to function space limitations of standard activation functions. Provides empirical guidelines for DNN implementation while highlighting theoretical constraints for challenging physical systems with discontinuities.

Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

</details>


### [28] [Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility](https://arxiv.org/abs/2509.14386)
*Arjun S. Nair,Kristina P. Sinaga*

Main category: cs.LG

TL;DR: Neural networks cannot learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision due to information-theoretic constraints.


<details>
  <summary>Details</summary>
Motivation: To understand why neural networks struggle with confidence calibration and diversity, and to prove this is a fundamental limitation rather than a methodological issue.

Method: Rigorous mathematical analysis and comprehensive empirical evaluation including negative reward training, symmetric loss functions, post-hoc calibration methods across MNIST, Fashion-MNIST, and CIFAR-10 datasets.

Result: Universal failure patterns: negative rewards cause extreme underconfidence (ECE > 0.8) and destroy confidence diversity (std < 0.05), symmetric losses fail to escape binary signal averaging, post-hoc methods achieve calibration (ECE < 0.02) only by compressing confidence distribution. 100% failure rate for all training methods, 33% success rate for post-hoc calibration.

Conclusion: Binary supervision creates an underspecified mapping problem where different confidence levels for correct predictions receive identical supervision. This explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary. Novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning are proposed to overcome these limitations.

Abstract: We prove a fundamental impossibility theorem: neural networks cannot
simultaneously learn well-calibrated confidence estimates with meaningful
diversity when trained using binary correct/incorrect supervision. Through
rigorous mathematical analysis and comprehensive empirical evaluation spanning
negative reward training, symmetric loss functions, and post-hoc calibration
methods, we demonstrate this is an information-theoretic constraint, not a
methodological failure. Our experiments reveal universal failure patterns:
negative rewards produce extreme underconfidence (ECE greater than 0.8) while
destroying confidence diversity (std less than 0.05), symmetric losses fail to
escape binary signal averaging, and post-hoc methods achieve calibration (ECE
less than 0.02) only by compressing the confidence distribution. We formalize
this as an underspecified mapping problem where binary signals cannot
distinguish between different confidence levels for correct predictions: a 60
percent confident correct answer receives identical supervision to a 90 percent
confident one. Crucially, our real-world validation shows 100 percent failure
rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while
post-hoc calibration's 33 percent success rate paradoxically confirms our
theorem by achieving calibration through transformation rather than learning.
This impossibility directly explains neural network hallucinations and
establishes why post-hoc calibration is mathematically necessary, not merely
convenient. We propose novel supervision paradigms using ensemble disagreement
and adaptive multi-agent learning that could overcome these fundamental
limitations without requiring human confidence annotations.

</details>


### [29] [Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs](https://arxiv.org/abs/2509.14391)
*Ye Qiao,Sitao Huang*

Main category: cs.LG

TL;DR: Combining RoPE position interpolation with post-training quantization causes accuracy degradation due to position-dependent noise. Q-ROAR introduces RoPE-aware stabilization through frequency band grouping and per-band scaling search to recover accuracy.


<details>
  <summary>Details</summary>
Motivation: Extending LLM context windows is crucial for long-range tasks, but combining position interpolation methods with post-training quantization leads to accuracy degradation due to coupled effects like long context aliasing and outlier shifting.

Method: Proposes Q-ROAR - a RoPE-aware weight-only stabilization that groups RoPE dimensions into frequency bands and performs a small search over per-band scales for W_Q and W_K matrices, using diagnostics like Interpolation Pressure and Tail Inflation Ratios.

Result: Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and maintaining compatibility with existing inference stacks.

Conclusion: The method effectively addresses the degradation issues when combining position interpolation with quantization, providing a no-fine-tuning solution that maintains performance across both short and long contexts.

Abstract: Extending LLM context windows is crucial for long range tasks. RoPE-based
position interpolation (PI) methods like linear and frequency-aware scaling
extend input lengths without retraining, while post-training quantization (PTQ)
enables practical deployment. We show that combining PI with PTQ degrades
accuracy due to coupled effects long context aliasing, dynamic range dilation,
axis grid anisotropy, and outlier shifting that induce position-dependent logit
noise. We provide the first systematic analysis of PI plus PTQ and introduce
two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)
and Tail Inflation Ratios (outlier shift from short to long contexts). To
address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that
groups RoPE dimensions into a few frequency bands and performs a small search
over per-band scales for W_Q,W_K, with an optional symmetric variant to
preserve logit scale. The diagnostics guided search uses a tiny long-context
dev set and requires no fine-tuning, kernel, or architecture changes.
Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces
GovReport perplexity by more than 10%, while preserving short-context
performance and compatibility with existing inference stacks.

</details>


### [30] [Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models](https://arxiv.org/abs/2509.14427)
*Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly*

Main category: cs.LG

TL;DR: Hashing-Baseline is a training-free hashing method that combines classical techniques (PCA, random orthogonal projection, threshold binarization) with frozen pretrained encoders to produce competitive binary embeddings for fast search without additional training.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art hashing methods require expensive, scenario-specific training, which limits scalability and practical deployment for fast search applications.

Method: Combines classical training-free hashing techniques (PCA, random orthogonal projection, threshold binarization) with frozen embeddings from state-of-the-art pretrained vision and audio encoders.

Result: Achieves competitive retrieval performance on standard image retrieval benchmarks and a newly introduced audio hashing benchmark without any additional learning or fine-tuning.

Conclusion: The approach demonstrates that strong hashing performance can be achieved without expensive training by leveraging powerful pretrained encoders and classical techniques, providing a practical and scalable solution for information retrieval.

Abstract: Information retrieval with compact binary embeddings, also referred to as
hashing, is crucial for scalable fast search applications, yet state-of-the-art
hashing methods require expensive, scenario-specific training. In this work, we
introduce Hashing-Baseline, a strong training-free hashing method leveraging
powerful pretrained encoders that produce rich pretrained embeddings. We
revisit classical, training-free hashing techniques: principal component
analysis, random orthogonal projection, and threshold binarization, to produce
a strong baseline for hashing. Our approach combines these techniques with
frozen embeddings from state-of-the-art vision and audio encoders to yield
competitive retrieval performance without any additional learning or
fine-tuning. To demonstrate the generality and effectiveness of this approach,
we evaluate it on standard image retrieval benchmarks as well as a newly
introduced benchmark for audio hashing.

</details>


### [31] [FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport](https://arxiv.org/abs/2509.14444)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: FedAVOT addresses FL bias from client availability mismatch by using optimal transport to align availability and importance distributions, achieving better convergence and performance than FedAvg.


<details>
  <summary>Details</summary>
Motivation: Federated Learning suffers from biased updates when client participation distribution doesn't match the optimization objective distribution, leading to unstable training.

Method: Proposes FedAVOT which formulates aggregation as a masked optimal transport problem using Sinkhorn scaling to compute transport-based aggregation weights.

Result: Achieves O(1/√T) convergence rate independent of participating users per round, with drastically improved performance across heterogeneous, fairness-sensitive, and low-availability scenarios.

Conclusion: FedAVOT provides provable convergence guarantees and superior performance over FedAvg, even with as few as two clients per round.

Abstract: Federated Learning (FL) allows distributed model training without sharing raw
data, but suffers when client participation is partial. In practice, the
distribution of available users (\emph{availability distribution} $q$) rarely
aligns with the distribution defining the optimization objective
(\emph{importance distribution} $p$), leading to biased and unstable updates
under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal
Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal
transport problem aligning $q$ and $p$. Using Sinkhorn scaling,
\textbf{FedAVOT} computes transport-based aggregation weights with provable
convergence guarantees. \textbf{FedAVOT} achieves a standard
$\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent
of the number of participating users per round. Our experiments confirm
drastically improved performance compared to FedAvg across heterogeneous,
fairness-sensitive, and low-availability regimes, even when only two clients
participate per round.

</details>


### [32] [H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations](https://arxiv.org/abs/2509.14472)
*Mahsa Khazaei,Azim Ahmadzadeh,Alexei Pevtsov,Luca Bertello,Alexander Pevtsov*

Main category: cs.LG

TL;DR: Lightweight non-ML anomaly detection algorithm for H-alpha solar observations that outperforms existing methods while providing explainability and highlighting specific anomalous regions.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of astrophysical data requires quality control for ML models. GONG network's H-alpha observations produce continuous data since 2010 that needs reliable anomaly detection.

Method: Developed H-Alpha Anomalyzer - a lightweight non-machine learning algorithm that identifies anomalies based on user-defined criteria, highlights specific anomalous regions, and quantifies anomaly likelihood.

Result: Outperforms existing methods and provides explainability for domain expert evaluation. Created and released a dataset of 2,000 observations (50% anomalous, 50% normal) for comparative analysis.

Conclusion: The proposed model effectively detects anomalies in solar H-alpha observations while offering transparency and interpretability that black-box ML algorithms lack.

Abstract: The plethora of space-borne and ground-based observatories has provided
astrophysicists with an unprecedented volume of data, which can only be
processed at scale using advanced computing algorithms. Consequently, ensuring
the quality of data fed into machine learning (ML) models is critical. The
H$\alpha$ observations from the GONG network represent one such data stream,
producing several observations per minute, 24/7, since 2010. In this study, we
introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha
Anomalyzer, designed to identify anomalous observations based on user-defined
criteria. Unlike many black-box algorithms, our approach highlights exactly
which regions triggered the anomaly flag and quantifies the corresponding
anomaly likelihood. For our comparative analysis, we also created and released
a dataset of 2,000 observations, equally divided between anomalous and
non-anomalous cases. Our results demonstrate that the proposed model not only
outperforms existing methods but also provides explainability, enabling
qualitative evaluation by domain experts.

</details>


### [33] [Decentralized Optimization with Topology-Independent Communication](https://arxiv.org/abs/2509.14488)
*Ying Lin,Yao Kuang,Ahmet Alacaoglu,Michael P. Friedlander*

Main category: cs.LG

TL;DR: Randomized local coordination method reduces communication from O(m) to exactly 2 messages per iteration for graph-guided regularizers by sampling one regularizer per node and coordinating only with relevant nodes.


<details>
  <summary>Details</summary>
Motivation: Full synchronization in distributed optimization scales poorly, with standard methods requiring O(m) communications per iteration when n nodes collaborate through m pairwise regularizers.

Method: Each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term, replacing the proximal map of the sum with the proximal map of a single randomly selected regularizer.

Result: Achieves ~O(ε⁻²) iterations for convex objectives, O(ε⁻¹) to ε-solution under strong convexity, and O(log(1/ε)) to a neighborhood. Communication drops to exactly 2 messages per iteration for graph-guided regularizers.

Conclusion: Randomized local coordination preserves convergence while eliminating global coordination, validated by experiments showing both convergence rates and communication efficiency on synthetic and real-world datasets.

Abstract: Distributed optimization requires nodes to coordinate, yet full
synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise
regularizers, standard methods demand $\mathcal{O}(m)$ communications per
iteration. This paper proposes randomized local coordination: each node
independently samples one regularizer uniformly and coordinates only with nodes
sharing that term. This exploits partial separability, where each regularizer
$G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For
graph-guided regularizers where $|S_j|=2$, expected communication drops to
exactly 2 messages per iteration. This method achieves
$\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and
under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an
$\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a
neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the
proximal map of a single randomly selected regularizer $G_j$ preserves
convergence while eliminating global coordination. Experiments validate both
convergence rates and communication efficiency across synthetic and real-world
datasets.

</details>


### [34] [BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning](https://arxiv.org/abs/2509.14519)
*Wadduwage Shanika Perera,Haodi Jiang*

Main category: cs.LG

TL;DR: BEACON is a deep learning framework that uses large language models to generate behavioral embeddings from malware sandbox reports, achieving superior malware classification performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional static malware analysis fails against modern obfuscation and evasion techniques, creating a need for more reliable behavioral detection methods that monitor runtime activities.

Method: Proposes BEACON framework that leverages LLMs to generate dense contextual embeddings from raw sandbox behavior reports, then uses 1D CNN for multi-class malware classification.

Result: Evaluated on Avast-CTU Public CAPE Dataset, consistently outperforms existing methods, demonstrating effectiveness of LLM-based behavioral embeddings.

Conclusion: The framework highlights the effectiveness of combining LLMs with behavioral analysis for robust malware classification against modern threats.

Abstract: Malware is becoming increasingly complex and widespread, making it essential
to develop more effective and timely detection methods. Traditional static
analysis often fails to defend against modern threats that employ code
obfuscation, polymorphism, and other evasion techniques. In contrast,
behavioral malware detection, which monitors runtime activities, provides a
more reliable and context-aware solution. In this work, we propose BEACON, a
novel deep learning framework that leverages large language models (LLMs) to
generate dense, contextual embeddings from raw sandbox-generated behavior
reports. These embeddings capture semantic and structural patterns of each
sample and are processed by a one-dimensional convolutional neural network (1D
CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public
CAPE Dataset, our framework consistently outperforms existing methods,
highlighting the effectiveness of LLM-based behavioral embeddings and the
overall design of BEACON for robust malware classification.

</details>


### [35] [Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach](https://arxiv.org/abs/2509.14536)
*Muhammad Awais Ali,Marlon Dumas,Fredrik Milani*

Main category: cs.LG

TL;DR: Proposes a technique for predicting business process case suffixes with start and end timestamps to enable resource capacity planning, using a sweep-line approach that predicts all ongoing cases simultaneously rather than in isolation.


<details>
  <summary>Details</summary>
Motivation: Existing case suffix prediction approaches only generate sequences with single timestamps, which is insufficient for resource capacity planning that requires reasoning about when resources will be busy performing work.

Method: Introduces a sweep-line approach that predicts case suffixes consisting of activities with both start and end timestamps, predicting waiting and processing times for all ongoing cases in lockstep rather than in isolation.

Result: Evaluation on real-life and synthetic datasets shows advantages of this multi-model approach for case suffix prediction accuracy.

Conclusion: The proposed technique provides more comprehensive predictions suitable for resource capacity planning by capturing both waiting and processing times through simultaneous prediction of all ongoing cases.

Abstract: Predictive process monitoring techniques support the operational decision
making by predicting future states of ongoing cases of a business process. A
subset of these techniques predict the remaining sequence of activities of an
ongoing case (case suffix prediction). Existing approaches for case suffix
prediction generate sequences of activities with a single timestamp (e.g. the
end timestamp). This output is insufficient for resource capacity planning,
where we need to reason about the periods of time when resources will be busy
performing work. This paper introduces a technique for predicting case suffixes
consisting of activities with start and end timestamps. In other words, the
proposed technique predicts both the waiting time and the processing time of
each activity. Since the waiting time of an activity in a case depends on how
busy resources are in other cases, the technique adopts a sweep-line approach,
wherein the suffixes of all ongoing cases in the process are predicted in
lockstep, rather than predictions being made for each case in isolation. An
evaluation on real-life and synthetic datasets compares the accuracy of
different instantiations of this approach, demonstrating the advantages of a
multi-model approach to case suffix prediction.

</details>


### [36] [LiMuon: Light and Fast Muon Optimizer for Large Models](https://arxiv.org/abs/2509.14562)
*Feihu Huang,Yuning Luo,Songcan Chen*

Main category: cs.LG

TL;DR: LiMuon optimizer: a light and fast Muon variant using momentum-based variance reduction and randomized SVD for efficient large model training with lower memory and O(ε⁻³) sample complexity under both smooth and generalized smooth conditions.


<details>
  <summary>Details</summary>
Motivation: Existing Muon optimizers suffer from high sample complexity or high memory requirements for large models, and current convergence analysis relies on strict Lipschitz smooth assumptions that don't hold for tasks like LLM training.

Method: Proposes LiMuon optimizer based on momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD) to reduce memory usage while maintaining efficiency.

Result: LiMuon achieves lower memory than current Muon variants and O(ε⁻³) sample complexity for finding ε-stationary solutions under both smooth and generalized smooth conditions. Experimental results on DistilGPT2 and ViT models verify efficiency.

Conclusion: LiMuon provides an effective solution for efficient large model training with reduced memory requirements and proven convergence guarantees under more realistic smoothness conditions encountered in AI tasks like LLM training.

Abstract: Large models recently are widely applied in artificial intelligence, so
efficient training of large models has received widespread attention. More
recently, a useful Muon optimizer is specifically designed for
matrix-structured parameters of large models. Although some works have begun to
studying Muon optimizer, the existing Muon and its variants still suffer from
high sample complexity or high memory for large models. To fill this gap, we
propose a light and fast Muon (LiMuon) optimizer for training large models,
which builds on the momentum-based variance reduced technique and randomized
Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory
than the current Muon and its variants. Moreover, we prove that our LiMuon has
a lower sample complexity of $O(\epsilon^{-3})$ for finding an
$\epsilon$-stationary solution of non-convex stochastic optimization under the
smooth condition. Recently, the existing convergence analysis of Muon optimizer
mainly relies on the strict Lipschitz smooth assumption, while some artificial
intelligence tasks such as training large language models (LLMs) do not satisfy
this condition. We also proved that our LiMuon optimizer has a sample
complexity of $O(\epsilon^{-3})$ under the generalized smooth condition.
Numerical experimental results on training DistilGPT2 and ViT models verify
efficiency of our LiMuon optimizer.

</details>


### [37] [Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework](https://arxiv.org/abs/2509.14563)
*Shiyuan Luo,Runlong Yu,Chonghao Qiu,Rahul Ghosh,Robert Ladwig,Paul C. Hanson,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: A$^2$SL framework improves environmental modeling by retrieving relevant observational data and using adaptive augmentation for data-scarce and atypical conditions, demonstrated with freshwater ecosystem case study.


<details>
  <summary>Details</summary>
Motivation: High cost of environmental data collection and poor generalization of existing ML approaches in data-sparse or atypical conditions.

Method: Augmentation-Adaptive Self-Supervised Learning with multi-level pairwise learning loss for scenario similarity, retrieval mechanism, and selective data augmentation for extreme conditions.

Result: Significantly improves predictive accuracy and robustness for water temperature and dissolved oxygen modeling in real-world lakes under data-scarce scenarios.

Conclusion: A$^2$SL provides a broadly applicable solution for various scientific domains beyond freshwater ecosystems, addressing data scarcity and atypical condition challenges.

Abstract: The discovery of environmental knowledge depends on labeled task-specific
data, but is often constrained by the high cost of data collection. Existing
machine learning approaches usually struggle to generalize in data-sparse or
atypical conditions. To this end, we propose an Augmentation-Adaptive
Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant
observational samples to enhance modeling of the target ecosystem.
Specifically, we introduce a multi-level pairwise learning loss to train a
scenario encoder that captures varying degrees of similarity among scenarios.
These learned similarities drive a retrieval mechanism that supplements a
target scenario with relevant data from different locations or time periods.
Furthermore, to better handle variable scenarios, particularly under atypical
or extreme conditions where traditional models struggle, we design an
augmentation-adaptive mechanism that selectively enhances these scenarios
through targeted data augmentation. Using freshwater ecosystems as a case
study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen
dynamics in real-world lakes. Experimental results show that A$^2$SL
significantly improves predictive accuracy and enhances robustness in
data-scarce and atypical scenarios. Although this study focuses on freshwater
ecosystems, the A$^2$SL framework offers a broadly applicable solution in
various scientific domains.

</details>


### [38] [Evidential Physics-Informed Neural Networks for Scientific Discovery](https://arxiv.org/abs/2509.14568)
*Hai Siong Tan,Kuancheng Wang,Rafe McBeth*

Main category: cs.LG

TL;DR: E-PINN is a novel uncertainty-aware Physics-Informed Neural Network that uses evidential deep learning for uncertainty estimation and parameter inference, outperforming Bayesian PINN and Deep Ensemble methods in calibration.


<details>
  <summary>Details</summary>
Motivation: To develop a more reliable uncertainty-aware PINN framework that can better quantify uncertainty in PDE solutions and parameter estimation, addressing limitations of existing methods like Bayesian PINN and Deep Ensembles.

Method: Leverages marginal distribution loss function from evidential deep learning to estimate output uncertainty and infers unknown PDE parameters through learned posterior distributions. Validated on 1D Poisson equation and 2D Fisher-KPP equation.

Result: E-PINN generated significantly better calibrated empirical coverage probabilities compared to Bayesian PINN and Deep Ensemble methods. Demonstrated real-world applicability on clinical glucose-insulin datasets for diabetes research.

Conclusion: E-PINN provides a superior uncertainty quantification framework for physics-informed neural networks with better calibration performance and practical applicability to real-world scientific problems.

Abstract: We present the fundamental theory and implementation guidelines underlying
Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of
uncertainty-aware PINN. It leverages the marginal distribution loss function of
evidential deep learning for estimating uncertainty of outputs, and infers
unknown parameters of the PDE via a learned posterior distribution. Validating
our model on two illustrative case studies -- the 1D Poisson equation with a
Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated
empirical coverage probabilities that were calibrated significantly better than
Bayesian PINN and Deep Ensemble methods. To demonstrate real-world
applicability, we also present a brief case study on applying E-PINN to analyze
clinical glucose-insulin datasets that have featured in medical research on
diabetes pathophysiology.

</details>


### [39] [Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition](https://arxiv.org/abs/2509.14577)
*Yang Xu,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: SPMD-LRT is a tensor-based classifier that preserves data structure while optimizing margin distribution, outperforming traditional methods on high-dimensional tensor data.


<details>
  <summary>Details</summary>
Motivation: Existing LMDM methods require flattening tensor data, destroying structural information and increasing computational burden for high-dimensional data.

Method: Proposes SPMD-LRT that operates directly on tensor representations, incorporates first/second-order margin statistics, and uses low-rank tensor decomposition (CP/Tucker) with alternating optimization.

Result: Superior classification accuracy on MNIST, images, and fMRI data compared to SVM, vector-based LMDM, and tensor SVM extensions. Tucker decomposition achieved highest accuracy.

Conclusion: SPMD-LRT effectively handles high-dimensional tensor data while preserving structural information and optimizing margin distribution for improved classification performance.

Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in
classifier design that optimizes not just the minimum margin (as in SVM) but
the entire margin distribution, thereby improving generalization. However,
existing LMDM formulations are limited to vectorized inputs and struggle with
high-dimensional tensor data due to the need for flattening, which destroys the
data's inherent multi-mode structure and increases computational burden. In
this paper, we propose a Structure-Preserving Margin Distribution Learning for
High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates
directly on tensor representations without vectorization. The SPMD-LRT
preserves multi-dimensional spatial structure by incorporating first-order and
second-order tensor statistics (margin mean and variance) into the objective,
and it leverages low-rank tensor decomposition techniques including rank-1(CP),
higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An
alternating optimization (double-gradient descent) algorithm is developed to
efficiently solve the SPMD-LRT, iteratively updating factor matrices and core
tensor. This approach enables SPMD-LRT to maintain the structural information
of high-order data while optimizing margin distribution for improved
classification. Extensive experiments on diverse datasets (including MNIST,
images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior
classification accuracy compared to conventional SVM, vector-based LMDM, and
prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker
Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest
accuracy, highlighting the benefit of structure preservation. These results
confirm the effectiveness and robustness of SPMD-LRT in handling
high-dimensional tensor data for classification.

</details>


### [40] [Online reinforcement learning via sparse Gaussian mixture model Q-functions](https://arxiv.org/abs/2509.14585)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: Online RL framework using sparse Gaussian mixture model Q-functions that achieves performance comparable to dense DeepRL methods with fewer parameters and better generalization in low-parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable and structured online policy-iteration framework that extends previous offline GMM-QF work to online settings, enabling better exploration and mitigating overfitting while maintaining expressiveness.

Method: Proposes sparse Gaussian mixture model Q-functions (S-GMM-QFs) with sparsification via Hadamard overparametrization, uses Riemannian manifold structure for principled parameter updates through online gradient descent on smooth objectives.

Result: S-GMM-QFs match performance of dense DeepRL methods on standard benchmarks while using significantly fewer parameters, and maintain strong performance in low-parameter-count regimes where sparsified DeepRL methods fail.

Conclusion: The framework provides an effective online RL approach that combines interpretability, parameter efficiency, and strong generalization capabilities through structured sparse modeling and Riemannian optimization.

Abstract: This paper introduces a structured and interpretable online policy-iteration
framework for reinforcement learning (RL), built around the novel class of
sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work
that trained GMM-QFs offline, the proposed framework develops an online scheme
that leverages streaming data to encourage exploration. Model complexity is
regulated through sparsification by Hadamard overparametrization, which
mitigates overfitting while preserving expressiveness. The parameter space of
S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing
for principled parameter updates via online gradient descent on a smooth
objective. Numerical tests show that S-GMM-QFs match the performance of dense
deep RL (DeepRL) methods on standard benchmarks while using significantly fewer
parameters, and maintain strong performance even in low-parameter-count regimes
where sparsified DeepRL methods fail to generalize.

</details>


### [41] [TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics](https://arxiv.org/abs/2509.14600)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Razvan Marinescu*

Main category: cs.LG

TL;DR: Adding energy matching to coarse-grained machine learning models for molecular dynamics shows potential for improving free energy surface generalization but no significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations are computationally expensive for long timescales, and conventional force matching approaches often fail to capture the full thermodynamic landscape of biomolecular systems.

Method: Incorporated a complementary energy matching term into the loss function of CGSchNet model, systematically varying the weight of the energy loss term, and evaluated on Chignolin protein.

Result: Energy matching did not yield statistically significant improvements in accuracy but revealed distinct tendencies in how models generalize the free energy surface.

Conclusion: Future opportunities exist to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.

Abstract: Molecular dynamics (MD) simulations provide atomistic insight into
biomolecular systems but are often limited by high computational costs required
to access long timescales. Coarse-grained machine learning models offer a
promising avenue for accelerating sampling, yet conventional force matching
approaches often fail to capture the full thermodynamic landscape as fitting a
model on the gradient may not fit the absolute differences between low-energy
conformational states. In this work, we incorporate a complementary energy
matching term into the loss function. We evaluate our framework on the
Chignolin protein using the CGSchNet model, systematically varying the weight
of the energy loss term. While energy matching did not yield statistically
significant improvements in accuracy, it revealed distinct tendencies in how
models generalize the free energy surface. Our results suggest future
opportunities to enhance coarse-grained modeling through improved energy
estimation techniques and multi-modal loss formulations.

</details>


### [42] [Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking](https://arxiv.org/abs/2509.14603)
*Xingchen Wang,Feijie Wu,Chenglin Miao,Tianchun Li,Haoyu Hu,Qiming Cao,Jing Gao,Lu Su*

Main category: cs.LG

TL;DR: PM-SFL is a privacy-preserving Split Federated Learning framework that uses probabilistic mask training instead of noise injection to protect against data reconstruction attacks while maintaining model performance, with additional features for handling data and system heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning reduces client computation but introduces privacy risks from intermediate data exchange. Existing noise-based defenses degrade model performance, creating a need for better privacy-preserving solutions.

Method: Uses probabilistic mask training to add structured randomness without explicit noise, personalized mask learning for data heterogeneity, and layer-wise knowledge compensation for system heterogeneity with adaptive model splitting.

Result: Theoretical privacy protection confirmed, with experiments showing improved accuracy, communication efficiency, and robustness to privacy attacks, especially under data and system heterogeneity conditions.

Conclusion: PM-SFL provides an effective privacy-preserving SFL framework that maintains model utility while addressing both privacy risks and heterogeneity challenges through innovative mask training and compensation mechanisms.

Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.

</details>


### [43] [HD3C: Efficient Medical Data Classification for Embedded Devices](https://arxiv.org/abs/2509.14617)
*Jianglan Wei,Zhenyu Zhang,Pengcheng Wang,Mingjie Zeng,Zhigang Zeng*

Main category: cs.LG

TL;DR: HD3C is an energy-efficient hyperdimensional computing framework for medical data classification that achieves 350x better energy efficiency than Bayesian ResNet with minimal accuracy loss, while providing robustness to noise and limited data.


<details>
  <summary>Details</summary>
Motivation: Energy-efficient medical data classification is needed for home and field healthcare applications where embedded devices have limited power resources, but current deep learning models consume too much energy and require GPUs.

Method: HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace.

Result: HD3C achieves 350x better energy efficiency than Bayesian ResNet on heart sound classification with less than 1% accuracy difference, and demonstrates exceptional robustness to noise, limited training data, and hardware errors.

Conclusion: HD3C provides a lightweight, energy-efficient classification framework suitable for reliable deployment in real-world low-power medical applications, with both theoretical and empirical support for its robustness.

Abstract: Energy-efficient medical data classification is essential for modern disease
screening, particularly in home and field healthcare where embedded devices are
prevalent. While deep learning models achieve state-of-the-art accuracy, their
substantial energy consumption and reliance on GPUs limit deployment on such
platforms. We present Hyperdimensional Computing with Class-Wise Clustering
(HD3C), a lightweight classification framework designed for low-power
environments. HD3C encodes data into high-dimensional hypervectors, aggregates
them into multiple cluster-specific prototypes, and performs classification
through similarity search in hyperspace. We evaluate HD3C across three medical
classification tasks; on heart sound classification, HD3C is $350\times$ more
energy-efficient than Bayesian ResNet with less than 1% accuracy difference.
Moreover, HD3C demonstrates exceptional robustness to noise, limited training
data, and hardware error, supported by both theoretical analysis and empirical
results, highlighting its potential for reliable deployment in real-world
settings. Code is available at https://github.com/jianglanwei/HD3C.

</details>


### [44] [CUFG: Curriculum Unlearning Guided by the Forgetting Gradient](https://arxiv.org/abs/2509.14633)
*Jiaxing Miao,Liang Hu,Qi Zhang,Lai Zhong Yuan,Usman Naseem*

Main category: cs.LG

TL;DR: CUFG is a novel curriculum-based unlearning framework that uses forgetting gradients and progressive data scheduling to enable more stable and reliable machine unlearning compared to aggressive existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods prioritize efficiency and aggressive forgetting too much, causing model instability, weight collapse, and reduced reliability through radical interventions like gradient ascent and random label noise.

Method: Proposes CUFG framework with two innovations: 1) gradient corrector guided by forgetting gradients for fine-tuning-based unlearning, and 2) curriculum unlearning paradigm that progressively forgets from easy to hard data.

Result: Extensive experiments across various forgetting scenarios validate the approach, showing CUFG narrows the gap with gold-standard Retrain method and improves both effectiveness and reliability.

Conclusion: CUFG provides more stable and progressive unlearning, and the curriculum unlearning concept offers substantial research potential and forward-looking insights for machine unlearning field development.

Abstract: As privacy and security take center stage in AI, machine unlearning, the
ability to erase specific knowledge from models, has garnered increasing
attention. However, existing methods overly prioritize efficiency and
aggressive forgetting, which introduces notable limitations. In particular,
radical interventions like gradient ascent, influence functions, and random
label noise can destabilize model weights, leading to collapse and reduced
reliability. To address this, we propose CUFG (Curriculum Unlearning via
Forgetting Gradients), a novel framework that enhances the stability of
approximate unlearning through innovations in both forgetting mechanisms and
data scheduling strategies. Specifically, CUFG integrates a new gradient
corrector guided by forgetting gradients for fine-tuning-based unlearning and a
curriculum unlearning paradigm that progressively forgets from easy to hard.
These innovations narrow the gap with the gold-standard Retrain method by
enabling more stable and progressive unlearning, thereby improving both
effectiveness and reliability. Furthermore, we believe that the concept of
curriculum unlearning has substantial research potential and offers
forward-looking insights for the development of the MU field. Extensive
experiments across various forgetting scenarios validate the rationale and
effectiveness of our approach and CUFG. Codes are available at
https://anonymous.4open.science/r/CUFG-6375.

</details>


### [45] [DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers](https://arxiv.org/abs/2509.14640)
*Habib Irani,Vangelis Metsis*

Main category: cs.LG

TL;DR: DyWPE is a signal-aware positional encoding method that uses Discrete Wavelet Transform to generate embeddings directly from time series data, outperforming traditional signal-agnostic methods.


<details>
  <summary>Details</summary>
Motivation: Existing positional encoding methods ignore signal characteristics and are problematic for time series analysis with complex, non-stationary dynamics across multiple temporal scales.

Method: Dynamic Wavelet Positional Encoding (DyWPE) framework that generates positional embeddings directly from input time series using Discrete Wavelet Transform (DWT).

Result: Outperformed 8 state-of-the-art positional encoding methods across 10 diverse time series datasets, achieving 9.1% average improvement over baseline sinusoidal encoding in biomedical signals while maintaining computational efficiency.

Conclusion: Signal-aware positional encoding using wavelet transforms significantly improves performance in time series analysis compared to traditional signal-agnostic approaches.

Abstract: Existing positional encoding methods in transformers are fundamentally
signal-agnostic, deriving positional information solely from sequence indices
while ignoring the underlying signal characteristics. This limitation is
particularly problematic for time series analysis, where signals exhibit
complex, non-stationary dynamics across multiple temporal scales. We introduce
Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework
that generates positional embeddings directly from input time series using the
Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time
series datasets demonstrate that DyWPE consistently outperforms eight existing
state-of-the-art positional encoding methods, achieving average relative
improvements of 9.1\% compared to baseline sinusoidal absolute position
encoding in biomedical signals, while maintaining competitive computational
efficiency.

</details>


### [46] [DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training](https://arxiv.org/abs/2509.14642)
*Yuemin Wu,Zhongze Wu,Xiu Su,Feng Yang,Hongyan Xu,Xi Lin,Wenti Huang,Shan You,Chang Xu*

Main category: cs.LG

TL;DR: DeCoP is a novel time series pre-training framework that addresses dynamic temporal dependencies and distribution shifts through instance-wise patch normalization and hierarchical dependency controlled learning, achieving state-of-the-art results with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing time series pre-training models struggle with dynamic temporal dependencies caused by distribution shifts and multi-scale patterns, leading to poor generalization and susceptibility to spurious correlations in downstream tasks.

Method: DeCoP uses Instance-wise Patch Normalization (IPN) to handle distributional shifts while preserving patch characteristics, and a hierarchical Dependency Controlled Learning (DCL) strategy with Instance-level Contrastive Module (ICM) to model multi-scale inter-patch dependencies and learn instance-discriminative representations.

Result: DeCoP achieves state-of-the-art performance on ten datasets with 3% MSE improvement on ETTh1 over PatchTST while using only 37% of the FLOPs, demonstrating superior efficiency and effectiveness.

Conclusion: The proposed framework successfully addresses temporal variability challenges in time series pre-training through explicit modeling of dynamic multi-scale dependencies, offering improved generalization and computational efficiency.

Abstract: Modeling dynamic temporal dependencies is a critical challenge in time series
pre-training, which evolve due to distribution shifts and multi-scale patterns.
This temporal variability severely impairs the generalization of pre-trained
models to downstream tasks. Existing frameworks fail to capture the complex
interactions of short- and long-term dependencies, making them susceptible to
spurious correlations that degrade generalization. To address these
limitations, we propose DeCoP, a Dependency Controlled Pre-training framework
that explicitly models dynamic, multi-scale dependencies by simulating evolving
inter-patch dependencies. At the input level, DeCoP introduces Instance-wise
Patch Normalization (IPN) to mitigate distributional shifts while preserving
the unique characteristics of each patch, creating a robust foundation for
representation learning. At the latent level, a hierarchical Dependency
Controlled Learning (DCL) strategy explicitly models inter-patch dependencies
across multiple temporal scales, with an Instance-level Contrastive Module
(ICM) enhances global generalization by learning instance-discriminative
representations from time-invariant positive pairs. DeCoP achieves
state-of-the-art results on ten datasets with lower computing resources,
improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.

</details>


### [47] [Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](https://arxiv.org/abs/2509.14678)
*Hyungjoon Soh,Junghyo Jo*

Main category: cs.LG

TL;DR: A novel attention mechanism using learned nonnegative clocks for continuous sequences that enforces alignment continuity and monotonicity without external positional regularizers, improving stability and robustness in sequence-to-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Standard scaled dot-product attention lacks enforcement of continuity and monotonicity, which are crucial for frame-synchronous targets in sequence-to-sequence tasks.

Method: Proposed learned nonnegative clocks for source and target sequences, modeling attention as meeting probability of these clocks. Uses path-integral derivation to create Gaussian-like scoring with intrinsic bias toward causal, smooth, near-diagonal alignments.

Result: In Transformer text-to-speech testbed, produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines.

Conclusion: The clock-based attention framework provides effective drop-in replacements for both parallel and autoregressive decoding, with potential applicability to other continuous targets like video and temporal signal modeling.

Abstract: We formulate an attention mechanism for continuous and ordered sequences that
explicitly functions as an alignment model, which serves as the core of many
sequence-to-sequence tasks. Standard scaled dot-product attention relies on
positional encodings and masks but does not enforce continuity or monotonicity,
which are crucial for frame-synchronous targets. We propose learned nonnegative
\emph{clocks} to source and target and model attention as the meeting
probability of these clocks; a path-integral derivation yields a closed-form,
Gaussian-like scoring rule with an intrinsic bias toward causal, smooth,
near-diagonal alignments, without external positional regularizers. The
framework supports two complementary regimes: normalized clocks for parallel
decoding when a global length is available, and unnormalized clocks for
autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In
a Transformer text-to-speech testbed, this construction produces more stable
alignments and improved robustness to global time-scaling while matching or
improving accuracy over scaled dot-product baselines. We hypothesize
applicability to other continuous targets, including video and temporal signal
modeling.

</details>


### [48] [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang*

Main category: cs.LG

TL;DR: DSCL framework improves RL efficiency for LLM tool learning by dynamically sampling valuable data and adapting curriculum to focus on challenging sub-tasks, achieving 3.29% improvement on BFCLv3 benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic sampling techniques are inadequate for tool learning's multi-task structure and fine-grained reward mechanisms, leading to inefficient training with diminishing returns from simple samples.

Method: Two core components: 1) Reward-Based Dynamic Sampling using multi-dimensional reward statistics (mean/variance) to prioritize valuable data, and 2) Task-Based Dynamic Curriculum Learning that adaptively focuses training on less-mastered sub-tasks.

Result: Significantly improves training efficiency and model performance over strong baselines, achieving 3.29% improvement on the BFCLv3 benchmark.

Conclusion: DSCL provides a tailored solution that effectively leverages complex reward signals and sub-task dynamics within tool learning to achieve superior results.

Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool
learning, its efficiency is often hampered by an overabundance of simple
samples that provide diminishing learning value as training progresses.
Existing dynamic sampling techniques are ill-suited for the multi-task
structure and fine-grained reward mechanisms inherent to tool learning. This
paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework
specifically designed to address this challenge by targeting the unique
characteristics of tool learning: its multiple interdependent sub-tasks and
multi-valued reward functions. DSCL features two core components: Reward-Based
Dynamic Sampling, which uses multi-dimensional reward statistics (mean and
variance) to prioritize valuable data, and Task-Based Dynamic Curriculum
Learning, which adaptively focuses training on less-mastered sub-tasks. Through
extensive experiments, we demonstrate that DSCL significantly improves training
efficiency and model performance over strong baselines, achieving a 3.29\%
improvement on the BFCLv3 benchmark. Our method provides a tailored solution
that effectively leverages the complex reward signals and sub-task dynamics
within tool learning to achieve superior results.

</details>


### [49] [Towards Pre-trained Graph Condensation via Optimal Transport](https://arxiv.org/abs/2509.14722)
*Yeyu Yan,Shuai Zheng,Wenjun Hui,Xiangkai Zhu,Dong Chen,Zhenfeng Zhu,Yao Zhao,Kunlun He*

Main category: cs.LG

TL;DR: PreGC is a novel graph condensation method that uses optimal transport to create task-agnostic condensed graphs that work with any GNN architecture, overcoming limitations of traditional task-specific approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional graph condensation methods are heavily dependent on specific GNN architectures and task-specific supervision, which limits their reusability and generalization across different tasks and models.

Method: Proposes Pre-trained Graph Condensation (PreGC) using optimal transport with hybrid-interval graph diffusion augmentation to enhance generalization, and establishes matching between optimal graph transport plan and representation transport plan to maintain semantic consistency.

Result: Extensive experiments show PreGC achieves superior performance and versatility, demonstrating task-independent nature and seamless compatibility with arbitrary GNN architectures.

Conclusion: PreGC successfully transcends the limitations of traditional task- and architecture-dependent graph condensation methods, providing a generalized solution that maintains semantic consistency while being adaptable to various downstream tasks.

Abstract: Graph condensation (GC) aims to distill the original graph into a small-scale
graph, mitigating redundancy and accelerating GNN training. However,
conventional GC approaches heavily rely on rigid GNNs and task-specific
supervision. Such a dependency severely restricts their reusability and
generalization across various tasks and architectures. In this work, we revisit
the goal of ideal GC from the perspective of GNN optimization consistency, and
then a generalized GC optimization objective is derived, by which those
traditional GC methods can be viewed nicely as special cases of this
optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)
via optimal transport is proposed to transcend the limitations of task- and
architecture-dependent GC methods. Specifically, a hybrid-interval graph
diffusion augmentation is presented to suppress the weak generalization ability
of the condensed graph on particular architectures by enhancing the uncertainty
of node states. Meanwhile, the matching between optimal graph transport plan
and representation transport plan is tactfully established to maintain semantic
consistencies across source graph and condensed graph spaces, thereby freeing
graph condensation from task dependencies. To further facilitate the adaptation
of condensed graphs to various downstream tasks, a traceable semantic
harmonizer from source nodes to condensed nodes is proposed to bridge semantic
associations through the optimized representation transport plan in
pre-training. Extensive experiments verify the superiority and versatility of
PreGC, demonstrating its task-independent nature and seamless compatibility
with arbitrary GNNs.

</details>


### [50] [Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models](https://arxiv.org/abs/2509.14723)
*Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda*

Main category: cs.LG

TL;DR: Training a transcoder on cell2sentence model to extract interpretable decision circuits from single-cell foundation models, revealing biologically meaningful pathways.


<details>
  <summary>Details</summary>
Motivation: Single-cell foundation models lack interpretability compared to traditional methods, making their decision processes opaque despite superior performance.

Method: Train a transcoder on the cell2sentence (C2S) model to extract internal decision-making circuits from this state-of-the-art single-cell foundation model.

Result: The extracted circuits correspond to real-world biological mechanisms, demonstrating transcoders can uncover biologically plausible pathways within complex models.

Conclusion: Transcoders show promising potential for making single-cell foundation models more interpretable by revealing their internal biological decision pathways.

Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art
performance on various tasks, such as cell-type annotation and perturbation
response prediction, by learning gene regulatory networks from large-scale
transcriptome data. However, a significant challenge remains: the
decision-making processes of these models are less interpretable compared to
traditional methods like differential gene expression analysis. Recently,
transcoders have emerged as a promising approach for extracting interpretable
decision circuits from large language models (LLMs). In this work, we train a
transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By
leveraging the trained transcoder, we extract internal decision-making circuits
from the C2S model. We demonstrate that the discovered circuits correspond to
real-world biological mechanisms, confirming the potential of transcoders to
uncover biologically plausible pathways within complex single-cell models.

</details>


### [51] [One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning](https://arxiv.org/abs/2509.14724)
*Zhiyuan Xue,Ben Yang,Xuetao Zhang,Fei Wang,Zhiping Lin*

Main category: cs.LG

TL;DR: Proposes OMCAL, a one-step multi-view clustering method with adaptive low-rank anchor-graph learning to address redundancy and noise issues in existing anchor graph-based methods while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing anchor graph-based multi-view clustering methods suffer from redundant information and noise in consensus anchor graphs, and require independent post-processing that reduces both effectiveness and efficiency.

Method: Develops a nuclear norm-based adaptive consensus anchor graph learning model to handle information redundancy and noise, and integrates category indicator acquisition with anchor graph learning into a unified one-step framework.

Result: Extensive experiments on ordinary and large-scale datasets show that OMCAL outperforms state-of-the-art methods in both clustering effectiveness and efficiency.

Conclusion: OMCAL successfully addresses the limitations of existing AGMC methods by providing a unified framework that handles redundancy and noise while improving clustering performance and computational efficiency.

Abstract: In light of their capability to capture structural information while reducing
computing complexity, anchor graph-based multi-view clustering (AGMC) methods
have attracted considerable attention in large-scale clustering problems.
Nevertheless, existing AGMC methods still face the following two issues: 1)
They directly embedded diverse anchor graphs into a consensus anchor graph
(CAG), and hence ignore redundant information and numerous noises contained in
these anchor graphs, leading to a decrease in clustering effectiveness; 2) They
drop effectiveness and efficiency due to independent post-processing to acquire
clustering indicators. To overcome the aforementioned issues, we deliver a
novel one-step multi-view clustering method with adaptive low-rank anchor-graph
learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear
norm-based adaptive CAG learning model against information redundancy and noise
interference. Then, to boost clustering effectiveness and efficiency
substantially, we incorporate category indicator acquisition and CAG learning
into a unified framework. Numerous studies conducted on ordinary and
large-scale datasets indicate that OMCAL outperforms existing state-of-the-art
methods in terms of clustering effectiveness and efficiency.

</details>


### [52] [FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration](https://arxiv.org/abs/2509.14775)
*Shuangshuang He,Yuanting Zhang,Hongli Liang,Qingye Meng,Xingyuan Yuan*

Main category: cs.LG

TL;DR: FlowCast-ODE is a continuous flow modeling framework for accurate hourly weather forecasting that addresses error accumulation and temporal discontinuities in ERA5 data through dynamic flow matching and ODE solvers.


<details>
  <summary>Details</summary>
Motivation: Accurate hourly weather forecasting is critical but challenging due to error accumulation in autoregressive rollouts and temporal discontinuities in ERA5's 12-hour assimilation cycle.

Method: Models atmospheric state evolution as continuous flow using conditional flow paths, employs coarse-to-fine training with dynamic flow matching on 6-hour data, refines with ODE solver on hourly data, and uses lightweight low-rank AdaLN-Zero modulation to reduce model size.

Result: Outperforms baselines with lower RMSE, better energy conservation, reduced blurring, preserved fine-scale spatial details, comparable performance on extreme events like typhoons, and alleviates temporal discontinuities.

Conclusion: FlowCast-ODE provides an effective framework for stable and accurate hourly weather forecasting by modeling atmospheric dynamics as continuous flows with improved computational efficiency and reduced model size.

Abstract: Accurate hourly weather forecasting is critical for numerous applications.
Recent deep learning models have demonstrated strong capability on 6-hour
intervals, yet achieving accurate and stable hourly predictions remains a
critical challenge. This is primarily due to the rapid accumulation of errors
in autoregressive rollouts and temporal discontinuities within the ERA5 data's
12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a
framework that models atmospheric state evolution as a continuous flow.
FlowCast-ODE learns the conditional flow path directly from the previous state,
an approach that aligns more naturally with physical dynamic systems and
enables efficient computation. A coarse-to-fine strategy is introduced to train
the model on 6-hour data using dynamic flow matching and then refined on hourly
data that incorporates an Ordinary Differential Equation (ODE) solver to
achieve temporally coherent forecasts. In addition, a lightweight low-rank
AdaLN-Zero modulation mechanism is proposed and reduces model size by 15%
without compromising accuracy. Experiments demonstrate that FlowCast-ODE
outperforms strong baselines, yielding lower root mean square error (RMSE) and
better energy conservation, which reduces blurring and preserves more
fine-scale spatial details. It also shows comparable performance to the
state-of-the-art model in forecasting extreme events like typhoons.
Furthermore, the model alleviates temporal discontinuities associated with
assimilation cycle transitions.

</details>


### [53] [Pre-training under infinite compute](https://arxiv.org/abs/2509.14786)
*Konwoo Kim,Suhas Kotha,Percy Liang,Tatsunori Hashimoto*

Main category: cs.LG

TL;DR: This paper shows that with proper regularization and ensembling techniques, language models can achieve significantly better data efficiency during pre-training, achieving up to 17.5x data efficiency improvements over baseline approaches.


<details>
  <summary>Details</summary>
Motivation: As compute resources grow faster than available web text for language model pre-training, the research addresses how to optimize pre-training under fixed data constraints with unlimited compute resources.

Method: The authors developed a regularized training recipe with much larger weight decay (30x standard), parameter scaling, ensemble scaling, and knowledge distillation techniques to improve data efficiency while preventing overfitting.

Result: The approach achieved 5.17x less data usage at 200M tokens, 83% retention of ensemble benefits in distilled models 8x smaller, and 17.5x data efficiency improvement on math tasks compared to continued pre-training.

Conclusion: Simple algorithmic improvements like proper regularization and ensembling can enable significantly more data-efficient pre-training, making better use of limited data resources in compute-rich environments.

Abstract: Since compute grows much faster than web text available for language model
pre-training, we ask how one should approach pre-training under fixed data and
no compute constraints. We first show that existing data-constrained approaches
of increasing epoch count and parameter count eventually overfit, and we
significantly improve upon such recipes by properly tuning regularization,
finding that the optimal weight decay is $30\times$ larger than standard
practice. Since our regularized recipe monotonically decreases loss following a
simple power law in parameter count, we estimate its best possible performance
via the asymptote of its scaling law rather than the performance at a fixed
compute budget. We then identify that ensembling independently trained models
achieves a significantly lower loss asymptote than the regularized recipe. Our
best intervention combining epoching, regularization, parameter scaling, and
ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less
data than our baseline, and our data scaling laws predict that this improvement
persists at higher token budgets. We find that our data efficiency gains can be
realized at much smaller parameter counts as we can distill an ensemble into a
student model that is 8$\times$ smaller and retains $83\%$ of the ensembling
benefit. Finally, our interventions designed for validation loss generalize to
downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and
a $17.5\times$ data efficiency improvement over continued pre-training on math
mid-training data. Our results show that simple algorithmic improvements can
enable significantly more data-efficient pre-training in a compute-rich future.

</details>


### [54] [Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery](https://arxiv.org/abs/2509.14788)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.LG

TL;DR: A sequence-based drug-target interaction framework that integrates structural priors while maintaining high-throughput screening capability, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability but often lack structural awareness.

Method: Sequence-based DTI framework that integrates structural priors into protein representations, using learned aggregation, bilinear attention, and contrastive alignment techniques.

Result: Achieves state-of-the-art performance on Human and BioSNAP datasets, remains competitive on BindingDB, and surpasses prior methods in virtual screening tasks on LIT-PCBA with substantial gains in AUROC and BEDROC metrics.

Conclusion: The framework demonstrates utility for scalable and structure-aware DTI prediction, with embedding visualizations confirming improved spatial correspondence with known binding pockets and interpretable attention patterns.

Abstract: Accurate identification of drug-target interactions (DTI) remains a central
challenge in computational pharmacology, where sequence-based methods offer
scalability. This work introduces a sequence-based drug-target interaction
framework that integrates structural priors into protein representations while
maintaining high-throughput screening capability. Evaluated across multiple
benchmarks, the model achieves state-of-the-art performance on Human and
BioSNAP datasets and remains competitive on BindingDB. In virtual screening
tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in
AUROC and BEDROC. Ablation studies confirm the critical role of learned
aggregation, bilinear attention, and contrastive alignment in enhancing
predictive robustness. Embedding visualizations reveal improved spatial
correspondence with known binding pockets and highlight interpretable attention
patterns over ligand-residue contacts. These results validate the framework's
utility for scalable and structure-aware DTI prediction.

</details>


### [55] [STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models](https://arxiv.org/abs/2509.14801)
*Julian F. Schumann,Anna Mészáros,Jens Kober,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: STEP is a new benchmarking framework for trajectory prediction models that addresses limitations of existing frameworks by providing unified dataset interfaces, consistent evaluation conditions, and support for diverse prediction models including joint modeling approaches.


<details>
  <summary>Details</summary>
Motivation: Standardized evaluation practices for trajectory prediction models are underdeveloped, with existing frameworks lacking support for heterogeneous traffic scenarios, joint prediction models, and proper user documentation, making fair comparisons difficult.

Method: The authors introduce STEP framework that provides a unified interface for multiple datasets, enforces consistent training and evaluation conditions, and supports a wide range of prediction models including joint modeling approaches.

Result: Experiments reveal limitations of current testing procedures, demonstrate the importance of joint agent modeling for interaction prediction, and show vulnerability of state-of-the-art models to distribution shifts and adversarial attacks.

Conclusion: STEP aims to shift focus from leaderboard rankings to deeper insights about model behavior and generalization in complex multi-agent settings, providing a more comprehensive evaluation framework for trajectory prediction research.

Abstract: While trajectory prediction plays a critical role in enabling safe and
effective path-planning in automated vehicles, standardized practices for
evaluating such models remain underdeveloped. Recent efforts have aimed to
unify dataset formats and model interfaces for easier comparisons, yet existing
frameworks often fall short in supporting heterogeneous traffic scenarios,
joint prediction models, or user documentation. In this work, we introduce STEP
-- a new benchmarking framework that addresses these limitations by providing a
unified interface for multiple datasets, enforcing consistent training and
evaluation conditions, and supporting a wide range of prediction models. We
demonstrate the capabilities of STEP in a number of experiments which reveal 1)
the limitations of widely-used testing procedures, 2) the importance of joint
modeling of agents for better predictions of interactions, and 3) the
vulnerability of current state-of-the-art models against both distribution
shifts and targeted attacks by adversarial agents. With STEP, we aim to shift
the focus from the ``leaderboard'' approach to deeper insights about model
behavior and generalization in complex multi-agent settings.

</details>


### [56] [Precision Neural Networks: Joint Graph And Relational Learning](https://arxiv.org/abs/2509.14821)
*Andrea Cavallo,Samuel Rey,Antonio G. Marques,Elvin Isufi*

Main category: cs.LG

TL;DR: Precision Neural Networks (PNNs) extend VNNs by using precision matrices instead of covariance matrices, enabling joint learning of network parameters and precision estimation for better task-aware performance.


<details>
  <summary>Details</summary>
Motivation: Covariance matrices are dense, don't encode conditional independence, and are often precomputed in a task-agnostic way, which limits performance of VNNs.

Method: Formulate joint optimization problem for network parameters and precision matrix, solve via alternating optimization with sequential updates of weights and precision estimates.

Result: Theoretical bounds on precision matrix estimation error and demonstrated effectiveness over two-step approaches on synthetic and real-world data.

Conclusion: PNNs provide improved performance by leveraging precision matrices' sparsity, independence encoding, and joint task-aware estimation.

Abstract: CoVariance Neural Networks (VNNs) perform convolutions on the graph
determined by the covariance matrix of the data, which enables expressive and
stable covariance-based learning. However, covariance matrices are typically
dense, fail to encode conditional independence, and are often precomputed in a
task-agnostic way, which may hinder performance. To overcome these limitations,
we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix
-- the inverse covariance. The precision matrix naturally encodes statistical
independence, often exhibits sparsity, and preserves the covariance spectral
structure. To make precision estimation task-aware, we formulate an
optimization problem that jointly learns the network parameters and the
precision matrix, and solve it via alternating optimization, by sequentially
updating the network weights and the precision estimate. We theoretically bound
the distance between the estimated and true precision matrices at each
iteration, and demonstrate the effectiveness of joint estimation compared to
two-step approaches on synthetic and real-world data.

</details>


### [57] [Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization](https://arxiv.org/abs/2509.14832)
*Stelios Zarifis,Ioannis Kordonis,Petros Maragos*

Main category: cs.LG

TL;DR: DST is a diffusion-based framework for creating scenario trees that enables better stochastic optimization in energy markets by handling uncertainty more effectively than conventional methods.


<details>
  <summary>Details</summary>
Motivation: Stochastic forecasting is essential for decision-making in uncertain systems like energy markets, where full distribution estimation of future scenarios is needed for optimal performance.

Method: Proposes Diffusion Scenario Tree (DST) framework that uses diffusion models to recursively sample future trajectories and organizes them into trees via clustering while maintaining non-anticipativity.

Result: DST consistently outperforms conventional scenario tree models and Model-Free Reinforcement Learning baselines in energy arbitrage optimization tasks in New York's electricity market.

Conclusion: Using DST for stochastic optimization yields more efficient decision policies with higher performance by better handling uncertainty compared to deterministic and stochastic MPC variants using the same diffusion forecaster.

Abstract: Stochastic forecasting is critical for efficient decision-making in uncertain
systems, such as energy markets and finance, where estimating the full
distribution of future scenarios is essential. We propose Diffusion Scenario
Tree (DST), a general framework for constructing scenario trees for
multivariate prediction tasks using diffusion-based probabilistic forecasting
models. DST recursively samples future trajectories and organizes them into a
tree via clustering, ensuring non-anticipativity (decisions depending only on
observed history) at each stage. We evaluate the framework on the optimization
task of energy arbitrage in New York State's day-ahead electricity market.
Experimental results show that our approach consistently outperforms the same
optimization algorithms that use scenario trees from more conventional models
and Model-Free Reinforcement Learning baselines. Furthermore, using DST for
stochastic optimization yields more efficient decision policies, achieving
higher performance by better handling uncertainty than deterministic and
stochastic MPC variants using the same diffusion-based forecaster.

</details>


### [58] [Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study](https://arxiv.org/abs/2509.14863)
*Zhengwei Wang,Gang Wu*

Main category: cs.LG

TL;DR: G2LFormer is a Graph Transformer that reverses traditional attention schemes by using global attention in shallow layers and local GNN modules in deeper layers, preventing local information loss while maintaining linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Transformers integrate GNNs with global attention in ways that may dilute local neighborhood information learned by GNNs, causing information loss.

Method: Proposes global-to-local attention scheme: shallow layers use attention for global information, deeper layers use GNNs for local structure, with cross-layer fusion to retain beneficial global information.

Result: G2LFormer shows excellent performance on node-level and graph-level tasks while maintaining linear complexity, outperforming state-of-the-art linear GTs and GNNs.

Conclusion: The global-to-local attention scheme effectively prevents nodes from ignoring immediate neighbors and alleviates information loss with acceptable scalability trade-offs.

Abstract: Graph Transformers (GTs) show considerable potential in graph representation
learning. The architecture of GTs typically integrates Graph Neural Networks
(GNNs) with global attention mechanisms either in parallel or as a precursor to
attention mechanisms, yielding a local-and-global or local-to-global attention
scheme. However, as the global attention mechanism primarily captures
long-range dependencies between nodes, these integration schemes may suffer
from information loss, where the local neighborhood information learned by GNN
could be diluted by the attention mechanism. Therefore, we propose G2LFormer,
featuring a novel global-to-local attention scheme where the shallow network
layers use attention mechanisms to capture global information, while the deeper
layers employ GNN modules to learn local structural information, thereby
preventing nodes from ignoring their immediate neighbors. An effective
cross-layer information fusion strategy is introduced to allow local layers to
retain beneficial information from global layers and alleviate information
loss, with acceptable trade-offs in scalability. To validate the feasibility of
the global-to-local attention scheme, we compare G2LFormer with
state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The
results indicate that G2LFormer exhibits excellent performance while keeping
linear complexity.

</details>


### [59] [Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization](https://arxiv.org/abs/2509.14848)
*Houssem Sifaou,Osvaldo Simeone*

Main category: cs.LG

TL;DR: MF-HRL-IGM is a multi-fidelity hybrid RL algorithm that optimizes policy training by selecting simulation fidelity levels based on information gain maximization, achieving no-regret performance under fixed cost constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional RL requires expensive high-fidelity simulator interactions, while offline RL is limited by dataset quality. Real-world scenarios often have multiple simulators with varying fidelity and cost, but existing methods don't effectively leverage this multi-fidelity setup.

Method: Proposes MF-HRL-IGM algorithm that uses information gain maximization through bootstrapping to select appropriate fidelity levels for hybrid offline-online RL training under fixed cost budget.

Result: Theoretical analysis proves the algorithm has no-regret property. Empirical evaluations show superior performance compared to existing benchmarks.

Conclusion: MF-HRL-IGM effectively leverages multiple simulators with varying fidelity levels to optimize RL policy training while respecting cost constraints, outperforming traditional approaches.

Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive
interactions with a high-fidelity simulator of the environment, which are often
costly or impractical. Offline RL addresses this problem by allowing training
from pre-collected data, but its effectiveness is strongly constrained by the
size and quality of the dataset. Hybrid offline-online RL leverages both
offline data and interactions with a single simulator of the environment. In
many real-world scenarios, however, multiple simulators with varying levels of
fidelity and computational cost are available. In this work, we study
multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We
introduce multi-fidelity hybrid RL via information gain maximization
(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity
selection based on information gain maximization through a bootstrapping
approach. Theoretical analysis establishes the no-regret property of
MF-HRL-IGM, while empirical evaluations demonstrate its superior performance
compared to existing benchmarks.

</details>


### [60] [DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.14868)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei*

Main category: cs.LG

TL;DR: Ablation studies confirm DPANet's dual-domain architecture and cross-attention fusion are essential for optimal performance


<details>
  <summary>Details</summary>
Motivation: To validate the importance of combining temporal and frequency domains through interactive fusion in DPANet architecture

Method: Conducted ablation studies comparing full DPANet against Temporal-Only and Frequency-Only variants, and tested without cross-attention fusion mechanism

Result: Full model consistently outperformed all variants; both single-domain versions underperformed significantly; removing cross-attention caused the most severe performance degradation

Conclusion: Fusion of heterogeneous temporal and frequency information is critical, with cross-attention mechanism being the most essential component

Abstract: We conducted rigorous ablation studies to validate DPANet's key components
(Table \ref{tab:ablation-study}). The full model consistently outperforms all
variants. To test our dual-domain hypothesis, we designed two specialized
versions: a Temporal-Only model (fusing two identical temporal pyramids) and a
Frequency-Only model (fusing two spectral pyramids). Both variants
underperformed significantly, confirming that the fusion of heterogeneous
temporal and frequency information is critical. Furthermore, replacing the
cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the
most severe performance degradation. This result underscores that our
interactive fusion block is the most essential component.

</details>


### [61] [Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](https://arxiv.org/abs/2509.15024)
*Xuanting Xie,Bingheng Li,Erlin Pan,Rui Hou,Wenyu Chen,Zhao Kang*

Main category: cs.LG

TL;DR: AGCN is a novel graph clustering network that integrates attention mechanisms directly into graph structure to overcome limitations of both GNNs (over-localizing) and Transformers (over-globalizing), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current attention mechanisms underperform on graph data compared to GNNs. GNNs overemphasize neighborhood aggregation causing homogenization, while Transformers over-globalize and miss local patterns. The paper questions if attention is redundant for unsupervised graph learning.

Method: Proposes Attentive Graph Clustering Network (AGCN) that embeds attention directly into graph structure. Includes KV cache for computational efficiency and pairwise margin contrastive loss to enhance attention space discriminability. Theoretical analysis contrasts AGCN with GNN and Transformer behaviors.

Result: Extensive experiments show AGCN outperforms state-of-the-art methods in graph clustering tasks.

Conclusion: Attention is not redundant for graph learning when properly integrated. AGCN successfully combines the strengths of both GNNs and Transformers while mitigating their weaknesses, demonstrating effective global information extraction with local topological sensitivity.

Abstract: Attention mechanisms have become a cornerstone in modern neural networks,
driving breakthroughs across diverse domains. However, their application to
graph structured data, where capturing topological connections is essential,
remains underexplored and underperforming compared to Graph Neural Networks
(GNNs), particularly in the graph clustering task. GNN tends to overemphasize
neighborhood aggregation, leading to a homogenization of node representations.
Conversely, Transformer tends to over globalize, highlighting distant nodes at
the expense of meaningful local patterns. This dichotomy raises a key question:
Is attention inherently redundant for unsupervised graph learning? To address
this, we conduct a comprehensive empirical analysis, uncovering the
complementary weaknesses of GNN and Transformer in graph clustering. Motivated
by these insights, we propose the Attentive Graph Clustering Network (AGCN) a
novel architecture that reinterprets the notion that graph is attention. AGCN
directly embeds the attention mechanism into the graph structure, enabling
effective global information extraction while maintaining sensitivity to local
topological cues. Our framework incorporates theoretical analysis to contrast
AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV
cache mechanism to improve computational efficiency, and (2) a pairwise margin
contrastive loss to boost the discriminative capacity of the attention space.
Extensive experimental results demonstrate that AGCN outperforms
state-of-the-art methods.

</details>


### [62] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: DEER is a novel experience replay method that uses environmental discrepancy detection and adaptive prioritization to improve reinforcement learning in non-stationary environments, achieving 11.54% performance improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional experience replay methods struggle in non-stationary environments because they cannot distinguish between policy-induced changes and actual environmental shifts, leading to inefficient learning when dynamics and rewards change over time.

Method: Proposes Discrepancy of Environment Dynamics (DoE) metric to isolate environmental shift effects, and DEER framework that uses a binary classifier to detect environment changes and applies different prioritization strategies before/after each shift.

Result: Experiments on four non-stationary benchmarks show DEER improves performance of off-policy algorithms by 11.54% compared to the best state-of-the-art experience replay methods.

Conclusion: DEER successfully addresses the challenge of non-stationary environments by adaptively prioritizing experiences based on detected environmental changes, enabling more sample-efficient reinforcement learning.

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [63] [Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis](https://arxiv.org/abs/2509.14887)
*Hoang-Son Nguyen,Hoi-To Wai*

Main category: cs.LG

TL;DR: Vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals, recovering ground truth graph topology even with hidden nodes.


<details>
  <summary>Details</summary>
Motivation: Existing graph learning methods are vulnerable to corruption from hidden nodes whose signals are unobservable, but robustness analysis of naive approaches is underexplored.

Method: Extend the restricted isometry property (RIP) to Dirichlet energy function used in graph learning objectives, showing smoothness-based formulations like GL-SigRep can recover ground truth topology from partial observations.

Result: Theoretical analysis demonstrates implicit robustness, and synthetic/real data experiments corroborate that vanilla methods can recover correct graph topology for observed nodes despite hidden nodes.

Conclusion: Vanilla graph learning approaches possess inherent robustness to partial observations of low-pass filtered signals, providing theoretical foundation for their effectiveness even with unobserved hidden nodes.

Abstract: Learning the graph underlying a networked system from nodal signals is
crucial to downstream tasks in graph signal processing and machine learning.
The presence of hidden nodes whose signals are not observable might corrupt the
estimated graph. While existing works proposed various robustifications of
vanilla graph learning objectives by explicitly accounting for the presence of
these hidden nodes, a robustness analysis of "naive", hidden-node agnostic
approaches is still underexplored. This work demonstrates that vanilla graph
topology learning methods are implicitly robust to partial observations of
low-pass filtered graph signals. We achieve this theoretical result through
extending the restricted isometry property (RIP) to the Dirichlet energy
function used in graph learning objectives. We show that smoothness-based graph
learning formulation (e.g., the GL-SigRep method) on partial observations can
recover the ground truth graph topology corresponding to the observed nodes.
Synthetic and real data experiments corroborate our findings.

</details>


### [64] [From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets](https://arxiv.org/abs/2509.15040)
*Juwon Kim,Hyunwook Lee,Hyotaek Jeon,Seungmin Jin,Sungahn Ko*

Main category: cs.LG

TL;DR: A two-stage framework combining unsupervised pattern extraction with interpretable forecasting for financial directional prediction, achieving top performance while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between accurate but opaque deep learning models and interpretable but limited traditional pattern-based approaches in financial forecasting.

Method: Two-stage approach: (1) SIMPC for segmenting/clustering multivariate time series to extract amplitude and time-invariant patterns, (2) JISC-Net shapelet-based classifier using pattern beginnings to forecast short-term directional movements.

Result: Ranked first or second in 11 out of 12 metric-dataset combinations on Bitcoin and S&P 500 equities, consistently outperforming baselines.

Conclusion: The framework provides both high accuracy and interpretability by revealing underlying pattern structures that drive predictions, enabling transparent decision-making in financial markets.

Abstract: Directional forecasting in financial markets requires both accuracy and
interpretability. Before the advent of deep learning, interpretable approaches
based on human-defined patterns were prevalent, but their structural vagueness
and scale ambiguity hindered generalization. In contrast, deep learning models
can effectively capture complex dynamics, yet often offer limited transparency.
To bridge this gap, we propose a two-stage framework that integrates
unsupervised pattern extracion with interpretable forecasting. (i) SIMPC
segments and clusters multivariate time series, extracting recurrent patterns
that are invariant to amplitude scaling and temporal distortion, even under
varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses
the initial part of extracted patterns as input and forecasts subsequent
partial sequences for short-term directional movement. Experiments on Bitcoin
and three S&P 500 equities demonstrate that our method ranks first or second in
11 out of 12 metric--dataset combinations, consistently outperforming
baselines. Unlike conventional deep learning models that output buy-or-sell
signals without interpretable justification, our approach enables transparent
decision-making by revealing the underlying pattern structures that drive
predictive outcomes.

</details>


### [65] [Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics](https://arxiv.org/abs/2509.14894)
*Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin*

Main category: cs.LG

TL;DR: A novel RL-GA hybrid approach to systematically identify critical background processes in beauty hadron decay measurements, addressing sparse rewards and large search spaces.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in beauty hadron decay analysis where traditional methods rely on physicist intuition and computational limitations restrict background simulation to only the most relevant processes.

Method: Combines Reinforcement Learning with Genetic Algorithms, using GAs to explore large trajectory spaces and identify successful paths, then guides RL agent training with transformer architecture to handle token sequences representing decays.

Result: Developed a systematic method to determine critical background processes that was previously unavailable, with broad applicability beyond beauty hadron physics to other particle physics measurements.

Conclusion: The RL-GA hybrid approach successfully addresses sparse reward environments and large search spaces, providing a systematic solution for background identification in particle physics that reduces reliance on expert intuition.

Abstract: Experimental studies of beauty hadron decays face significant challenges due
to a wide range of backgrounds arising from the numerous possible decay
channels with similar final states. For a particular signal decay, the process
for ascertaining the most relevant background processes necessitates a detailed
analysis of final state particles, potential misidentifications, and kinematic
overlaps, which, due to computational limitations, is restricted to the
simulation of only the most relevant backgrounds. Moreover, this process
typically relies on the physicist's intuition and expertise, as no systematic
method exists.
  This paper has two primary goals. First, from a particle physics perspective,
we present a novel approach that utilises Reinforcement Learning (RL) to
overcome the aforementioned challenges by systematically determining the
critical backgrounds affecting beauty hadron decay measurements. While beauty
hadron physics serves as the case study in this work, the proposed strategy is
broadly adaptable to other types of particle physics measurements. Second, from
a Machine Learning perspective, we introduce a novel algorithm which exploits
the synergy between RL and Genetic Algorithms (GAs) for environments with
highly sparse rewards and a large trajectory space. This strategy leverages GAs
to efficiently explore the trajectory space and identify successful
trajectories, which are used to guide the RL agent's training. Our method also
incorporates a transformer architecture for the RL agent to handle token
sequences representing decays.

</details>


### [66] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: Hybrid training combining offline imitation learning with online RL for 2D shooter game agents, using multi-head neural network with attention mechanisms to address sparse rewards and training instability.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning agents in complex game environments suffer from sparse rewards, training instability, and poor sample efficiency, making pure exploration insufficient.

Method: Multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Starts with behavioral cloning on demonstration data, then transitions to reinforcement learning.

Result: Achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure RL methods which showed high variance and frequent performance degradation.

Conclusion: Combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments.

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [67] [Robust Barycenters of Persistence Diagrams](https://arxiv.org/abs/2509.14904)
*Keanu Sisouk,Eloi Tanguy,Julie Delon,Julien Tierny*

Main category: cs.LG

TL;DR: General approach for computing robust Wasserstein barycenters of persistence diagrams using fixed-point method for q>1 transportation costs, particularly robust q∈(1,2) cases.


<details>
  <summary>Details</summary>
Motivation: Classical methods only work for q=2 Wasserstein distance, limiting robustness to outliers. Need for generalized framework that handles q>1 transportation costs for outlier-resistant barycenter computation.

Method: Adapted fixed-point method to compute barycenter diagrams for generic transportation costs (q>1), particularly focusing on robust q∈(1,2) cases that are resistant to outliers.

Result: Successfully implemented robust barycenter computation for persistence diagrams. Demonstrated utility in clustering persistence diagrams and dictionary encoding, showing improved robustness to outliers compared to classical q=2 methods.

Conclusion: The generalized framework provides robust Wasserstein barycenter computation for persistence diagrams with q>1 transportation costs, offering significant outlier resistance particularly for q∈(1,2), with practical applications in clustering and dictionary encoding tasks.

Abstract: This short paper presents a general approach for computing robust Wasserstein
barycenters of persistence diagrams. The classical method consists in computing
assignment arithmetic means after finding the optimal transport plans between
the barycenter and the persistence diagrams. However, this procedure only works
for the transportation cost related to the $q$-Wasserstein distance $W_q$ when
$q=2$. We adapt an alternative fixed-point method to compute a barycenter
diagram for generic transportation costs ($q > 1$), in particular those robust
to outliers, $q \in (1,2)$. We show the utility of our work in two
applications: \emph{(i)} the clustering of persistence diagrams on their metric
space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both
scenarios, we demonstrate the added robustness to outliers provided by our
generalized framework. Our Python implementation is available at this address:
https://github.com/Keanu-Sisouk/RobustBarycenter .

</details>


### [68] [Credit Card Fraud Detection](https://arxiv.org/abs/2509.15044)
*Iva Popova,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: Evaluation of 5 ML models for credit card fraud detection using undersampling, SMOTE, and hybrid approaches on imbalanced data, with hybrid method achieving best recall-precision balance.


<details>
  <summary>Details</summary>
Motivation: Credit card fraud detection faces challenges due to class imbalance and fraudsters mimicking legitimate behavior, requiring effective ML approaches.

Method: Tested Logistic Regression, Random Forest, XGBoost, KNN, and MLP on real-world dataset using undersampling, SMOTE, and hybrid approach, evaluated on original imbalanced test set.

Result: Hybrid method achieved the best balance between recall and precision, particularly improving performance of MLP and KNN models.

Conclusion: Hybrid sampling approach shows superior performance in handling class imbalance for credit card fraud detection compared to individual sampling methods.

Abstract: Credit card fraud remains a significant challenge due to class imbalance and
fraudsters mimicking legitimate behavior. This study evaluates five machine
learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest
Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using
undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the
original imbalanced test set to better reflect real-world performance. Results
show that the hybrid method achieves the best balance between recall and
precision, especially improving MLP and KNN performance.

</details>


### [69] [Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation](https://arxiv.org/abs/2509.14925)
*Konrad Nowosadko,Franco Ruggeri,Ahmad Terra*

Main category: cs.LG

TL;DR: Proposes using Self-Explaining Neural Networks (SENNs) to make deep reinforcement learning more interpretable while maintaining competitive performance on low-dimensional tasks like mobile network resource allocation.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning methods lack transparency and interpretability, which reduces trustworthiness in critical domains. The black-box nature of DNNs hinders understanding of model behavior.

Method: Uses Self-Explaining Neural Networks (SENNs) with explanation extraction methods to enhance interpretability while preserving predictive accuracy. Focuses on low-dimensionality problems to generate robust local and global explanations.

Result: The approach demonstrates competitive performance on par with state-of-the-art methods while providing robust explanations. Successfully evaluated on mobile network resource allocation problems.

Conclusion: SENNs show potential to improve transparency and trust in AI-driven decision-making for low-dimensional tasks, offering interpretable solutions without sacrificing performance.

Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks
(DNN), though powerful, often lack transparency. Their black-box characteristic
hinders interpretability and reduces trustworthiness, particularly in critical
domains. To address this challenge in RL tasks, we propose a solution based on
Self-Explaining Neural Networks (SENNs) along with explanation extraction
methods to enhance interpretability while maintaining predictive accuracy. Our
approach targets low-dimensionality problems to generate robust local and
global explanations of the model's behaviour. We evaluate the proposed method
on the resource allocation problem in mobile networks, demonstrating that SENNs
can constitute interpretable solutions with competitive performance. This work
highlights the potential of SENNs to improve transparency and trust in
AI-driven decision-making for low-dimensional tasks. Our approach strong
performance on par with the existing state-of-the-art methods, while providing
robust explanations.

</details>


### [70] [Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning](https://arxiv.org/abs/2509.15057)
*Quincy Hershey,Randy Paffenroth*

Main category: cs.LG

TL;DR: Novel hyperparameters for sparse RNNs with varying sparsity levels, introducing hidden proportion metric that balances unknowns and predicts performance, enabling meta-learning applications.


<details>
  <summary>Details</summary>
Motivation: To develop improved sparse RNN architectures with better performance predictability and explanatory power for model optimization based on dataset characteristics.

Method: Developed alternative hyperparameters for specifying sparse RNNs with varying sparsity in weight matrices, and created a hidden proportion metric to balance unknown distribution.

Result: Achieved significant performance gains with improved a priori performance expectations through the combined sparse RNN architecture and hidden proportion metric.

Conclusion: The approach provides a path toward generalized meta-learning applications and model optimization based on intrinsic dataset characteristics like input/output dimensions.

Abstract: This paper develops alternative hyperparameters for specifying sparse
Recurrent Neural Networks (RNNs). These hyperparameters allow for varying
sparsity within the trainable weight matrices of the model while improving
overall performance. This architecture enables the definition of a novel
metric, hidden proportion, which seeks to balance the distribution of unknowns
within the model and provides significant explanatory power of model
performance. Together, the use of the varied sparsity RNN architecture combined
with the hidden proportion metric generates significant performance gains while
improving performance expectations on an a priori basis. This combined approach
provides a path forward towards generalized meta-learning applications and
model optimization based on intrinsic characteristics of the data set,
including input and output dimensions.

</details>


### [71] [DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2509.14933)
*Xiangfei Qiu,Yuhan Zhu,Zhengyu Li,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: Proposes DAG framework with dual causal networks (temporal and channel dimensions) to better leverage exogenous variables, especially future ones, for improved time series forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for time series forecasting with exogenous variables (TSF-X) fail to leverage future exogenous variables and ignore causal relationships between endogenous and exogenous variables, leading to suboptimal performance.

Method: Dual causal network framework with: 1) Temporal Causal Module - causal discovery of historical exogenous variables affecting future exogenous variables, and causal injection for forecasting; 2) Channel Causal Module - causal discovery of historical exogenous variables influencing historical endogenous variables, and causal injection using future exogenous variables.

Result: The proposed DAG framework aims to improve forecasting accuracy by properly incorporating causal relationships and future exogenous variables.

Conclusion: The DAG framework provides a comprehensive approach to leverage exogenous variables through causal modeling in both temporal and channel dimensions, addressing limitations of existing TSF-X methods.

Abstract: Time series forecasting is crucial in various fields such as economics,
traffic, and AIOps. However, in real-world applications, focusing solely on the
endogenous variables (i.e., target variables), is often insufficient to ensure
accurate predictions. Considering exogenous variables (i.e., covariates)
provides additional predictive information, thereby improving forecasting
accuracy. However, existing methods for time series forecasting with exogenous
variables (TSF-X) have the following shortcomings: 1) they do not leverage
future exogenous variables, 2) they fail to account for the causal
relationships between endogenous and exogenous variables. As a result, their
performance is suboptimal. In this study, to better leverage exogenous
variables, especially future exogenous variable, we propose a general framework
DAG, which utilizes dual causal network along both the temporal and channel
dimensions for time series forecasting with exogenous variables. Specifically,
we first introduce the Temporal Causal Module, which includes a causal
discovery module to capture how historical exogenous variables affect future
exogenous variables. Following this, we construct a causal injection module
that incorporates the discovered causal relationships into the process of
forecasting future endogenous variables based on historical endogenous
variables. Next, we propose the Channel Causal Module, which follows a similar
design principle. It features a causal discovery module models how historical
exogenous variables influence historical endogenous variables, and a causal
injection module incorporates the discovered relationships to enhance the
prediction of future endogenous variables based on future exogenous variables.

</details>


### [72] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Simone Scardapane*

Main category: cs.LG

TL;DR: ADC framework reduces communication overhead in Split Learning for Vision Transformers by using two compression strategies: merging similar activations based on attention scores and discarding least meaningful tokens.


<details>
  <summary>Details</summary>
Motivation: To address the high communication overhead required for transmitting intermediate Vision Transformers activations during Split Learning training process.

Method: Proposes Attention-based Double Compression (ADC) with two parallel strategies: 1) class-agnostic merging of similar activations based on average attention scores, and 2) discarding least meaningful tokens to further reduce communication cost.

Result: ADC outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.

Conclusion: The combined compression strategies enable sending less data during forward pass and naturally compress gradients, allowing whole model training without additional tuning or gradient approximations.

Abstract: This paper proposes a novel communication-efficient Split Learning (SL)
framework, named Attention-based Double Compression (ADC), which reduces the
communication overhead required for transmitting intermediate Vision
Transformers activations during the SL training process. ADC incorporates two
parallel compression strategies. The first one merges samples' activations that
are similar, based on the average attention score calculated in the last client
layer; this strategy is class-agnostic, meaning that it can also merge samples
having different classes, without losing generalization ability nor decreasing
final results. The second strategy follows the first and discards the least
meaningful tokens, further reducing the communication cost. Combining these
strategies not only allows for sending less during the forward pass, but also
the gradients are naturally compressed, allowing the whole model to be trained
without additional tuning or approximations of the gradients. Simulation
results demonstrate that Attention-based Double Compression outperforms
state-of-the-art SL frameworks by significantly reducing communication
overheads while maintaining high accuracy.

</details>


### [73] [A Comparative Analysis of Transformer Models in Social Bot Detection](https://arxiv.org/abs/2509.14936)
*Rohan Veit,Michael Lones*

Main category: cs.LG

TL;DR: Comparison of encoder vs decoder transformer models for bot detection, showing encoders are more accurate but decoders have better generalization potential.


<details>
  <summary>Details</summary>
Motivation: Social media bots using advanced text generation tools like LLMs are misleading users and manipulating online discussions, requiring effective detection methods.

Method: Developed evaluation pipelines to test performance of encoder-based and decoder-based transformer classifiers for bot detection.

Result: Encoder-based classifiers demonstrated greater accuracy and robustness, while decoder-based models showed better adaptability through task-specific alignment.

Conclusion: Both encoder and decoder approaches contribute to preventing digital manipulation, with encoders offering better accuracy and decoders providing superior generalization potential across different use cases.

Abstract: Social media has become a key medium of communication in today's society.
This realisation has led to many parties employing artificial users (or bots)
to mislead others into believing untruths or acting in a beneficial manner to
such parties. Sophisticated text generation tools, such as large language
models, have further exacerbated this issue. This paper aims to compare the
effectiveness of bot detection models based on encoder and decoder
transformers. Pipelines are developed to evaluate the performance of these
classifiers, revealing that encoder-based classifiers demonstrate greater
accuracy and robustness. However, decoder-based models showed greater
adaptability through task-specific alignment, suggesting more potential for
generalisation across different use cases in addition to superior observa.
These findings contribute to the ongoing effort to prevent digital environments
being manipulated while protecting the integrity of online discussion.

</details>


### [74] [Hierarchical Federated Learning for Social Network with Mobility](https://arxiv.org/abs/2509.14938)
*Zeyu Chen,Wen Chen,Jun Li,Qingqing Wu,Ming Ding,Xuefeng Han,Xiumei Deng,Liwei Wang*

Main category: cs.LG

TL;DR: Proposes HFL-SNM, a hierarchical federated learning framework that incorporates social networks and client mobility to optimize resource allocation and energy consumption while maintaining data privacy.


<details>
  <summary>Details</summary>
Motivation: Traditional FL frameworks assume absolute data privacy but neglect client mobility patterns, leading to inefficient resource usage and higher energy consumption during collaborative training.

Method: Developed HFL-SNM framework with concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. Formulated joint optimization problem for resource allocation and client scheduling, then decoupled into sub-problems solved by DO-SNM algorithm.

Result: Experimental results show superior model performance with significantly reduced energy consumption compared to traditional baseline algorithms.

Conclusion: The proposed HFL-SNM framework successfully addresses mobility-aware federated learning challenges, achieving better model performance while optimizing energy efficiency through social network-based resource allocation.

Abstract: Federated Learning (FL) offers a decentralized solution that allows
collaborative local model training and global aggregation, thereby protecting
data privacy. In conventional FL frameworks, data privacy is typically
preserved under the assumption that local data remains absolutely private,
whereas the mobility of clients is frequently neglected in explicit modeling.
In this paper, we propose a hierarchical federated learning framework based on
the social network with mobility namely HFL-SNM that considers both data
sharing among clients and their mobility patterns. Under the constraints of
limited resources, we formulate a joint optimization problem of resource
allocation and client scheduling, which objective is to minimize the energy
consumption of clients during the FL process. In social network, we introduce
the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.
We analyze the impact of effective data and redundant data on the model
performance through preliminary experiments. We decouple the optimization
problem into multiple sub-problems, analyze them based on preliminary
experimental results, and propose Dynamic Optimization in Social Network with
Mobility (DO-SNM) algorithm. Experimental results demonstrate that our
algorithm achieves superior model performance while significantly reducing
energy consumption, compared to traditional baseline algorithms.

</details>


### [75] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin*

Main category: cs.LG

TL;DR: FlowRL proposes distribution matching instead of reward maximization in LLM RL to improve diversity and avoid over-optimizing dominant rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional reward-maximizing methods like PPO and GRPO tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, reducing diversity in reasoning trajectories.

Method: Transform scalar rewards into normalized target distribution using learnable partition function, then minimize reverse KL divergence between policy and target distribution through flow-balanced optimization.

Result: Achieves 10.0% average improvement over GRPO and 5.1% over PPO on math benchmarks, with consistent better performance on code reasoning tasks.

Conclusion: Reward distribution-matching is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing
instead of maximizing rewards in large language model (LLM) reinforcement
learning (RL). Recent advanced reasoning models adopt reward-maximizing methods
(\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while
neglecting less frequent but valid reasoning paths, thus reducing diversity. In
contrast, we transform scalar rewards into a normalized target distribution
using a learnable partition function, and then minimize the reverse KL
divergence between the policy and the target distribution. We implement this
idea as a flow-balanced optimization method that promotes diverse exploration
and generalizable reasoning trajectories. We conduct experiments on math and
code reasoning tasks: FlowRL achieves a significant average improvement of
$10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs
consistently better on code reasoning tasks. These results highlight reward
distribution-matching as a key step toward efficient exploration and diverse
reasoning in LLM reinforcement learning.

</details>


### [76] [Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models](https://arxiv.org/abs/2509.14945)
*Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie*

Main category: cs.LG

TL;DR: Ensemble machine learning models effectively predict malnutrition among pregnant Ethiopian women with 97.87% accuracy using demographic and health survey data.


<details>
  <summary>Details</summary>
Motivation: Malnutrition among pregnant women in Ethiopia poses serious public health risks, and traditional statistical methods fail to capture the complex multidimensional determinants of nutritional status.

Method: Used ensemble ML techniques (XGBoost, Random Forest, CatBoost, AdaBoost) on Ethiopian DHS data (2005-2020, 18,108 records, 30 attributes) with preprocessing including SMOTE balancing and feature selection.

Result: Random Forest achieved best performance: 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC for classifying four nutritional categories.

Conclusion: Ensemble learning effectively captures hidden patterns in complex datasets, providing timely insights for early nutritional risk detection and supporting data-driven strategies for improving maternal health outcomes.

Abstract: Malnutrition among pregnant women is a major public health challenge in
Ethiopia, increasing the risk of adverse maternal and neonatal outcomes.
Traditional statistical approaches often fail to capture the complex and
multidimensional determinants of nutritional status. This study develops a
predictive model using ensemble machine learning techniques, leveraging data
from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108
records with 30 socio-demographic and health attributes. Data preprocessing
included handling missing values, normalization, and balancing with SMOTE,
followed by feature selection to identify key predictors. Several supervised
ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost
were applied to classify nutritional status. Among them, the Random Forest
model achieved the best performance, classifying women into four categories
(normal, moderate malnutrition, severe malnutrition, and overnutrition) with
97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%
ROC AUC. These findings demonstrate the effectiveness of ensemble learning in
capturing hidden patterns from complex datasets and provide timely insights for
early detection of nutritional risks. The results offer practical implications
for healthcare providers, policymakers, and researchers, supporting data-driven
strategies to improve maternal nutrition and health outcomes in Ethiopia.

</details>


### [77] [Stochastic Bilevel Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2509.14952)
*Zhuanghua Liu,Luo Luo*

Main category: cs.LG

TL;DR: Proposes N²SBA algorithm for stochastic bilevel optimization with heavy-tailed noise, achieving improved complexity bounds for finding ϵ-stationary points in nonconvex-strongly convex bilevel problems and nonconvex-strongly concave minimax problems.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of stochastic bilevel optimization with heavy-tailed noise, which is prevalent in machine learning applications like large language model training and reinforcement learning, where traditional methods assuming bounded variance may fail.

Method: Develops a nested-loop normalized stochastic bilevel approximation (N²SBA) algorithm that handles heavy-tailed noise by leveraging central moment assumptions rather than bounded variance, using stochastic first-order oracles with normalized gradient updates.

Result: Achieves SFO complexity of Ξ(κ⁷ᵈ⁻³/ᵈ⁻¹ σᵈ/ᵈ⁻¹ ϵ⁻⁴ᵈ⁻²/ᵈ⁻¹) for bilevel optimization and Ξ(κ²ᵈ⁻¹/ᵈ⁻¹ σᵈ/ᵈ⁻¹ ϵ⁻³ᵈ⁻²/ᵈ⁻¹) for minimax optimization, matching best-known results when p=2 (bounded variance case).

Conclusion: The proposed N²SBA algorithm provides an effective solution for stochastic bilevel and minimax optimization under heavy-tailed noise conditions, generalizing existing bounded variance results and demonstrating practical relevance for modern machine learning applications.

Abstract: This paper considers the smooth bilevel optimization in which the lower-level
problem is strongly convex and the upper-level problem is possibly nonconvex.
We focus on the stochastic setting that the algorithm can access the unbiased
stochastic gradient evaluation with heavy-tailed noise, which is prevalent in
many machine learning applications such as training large language models and
reinforcement learning. We propose a nested-loop normalized stochastic bilevel
approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the
stochastic first-order oracle (SFO) complexity of
$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number,
$p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the
noise level. Furthermore, we specialize our idea to solve the
nonconvex-strongly-concave minimax optimization problem, achieving an
$\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal
O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known
results under the special case of the bounded variance setting, i.e., $p=2$.

</details>


### [78] [FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference](https://arxiv.org/abs/2509.14968)
*Carlos Barroso-Fernández,Alejandro Calvillo-Fernandez,Antonio de la Oliva,Carlos J. Bernardos*

Main category: cs.LG

TL;DR: FAWN is a MultiEncoder Fusion-Attention Wave Network that fuses Wi-Fi and 5G signals for indoor scene inference using ISAC passive sensing, achieving sub-0.6m accuracy 84% of the time without interfering with communications.


<details>
  <summary>Details</summary>
Motivation: Current ISAC passive sensing solutions are limited to single technologies (Wi-Fi or 5G), constraining accuracy. Different technologies work with different spectrums, creating a need to integrate multiple technologies to augment coverage area and improve environmental perception.

Method: Proposed FAWN network based on transformers architecture with MultiEncoder Fusion-Attention to fuse information from both Wi-Fi and 5G signals for passive sensing. Built a prototype and tested in real scenarios.

Result: Achieved errors below 0.6 meters around 84% of the time in indoor scene inference tasks.

Conclusion: FAWN demonstrates that fusing multiple wireless technologies (Wi-Fi and 5G) through transformer-based architecture enables accurate indoor scene inference without interfering with existing communications, advancing ISAC passive sensing capabilities.

Abstract: The upcoming generations of wireless technologies promise an era where
everything is interconnected and intelligent. As the need for intelligence
grows, networks must learn to better understand the physical world. However,
deploying dedicated hardware to perceive the environment is not always
feasible, mainly due to costs and/or complexity. Integrated Sensing and
Communication (ISAC) has made a step forward in addressing this challenge.
Within ISAC, passive sensing emerges as a cost-effective solution that reuses
wireless communications to sense the environment, without interfering with
existing communications. Nevertheless, the majority of current solutions are
limited to one technology (mostly Wi-Fi or 5G), constraining the maximum
accuracy reachable. As different technologies work with different spectrums, we
see a necessity in integrating more than one technology to augment the coverage
area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a
MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.
FAWN is based on the original transformers architecture, to fuse information
from Wi-Fi and 5G, making the network capable of understanding the physical
world without interfering with the current communication. To test our solution,
we have built a prototype and integrated it in a real scenario. Results show
errors below 0.6 m around 84% of times.

</details>


### [79] [Stochastic Adaptive Gradient Descent Without Descent](https://arxiv.org/abs/2509.14969)
*Jean-François Aujol,Jérémie Bigot,Camille Castera*

Main category: cs.LG

TL;DR: New adaptive step-size strategy for stochastic convex optimization that requires no hyperparameter tuning and uses only first-order stochastic oracle information.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretically-grounded adaptive step-size method for stochastic gradient descent that automatically adapts to local geometry without requiring manual hyperparameter tuning.

Method: Adaptation of the Adaptive Gradient Descent Without Descent method to stochastic setting, using first-order stochastic oracle to exploit local geometry of objective function.

Result: Proven convergence under various assumptions and empirical demonstration of competitiveness against tuned baseline methods.

Conclusion: The proposed adaptive step-size strategy provides an effective, theoretically-sound approach for stochastic convex optimization that eliminates the need for hyperparameter tuning while maintaining competitive performance.

Abstract: We introduce a new adaptive step-size strategy for convex optimization with
stochastic gradient that exploits the local geometry of the objective function
only by means of a first-order stochastic oracle and without any
hyper-parameter tuning. The method comes from a theoretically-grounded
adaptation of the Adaptive Gradient Descent Without Descent method to the
stochastic setting. We prove the convergence of stochastic gradient descent
with our step-size under various assumptions, and we show that it empirically
competes against tuned baselines.

</details>


### [80] [Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection](https://arxiv.org/abs/2509.15033)
*Padmaksha Roy,Almuatazbellah Boker,Lamine Mili*

Main category: cs.LG

TL;DR: A novel approach for multivariate anomaly detection that models time-varying non-linear spatio-temporal correlations using transformer encoders, multivariate likelihood, and copula models trained with contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing approaches oversimplify real-world interactions by assuming time series variables are independent, missing the collective behavior patterns that indicate anomalies when no individual series shows clear abnormalities.

Method: Models joint dependencies in latent space by decoupling marginal distributions, temporal dynamics, and inter-variable dependencies. Uses transformer encoder for temporal patterns, multivariate likelihood and copula for spatial dependencies, trained jointly with self-supervised contrastive learning.

Result: The approach learns meaningful feature representations that effectively separate normal and anomaly samples by capturing complex spatio-temporal correlations.

Conclusion: The proposed method successfully addresses the limitations of independence assumptions in multivariate anomaly detection by modeling comprehensive spatio-temporal dependencies through a joint training framework.

Abstract: In this paper, we aim to improve multivariate anomaly detection (AD) by
modeling the \textit{time-varying non-linear spatio-temporal correlations}
found in multivariate time series data . In multivariate time series data, an
anomaly may be indicated by the simultaneous deviation of interrelated time
series from their expected collective behavior, even when no individual time
series exhibits a clearly abnormal pattern on its own. In many existing
approaches, time series variables are assumed to be (conditionally)
independent, which oversimplifies real-world interactions. Our approach
addresses this by modeling joint dependencies in the latent space and
decoupling the modeling of \textit{marginal distributions, temporal dynamics,
and inter-variable dependencies}. We use a transformer encoder to capture
temporal patterns, and to model spatial (inter-variable) dependencies, we fit a
multi-variate likelihood and a copula. The temporal and the spatial components
are trained jointly in a latent space using a self-supervised contrastive
learning objective to learn meaningful feature representations to separate
normal and anomaly samples.

</details>


### [81] [Probabilistic and nonlinear compressive sensing](https://arxiv.org/abs/2509.15060)
*Lukas Silvester Barth,Paulo von Petersenn*

Main category: cs.LG

TL;DR: A smooth probabilistic reformulation of ℓ0 regularized regression that enables exact gradient computation and rapid convergence, outperforming Monte Carlo approaches and compressive sensing algorithms like IHT and Lasso. Also explores nonlinear compressive sensing with theoretical and empirical analysis showing fundamental differences from linear cases.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of ℓ0 regularized regression by developing a method that avoids Monte Carlo sampling and enables exact gradient computation for faster convergence. Additionally, to investigate the feasibility of parameter recovery in nonlinear compressive sensing scenarios.

Method: Developed a smooth probabilistic reformulation of ℓ0 regularization that computes exact gradients without Monte Carlo sampling. For nonlinear compressive sensing, built upon Fefferman and Markel theorems and implemented a normal-form algorithm to select canonical representatives within symmetry classes for empirical validation.

Result: The method significantly improves convergence speed compared to Monte Carlo approaches and outperforms compressive sensing algorithms (IHT, Relaxed-Lasso) across various settings and SNR ratios. For nonlinear cases, compression improves test loss but exact parameter recovery is impossible even up to symmetries, with observed rebound effect where configurations initially converge then diverge.

Conclusion: The proposed probabilistic reformulation provides efficient ℓ0 regularization with exact gradients. Nonlinear compressive sensing fundamentally differs from linear cases - while compression can improve performance, exact parameter recovery is not achievable even considering symmetries, indicating inherent limitations in nonlinear settings.

Abstract: We present a smooth probabilistic reformulation of $\ell_0$ regularized
regression that does not require Monte Carlo sampling and allows for the
computation of exact gradients, facilitating rapid convergence to local optima
of the best subset selection problem. The method drastically improves
convergence speed compared to similar Monte Carlo based approaches.
Furthermore, we empirically demonstrate that it outperforms compressive sensing
algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and
signal-to-noise ratios. The implementation runs efficiently on both CPUs and
GPUs and is freely available at
https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive
sensing by investigating when parameter recovery of a nonlinear teacher network
is possible through compression of a student network. Building upon theorems of
Fefferman and Markel, we show theoretically that the global optimum in the
infinite-data limit enforces recovery up to certain symmetries. For empirical
validation, we implement a normal-form algorithm that selects a canonical
representative within each symmetry class. However, while compression can help
to improve test loss, we find that exact parameter recovery is not even
possible up to symmetries. In particular, we observe a surprising rebound
effect where teacher and student configurations initially converge but
subsequently diverge despite continuous decrease in test loss. These findings
indicate fundamental differences between linear and nonlinear compressive
sensing.

</details>


### [82] [Improving Internet Traffic Matrix Prediction via Time Series Clustering](https://arxiv.org/abs/2509.15072)
*Martha Cash,Alexander Wyglinski*

Main category: cs.LG

TL;DR: A framework using time series clustering to improve internet traffic matrix prediction by grouping flows with similar temporal patterns before training deep learning models, achieving significant error reduction and better network optimization.


<details>
  <summary>Details</summary>
Motivation: Traffic flows in internet traffic matrices exhibit diverse temporal behaviors that hinder prediction accuracy when using a single model for all flows, necessitating a clustering approach to create homogeneous data subsets.

Method: Proposes two clustering strategies (source clustering and histogram clustering) to group flows with similar temporal patterns before training deep learning models, enabling better pattern capture and generalization.

Result: Reduces RMSE by up to 92% for Abilene and 75% for GÉANT compared to existing methods, and reduces maximum link utilization bias by 18% and 21% respectively in routing scenarios.

Conclusion: Clustering-based approach significantly improves traffic matrix prediction accuracy and demonstrates practical benefits for network optimization applications.

Abstract: We present a novel framework that leverages time series clustering to improve
internet traffic matrix (TM) prediction using deep learning (DL) models.
Traffic flows within a TM often exhibit diverse temporal behaviors, which can
hinder prediction accuracy when training a single model across all flows. To
address this, we propose two clustering strategies, source clustering and
histogram clustering, that group flows with similar temporal patterns prior to
model training. Clustering creates more homogeneous data subsets, enabling
models to capture underlying patterns more effectively and generalize better
than global prediction approaches that fit a single model to the entire TM.
Compared to existing TM prediction methods, our method reduces RMSE by up to
92\% for Abilene and 75\% for G\'EANT. In routing scenarios, our clustered
predictions also reduce maximum link utilization (MLU) bias by 18\% and 21\%,
respectively, demonstrating the practical benefits of clustering when TMs are
used for network optimization.

</details>


### [83] [Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits](https://arxiv.org/abs/2509.15073)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing
environments by incorporating mechanisms to detect and respond to shifts in
reward distributions, making them well-suited for dynamic settings. However,
existing approaches typically assume that reward feedback is available at every
round - an assumption that overlooks many real-world scenarios where feedback
is limited. In this paper, we take a significant step forward by introducing a
new model of constrained feedback in non-stationary multi-armed bandits, where
the availability of reward feedback is restricted. We propose the first
prior-free algorithm - that is, one that does not require prior knowledge of
the degree of non-stationarity - that achieves near-optimal dynamic regret in
this setting. Specifically, our algorithm attains a dynamic regret of
$\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the
number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$
is the variation budget capturing the degree of non-stationarity.

</details>


### [84] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: AI-driven system predicts air pollution from sky images and generates realistic pollution visualizations using generative modeling and vision-language models for improved public awareness and decision-making.


<details>
  <summary>Details</summary>
Motivation: Conventional air pollution monitoring systems have limited spatial coverage and accessibility, creating a need for more accessible and transparent methods to help the public understand and respond to air quality conditions.

Method: Combines statistical texture analysis with supervised learning for pollution classification, and uses vision-language model (VLM)-guided image generation to create interpretable pollution visualizations that simulate varying air quality conditions.

Result: Validated on urban sky image dataset, demonstrating effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system provides realistic pollution scenario visualizations for user interfaces.

Conclusion: The approach offers a foundation for intelligent applications that enhance situational awareness and support environmental decision-making. Future work will incorporate green CNN architecture with FPGA-based incremental learning for scalable, energy-efficient real-time deployment on edge platforms.

Abstract: Air pollution remains a critical threat to public health and environmental
sustainability, yet conventional monitoring systems are often constrained by
limited spatial coverage and accessibility. This paper proposes an AI-driven
agent that predicts ambient air pollution levels from sky images and
synthesizes realistic visualizations of pollution scenarios using generative
modeling. Our approach combines statistical texture analysis with supervised
learning for pollution classification, and leverages vision-language model
(VLM)-guided image generation to produce interpretable representations of air
quality conditions. The generated visuals simulate varying degrees of
pollution, offering a foundation for user-facing interfaces that improve
transparency and support informed environmental decision-making. These outputs
can be seamlessly integrated into intelligent applications aimed at enhancing
situational awareness and encouraging behavioral responses based on real-time
forecasts. We validate our method using a dataset of urban sky images and
demonstrate its effectiveness in both pollution level estimation and
semantically consistent visual synthesis. The system design further
incorporates human-centered user experience principles to ensure accessibility,
clarity, and public engagement in air quality forecasting. To support scalable
and energy-efficient deployment, future iterations will incorporate a green CNN
architecture enhanced with FPGA-based incremental learning, enabling real-time
inference on edge platforms.

</details>


### [85] [Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](https://arxiv.org/abs/2509.15087)
*Lei Wang,Jieming Bian,Letian Zhang,Jie Xu*

Main category: cs.LG

TL;DR: FedLEASE is a federated learning framework that adaptively allocates and selects LoRA experts for efficient LLM fine-tuning across heterogeneous clients while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for domain-specific applications requires substantial domain data often distributed across organizations, but federated learning faces computational constraints and single LoRA modules struggle with heterogeneous data.

Method: Proposes FedLEASE framework that clusters clients based on representation similarity to allocate domain-specific LoRA experts, and uses adaptive top-M Mixture-of-Experts mechanism for optimal expert selection per client.

Result: Extensive experiments on diverse benchmark datasets show FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.

Conclusion: FedLEASE effectively addresses the challenges of determining optimal LoRA expert allocation and enabling selective expert utilization in federated learning environments with heterogeneous data.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
various tasks, but fine-tuning them for domain-specific applications often
requires substantial domain-specific data that may be distributed across
multiple organizations. Federated Learning (FL) offers a privacy-preserving
solution, but faces challenges with computational constraints when applied to
LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient
fine-tuning approach, though a single LoRA module often struggles with
heterogeneous data across diverse domains. This paper addresses two critical
challenges in federated LoRA fine-tuning: 1. determining the optimal number and
allocation of LoRA experts across heterogeneous clients, and 2. enabling
clients to selectively utilize these experts based on their specific data
characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation
and SElection), a novel framework that adaptively clusters clients based on
representation similarity to allocate and train domain-specific LoRA experts.
It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows
each client to select the optimal number of utilized experts. Our extensive
experiments on diverse benchmark datasets demonstrate that FedLEASE
significantly outperforms existing federated fine-tuning approaches in
heterogeneous client settings while maintaining communication efficiency.

</details>


### [86] [Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)
*Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi*

Main category: cs.LG

TL;DR: Strategic competition among multiple misaligned AI agents can yield outcomes comparable to perfect alignment when user utility lies within the convex hull of agents' utilities, especially with increased model diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing the fundamental challenge of aligning AI systems with human values, exploring whether imperfect alignment can still provide benefits through strategic interaction with multiple misaligned agents.

Method: Modeled as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, with theoretical analysis and two sets of experiments.

Result: Three key theoretical results: (1) user can learn Bayes-optimal action under convex hull condition, (2) near-optimal utility achieved with quantal response, (3) equilibrium guarantees remain near-optimal when selecting best single AI after evaluation.

Conclusion: Strategic competition among diverse misaligned AI agents can effectively substitute for perfect alignment, providing comparable benefits without requiring individually well-aligned models.

Abstract: Aligning AI systems with human values remains a fundamental challenge, but
does our inability to create perfectly aligned models preclude obtaining the
benefits of alignment? We study a strategic setting where a human user
interacts with multiple differently misaligned AI agents, none of which are
individually well-aligned. Our key insight is that when the users utility lies
approximately within the convex hull of the agents utilities, a condition that
becomes easier to satisfy as model diversity increases, strategic competition
can yield outcomes comparable to interacting with a perfectly aligned model. We
model this as a multi-leader Stackelberg game, extending Bayesian persuasion to
multi-round conversations between differently informed parties, and prove three
results: (1) when perfect alignment would allow the user to learn her
Bayes-optimal action, she can also do so in all equilibria under the convex
hull condition (2) under weaker assumptions requiring only approximate utility
learning, a non-strategic user employing quantal response achieves near-optimal
utility in all equilibria and (3) when the user selects the best single AI
after an evaluation period, equilibrium guarantees remain near-optimal without
further distributional assumptions. We complement the theory with two sets of
experiments.

</details>


### [87] [The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](https://arxiv.org/abs/2509.15097)
*Mohammad Saleh Vahdatpour,Huaiyuan Chu,Yanqing Zhang*

Main category: cs.LG

TL;DR: A hybrid framework combining hierarchical decomposition with FPGA-based equation solving and incremental learning to reduce computational and energy demands of large language models while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the unsustainable computational and energy requirements of deep learning, particularly in large-scale architectures like foundation models and LLMs, where traditional gradient-based training is inefficient and power-hungry.

Method: Divides neural network into two tiers: lower layers use FPGA-based single-step equation solving for efficient feature extraction, while higher layers employ adaptive incremental learning. Introduces Compound LLM framework with lower-level LLM for representation learning and upper-level LLM for adaptive decision-making.

Result: Significantly reduces computational costs while preserving high model performance, making it suitable for edge deployment and real-time adaptation in energy-constrained environments.

Conclusion: The proposed hybrid framework enhances scalability, reduces redundant computation, and aligns with sustainable AI principles by combining efficient hardware acceleration with adaptive learning strategies.

Abstract: The rising computational and energy demands of deep learning, particularly in
large-scale architectures such as foundation models and large language models
(LLMs), pose significant challenges to sustainability. Traditional
gradient-based training methods are inefficient, requiring numerous iterative
updates and high power consumption. To address these limitations, we propose a
hybrid framework that combines hierarchical decomposition with FPGA-based
direct equation solving and incremental learning. Our method divides the neural
network into two functional tiers: lower layers are optimized via single-step
equation solving on FPGAs for efficient and parallelizable feature extraction,
while higher layers employ adaptive incremental learning to support continual
updates without full retraining. Building upon this foundation, we introduce
the Compound LLM framework, which explicitly deploys LLM modules across both
hierarchy levels. The lower-level LLM handles reusable representation learning
with minimal energy overhead, while the upper-level LLM performs adaptive
decision-making through energy-aware updates. This integrated design enhances
scalability, reduces redundant computation, and aligns with the principles of
sustainable AI. Theoretical analysis and architectural insights demonstrate
that our method reduces computational costs significantly while preserving high
model performance, making it well-suited for edge deployment and real-time
adaptation in energy-constrained environments.

</details>


### [88] [Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting](https://arxiv.org/abs/2509.15105)
*Liran Nochumsohn,Raz Marshanski,Hedi Zisling,Omri Azencot*

Main category: cs.LG

TL;DR: Super-Linear is a lightweight mixture-of-experts model that uses frequency-specialized linear experts and spectral gating for efficient time series forecasting, matching SOTA performance with superior efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing large pre-trained models for time series forecasting suffer from high computational costs despite strong zero-shot performance, creating a need for more efficient solutions.

Method: Replaces deep architectures with simple linear experts specialized for different frequencies, trained on resampled data, and uses a lightweight spectral gating mechanism to dynamically select relevant experts.

Result: Matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability.

Conclusion: Super-Linear provides an effective lightweight alternative to computationally expensive models for general time series forecasting across diverse domains.

Abstract: Time series forecasting (TSF) is critical in domains like energy, finance,
healthcare, and logistics, requiring models that generalize across diverse
datasets. Large pre-trained models such as Chronos and Time-MoE show strong
zero-shot (ZS) performance but suffer from high computational costs. In this
work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts
(MoE) model for general forecasting. It replaces deep architectures with simple
frequency-specialized linear experts, trained on resampled data across multiple
frequency regimes. A lightweight spectral gating mechanism dynamically selects
relevant experts, enabling efficient, accurate forecasting. Despite its
simplicity, Super-Linear matches state-of-the-art performance while offering
superior efficiency, robustness to various sampling rates, and enhanced
interpretability. The implementation of Super-Linear is available at
\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}

</details>


### [89] [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](https://arxiv.org/abs/2509.15107)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.LG

TL;DR: Current chest X-ray AI datasets have significant limitations including label errors from automated extraction, domain shift issues, and population biases that reduce clinical applicability and generalizability.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze the limitations of current public chest X-ray datasets (MIMIC-CXR, ChestX-ray14, PadChest, CheXpert) that are widely used for AI development but suffer from label quality issues, domain shift, and population biases.

Method: Conducted cross-dataset domain shift evaluation across multiple model architectures, trained source-classification models to detect dataset bias, performed subgroup analyses for demographic groups, and conducted expert review by two board-certified radiologists to validate label accuracy.

Result: Found substantial external performance degradation with reduced AUPRC and F1 scores, near-perfect dataset distinguishability indicating bias, reduced performance for minority age/sex groups, and significant disagreement between radiologist review and automated labels.

Conclusion: Current chest X-ray benchmarks have important clinical weaknesses, highlighting the need for clinician-validated datasets and fairer evaluation frameworks to ensure reliable AI performance in real clinical settings.

Abstract: Artificial intelligence has shown significant promise in chest radiography,
where deep learning models can approach radiologist-level diagnostic
performance. Progress has been accelerated by large public datasets such as
MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of
thousands of labelled images with pathology annotations. However, these
datasets also present important limitations. Automated label extraction from
radiology reports introduces errors, particularly in handling uncertainty and
negation, and radiologist review frequently disagrees with assigned labels. In
addition, domain shift and population bias restrict model generalisability,
while evaluation practices often overlook clinically meaningful measures. We
conduct a systematic analysis of these challenges, focusing on label quality,
dataset bias, and domain shift. Our cross-dataset domain shift evaluation
across multiple model architectures revealed substantial external performance
degradation, with pronounced reductions in AUPRC and F1 scores relative to
internal testing. To assess dataset bias, we trained a source-classification
model that distinguished datasets with near-perfect accuracy, and performed
subgroup analyses showing reduced performance for minority age and sex groups.
Finally, expert review by two board-certified radiologists identified
significant disagreement with public dataset labels. Our findings highlight
important clinical weaknesses of current benchmarks and emphasise the need for
clinician-validated datasets and fairer evaluation frameworks.

</details>


### [90] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: TDRM introduces temporal-difference regularization to create smoother, more reliable reward models that improve RL training stability and alignment with long-term objectives.


<details>
  <summary>Details</summary>
Motivation: Existing reward models lack temporal consistency, leading to ineffective policy updates and unstable reinforcement learning training.

Method: TDRM minimizes temporal differences during training through TD regularization to produce smooth rewards and improve alignment with long-term objectives.

Result: TD-trained process reward models improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. Combined with RLVR, they achieve comparable performance with just 2.5k data vs 50.1k for baselines, yielding higher-quality policies on 8 model variants.

Conclusion: TDRM is an effective supplement to verifiable reward methods that produces more data-efficient RL training and higher-quality language model policies across multiple model series.

Abstract: Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

</details>


### [91] [Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers](https://arxiv.org/abs/2509.15113)
*Andrei Chertkov,Artem Basharin,Mikhail Saygin,Evgeny Frolov,Stanislav Straupe,Ivan Oseledets*

Main category: cs.LG

TL;DR: A framework for end-to-end training of hybrid digital-physical neural networks using stochastic zeroth-order optimization and dynamic low-rank surrogate models to handle non-differentiable physical components.


<details>
  <summary>Details</summary>
Motivation: The need to integrate energy-efficient physical computing components (photonic, neuromorphic) into deep learning pipelines despite their non-differentiable nature and limited expressiveness.

Method: Combines stochastic zeroth-order optimization for physical layer updates with a dynamic low-rank surrogate model for gradient propagation, using an implicit projector-splitting integrator algorithm to minimize hardware queries.

Result: Achieves near-digital baseline accuracy across computer vision, audio classification, and language modeling tasks with various non-differentiable physical components.

Conclusion: Bridges hardware-aware deep learning and gradient-free optimization, providing a practical pathway for integrating non-differentiable physical components into scalable AI systems.

Abstract: The growing demand for energy-efficient, high-performance AI systems has led
to increased attention on alternative computing platforms (e.g., photonic,
neuromorphic) due to their potential to accelerate learning and inference.
However, integrating such physical components into deep learning pipelines
remains challenging, as physical devices often offer limited expressiveness,
and their non-differentiable nature renders on-device backpropagation difficult
or infeasible. This motivates the development of hybrid architectures that
combine digital neural networks with reconfigurable physical layers, which
effectively behave as black boxes. In this work, we present a framework for the
end-to-end training of such hybrid networks. This framework integrates
stochastic zeroth-order optimization for updating the physical layer's internal
parameters with a dynamic low-rank surrogate model that enables gradient
propagation through the physical layer. A key component of our approach is the
implicit projector-splitting integrator algorithm, which updates the
lightweight surrogate model after each forward pass with minimal hardware
queries, thereby avoiding costly full matrix reconstruction. We demonstrate our
method across diverse deep learning tasks, including: computer vision, audio
classification, and language modeling. Notably, across all modalities, the
proposed approach achieves near-digital baseline accuracy and consistently
enables effective end-to-end training of hybrid models incorporating various
non-differentiable physical components (spatial light modulators, microring
resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware
deep learning and gradient-free optimization, thereby offering a practical
pathway for integrating non-differentiable physical components into scalable,
end-to-end trainable AI systems.

</details>


### [92] [Efficient Conformal Prediction for Regression Models under Label Noise](https://arxiv.org/abs/2509.15120)
*Yahav Cohen,Jacob Goldberger,Tom Tirer*

Main category: cs.LG

TL;DR: Conformal Prediction for regression with noisy labels in medical imaging, achieving near-clean-label performance


<details>
  <summary>Details</summary>
Motivation: In high-stakes medical imaging applications, reliable confidence intervals are critical, but existing Conformal Prediction methods assume clean labels while real-world calibration sets often contain noisy labels

Method: Developed a mathematically grounded procedure to estimate noise-free CP threshold, then created a practical algorithm to handle continuous regression challenges with Gaussian label noise

Result: Significantly outperforms existing alternatives on two medical imaging regression datasets, achieving performance close to clean-label setting

Conclusion: The proposed method successfully addresses the challenge of applying Conformal Prediction to regression models with noisy calibration labels, providing reliable confidence intervals in practical medical imaging scenarios

Abstract: In high-stakes scenarios, such as medical imaging applications, it is
critical to equip the predictions of a regression model with reliable
confidence intervals. Recently, Conformal Prediction (CP) has emerged as a
powerful statistical framework that, based on a labeled calibration set,
generates intervals that include the true labels with a pre-specified
probability. In this paper, we address the problem of applying CP for
regression models when the calibration set contains noisy labels. We begin by
establishing a mathematically grounded procedure for estimating the noise-free
CP threshold. Then, we turn it into a practical algorithm that overcomes the
challenges arising from the continuous nature of the regression problem. We
evaluate the proposed method on two medical imaging regression datasets with
Gaussian label noise. Our method significantly outperforms the existing
alternative, achieving performance close to the clean-label setting.

</details>


### [93] [Optimal Learning from Label Proportions with General Loss Functions](https://arxiv.org/abs/2509.15145)
*Lorne Applebaum,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren*

Main category: cs.LG

TL;DR: Novel low-variance de-biasing method for Learning from Label Proportions (LLP) that works with various loss functions in binary and multi-class classification, improving sample complexity and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing problems in online advertising where training data consists of groups (bags) with only average label values available, requiring individual example prediction.

Method: Introduces a versatile low-variance de-biasing methodology to learn from aggregate label information, combining estimators with standard techniques.

Result: Significantly advances state of the art in LLP, improves sample complexity guarantees, and demonstrates compelling empirical advantages across diverse benchmark datasets.

Conclusion: The proposed approach effectively handles LLP problems with flexibility across various loss functions and classification settings, showing both theoretical and practical improvements.

Abstract: Motivated by problems in online advertising, we address the task of Learning
from Label Proportions (LLP). In this partially-supervised setting, training
data consists of groups of examples, termed bags, for which we only observe the
average label value. The main goal, however, remains the design of a predictor
for the labels of individual examples. We introduce a novel and versatile
low-variance de-biasing methodology to learn from aggregate label information,
significantly advancing the state of the art in LLP. Our approach exhibits
remarkable flexibility, seamlessly accommodating a broad spectrum of
practically relevant loss functions across both binary and multi-class
classification settings. By carefully combining our estimators with standard
techniques, we substantially improve sample complexity guarantees for a large
class of losses of practical relevance. We also empirically validate the
efficacy of our proposed approach across a diverse array of benchmark datasets,
demonstrating compelling empirical advantages over standard baselines.

</details>


### [94] [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](https://arxiv.org/abs/2509.15147)
*Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: Logit-based federated learning reduces communication costs by sharing only logits instead of full model weights/gradients, with three aggregation methods showing competitive accuracy to centralized training.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning shares model weights or gradients which is costly for large models. Logit-based FL reduces communication overhead by sharing only logits computed on a public proxy dataset, but aggregating information from heterogeneous clients remains challenging.

Method: Three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10 datasets.

Result: The methods reduce communication overhead, improve robustness under non-IID data distributions, and achieve accuracy competitive with centralized training.

Conclusion: Logit-based federated learning with effective aggregation strategies provides a communication-efficient alternative to traditional FL while maintaining competitive performance, especially valuable for large models and heterogeneous client data.

Abstract: Federated learning (FL) usually shares model weights or gradients, which is
costly for large models. Logit-based FL reduces this cost by sharing only
logits computed on a public proxy dataset. However, aggregating information
from heterogeneous clients is still challenging. This paper studies this
problem, introduces and compares three logit aggregation methods: simple
averaging, uncertainty-weighted averaging, and a learned meta-aggregator.
Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead,
improve robustness under non-IID data, and achieve accuracy competitive with
centralized training.

</details>


### [95] [Self-Improving Embodied Foundation Models](https://arxiv.org/abs/2509.15155)
*Seyed Kamyar Seyed Ghasemipour,Ayzaan Wahid,Jonathan Tompson,Pannag Sanketi,Igor Mordatch*

Main category: cs.LG

TL;DR: A two-stage post-training approach combining supervised fine-tuning and self-improvement enables autonomous skill acquisition in robotics, outperforming behavioral cloning in sample efficiency and success rates.


<details>
  <summary>Details</summary>
Motivation: Foundation models have transformed robotics but remain limited to behavioral cloning for low-level control. The success of reinforcement learning in fine-tuning large language models inspired this approach to enable autonomous skill acquisition.

Method: Two-stage approach: 1) Supervised Fine-Tuning (SFT) with behavioral cloning and steps-to-go prediction objectives, 2) Self-Improvement stage where steps-to-go prediction enables reward function extraction and success detection for autonomous practice by robot fleets.

Result: Significantly more sample-efficient than scaling imitation data collection, leads to higher success rates, and uniquely enables autonomous acquisition of novel skills beyond behaviors in training datasets.

Conclusion: Combining pretrained foundation models with online self-improvement has transformative potential for autonomous skill acquisition in robotics.

Abstract: Foundation models trained on web-scale data have revolutionized robotics, but
their application to low-level control remains largely limited to behavioral
cloning. Drawing inspiration from the success of the reinforcement learning
stage in fine-tuning large language models, we propose a two-stage
post-training approach for robotics. The first stage, Supervised Fine-Tuning
(SFT), fine-tunes pretrained foundation models using both: a) behavioral
cloning, and b) steps-to-go prediction objectives. In the second stage,
Self-Improvement, steps-to-go prediction enables the extraction of a
well-shaped reward function and a robust success detector, enabling a fleet of
robots to autonomously practice downstream tasks with minimal human
supervision. Through extensive experiments on real-world and simulated robot
embodiments, our novel post-training recipe unveils significant results on
Embodied Foundation Models. First, we demonstrate that the combination of SFT
and Self-Improvement is significantly more sample-efficient than scaling
imitation data collection for supervised learning, and that it leads to
policies with significantly higher success rates. Further ablations highlight
that the combination of web-scale pretraining and Self-Improvement is the key
to this sample-efficiency. Next, we demonstrate that our proposed combination
uniquely unlocks a capability that current methods cannot achieve: autonomously
practicing and acquiring novel skills that generalize far beyond the behaviors
observed in the imitation learning datasets used during training. These
findings highlight the transformative potential of combining pretrained
foundation models with online Self-Improvement to enable autonomous skill
acquisition in robotics. Our project website can be found at
https://self-improving-efms.github.io .

</details>


### [96] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin*

Main category: cs.LG

TL;DR: A data rewriting framework for off-policy supervised fine-tuning that proactively reduces policy gaps by keeping correct solutions and rewriting incorrect ones with guided re-solving, improving training stability and performance.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning suffers from distribution mismatch between expert demonstrations and target policy, leading to high variance and instability in off-policy learning through importance sampling.

Method: Proactive data rewriting framework that aligns training distribution with target policy by keeping correct solutions as on-policy data, rewriting incorrect ones with guided re-solving, and only using expert demonstrations when necessary.

Result: Experiments on five mathematical reasoning benchmarks show consistent and significant gains over vanilla SFT and state-of-the-art Dynamic Fine-Tuning approach.

Conclusion: The proposed data rewriting framework effectively reduces importance sampling variance and stabilizes off-policy fine-tuning by proactively shrinking policy gaps before optimization.

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an
off-policy learning problem, where expert demonstrations come from a fixed
behavior policy while training aims to optimize a target policy. Importance
sampling is the standard tool for correcting this distribution mismatch, but
large policy gaps lead to high variance and training instability. Existing
approaches mitigate this issue using KL penalties or clipping, which passively
constrain updates rather than actively reducing the gap. We propose a simple
yet effective data rewriting framework that proactively shrinks the policy gap
by keeping correct solutions as on-policy data and rewriting incorrect ones
with guided re-solving, falling back to expert demonstrations only when needed.
This aligns the training distribution with the target policy before
optimization, reducing importance sampling variance and stabilizing off-policy
fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate
consistent and significant gains over both vanilla SFT and the state-of-the-art
Dynamic Fine-Tuning (DFT) approach. The data and code will be released at
https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [97] [MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration](https://arxiv.org/abs/2509.15187)
*Giorgos Armeniakos,Alexis Maras,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: Proposed MaRVIn framework with novel ISA extensions and micro-architecture for efficient mixed-precision NN execution on RISC-V, achieving 17.6x speedup with <1% accuracy loss and up to 1.8 TOPs/W.


<details>
  <summary>Details</summary>
Motivation: Existing embedded microprocessors lack architectural support for mixed-precision NNs, causing inefficiencies like excessive data packing/unpacking and underutilized arithmetic units.

Method: Cross-layer hardware-software co-design with enhanced ALU for configurable mixed-precision arithmetic (2,4,8 bits), multi-pumping, soft SIMD, pruning-aware fine-tuning, greedy DSE for Pareto-optimal models, and voltage scaling.

Result: Achieved average 17.6x speedup with <1% accuracy loss on CIFAR10 and ImageNet datasets, outperforming state-of-the-art RISC-V cores with up to 1.8 TOPs/W power efficiency.

Conclusion: MaRVIn framework successfully addresses mixed-precision execution inefficiencies through hardware-software co-design, delivering significant performance and energy efficiency improvements for deep learning inference on RISC-V architectures.

Abstract: The evolution of quantization and mixed-precision techniques has unlocked new
possibilities for enhancing the speed and energy efficiency of NNs. Several
recent studies indicate that adapting precision levels across different
parameters can maintain accuracy comparable to full-precision models while
significantly reducing computational demands. However, existing embedded
microprocessors lack sufficient architectural support for efficiently executing
mixed-precision NNs, both in terms of ISA extensions and hardware design,
resulting in inefficiencies such as excessive data packing/unpacking and
underutilized arithmetic units. In this work, we propose novel ISA extensions
and a micro-architecture implementation specifically designed to optimize
mixed-precision execution, enabling energy-efficient deep learning inference on
RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software
co-design framework that enhances power efficiency and performance through a
combination of hardware improvements, mixed-precision quantization, ISA-level
optimizations, and cycle-accurate emulation. At the hardware level, we enhance
the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for
weights/activations and employ multi-pumping to reduce execution latency while
implementing soft SIMD for efficient 2-bit ops. At the software level, we
integrate a pruning-aware fine-tuning method to optimize model compression and
a greedy-based DSE approach to efficiently search for Pareto-optimal
mixed-quantized models. Additionally, we incorporate voltage scaling to boost
the power efficiency of our system. Our experimental evaluation over widely
used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our
framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss
and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up
to 1.8 TOPs/W.

</details>


### [98] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou,Zhenwen Liang,Haolin Liu,Wenhao Yu,Kishan Panaganti,Linfeng Song,Dian Yu,Xiangliang Zhang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: EVOL-RL is a label-free reinforcement learning method that prevents entropy collapse by combining majority-vote stability with novelty-based variation, enabling LLMs to self-improve without sacrificing exploration diversity.


<details>
  <summary>Details</summary>
Motivation: Existing label-free RL methods for LLMs cause entropy collapse where generations become shorter and less diverse. Current approaches like TTRL focus on immediate dataset adaptation rather than general improvement while maintaining exploration capacity.

Method: EVOL-RL uses majority-voted answers as stable anchors plus novelty-aware rewards that favor semantically different responses. Implemented with GRPO, it includes asymmetric clipping and entropy regularization to maintain search diversity.

Result: EVOL-RL significantly outperforms TTRL baseline, improving Qwen3-4B-Base AIME25 pass@1 from 4.6% to 16.4% and pass@16 from 18.5% to 37.9%. It prevents diversity collapse and enhances generalization across domains like GPQA.

Conclusion: EVOL-RL effectively enables LLMs to self-improve without labels while maintaining exploration diversity and generalization ability, working well in both label-free and RLVR settings with broad applicability.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning from verifiable rewards (RLVR), yet real-world deployment demands
models that can self-improve without labels or external judges. Existing
label-free methods, confidence minimization, self-consistency, or majority-vote
objectives, stabilize learning but steadily shrink exploration, causing an
entropy collapse: generations become shorter, less diverse, and brittle. Unlike
prior approaches such as Test-Time Reinforcement Learning (TTRL), which
primarily adapt models to the immediate unlabeled dataset at hand, our goal is
broader: to enable general improvements without sacrificing the model's
inherent exploration capacity and generalization ability, i.e., evolving. We
formalize this issue and propose EVolution-Oriented and Label-free
Reinforcement Learning (EVOL-RL), a simple rule that couples stability with
variation under a label-free setting. EVOL-RL keeps the majority-voted answer
as a stable anchor (selection) while adding a novelty-aware reward that favors
responses whose reasoning differs from what has already been produced
(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also
uses asymmetric clipping to preserve strong signals and an entropy regularizer
to sustain search. This majority-for-selection + novelty-for-variation design
prevents collapse, maintains longer and more informative chains of thought, and
improves both pass@1 and pass@n. EVOL-RL consistently outperforms the
majority-only TTRL baseline; e.g., training on label-free AIME24 lifts
Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%
to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks
stronger generalization across domains (e.g., GPQA). Furthermore, we
demonstrate that EVOL-RL also boosts performance in the RLVR setting,
highlighting its broad applicability.

</details>


### [99] [Explaining deep learning for ECG using time-localized clusters](https://arxiv.org/abs/2509.15198)
*Ahcène Boubekki,Konstantinos Patlatzoglou,Joseph Barker,Fu Siong Ng,Antônio H. Ribeiro*

Main category: cs.LG

TL;DR: Novel interpretability method for CNN-based ECG analysis that extracts time-localized clusters from model representations to visualize waveform contributions and quantify prediction uncertainty.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for ECG analysis lack interpretability, limiting clinical trust and knowledge extraction from these advanced diagnostic tools.

Method: Extracts time-localized clusters from CNN's internal representations to segment ECG based on learned characteristics while quantifying representation uncertainty.

Result: Enables visualization of how different ECG waveform regions contribute to predictions and assessment of decision certainty.

Conclusion: Provides structured interpretability for ECG deep learning models, enhancing trust in AI diagnostics and facilitating discovery of clinically relevant electrophysiological patterns.

Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis,
enabling automatic annotation, disease screening, and prognosis beyond
traditional clinical capabilities. However, understanding these models remains
a challenge, limiting interpretation and gaining knowledge from these
developments. In this work, we propose a novel interpretability method for
convolutional neural networks applied to ECG analysis. Our approach extracts
time-localized clusters from the model's internal representations, segmenting
the ECG according to the learned characteristics while quantifying the
uncertainty of these representations. This allows us to visualize how different
waveform regions contribute to the model's predictions and assess the certainty
of its decisions. By providing a structured and interpretable view of deep
learning models for ECG, our method enhances trust in AI-driven diagnostics and
facilitates the discovery of clinically relevant electrophysiological patterns.

</details>


### [100] [CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness](https://arxiv.org/abs/2509.15199)
*Ying Zheng,Yangfan Jiang,Kian-Lee Tan*

Main category: cs.LG

TL;DR: CausalPre is a scalable causality-guided data pre-processing framework that achieves causal fairness without requiring strong causal model assumptions, outperforming previous approaches in both effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing causal fairness methods either assume known causal models or enforce constraints that fail to capture broader attribute relationships, limiting their utility. The paper addresses whether causal fairness can be achieved efficiently without strong causal model assumptions.

Method: CausalPre reformulates the causal fairness extraction task into a distribution estimation problem, using low-dimensional marginal factorization to approximate joint distributions and a heuristic algorithm for computational efficiency.

Result: Extensive experiments show CausalPre is both effective and scalable, achieving strong causal fairness while maintaining utility, challenging the belief that causal fairness requires trading off relationship coverage.

Conclusion: CausalPre demonstrates that causal fairness can be achieved efficiently without strong causal model assumptions, providing a practical solution that maintains both fairness and utility in database applications.

Abstract: Causal fairness in databases is crucial to preventing biased and inaccurate
outcomes in downstream tasks. While most prior work assumes a known causal
model, recent efforts relax this assumption by enforcing additional
constraints. However, these approaches often fail to capture broader attribute
relationships that are critical to maintaining utility. This raises a
fundamental question: Can we harness the benefits of causal reasoning to design
efficient and effective fairness solutions without relying on strong
assumptions about the underlying causal model? In this paper, we seek to answer
this question by introducing CausalPre, a scalable and effective
causality-guided data pre-processing framework that guarantees justifiable
fairness, a strong causal notion of fairness. CausalPre extracts causally fair
relationships by reformulating the originally complex and computationally
infeasible extraction task into a tailored distribution estimation problem. To
ensure scalability, CausalPre adopts a carefully crafted variant of
low-dimensional marginal factorization to approximate the joint distribution,
complemented by a heuristic algorithm that efficiently tackles the associated
computational challenge. Extensive experiments on benchmark datasets
demonstrate that CausalPre is both effective and scalable, challenging the
conventional belief that achieving causal fairness requires trading off
relationship coverage for relaxed model assumptions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [101] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: CoDiCon introduces competitive incentives in cooperative MARL through ranking-based intrinsic rewards to foster strategic diversity and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing MARL diversity methods focus on individual agent characteristics but neglect agent interplay and mutual influence during policy formation.

Method: Uses competitive intrinsic rewards with ranking features, centralized reward module for balanced competition-cooperation, and bilevel optimization to maximize environmental rewards.

Result: Outperforms state-of-the-art methods in SMAC and GRF environments, with competitive rewards effectively promoting diverse and adaptive strategies.

Conclusion: Incorporating constructive competition through ranking-based intrinsic rewards enhances cooperative MARL by fostering strategic diversity and improving overall performance.

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [102] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: LEED framework uses LLM-generated expert demonstrations to improve multi-agent reinforcement learning, achieving better efficiency and scalability than existing methods.


<details>
  <summary>Details</summary>
Motivation: Multi-agent reinforcement learning faces coordination and scalability challenges as the number of agents increases, limiting its effectiveness in complex environments.

Method: Two-component framework: 1) DG module uses large language models to generate high-quality expert demonstrations, 2) PO module uses decentralized training with expert policy loss integrated with individual policy loss for personalized optimization.

Result: Experimental results show superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.

Conclusion: LEED framework effectively addresses coordination and scalability issues in MARL by leveraging LLM-generated expert demonstrations, enabling more efficient and scalable multi-agent decision-making.

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [103] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: This paper proposes a method to identify the most vulnerable agents in large-scale multi-agent systems by framing the problem as Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC) and solving it through decomposition and reformulation.


<details>
  <summary>Details</summary>
Motivation: As systems scale up, partial agent failure becomes inevitable, making it crucial to identify which agents' compromise would most severely degrade overall system performance.

Method: The authors frame the Vulnerable Agent Identification (VAI) problem as HAD-MFC, then decouple the hierarchical process using Fenchel-Rockafellar transform, resulting in a regularized mean-field Bellman operator. They reformulate the upper-level combinatorial problem as an MDP with dense rewards to sequentially identify vulnerable agents using greedy and RL algorithms.

Result: Experiments show the method effectively identifies more vulnerable agents in large-scale MARL and rule-based systems, causes worse system failures, and learns a value function that reveals each agent's vulnerability.

Conclusion: The proposed decomposition approach successfully solves the challenging HAD-MFC problem while preserving optimal solutions, enabling efficient identification of the most critical agents whose failure would maximally impact system performance.

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>
