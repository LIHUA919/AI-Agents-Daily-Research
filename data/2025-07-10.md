<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: MEWI is a 3D multiplayer simulation for medical evacuation training, improving decision-making and learning in classroom settings.


<details>
  <summary>Details</summary>
Motivation: Lack of tools to simulate and evaluate medical evacuation networks in training environments.

Method: Developed MEWI in Unity, modeling battlefield constraints and patient interactions in scenarios like amphibious assaults and Eurasian conflicts.

Result: MEWI participation enhances learning and cooperative decision-making, validated by student surveys and observer notes.

Conclusion: MEWI advances high-fidelity medical training, offering insights for improving evacuation education and operations.

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [2] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: The paper introduces the Prompt Declaration Language (PDL) to simplify prompt engineering for LLMs, enabling customization and optimization, and demonstrates its effectiveness with a 4x performance improvement in a case study.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for prompt engineering are either too restrictive or inflexible, hindering sophisticated agentic programming.

Method: The authors propose PDL, a declarative language for prompt representation, allowing manual and automatic tuning while integrating LLM calls, rule-based code, and external tools.

Result: PDL improved performance by up to 4x in a real-world compliance agent case study compared to traditional methods.

Conclusion: PDL effectively addresses prompt engineering complexity, enhancing productivity and optimization potential.

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [3] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: The paper explores the Jolting Technologies Hypothesis, proposing superexponential AI growth, and develops tools for future empirical validation.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the accelerating growth of AI capabilities and its implications for AGI emergence.

Method: Develops a theoretical framework and validates detection methods via Monte Carlo simulations, focusing on idea-to-action intervals and iterative AI improvements.

Result: Provides mathematical tools for detecting and analyzing jolt dynamics in AI development, pending empirical validation.

Conclusion: The study lays groundwork for future research and policy insights into AI trajectories and AGI emergence.

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [4] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: The paper proves q-dialectical systems are more powerful than p-dialectical systems, which are stronger than (d-)dialectical systems, highlighting the roles of counterexample and contradiction in belief revision.


<details>
  <summary>Details</summary>
Motivation: To address an open problem about the relative power of dialectical systems in modeling belief revision for automated agents and human reasoning.

Method: Comparative analysis of three dialectical systems (d-, p-, and q-dialectical) to evaluate their capabilities in belief revision.

Result: q-dialectical systems are strictly more powerful than p-dialectical systems, which are stronger than (d-)dialectical systems.

Conclusion: The findings emphasize the complementary roles of counterexample and contradiction in belief revision, applicable to both automated agents and human reasoning.

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [5] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: The paper addresses the limitations of SCC-recursive semantics in infinite argumentation frameworks (AFs) and proposes two extensions. It evaluates these using Baroni and Giacomin's criteria, finding general failure of directionality but partial success in finitary frameworks.


<details>
  <summary>Details</summary>
Motivation: SCC-recursive semantics, effective for finite AFs, fail in infinite settings due to well-foundedness issues. This motivates extending SCC-recursiveness to infinite AFs.

Method: Two approaches to extend SCC-recursiveness to infinite AFs are proposed and evaluated using Baroni and Giacomin's criteria.

Result: Directionality fails in general for infinite AFs, but some semantics satisfy it in finitary frameworks.

Conclusion: The work advances infinite argumentation theory and supports reasoning systems for unbounded or evolving domains.

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [6] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: The paper proposes a systematic framework for constructing high-quality instruction datasets to improve model performance, addressing limitations in coverage and depth of existing datasets.


<details>
  <summary>Details</summary>
Motivation: Current instruction datasets lack sufficient coverage and depth, limiting model performance on complex tasks and rare domains.

Method: A hierarchical labeling system, seed selection algorithm, evolutionary data synthesis, and deficiency diagnosis are integrated into an iterative framework.

Result: The constructed dataset, InfinityInstruct-Subject (~1.5M instructions), improves instruction-following capabilities in experiments.

Conclusion: The framework enables qualitative improvement of instruction datasets, shifting focus from quantity to quality.

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [7] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: The paper proposes a dynamic travel-planning system using three cooperative agents to address gaps in intelligent trip planning, precise navigation, and itinerary adaptation, showing improved performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: Traditional travel-planning systems are static and fragmented, failing to handle real-world complexities like environmental changes and disruptions, leading to poor user experience.

Method: Three agents are introduced: a Travel Planning Agent for multi-modal queries, a Destination Assistant Agent for fine-grained navigation, and a Local Discovery Agent for disruption response using image embeddings and RAG.

Result: Evaluations show significant improvements in query interpretation, navigation accuracy, and disruption resilience.

Conclusion: The system holds promise for applications ranging from urban exploration to emergency response.

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [8] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: FR3E improves RLVR by targeting high-uncertainty points in reasoning, enabling stable training and better LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: RLVR struggles with unstable exploration in LLMs, limiting reasoning improvements.

Method: FR3E identifies uncertain decision points and uses targeted rollouts for grounded feedback.

Result: FR3E enhances stability, response coherence, and correctness in mathematical reasoning benchmarks.

Conclusion: FR3E effectively improves LLM reasoning through structured exploration.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: Proposes HADES-NN, a neural network-based method for estimating parameters in non-autonomous differential equations with abrupt signal changes, improving accuracy and applicability.


<details>
  <summary>Details</summary>
Motivation: Non-autonomous differential equations are hard to fit when signals change abruptly, limiting their practical use.

Method: HADES-NN uses neural networks in two stages: smooth approximation of discontinuous signals, followed by parameter estimation.

Result: Achieves accurate parameter estimates in applications like circadian clocks and yeast mating response.

Conclusion: HADES-NN broadens the scope of models that can be fitted to real-world data.

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [10] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: Decentralized learning with optimized communication scheduling, especially concentrated in later stages, matches centralized training performance and preserves model mergeability.


<details>
  <summary>Details</summary>
Motivation: To address the performance limitations of decentralized learning due to constrained peer-to-peer communication and challenge the belief that it generalizes poorly under data heterogeneity.

Method: Study communication scheduling in decentralized training, focusing on timing and frequency of device synchronization, including a final global merging step.

Result: Concentrating communication in later stages improves generalization; a single global merging matches centralized training performance. Low communication preserves model mergeability.

Conclusion: Decentralized learning can perform as well as centralized methods with proper communication scheduling, offering new insights into model merging and loss landscapes.

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [11] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS is a sample-efficient RL framework for adaptive DBS, improving convergence, beta-band suppression, and hardware compatibility.


<details>
  <summary>Details</summary>
Motivation: Conventional DBS lacks adaptability and efficiency; RL-based aDBS faces high sample complexity and hardware constraints.

Method: SEA-DBS integrates a predictive reward model and Gumbel Softmax for stable exploration in binary action spaces.

Result: Faster convergence, better beta-band suppression, and resilience to hardware constraints like FP16 quantization.

Conclusion: SEA-DBS is a practical RL solution for real-time, resource-constrained adaptive neurostimulation.

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [12] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: The paper presents an imitation learning method for learning constrained maximum entropy policies using expert trajectories, with a focus on KL-divergence bounds and probabilistic inference.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning policies that adhere to constraints demonstrated by experts while maximizing entropy, ensuring robust and generalizable behavior.

Method: The method uses dual gradient descent to optimize a learning objective derived from probabilistic inference, incorporating KL-divergence bounds between expert and learned policies.

Result: Experiments demonstrate the method's effectiveness in learning constraint-compliant policies across diverse settings and constraints.

Conclusion: The proposed approach successfully learns generalizable, constraint-abiding policies, validated through experimental results.

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [13] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux is a deep learning framework for symbolic regression to identify Hamiltonian functions from vector fields using hybrid CNN-LSTM architectures.


<details>
  <summary>Details</summary>
Motivation: To automate the discovery of Hamiltonian functions from vector fields, advancing Hamiltonian mechanics.

Method: Uses hybrid CNN-LSTM architectures to learn and output symbolic expressions of Hamiltonians, trained on new datasets of Hamiltonian vector fields.

Result: Effectively recovers symbolic Hamiltonian expressions, demonstrating accuracy.

Conclusion: SymFlux advances automated discovery in Hamiltonian mechanics by accurately identifying Hamiltonian functions.

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [14] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: The paper introduces DICE, a method to estimate data influence cascades in decentralized learning, addressing the challenge of fair incentives for participation.


<details>
  <summary>Details</summary>
Motivation: Decentralized learning lacks proper incentives, discouraging participation. Fair attribution of contributions is needed but challenging due to influence cascades in decentralized networks.

Method: The authors design DICE to estimate data influence cascades, considering data, communication topology, and loss landscape curvature.

Result: DICE provides tractable approximations of influence cascades and supports applications like collaborator selection and malicious behavior detection.

Conclusion: DICE offers a foundational solution for fair incentives in decentralized learning, with practical applications.

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [15] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: The paper introduces DecoyDB, a large-scale dataset for self-supervised graph contrastive learning (GCL) to improve protein-ligand binding affinity prediction, addressing data scarcity and unique challenges in the field.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, high-quality binding affinity labels hinders progress in drug discovery. Existing datasets like PDBbind are limited, and self-supervised learning, particularly GCL, offers a solution by leveraging unlabeled data.

Method: The authors propose DecoyDB, a dataset with ground truth and decoy protein-ligand complexes, annotated with RMSD values. They also design a customized GCL framework for pre-training graph neural networks on DecoyDB and fine-tuning on labeled data.

Result: Models pre-trained with DecoyDB show superior accuracy, label efficiency, and generalizability compared to existing methods.

Conclusion: DecoyDB and the customized GCL framework effectively address data scarcity and improve binding affinity prediction, advancing drug discovery efforts.

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [16] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: The paper shows that the gradient flow for learning linear convolutional networks can be written as a Riemannian gradient flow on function space, regardless of initialization, for certain conditions.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of gradient flow properties from linear fully connected networks to linear convolutional networks.

Method: Analyzing the gradient flow on parameter space for linear convolutional networks and establishing its equivalence to a Riemannian gradient flow on function space.

Result: The equivalence holds for $D$-dimensional convolutions ($D \geq 2$) and for $D=1$ with strides greater than one, with the Riemannian metric depending on initialization.

Conclusion: The findings generalize the balancedness condition to convolutional networks, providing insights into their optimization dynamics.

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [17] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs dynamically generates and compresses neural network weights, reducing memory usage without significant accuracy loss, using PCA and SVR.


<details>
  <summary>Details</summary>
Motivation: To address the high memory requirements of neural networks, especially for resource-constrained edge applications.

Method: Uses PCA for dimensionality reduction and SVR to predict weights in FC networks, and sensitivity-aware compression for CNNs.

Result: Achieves 53x compression for FC layers, 28x for AlexNet (MNIST), and 18x for AlexNet (CIFAR-10) with minimal accuracy loss.

Conclusion: WINGs offers a memory-efficient, secure, and high-throughput solution for DNN inference on edge devices.

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [18] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: The paper decomposes gradient flow in recurrent dynamical systems into two operators (K and P) to analyze latent dynamics and multi-task alignment, validated experimentally and theoretically.


<details>
  <summary>Details</summary>
Motivation: To rigorously understand the mechanisms shaping learned representations in finite, non-linear recurrent models like RNNs, Neural ODEs, and GRUs.

Method: Decomposes gradient flow into a Parameter Operator (K) and a Linearized Flow Propagator (P), analyzing their interplay.

Result: Demonstrates low-dimensional latent dynamics under GD and measures alignment of multi-task objectives.

Conclusion: Advances theoretical understanding of GD in non-linear recurrent models, supported by experimental validation and a practical toolkit (KPFlow).

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [19] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: The paper evaluates CNN, ResNet, and hybrid Transformer-CNN models for ECG tamper detection and Siamese networks for identity verification, achieving high accuracy (up to 100%) in various scenarios.


<details>
  <summary>Details</summary>
Motivation: Protecting ECG signal integrity against tampering is critical for health monitoring and authentication systems.

Method: Uses CNN, ResNet, and hybrid Transformer-CNN models for tamper detection, and Siamese networks for identity verification. ECG signals are transformed into 2D time-frequency representations using CWT.

Result: Models achieved over 99.5% accuracy for fragmented manipulations and 98% for subtle ones. The hybrid CNN-Transformer Siamese model achieved 100% accuracy for identity verification.

Conclusion: Hybrid models, especially CNN-Transformer, are highly effective for ECG tamper detection and identity verification, ensuring robust security.

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [20] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare is a deep learning framework for predicting clinical outcomes in rare ICU conditions, overcoming data scarcity and heterogeneity by leveraging self-supervised pre-training and a condition knowledge graph. It outperforms existing models and ICU scoring systems.


<details>
  <summary>Details</summary>
Motivation: Rare conditions in the ICU are underserved due to data scarcity and heterogeneity. Existing AI solutions often fail to address these challenges.

Method: KnowRare uses self-supervised pre-training for condition-agnostic learning and a condition knowledge graph to adapt knowledge from similar conditions.

Result: KnowRare outperformed state-of-the-art models and ICU scoring systems across five clinical prediction tasks.

Conclusion: KnowRare is a robust solution for improving care for rare ICU conditions, demonstrating flexibility, generalization, and rational source selection.

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [21] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: eegFloss is an open-source Python package using ML to detect artifacts in sleep EEG, improving sleep study accuracy.


<details>
  <summary>Details</summary>
Motivation: EEG signals in sleep studies are prone to artifacts, leading to erroneous sleep staging.

Method: Introduces eegUsability, an ML model trained on labeled EEG data to detect artifacts, and eegFloss for additional features like time-in-bed detection.

Result: eegUsability achieves high performance (F1-score ~0.85, Cohen's kappa 0.78) and 94% recall in identifying usable EEG data.

Conclusion: eegFloss enhances sleep study precision and reliability by addressing artifact challenges.

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [22] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: Interpretability tools can predict out-of-distribution (OOD) model behavior, with hierarchical attention patterns indicating hierarchical generalization.


<details>
  <summary>Details</summary>
Motivation: To explore if interpretability can predict unseen model behavior, specifically OOD generalization.

Method: Analyzed attention patterns and OOD generalization in hundreds of Transformer models trained on a synthetic task.

Result: Hierarchical in-distribution attention predicts hierarchical OOD generalization, even without direct reliance on these patterns.

Conclusion: Proof-of-concept for interpretability's potential in predicting unseen model behavior, encouraging further research.

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [23] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD is a novel Federated Learning approach for efficiently training Diffusion Models, addressing data heterogeneity and high communication costs with hierarchical FL and structured pruning.


<details>
  <summary>Details</summary>
Motivation: Challenges like high communication costs and data heterogeneity in training Diffusion Models in FL environments are not well-researched.

Method: FedPhD uses Hierarchical FL with homogeneity-aware model aggregation and selection policy, along with distributed structured pruning.

Result: FedPhD reduces communication costs by up to 88%, improves FID scores by at least 34%, and uses only 56% of total resources.

Conclusion: FedPhD effectively trains Diffusion Models in FL, outperforming baselines in performance and efficiency.

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [24] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: The paper introduces an automated framework for labeling neurons in protein language models (PLMs) with biological descriptions, enabling scalable analysis and novel protein generation with desired traits.


<details>
  <summary>Details</summary>
Motivation: Understanding the internal neuron representations of PLMs, which encode rich biological information, is currently limited.

Method: Developed an automated labeling framework for PLM neurons and a neuron activation-guided steering method for protein generation.

Result: Revealed neurons' sensitivity to diverse biochemical properties and enabled targeted protein generation. Also uncovered PLM scaling laws and structured neuron distribution.

Conclusion: The framework advances PLM interpretability and utility in protein design, demonstrating scalability and practical applications.

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [25] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: The paper proposes forward-forward algorithms for binary, stochastic units to reduce energy consumption in neural network training, achieving near real-valued performance with significant energy savings.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning's energy-intensive scaling necessitates alternatives to backpropagation, which is inefficient for hardware accelerators due to serial dependencies and memory demands.

Method: Derives forward-forward algorithms for binary, stochastic units, leveraging binarization for efficient hardware execution and stochasticity to overcome binary limitations. Uses p-bits for fast, cheap binary sampling.

Result: Performance close to real-valued forward-forward on MNIST, Fashion-MNIST, and CIFAR-10, with an estimated energy savings of about one order of magnitude.

Conclusion: The proposed method offers a viable, energy-efficient alternative to backpropagation for training neural networks, particularly suited for custom hardware accelerators.

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [26] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: The paper introduces SignSoftSGD (S3), a novel optimizer that enhances Adam's strengths and mitigates its limitations by using flexible p-th order momentum, unified exponential moving averages, and Nesterov's acceleration. S3 achieves stable training, faster convergence, and better performance.


<details>
  <summary>Details</summary>
Motivation: Adam's empirical success in training deep neural networks is underexplored, and it suffers from destabilizing loss spikes due to uncontrolled update scaling. The study aims to improve Adam's robustness and performance.

Method: Proposes SignSoftSGD (S3) with three innovations: flexible p-th order momentum, unified exponential moving averages to bound updates, and Nesterov's acceleration. Theoretical analysis shows optimal convergence rates.

Result: S3 achieves stable training, faster convergence, and better performance across vision and language tasks, even with aggressive learning rates. It outperforms AdamW with fewer training steps.

Conclusion: S3 is an effective optimizer that enhances Adam's advantages while addressing its limitations, offering improved efficiency and task performance.

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [27] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: FMSP leverages foundation models to enhance self-play, overcoming local optima and fostering diverse, high-quality solutions in multi-agent interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional self-play often gets stuck in local optima and lacks diversity. FMSP aims to address these limitations by using foundation models.

Method: FMSP includes three variants: vFMSP (competitive refinement), NSSP (diverse strategies), and QDSP (combining quality and diversity).

Result: FMSP outperforms human-designed strategies in Car Tag and successfully red-teams LLMs in Gandalf, also patching vulnerabilities.

Conclusion: FMSP is a promising approach for improving self-play, enabling more creative and open-ended strategy discovery.

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [28] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: The paper proposes MimbFD, a dual-view graph representation learning method to address message imbalance in fraud detection caused by fraudsters' behavior obfuscation and class imbalance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the imbalance in supervisory messages due to fraudsters' topological obfuscation and class imbalance, which affects GNN-based fraud detection.

Method: The method includes a topological message reachability module for better node representation and a local confounding debiasing module to balance class influence.

Result: Experiments on three public fraud datasets show MimbFD outperforms existing methods in fraud detection.

Conclusion: MimbFD effectively mitigates message imbalance and improves fraud detection performance by addressing topological and class imbalance issues.

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [29] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: FedDifRC introduces diffusion models into federated learning to address data heterogeneity by leveraging text-driven contrastive learning and noise-driven regularization.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning harms model convergence and performance, prompting the need for innovative solutions like diffusion models.

Method: FedDifRC uses text-driven diffusion contrasting and noise-driven regularization to align local instances with diffusion representations, ensuring semantic guidance and consistent convergence.

Result: Experiments validate FedDifRC's effectiveness in mitigating data heterogeneity and improving model performance across scenarios.

Conclusion: FedDifRC successfully integrates diffusion models into federated learning, offering a robust solution for data heterogeneity with theoretical and empirical support.

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [30] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time integrates time and frequency features in a Mixture of Experts (MoE) network for time series forecasting, achieving state-of-the-art performance on benchmarks and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to model both time and frequency characteristics in pretraining-finetuning paradigms, limiting performance in complex time series forecasting.

Method: MoFE-Time uses time and frequency cells as experts in an MoE network, leveraging pretraining-finetuning to transfer knowledge across datasets with varying periodicity.

Result: MoFE-Time reduces MSE and MAE by 6.95% and 6.02% compared to Time-MoE on benchmarks and excels on a proprietary dataset (NEV-sales).

Conclusion: MoFE-Time is effective for practical applications, outperforming existing methods by integrating time-frequency features and leveraging pretraining-finetuning.

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [31] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: Proposes monotonic post-hoc calibration methods for deep neural networks, ensuring expressiveness, robustness, and interpretability while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often produce miscalibrated, overconfident predictions, and existing calibration methods lack monotonicity guarantees or are under-parameterized/uninterpretable.

Method: Uses a constrained calibration map parameterized linearly with respect to the number of classes, formulated as a constrained optimization problem.

Result: Achieves state-of-the-art performance across datasets with different models, outperforming existing methods while being efficient.

Conclusion: The proposed methods ensure monotonicity, expressiveness, and interpretability, making them superior for post-hoc calibration.

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [32] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: AdaDPIGU is a differentially private SGD framework for deep neural networks, using importance-based gradient updates and adaptive clipping to improve performance in high-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: Existing differential privacy methods degrade performance in high-dimensional settings due to noise scaling with dimensionality.

Method: AdaDPIGU employs a differentially private Gaussian mechanism for parameter importance estimation, prunes low-importance coordinates, and uses adaptive clipping for sparse, noise-efficient updates.

Result: Achieves 99.12% accuracy on MNIST (ε=8) and 73.21% on CIFAR-10 (ε=4), outperforming non-private baselines.

Conclusion: AdaDPIGU enhances privacy and utility in high-dimensional settings, demonstrating adaptive sparsification's effectiveness.

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [33] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM is a magnitude-driven initialization scheme for LoRA, matching spectral methods' performance without their inefficiencies. It unifies hyperparameter tuning and outperforms benchmarks.


<details>
  <summary>Details</summary>
Motivation: Recent spectral initialization methods for LoRA improve performance but introduce computational overhead. The paper aims to address this inefficiency while maintaining performance.

Method: LoRAM uses deterministic orthogonal bases scaled by pretrained weight magnitudes to simulate spectral gains, optimizing update magnitude regulation.

Result: LoRAM matches or outperforms spectral initialization, retaining LoRA's efficiency and unifying hyperparameter tuning.

Conclusion: LoRAM provides an efficient, high-performance alternative to spectral initialization for LoRA, driven by update magnitude optimization.

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [34] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: A novel Bayesian optimization (BO) method is introduced, combining model and acquisition learning to minimize multi-step regret, outperforming traditional BO baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional BO methods rely on separate, hand-crafted components and operate myopically, limiting their effectiveness.

Method: The approach uses an ensemble of GPs and simulated BO trajectories to train a decision transformer for non-myopic query selection, with offline training and online refinement.

Result: The method achieves lower regret and more robust exploration in high-dimensional or noisy settings compared to baselines.

Conclusion: The proposed framework effectively integrates model and acquisition learning, demonstrating superior performance in BO tasks.

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [35] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: The paper introduces LPPO, a framework for reinforcement learning with verifiable rewards (RLVR), focusing on leveraging high-quality demonstrations through prefix-guided sampling and learning-progress weighting to improve LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in LLMs by optimizing the use of trusted demonstrations rather than scaling data volume, inspired by human problem-solving and learning strategies.

Method: Proposes prefix-guided sampling for online data augmentation using expert hints and learning-progress weighting to dynamically adjust training sample influence based on model progression.

Result: Outperforms baselines on mathematical-reasoning benchmarks, achieving faster convergence and higher performance.

Conclusion: LPPO effectively leverages high-quality demonstrations and dynamic learning strategies to advance RLVR in LLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [36] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGCL is a graph contrastive learning framework for AMS circuits, improving transferability and robustness in parasitic estimation tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges like scarce design data, unbalanced labels, and diverse circuit implementations in AMS circuit representation learning.

Method: Uses self-supervised learning with hyperspherical representation scattering and balanced MSE/softmax cross-entropy losses.

Result: Outperforms SOTA methods with significant improvements in edge regression (33.64%~44.20% R²) and node classification (0.9×~2.1× F1-score).

Conclusion: CircuitGCL effectively enhances transferability and robustness for parasitic estimation in AMS circuits.

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [37] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: The paper proposes using 'predicted information gain' to guide exploration in environments with unknown dynamics, leveraging reinforcement learning for effective policies.


<details>
  <summary>Details</summary>
Motivation: Explicit models of controllable dynamics are often unavailable, necessitating learned exploration strategies.

Method: Uses 'predicted information gain' to identify informative regions for exploration and applies reinforcement learning to derive suboptimal policies.

Result: The approach reliably estimates underlying dynamics and outperforms myopic exploration methods.

Conclusion: The method effectively guides exploration in environments without explicit models, improving dynamics estimation.

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [38] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS is a few-shot learning method for predicting parasitic effects in AMS circuits, using graph representation learning and a hybrid graph Transformer. It improves accuracy and reduces error compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of integrated circuit design data limits deep learning models for AMS circuits, necessitating a few-shot learning approach.

Method: CircuitGPS represents circuit netlists as heterogeneous graphs, pre-trains on link prediction, and fine-tunes on edge regression. It uses small-hop sampling and a hybrid graph Transformer with low-cost positional encoding.

Result: CircuitGPS improves coupling existence accuracy by 20% and reduces MAE of capacitance estimation by 0.067. It shows scalability and works via zero-shot learning.

Conclusion: CircuitGPS is effective for parasitic effect prediction in AMS circuits, offering scalability and insights for graph representation learning.

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [39] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: A novel framework balances disentanglement and reconstruction quality in generative models by using multiple β values and a diffusion model for smooth transitions.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between disentangled, interpretable latent representations and high-quality generation in generative models.

Method: Train a single VAE with a new loss function for multiple β values, then use a non-linear diffusion model to transition between representations.

Result: Achieves sharp reconstructions and supports standalone generation, with smooth latent space transitions.

Conclusion: The framework successfully balances disentanglement and generation quality, enabling flexible and high-quality outputs.

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [40] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: CTPG introduces a framework for multi-task RL by using guide policies to leverage cross-task similarities, improving performance with gating mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook direct cross-task guidance; CTPG addresses this by using proficient tasks to guide unmastered ones.

Method: CTPG trains guide policies to select behavior policies from all tasks, with gating mechanisms to filter non-beneficial policies and tasks.

Result: CTPG enhances performance in manipulation and locomotion benchmarks when combined with existing methods.

Conclusion: CTPG effectively leverages cross-task similarities for faster skill acquisition, improving multi-task RL performance.

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [41] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: A deep-learning-based 2-stage model predicts SRAM parasitics pre-layout, reducing errors by up to 19X and speeding up simulations by 598X.


<details>
  <summary>Details</summary>
Motivation: Parasitic effects in SRAM cause discrepancies between pre- and post-layout simulations, leading to design inefficiencies.

Method: Combines GNN classifier and MLP regressors with Focal Loss and subcircuit integration to predict parasitics.

Result: Achieves 19X error reduction and 598X simulation speedup on 4 SRAM designs.

Conclusion: The model effectively predicts parasitics pre-layout, improving design efficiency.

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [42] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill is a novel offline multi-task RL method that extracts reusable skills for better knowledge transfer and task performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in sharing knowledge across tasks in offline multi-task RL inspired the development of GO-Skill, mimicking human knowledge abstraction.

Method: Uses goal-oriented skill extraction, vector quantization for a discrete skill library, skill enhancement for balance, and hierarchical policy learning.

Result: Effective and versatile performance demonstrated on MetaWorld robotic manipulation tasks.

Conclusion: GO-Skill successfully enhances knowledge transfer and task performance in offline multi-task RL.

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [43] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: A novel algorithm for estimating individual treatment effects using disentangled representations, multi-head attention, and orthogonal regularization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurately estimating individual-level treatment effects from observational data, particularly in fields like education and healthcare, where disentangling covariates is crucial.

Method: Proposes a mixture of experts with multi-head attention and a linear orthogonal regularizer for soft decomposition of pre-treatment variables, combined with importance sampling re-weighting to reduce selection bias.

Result: Outperforms state-of-the-art methods on both semi-synthetic and real-world datasets.

Conclusion: The proposed algorithm effectively models causal relationships and improves treatment effect estimation accuracy.

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [44] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: The paper addresses the storage burden of Mixture-of-Experts (MoE) models on edge devices by optimizing expert caching for distributed inference, proposing algorithms for latency minimization with provable guarantees.


<details>
  <summary>Details</summary>
Motivation: The sheer number of expert networks in MoE models creates storage challenges for edge devices, necessitating efficient distributed inference solutions.

Method: Formulates a latency minimization problem under storage constraints, proposing greedy and dynamic programming-based algorithms for expert caching, with a max-convolution technique for acceleration.

Result: Simulations show the proposed method significantly reduces inference latency compared to baselines.

Conclusion: The work provides efficient solutions for MoE model deployment on edge networks, with theoretical guarantees and practical performance improvements.

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [45] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR introduces a method for federated learning that aggregates client models using variance-weighted classifiers and explainability-driven merging without architectural changes, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods require modifications to architecture or loss functions, which LIVAR avoids by leveraging intrinsic training signals.

Method: LIVAR uses variance-weighted classifier aggregation and SHAP-based LoRA merging for model updates.

Result: LIVAR achieves state-of-the-art performance on benchmarks without architectural overhead.

Conclusion: Effective model merging in FL can be done using existing training signals, setting a new standard for efficient aggregation.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [46] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: The paper analyzes prominent prototype models (ProtoPNet, ProtoPool, PIPNet) for explainable AI, introducing new metrics for interpretability and testing them on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive evaluation of prototype models in XAI, addressing gaps in interpretability metrics and performance across varied datasets.

Method: Analyzes prototype models using standard and newly proposed metrics, tested on fine-grained, Non-IID, and multi-label classification datasets.

Result: Demonstrates the models' performance and interpretability, supported by an open-source library for metric application and extensibility.

Conclusion: The study enhances understanding of prototype models in XAI, offering tools for future research and practical applications.

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [47] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: The paper proposes HeLo, a multi-modal emotion distribution learning framework, to address challenges in mining modality heterogeneity and exploiting label correlations in emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Existing emotion distribution learning (EDL) methods struggle with modality heterogeneity and underutilized label correlations, limiting their effectiveness in multi-modal emotion recognition.

Method: HeLo uses cross-attention for physiological data fusion, an OT-based module for heterogeneity mining, and learnable label embeddings with correlation alignment for label correlation learning.

Result: Experiments on two datasets show HeLo's superiority in emotion distribution learning.

Conclusion: HeLo effectively explores modality heterogeneity and label correlations, advancing multi-modal emotion recognition.

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [48] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: A generalization-focused RL framework for RAN control improves performance in diverse 5G scenarios, outperforming traditional methods and specialized RL.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based RRM algorithms underperform in dynamic RAN environments, and data-driven RL policies often overfit to training conditions, failing in unseen scenarios.

Method: The framework uses attention-based graph representations for topology, domain randomization for broader training, and a distributed data-generation and centralized training architecture.

Result: The policy improves throughput and spectral efficiency by ~10% over baselines, with up to 4-fold gains in specific benchmarks, and GAT models outperform MLP baselines by 30%.

Conclusion: The scalable and generalizable RL framework offers a promising approach for AI-native 6G RAN.

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [49] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: A real-time strategy game environment based on Generals.io, compatible with Gymnasium and PettingZoo, achieves top performance with a trained agent using supervised pre-training and self-play.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible yet challenging platform for advancing multi-agent reinforcement learning research.

Method: Built on Generals.io, incorporates potential-based reward shaping and memory features, and trains agents with supervised pre-training and self-play.

Result: The reference agent reaches the top 0.003% of the 1v1 human leaderboard in 36 hours on a single H100 GPU.

Conclusion: The modular RTS benchmark and competitive baseline agent offer a valuable tool for multi-agent reinforcement learning research.

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [50] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra is a generative framework using diffusion models to infer 2D/3D molecular structures from multi-modal spectral data, outperforming traditional and retrieval-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for molecular structure elucidation are unscalable and rely on expert interpretation, while existing machine learning methods lack generalization to novel molecules.

Method: DiffSpectra uses a diffusion-based generative framework with an SE(3)-equivariant denoising network (Diffusion Molecule Transformer) and a transformer-based spectral encoder (SpecFormer) for multi-modal spectral conditioning.

Result: Achieves 16.01% top-1 and 96.86% top-20 accuracy in recovering exact structures, benefiting from 3D geometric modeling and multi-modal conditioning.

Conclusion: DiffSpectra effectively unifies multi-modal spectral reasoning and joint 2D/3D generative modeling, advancing molecular structure elucidation.

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [51] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: ReMix introduces an off-policy RL method for efficient Reinforcement Finetuning of LLMs, reducing training costs while maintaining SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing on-policy RL methods for finetuning LLMs are inefficient due to underutilized past data, leading to high compute and time costs.

Method: ReMix combines mix-policy proximal policy gradient, KL-Convex policy constraint, and policy reincarnation to leverage off-policy data efficiently.

Result: ReMix achieves high accuracy (52.10%-64.39%) on math reasoning benchmarks with significantly reduced training costs (30x-450x less data).

Conclusion: ReMix is a cost-effective, high-performing off-policy RL method for LLM finetuning, with insights into off-policy challenges.

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [52] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: SAD-DPSGD improves medical image classification by addressing data leakage in imbalanced datasets with a linear decaying mechanism for noise and clipping thresholds.


<details>
  <summary>Details</summary>
Motivation: Data leakage in medical image classification, especially in small, imbalanced datasets like HAM10000, causes gradients from minority classes to lose crucial information, leading to suboptimal solutions.

Method: Proposes SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping thresholds, allocating more privacy budget and higher clipping thresholds early in training.

Result: Outperforms Auto-DPSGD on HAM10000, improving accuracy by 2.15% under ε=3.0, δ=10^-3.

Conclusion: SAD-DPSGD effectively mitigates data leakage in imbalanced medical datasets, enhancing model performance.

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [53] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: The paper evaluates whether foundation models truly understand underlying domain structures by testing their adaptability to synthetic datasets derived from postulated world models. It finds that while models excel at training tasks, they often fail to generalize or align with the world model's inductive biases.


<details>
  <summary>Details</summary>
Motivation: To assess if foundation models capture deeper domain understanding beyond surface-level sequence prediction, akin to how Kepler's predictions led to Newtonian mechanics.

Method: Develops an inductive bias probe technique, testing models on synthetic datasets generated from a world model to measure alignment with its inductive biases.

Result: Foundation models often fail to develop or apply the underlying world model's inductive biases when adapted to new tasks, relying instead on task-specific heuristics.

Conclusion: Foundation models may not inherently capture deeper domain structure, highlighting limitations in their generalization and adaptability.

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [54] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD is a universal outlier detection framework that avoids dataset-specific tuning and costly training by leveraging labeled datasets and graph-based node classification.


<details>
  <summary>Details</summary>
Motivation: Existing outlier detection methods require extensive hyperparameter tuning and model training, limiting their practicality. UniOD aims to simplify and improve this process.

Method: UniOD converts datasets into graphs, generates consistent node features, and frames outlier detection as a node-classification task, enabling generalization to unseen domains.

Result: UniOD outperforms 15 state-of-the-art baselines on 15 benchmark datasets, demonstrating its effectiveness.

Conclusion: UniOD reduces computational cost, avoids manual tuning, and leverages historical data, enhancing convenience and accuracy in outlier detection.

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [55] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: The paper analyzes the conditions under which Physics-Informed Neural Networks (PINNs) can achieve low empirical risk with noisy data, proving a lower bound on network size and showing empirical validation.


<details>
  <summary>Details</summary>
Motivation: Understanding the requirements for PINNs to perform effectively with noisy data, as real-world applications often involve noisy samples.

Method: Proves a theoretical lower bound on neural network size for PINNs to achieve empirical risk below noise variance and validates empirically, using the Hamilton–Jacobi–Bellman PDE as a case study.

Result: Shows that increasing noisy labels alone doesn't reduce empirical risk and provides conditions under which PINNs can achieve risks below noise variance.

Conclusion: The findings establish a foundation for quantifying parameter requirements for training PINNs in noisy settings.

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [56] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for interpreting and calibrating differentially private (DP) mechanisms using the hypothesis-testing interpretation ($f$-DP), offering tighter bounds and practical benefits like reduced noise and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mapping DP parameters to privacy risks are overly pessimistic and inconsistent, making DP mechanisms hard to interpret and calibrate.

Method: The work uses the hypothesis-testing interpretation of DP ($f$-DP) to derive unified bounds for re-identification, attribute inference, and data reconstruction risks.

Result: The proposed bounds are consistent, tunable, and empirically tighter than prior methods, reducing required noise by 20% and improving accuracy by over 15pp in tasks like text classification.

Conclusion: The framework provides a principled way to interpret and calibrate DP protection against specific privacy risks, addressing prior limitations.

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [57] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: A method to detect and prevent overfitting in mesh-like data regressions by using Laplace-operator derivatives for hyperparameter optimization and minimizing entropy.


<details>
  <summary>Details</summary>
Motivation: To address overfitting in regression tasks involving mesh-like data structures by leveraging the properties of the Laplace operator.

Method: Compute derivatives of training data on the original mesh for true labels, and derivatives of trained data on a staggered mesh to detect oscillations. Use Laplace-operator derivative loss for hyperparameter optimization.

Result: Reduction of unwanted oscillations and minimized entropy in the trained model, avoiding the need for splitting training data for testing.

Conclusion: The method effectively prevents overfitting by utilizing Laplace-operator derivatives and staggered meshes, providing a robust surrogate testing metric.

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [58] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed is a framework for synthesizing multi-table, time-series EHR data resembling raw records, addressing privacy concerns and outperforming baselines in fidelity and utility.


<details>
  <summary>Details</summary>
Motivation: Privacy and regulatory restrictions limit EHR data sharing, necessitating synthetic datasets that closely mimic raw EHRs.

Method: RawMed uses text-based representation and compression to capture complex structures and temporal dynamics with minimal preprocessing.

Result: Validated on two datasets, RawMed outperforms baselines in fidelity and utility.

Conclusion: RawMed advances EHR synthesis by closely resembling raw data and includes a new evaluation framework for synthetic EHRs.

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [59] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME is a novel MSA design model that enhances protein structure prediction for low-homology and orphan proteins by leveraging evolutionary embeddings and a conservation-diversity loss, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current protein folding models rely heavily on MSAs, limiting their effectiveness for low-homology and orphan proteins due to sparse or unavailable MSA data.

Method: PLAME uses pretrained protein language model embeddings, introduces a conservation-diversity loss, and proposes a novel MSA selection method and sequence quality assessment metric.

Result: PLAME achieves state-of-the-art performance on low-homology and orphan proteins, improves AlphaFold2 and AlphaFold3 predictions, and matches AlphaFold2 accuracy with ESMFold's speed.

Conclusion: PLAME effectively addresses MSA dependency limitations, enhances folding performance, and provides insights into MSA characteristics' impact on prediction quality.

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [60] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: The paper explores enhancing fuzzy systems by integrating machine learning and federated learning techniques to handle uncertainty more effectively, while acknowledging limitations and the need for further research.


<details>
  <summary>Details</summary>
Motivation: Fuzzy systems manage uncertainty better than binary systems, but improvements are possible by leveraging advancements in machine learning and federated learning.

Method: Proposes integrating machine learning for better results and federated learning for privacy and efficiency, focusing on updating fuzzy rules dynamically.

Result: Potential improvements identified, but limitations exist; further investigation is needed to quantify the enhancements.

Conclusion: The proposed enhancements show promise for advancing fuzzy systems, though additional research is required to validate their impact.

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [61] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: The paper proposes Heterogeneous Graph Attention Networks for multi-domain power system state forecasting, outperforming baselines by 35.5%.


<details>
  <summary>Details</summary>
Motivation: Increasing variability in power systems due to renewables necessitates reliable short-term state forecasting for stability and control. Existing GNN methods fail to handle heterogeneous sensor data across domains.

Method: Uses Heterogeneous Graph Attention Networks to model intra-domain and inter-domain relationships in hydraulic and electrical sensor data.

Result: Outperforms conventional baselines by 35.5% in normalized root mean square error.

Conclusion: The method effectively addresses multi-domain, multi-rate forecasting challenges in power systems.

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [62] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: The paper introduces a method for Imitation Learning from Observation (IfO) that handles nuanced data distributions, using a value function to transfer information between expert and non-expert data, improving scalability and robustness.


<details>
  <summary>Details</summary>
Motivation: Current IfO research focuses on idealized scenarios with bimodal-quality data, limiting practical applicability. This paper aims to address more realistic, nuanced distributions to advance scalable behavior learning.

Method: The method adapts RL-based imitation learning to action-free demonstrations, employing a value function to bridge expert and non-expert data.

Result: The study evaluates different data distributions, revealing limitations of existing methods and demonstrating the effectiveness of the proposed approach.

Conclusion: The findings offer insights for developing more robust and practical IfO techniques, moving toward scalable behavior learning through iterative self-improvement.

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [63] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: A novel Adaptive Physics-Informed Neural Network-based Observer (PINN-Obs) is introduced for accurate state estimation in nonlinear systems, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: State estimation in nonlinear systems is challenging with partial/noisy measurements, requiring a robust and adaptive solution.

Method: The framework integrates system dynamics and sensor data into a physics-informed learning process, adaptively learning an optimal gain matrix.

Result: Theoretical convergence guarantees are proven, and simulations on diverse systems (e.g., induction motor, satellite motion) validate superior accuracy and robustness.

Conclusion: PINN-Obs is a highly effective, adaptable, and robust solution for nonlinear state estimation, outperforming existing observer designs.

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [64] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: The MAD framework combines physical laws with data-driven learning to solve differential equations efficiently without relying on labeled datasets, outperforming traditional methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Current methods for solving differential equations using machine learning are limited by data dependency or efficiency-accuracy trade-offs. MAD aims to overcome these by integrating physics and synthetic data.

Method: MAD generates physics-embedded analytical solutions and synthetic data from differential equations' structure, eliminating the need for experimental or simulated training data.

Result: MAD demonstrates superior efficiency and accuracy in 2D parametric problems, handling complex parameter spaces and diverse DE scenarios.

Conclusion: MAD offers a universal paradigm for physics-informed machine intelligence, combining rigor and scalability for scientific computing.

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [65] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: Proposes a smooth, trainable activation function (Leaky Exponential Linear Unit) for better performance in nonlinear data regression, addressing issues like overfitting and sensitivity to parameters. Introduces a diffusion-loss metric for evaluating model performance.


<details>
  <summary>Details</summary>
Motivation: Nonlinear activation functions are essential for learning nonlinear datasets, but existing ones (e.g., ELU, SiLU, ReLU) have limitations like vanishing gradients or discontinuity, impacting model performance.

Method: Introduces a smooth, trainable activation function (Leaky Exponential Linear Unit) with non-zero gradients and proposes a diffusion-loss metric to assess overfitting.

Result: Demonstrates improved performance with the new activation function, addressing issues like overfitting and parameter sensitivity.

Conclusion: The proposed activation function and diffusion-loss metric enhance model performance in nonlinear regression tasks.

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [66] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: The paper introduces interpretable topological generalization bounds for stochastic optimization algorithms, avoiding intractable mutual information terms by using trajectory stability and topological data analysis (TDA).


<details>
  <summary>Details</summary>
Motivation: Existing topological generalization bounds rely on complex information-theoretic terms, limiting their practicality. The paper aims to simplify these bounds using trajectory stability.

Method: The authors propose a framework based on algorithmic stability, extending hypothesis set stability to trajectory stability. They bound generalization error using TDA quantities and trajectory stability parameters.

Result: Experiments show TDA terms in the bound are crucial, especially with more training samples, explaining the empirical success of topological bounds.

Conclusion: The paper provides a practical and interpretable alternative to existing topological generalization bounds, validated by empirical results.

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [67] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: A novel speech tokenizer is introduced, encoding both linguistic and acoustic features, enhancing fidelity in speech processing tasks like coding, voice conversion, and emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Existing RVQ-based tokenizers often overlook acoustic features, limiting their effectiveness in preserving prosodic and emotional content.

Method: The proposed approach simultaneously encodes linguistic and acoustic information, improving speech representation.

Result: Empirical evaluations show effectiveness in diverse tasks (speech coding, voice conversion, etc.) without extra training.

Conclusion: The method is versatile and promising for advancing AI-driven speech processing.

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [68] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: This paper proposes combining iterative methods and pathwise conditioning to scale Gaussian processes for large data and modern hardware.


<details>
  <summary>Details</summary>
Motivation: Classical Gaussian processes struggle with scalability for large datasets and parallel computation.

Method: Uses iterative linear system solvers and pathwise conditioning to reduce memory and leverage matrix multiplication.

Result: Enables Gaussian processes to handle larger datasets efficiently on modern hardware.

Conclusion: The synergy of iterative methods and pathwise conditioning significantly improves scalability for Gaussian processes.

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [69] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: The paper studies an online decision-making problem with contextual bandit-with-knapsack (BwK) instances, featuring non-stationary contexts and shared latent conversion models. It proposes an algorithm achieving sub-linear regret in the number of episodes, leveraging a confidence bound oracle.


<details>
  <summary>Details</summary>
Motivation: The work addresses dynamic decision-making in non-stationary environments, such as dynamic pricing and auctions, where resource constraints and varying contexts complicate optimization.

Method: The authors design an online algorithm assuming access to a confidence bound oracle, tackling challenges like unbounded state spaces and non-stationary contexts.

Result: The algorithm achieves sub-linear regret in the number of episodes, with improved bounds when unlabeled feature data is available.

Conclusion: The framework advances contextual BwK literature by handling non-stationarity and leveraging unlabeled data, offering practical applications in dynamic resource allocation.

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [70] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: The paper introduces a federated causal discovery method for both horizontal and vertical settings, using higher-order cumulants to address spurious relationships and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on horizontal federated settings, but real-world scenarios often involve incomplete variable sets across clients, leading to spurious causal relationships.

Method: The approach aggregates higher-order cumulant information from clients to construct global estimates, enabling recursive source identification and causal strength matrix derivation.

Result: The algorithm outperforms others in synthetic and real-world data experiments, accurately reconstructing causal graphs and estimating causal strengths.

Conclusion: The proposed method effectively addresses limitations of existing federated causal discovery approaches, offering robust performance in diverse settings.

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [71] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: A reinforcement learning-based method (RL-Window) dynamically optimizes sliding window sizes for multi-dimensional data streams, outperforming state-of-the-art techniques in accuracy, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Multi-dimensional data streams' high velocity, unbounded nature, and complex dependencies challenge fixed-size sliding windows, necessitating adaptive solutions for dynamic changes like concept drift.

Method: Proposes RL-Window, using a Dueling Deep Q-Network (DQN) with prioritized experience replay to learn adaptive window sizes based on stream characteristics (variance, correlations, trends).

Result: Outperforms ADWIN and CNN-Adaptive in classification accuracy, drift robustness, and computational efficiency on benchmark datasets (UCI HAR, PAMAP2, Yahoo! Finance Stream).

Conclusion: RL-Window's adaptability, stability, and efficiency make it suitable for real-time applications, supported by qualitative analyses and extended metrics.

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [72] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: Proposes an N-version machine learning (NVML) framework with safety-aware weighted soft voting to enhance traffic sign recognition robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in autonomous driving systems, particularly traffic sign recognition, which is vulnerable to adversarial attacks.

Method: Uses Failure Mode and Effects Analysis (FMEA) to assess risks and assigns dynamic weights in an ensemble. Evaluated against FGSM and PGD attacks.

Result: NVML significantly improves robustness and safety of traffic sign recognition under adversarial conditions.

Conclusion: The NVML framework effectively enhances the safety and robustness of autonomous driving systems against adversarial threats.

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [73] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: The paper introduces two novel loss functions, MV-InfoNCE and MV-DHEL, to address limitations in multi-view contrastive learning, improving alignment, uniformity, and scalability with view multiplicity.


<details>
  <summary>Details</summary>
Motivation: Current multi-view contrastive learning methods handle additional views suboptimally, leading to conflicting objectives, limited interactions, and alignment-uniformity coupling.

Method: Proposes MV-InfoNCE and MV-DHEL, which incorporate all view interactions and decouple alignment from uniformity, respectively.

Result: Empirical results show superior performance on ImageNet1K and other datasets, scalability with view multiplicity, and mitigation of dimensionality collapse.

Conclusion: The methods effectively address limitations of multi-view contrastive learning, enabling benefits observed in supervised settings.

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [74] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: The paper evaluates the accuracy of diffusion models in solving Gaussian deblurring tasks by comparing their outputs to theoretical solutions using Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: To address unresolved questions about the performance of diffusion models in Bayesian inverse problems, particularly for Gaussian deblurring.

Method: Analyze the discrepancy between theoretical and diffusion model solutions by computing the exact Wasserstein distance between their distributions.

Result: Provides precise comparisons of different algorithms' performance in solving the inverse problem.

Conclusion: The study offers insights into the accuracy of diffusion models and enables algorithm comparisons for Gaussian deblurring.

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [75] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: The paper explores edge-side model training on resource-limited smart meters, focusing on grid-edge intelligence and on-device training. It presents a case study on photovoltaic power forecasting using gradient boosting trees and recurrent neural networks, with precision-adapted training schemes. Results show feasibility for grid-edge intelligence.


<details>
  <summary>Details</summary>
Motivation: The study aims to enable grid-edge intelligence by leveraging on-device training on resource-limited smart meters, addressing the need for efficient and localized energy forecasting.

Method: The paper describes technical steps for on-device training and evaluates gradient boosting trees and recurrent neural networks for photovoltaic power forecasting. It introduces "mixed"- and "reduced"-precision training schemes to adapt to resource constraints.

Result: Experiments confirm the feasibility of achieving grid-edge intelligence economically using existing advanced metering infrastructures.

Conclusion: The study successfully demonstrates the practicality of on-device training for grid-edge intelligence, even under resource limitations.

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [76] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: The paper explores self-supervised learning (SSL) for edge devices, balancing performance and energy efficiency, and shows tailored SSL can reduce resource use by 4X.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying contrastive learning (CL) and SSL on resource-constrained edge devices due to high data and computational demands.

Method: Analyzes SSL techniques under limited computational, data, and energy budgets, and evaluates semi-supervised learning for reducing energy costs.

Result: Tailored SSL strategies achieve competitive performance while reducing resource consumption by up to 4X.

Conclusion: SSL is viable for energy-efficient learning on edge devices, with potential for significant resource savings.

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [77] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: The paper introduces an ensemble embedding approach for semantic caching in LLM systems, outperforming single-model methods by improving cache hit ratios and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing semantic caching frameworks use single embedding models, limiting their ability to capture diverse semantic relationships in queries.

Method: An ensemble embedding approach combines multiple models via a trained meta-encoder to enhance semantic similarity detection.

Result: Achieves 92% cache hit ratio for equivalent queries and 85% accuracy in rejecting non-equivalent ones, outperforming single-model methods.

Conclusion: Ensemble embedding significantly improves caching performance and reduces computational overhead in LLM systems.

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [78] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: The paper introduces the Dual-Balance Collaborative Experts (DCE) framework to address challenges in Domain-Incremental Learning (DIL) caused by intra-domain class imbalance and cross-domain shifts.


<details>
  <summary>Details</summary>
Motivation: DIL struggles with imbalanced data, where intra-domain imbalance underfits few-shot classes, and cross-domain shifts complicate knowledge retention and transfer.

Method: DCE uses a frequency-aware expert group with specialized loss functions and a dynamic expert selector leveraging balanced Gaussian sampling for pseudo-features.

Result: DCE achieves state-of-the-art performance on four benchmark datasets.

Conclusion: DCE effectively balances preserving historical knowledge and improving few-shot class performance in evolving domains.

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [79] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: Small batch sizes in language model training are stable and robust, outperforming larger batches per-FLOP. Scaling Adam hyperparameters for small batches is proposed, and SGD is viable without momentum. Gradient accumulation is discouraged unless necessary.


<details>
  <summary>Details</summary>
Motivation: Challenge the belief that small batch sizes are unstable in language model training and explore their potential advantages.

Method: Revisit small batch sizes (down to size one), propose scaling rules for Adam hyperparameters, and evaluate stability and performance.

Result: Small batches train stably, are robust to hyperparameters, perform better per-FLOP, and enable stable SGD training without momentum.

Conclusion: Small batch sizes are viable and often superior; gradient accumulation is unnecessary unless bandwidth-limited.

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [80] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: Compositional generalization in vision models is driven by data diversity, not scale. A linearly factored representational structure enables efficient learning. Pretrained models show partial but imperfect presence of this structure.


<details>
  <summary>Details</summary>
Motivation: To understand if contemporary vision models exhibit compositional generalization and whether scaling data and model sizes improves it.

Method: Controlled experiments varying data scale, concept diversity, and combination coverage, analyzing pretrained models (DINO, CLIP).

Result: Compositional generalization is driven by data diversity, not scale. Linearly factored representations enable perfect generalization from few combinations. Pretrained models show partial but imperfect performance.

Conclusion: Emphasize diverse datasets and linearly factored representational structures for efficient compositional learning.

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [81] [A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes](https://arxiv.org/abs/2507.06278)
*Kemboi Cheruiyot,Nickson Kiprotich,Vyacheslav Kungurtsev,Kennedy Mugo,Vivian Mwirigi,Marvin Ngesa*

Main category: cs.MA

TL;DR: A survey of three AI agent interaction topologies in autonomous systems: Federal RL (centrally coordinated), Decentralized RL (ad-hoc), and Noncooperative RL, comparing their structures, guarantees, and performance.


<details>
  <summary>Details</summary>
Motivation: To address the growing complexity of multiple AI agents interacting in environments, this paper surveys three key interaction topologies.

Method: The paper reviews and compares Federal RL, Decentralized RL, and Noncooperative RL, analyzing their formulations, theoretical guarantees, and numerical performance.

Result: The survey highlights structural similarities, distinctions, and limitations of each RL approach in multi-agent settings.

Conclusion: The paper provides a comprehensive overview of RL frameworks for multi-agent interactions, emphasizing recent advancements and open challenges.

Abstract: The increasing interest in research and innovation towards the development of
autonomous agents presents a number of complex yet important scenarios of
multiple AI Agents interacting with each other in an environment. The
particular setting can be understood as exhibiting three possibly topologies of
interaction - centrally coordinated cooperation, ad-hoc interaction and
cooperation, and settings with noncooperative incentive structures. This
article presents a comprehensive survey of all three domains, defined under the
formalism of Federal Reinforcement Learning (RL), Decentralized RL, and
Noncooperative RL, respectively. Highlighting the structural similarities and
distinctions, we review the state of the art in these subjects, primarily
explored and developed only recently in the literature. We include the
formulations as well as known theoretical guarantees and highlights and
limitations of numerical performance.

</details>


### [82] [Learning To Communicate Over An Unknown Shared Network](https://arxiv.org/abs/2507.06499)
*Shivangi Agarwal,Adi Asija,Sanjit K. Kaul,Arani Bhattacharya,Saket Anand*

Main category: cs.MA

TL;DR: A DRL model called QNet is proposed for agents to decide communication with edge-cloud nodes in shared wireless networks, trained via a simulation-to-real framework, and validated on WiFi and cellular networks.


<details>
  <summary>Details</summary>
Motivation: Agents in shared wireless networks lack awareness of network state, which is dynamic and influenced by other agents, making it challenging to optimize communication decisions.

Method: Proposes QNet, a DRL model, trained using a simulation-to-real framework with a single parameter to generalize across network configurations. Includes a learning algorithm to address training challenges.

Result: QNet is validated on real WiFi and cellular networks, showing efficacy in varying conditions (5-50 agents for WiFi, RTT 0.07-0.83s for cellular).

Conclusion: QNet effectively generalizes across network configurations and outperforms other policies, demonstrating the success of the simulation-to-real approach.

Abstract: As robots (edge-devices, agents) find uses in an increasing number of
settings and edge-cloud resources become pervasive, wireless networks will
often be shared by flows of data traffic that result from communication between
agents and corresponding edge-cloud. In such settings, agent communicating with
the edge-cloud is unaware of state of network resource, which evolves in
response to not just agent's own communication at any given time but also to
communication by other agents, which stays unknown to the agent. We address
challenge of an agent learning a policy that allows it to decide whether or not
to communicate with its cloud node, using limited feedback it obtains from its
own attempts to communicate, to optimize its utility. The policy generalizes
well to any number of other agents sharing the network and must not be trained
for any particular network configuration. Our proposed policy is a DRL model
Query Net (QNet) that we train using a proposed simulation-to-real framework.
Our simulation model has just one parameter and is agnostic to specific
configurations of any wireless network. It allows training an agent's policy
over a wide range of outcomes that an agent's communication with its edge-cloud
node may face when using a shared network, by suitably randomizing the
simulation parameter. We propose a learning algorithm that addresses challenges
observed in training QNet. We validate our simulation-to-real driven approach
through experiments conducted on real wireless networks including WiFi and
cellular. We compare QNet with other policies to demonstrate its efficacy. WiFi
experiments involved as few as five agents, resulting in barely any contention
for the network, to as many as fifty agents, resulting in severe contention.
The cellular experiments spanned a broad range of network conditions, with
baseline RTT ranging from a low of 0.07 second to a high of 0.83 second.

</details>


### [83] [Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration](https://arxiv.org/abs/2507.06520)
*Xinyuan Song,Zeyu Wang,Siyi Wu,Tianyu Shi,Lynn Ai*

Main category: cs.MA

TL;DR: Gradientsys is a multi-agent scheduling framework using MCP and ReAct for dynamic planning, achieving higher success rates and efficiency compared to baselines.


<details>
  <summary>Details</summary>
Motivation: To coordinate diverse AI agents efficiently, handle failures gracefully, and ensure transparency in multi-agent systems.

Method: Uses a typed Model-Context Protocol (MCP), ReAct-based dynamic planning, LLM-powered scheduling, and hybrid execution with retry mechanisms.

Result: Outperforms MinionS-style baselines in task success rates, latency, and API costs on the GAIA benchmark.

Conclusion: Gradientsys demonstrates effective LLM-driven multi-agent orchestration with improved performance and transparency.

Abstract: We present Gradientsys, a next-generation multi-agent scheduling framework
that coordinates diverse specialized AI agents using a typed Model-Context
Protocol (MCP) and a ReAct-based dynamic planning loop. At its core,
Gradientsys employs an LLM-powered scheduler for intelligent one-to-many task
dispatch, enabling parallel execution of heterogeneous agents such as PDF
parsers, web search modules, GUI controllers, and web builders. The framework
supports hybrid synchronous/asynchronous execution, respects agent capacity
constraints, and incorporates a robust retry-and-replan mechanism to handle
failures gracefully. To promote transparency and trust, Gradientsys includes an
observability layer streaming real-time agent activity and intermediate
reasoning via Server-Sent Events (SSE). We offer an architectural overview and
evaluate Gradientsys against existing frameworks in terms of extensibility,
scheduling topology, tool reusability, parallelism, and observability.
Experiments on the GAIA general-assistant benchmark show that Gradientsys
achieves higher task success rates with reduced latency and lower API costs
compared to a MinionS-style baseline, demonstrating the strength of its
LLM-driven multi-agent orchestration.

</details>


### [84] [Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments](https://arxiv.org/abs/2507.07074)
*Farhaan Ebadulla,Dharini Hindlatti,Srinivaasan NS,Apoorva VH,Ayman Aftab*

Main category: cs.MA

TL;DR: A graph-based coordination complexity metric for MARL is introduced, validated with strong correlation to empirical task difficulty, and applied in curriculum learning for improved performance in cooperative tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of validated task complexity metrics in MARL for cooperative coordination, hindering effective curriculum design.

Method: Proposes a metric combining agent dependency entropy, spatial interference, and goal overlap to predict task difficulty. Validated using MADDPG in MultiWalker and Simple Spread environments.

Result: Achieves 56x performance improvement in tight coordination tasks and systematic progression in cooperative navigation, with a strong correlation (rho = 0.952) between predicted complexity and empirical difficulty.

Conclusion: The complexity metric and curriculum framework enhance MARL performance, especially in tightly interdependent environments, offering guidelines for multi-robot coordination.

Abstract: Multi-agent reinforcement learning (MARL) faces significant challenges in
task sequencing and curriculum design, particularly for cooperative
coordination scenarios. While curriculum learning has demonstrated success in
single-agent domains, principled approaches for multi-agent coordination remain
limited due to the absence of validated task complexity metrics. This approach
presents a graph-based coordination complexity metric that integrates agent
dependency entropy, spatial interference patterns, and goal overlap analysis to
predict task difficulty in multi-agent environments. The complexity metric
achieves strong empirical validation with rho = 0.952 correlation (p < 0.001)
between predicted complexity and empirical difficulty determined by random
agent performance evaluation. This approach evaluates the curriculum learning
framework using MADDPG across two distinct coordination environments: achieving
56x performance improvement in tight coordination tasks (MultiWalker) and
demonstrating systematic task progression in cooperative navigation (Simple
Spread). Through systematic analysis, coordination tightness emerges as a
predictor of curriculum learning effectiveness, where environments requiring
strict agent interdependence benefit substantially from structured progression.
This approach provides a validated complexity metric for multi-agent curriculum
design and establishes empirical guidelines for multi-robot coordination
applications.

</details>
