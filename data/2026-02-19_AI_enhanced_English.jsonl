{"id": "2602.16012", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.16012", "abs": "https://arxiv.org/abs/2602.16012", "authors": ["Jieyi Bi", "Zhiguang Cao", "Jianan Zhou", "Wen Song", "Yaoxin Wu", "Jie Zhang", "Yining Ma", "Cathy Wu"], "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems", "comment": "Accepted by ICLR 2026", "summary": "Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.", "AI": {"tldr": "CaR is a general and efficient constraint-handling framework for neural routing solvers using explicit learning-based feasibility refinement, outperforming state-of-the-art methods in feasibility, quality, and efficiency.", "motivation": "Current neural solvers struggle with complex constraints due to inefficient constraint-handling schemes like feasibility masking or implicit feasibility awareness, which are inadequate for hard constraints.", "method": "Proposes Construct-and-Refine (CaR), a joint training framework with a construction module generating diverse, high-quality solutions for a lightweight improvement process (e.g., 10 steps vs. prior 5k steps), and introduces construction-improvement-shared representation to unify the encoder for knowledge sharing.", "result": "Evaluation on typical hard routing constraints shows CaR achieves superior feasibility, solution quality, and efficiency compared to classical and neural state-of-the-art solvers.", "conclusion": "CaR provides an effective and efficient approach for handling hard constraints in neural routing solvers, broadening their applicability in complex scenarios."}}
{"id": "2602.16037", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16037", "abs": "https://arxiv.org/abs/2602.16037", "authors": ["Cameron Cagan", "Pedram Fard", "Jiazi Tian", "Jingya Cheng", "Shawn N. Murphy", "Hossein Estiri"], "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection", "comment": null, "summary": "Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.", "AI": {"tldr": "Autonomous agentic workflows can degrade classifier performance via optimization instability, especially in low-prevalence tasks; retrospective selection with a selector agent effectively prevents failure and outperforms expert methods.", "motivation": "To characterize failure modes in autonomous agentic workflows, specifically optimization instability where continued autonomous improvement paradoxically reduces classifier performance, using Pythia for automated prompt optimization in clinical symptom classification.", "method": "Evaluated three clinical symptoms with varying prevalence using Pythia. Tested interventions: a guiding agent for active redirection and a selector agent for retrospective identification of best-performing iterations. Compared performance with expert-curated lexicons.", "result": "Validation sensitivity oscillated between 1.0 and 0.0, with severity inversely related to class prevalence (e.g., 95% accuracy but zero positive cases at 3% prevalence). Guiding agent amplified overfitting; selector agent prevented catastrophic failure, outperforming expert lexicons by 331% (F1) for brain fog and 7% for chest pain.", "conclusion": "Optimization instability is a critical failure mode in autonomous AI systems; retrospective selection via selector agents stabilizes performance better than active intervention, particularly for low-prevalence classification tasks, enabling robust autonomous workflows."}}
{"id": "2602.16039", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16039", "abs": "https://arxiv.org/abs/2602.16039", "authors": ["Hang Li", "Kaiqi Yang", "Xianxuan Long", "Fedor Filippov", "Yucheng Chu", "Yasemin Copur-Gencturk", "Peng He", "Cory Miller", "Namsoo Shin", "Joseph Krajcik", "Hui Liu", "Jiliang Tang"], "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment", "comment": null, "summary": "The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.", "AI": {"tldr": "This paper benchmarks uncertainty quantification methods for LLM-based automatic assessment in education, analyzing their patterns and impacts to inform future grading systems.", "motivation": "Large language models (LLMs) offer advantages in automatic educational assessment but introduce output uncertainty due to their probabilistic nature, which can lead to unstable pedagogical interventions if not properly calibrated, necessitating a systematic study.", "method": "The study benchmarks various uncertainty quantification methods, conducting comprehensive analyses across multiple assessment datasets, LLM families, and generation control settings to characterize uncertainty patterns in grading scenarios.", "result": "The research characterizes uncertainty patterns exhibited by LLMs in grading, evaluates the strengths and limitations of different uncertainty metrics, and analyzes the influence of key factors like model families, assessment tasks, and decoding strategies on uncertainty estimates.", "conclusion": "The study provides actionable insights into uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable uncertainty-aware grading systems, addressing challenges for future educational applications."}}
{"id": "2602.16662", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16662", "abs": "https://arxiv.org/abs/2602.16662", "authors": ["Richard Willis", "Jianing Zhao", "Yali Du", "Joel Z. Leibo"], "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents", "comment": null, "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.", "AI": {"tldr": "A framework for evaluating collective behavior of LLM-powered agents in social dilemmas, showing newer models may worsen societal outcomes when prioritizing individual gain.", "motivation": "To understand collective behavior of LLM-powered autonomous agents in social dilemmas, as their deployment increases in society, with a need for scalable evaluation.", "method": "Introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection before deployment and scaling to hundreds of agents, using cultural evolution to model user selection.", "result": "More recent models tend to produce worse societal outcomes than older models when agents prioritize individual gain over collective benefits; simulations show risk of convergence to poor societal equilibria, especially with diminishing cooperation benefits and larger populations.", "conclusion": "There is a significant risk of poor societal equilibria in LLM agent behavior, prompting the release of an evaluation suite for developers to assess collective outcomes."}}
{"id": "2602.16050", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16050", "abs": "https://arxiv.org/abs/2602.16050", "authors": ["Amir Hosseinian", "MohammadReza Zare Shahneh", "Umer Mansoor", "Gilbert Szeto", "Kirill Karlin", "Nima Aghaeepour"], "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination", "comment": null, "summary": "Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.", "AI": {"tldr": "Mirror, an evidence-grounded clinical reasoning system, outperformed frontier LLMs and human performance on an endocrinology exam, achieving high accuracy and citation traceability.", "motivation": "Large language models struggle with subspecialty clinical reasoning due to evolving guidelines and nuanced evidence, requiring improved systems for reliable medical decision-making.", "method": "Evaluated Mirror against GPT-5, GPT-5.2, and Gemini-3-Pro on a 120-question endocrinology board exam, using a curated evidence corpus with structured reasoning under closed-evidence constraints.", "result": "Mirror achieved 87.5% accuracy, surpassing human reference (62.3%) and LLMs (e.g., GPT-5.2 at 74.6%), with 76.7% on difficult questions and 74.2% outputs citing guideline sources accurately.", "conclusion": "Curated evidence with explicit provenance enhances clinical reasoning over unconstrained web retrieval, supporting auditability for deployment in subspecialty medicine."}}
{"id": "2602.16678", "categories": ["cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16678", "abs": "https://arxiv.org/abs/2602.16678", "authors": ["Harrison Perone", "Christopher W. Hays"], "title": "Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems", "comment": "14 pages, 4 figures. Submitted to the 48th Rocky Mountain American Astronautical Society's Guidance, Navigation and Control Conference", "summary": "In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.", "AI": {"tldr": "Decentralized task allocation algorithm for coordinating satellite observations to track objects using angles-only measurements, improving efficiency.", "motivation": "Ground-based tracking may not achieve required accuracy for close-proximity satellites and debris, necessitating space-based sensors with coordinated communication for better relative state estimates.", "method": "Uses a decentralized task allocation algorithm to schedule and coordinate observations among multiple satellites with limited field of view angles-only measurements.", "result": "Numerical simulations show the new method significantly outperforms current approaches in terms of fuel usage and catalog uncertainty, enhancing the Pareto frontier.", "conclusion": "The proposed decentralized approach effectively solves the observation scheduling problem, improving safety and efficiency for satellite missions through better relative state estimation."}}
