<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: AlphaEvolve discovers interpretable models of human and LLM behavior in strategic games, revealing LLMs can exhibit deeper strategic thinking than humans in iterated rock-paper-scissors.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly deployed in social and strategic scenarios, creating a need to understand where and why their behavior diverges from humans. Existing behavioral game theory models don't fully capture idiosyncratic human or black-box LLM behavior.

Method: Use AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, enabling open-ended discovery of structural factors driving behavior.

Result: Analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans.

Conclusion: The research provides a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions, with implications for deploying LLMs in social and strategic contexts.

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [2] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: LiveMedBench is a dynamic, contamination-free benchmark for evaluating LLMs in clinical settings, using real-world cases and rubric-based assessment. It reveals LLMs perform poorly (best model: 39.2%) and struggle with contextual application, not just factual knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks for LLMs are static and suffer from data contamination and temporal misalignment, failing to capture rapid medical knowledge evolution. Current evaluation metrics for open-ended clinical reasoning rely on shallow lexical overlap or subjective LLM-as-a-Judge scoring, which are inadequate for verifying clinical correctness. There is a need for a rigorous, contamination-free, and continuously updated benchmark to reliably assess LLMs in high-stakes clinical settings.

Method: The authors introduce LiveMedBench, a continuously updated benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from training data. They employ a Multi-Agent Clinical Curation Framework to filter noise and validate clinical integrity against evidence-based principles. For evaluation, they develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving strong alignment with expert physicians.

Result: LiveMedBench comprises 2,756 real-world cases across 38 medical specialties and multiple languages, with 16,702 unique evaluation criteria. Evaluation of 38 LLMs shows the best-performing model achieves only 39.2% accuracy, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis identifies contextual application as the dominant bottleneck, with 35-48% of failures due to inability to tailor medical knowledge to patient-specific constraints.

Conclusion: LiveMedBench addresses critical gaps in LLM evaluation for clinical settings by providing a dynamic, contamination-free, and rubric-based benchmark. It reveals significant performance limitations of current LLMs, with the best model achieving only 39.2% accuracy, and highlights contextual application as the primary bottleneck rather than factual knowledge. The benchmark's continuous update mechanism and rigorous evaluation framework offer a more reliable tool for assessing LLMs in evolving medical contexts.

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [3] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL: A platform that efficiently integrates Vision-Language Models into RL for autonomous driving using asynchronous batch inference and specialized supervision mechanisms to overcome VLM latency issues while maintaining semantic interpretability.


<details>
  <summary>Details</summary>
Motivation: RL is sample-inefficient and lacks semantic interpretability in complex autonomous driving scenarios. VLMs offer rich context-aware knowledge but have high inference latency that hinders integration into high-frequency RL training loops.

Method: Proposes Found-RL with: 1) Asynchronous batch inference framework to decouple heavy VLM reasoning from simulation loop, 2) Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to distill VLM expert suggestions into RL policy, 3) High-throughput CLIP for dense reward shaping with Conditional Contrastive Action Alignment to address CLIP's dynamic blindness.

Result: Found-RL enables lightweight RL models to achieve near-VLM performance compared to billion-parameter VLMs while sustaining real-time inference (~500 FPS), effectively resolving latency bottlenecks for real-time learning.

Conclusion: Found-RL bridges the gap between RL and foundation models for autonomous driving by efficiently integrating VLM knowledge through asynchronous processing and specialized supervision mechanisms, enabling sample-efficient, semantically interpretable learning with real-time capability.

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [4] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: A new utility-feedback framework with AgoraBench benchmark and human-preference learning pipeline improves LLMs' bargaining by aligning strategies with human preferences.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with bargaining due to limited strategic depth and difficulty adapting to complex human factors, and current benchmarks fail to capture these limitations.

Method: Developed AgoraBench, a benchmark with nine challenging negotiation settings (e.g., deception, monopoly), and introduced human-aligned metrics based on utility theory (agent utility, negotiation power, acquisition ratio). Created a human-preference grounded dataset and learning pipeline for prompting and finetuning LLMs.

Result: Baseline LLM strategies often diverge from human preferences, while the proposed mechanism improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

Conclusion: The proposed utility feedback framework, AgoraBench benchmark, and human-preference grounded learning pipeline substantially enhance LLMs' bargaining performance by aligning them with human strategic preferences and improving opponent awareness.

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [5] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: LLMs can generate Qualitative Numerical Planning abstractions for generalized planning with automated debugging to fix abstraction errors.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Large Language Models can serve as QNP abstraction generators for generalized planning problems and develop methods to fix abstractions through automated debugging.

Method: Proposed a prompt protocol where LLMs are given GP domains and training tasks to generate abstract features and create QNP abstractions, combined with an automated debugging method to detect and guide fixes for abstraction errors.

Result: Experiments show that with proper guidance from automated debugging, some LLMs can generate useful QNP abstractions for generalized planning problems.

Conclusion: LLMs can function as QNP abstraction generators when properly guided by automated debugging methods, enabling them to create useful abstractions for generalized planning.

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [6] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: FoSS introduces a GFlowNet framework for span generation that creates a DAG state space with dynamic span vocabulary, enabling diverse compositional paths and improving text generation quality.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive language models have tree-structured state spaces that limit flexibility, while existing dynamic vocabulary approaches overlook that sentences can be composed of spans of varying lengths, lacking explicit DAG modeling and causing restricted exploration of compositional paths with bias.

Method: FoSS constructs a dynamic span vocabulary by flexibly segmenting retrieved text to ensure DAG-structured state space, allowing GFlowNets to explore diverse compositional paths and improve generalization. Uses specialized reward models to generate diverse, high-quality text.

Result: Empirically improves MAUVE scores by up to 12.5% over Transformer on text generation, achieves 3.5% gains on knowledge-intensive tasks, consistently outperforms SOTA methods. Scaling experiments show FoSS benefits from larger models, more data, and richer retrieval corpora.

Conclusion: FoSS provides a principled GFlowNet framework for span generation that addresses limitations of existing approaches by creating DAG state spaces, enabling better exploration of compositional paths and improving text generation performance across various tasks.

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [7] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: NSAM is a neuro-symbolic framework that automatically learns symbolic models from high-dimensional states during DRL to mask infeasible actions, improving sample efficiency and reducing constraint violations.


<details>
  <summary>Details</summary>
Motivation: Existing DRL methods require manual specification of action masking and assume a symbol grounding function, which limits practicality and adaptability in constrained domains.

Method: The proposed Neuro-symbolic Action Masking (NSAM) framework learns symbolic models consistent with domain constraints in a minimally supervised manner during DRL, using these models to generate action masks that exclude infeasible actions.

Result: NSAM is evaluated on multiple domains with constraints, showing significant improvement in DRL sample efficiency and substantial reduction in constraint violations compared to existing approaches.

Conclusion: NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, with mutual reinforcement between symbolic grounding and policy learning, enhancing DRL performance in constrained environments.

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [8] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: Reasoning models don't consistently outperform non-reasoning models on Theory of Mind tasks, revealing fundamental limitations in transferring formal reasoning skills to social reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the step-by-step reasoning capabilities of Large Reasoning Models (LRMs) that benefit mathematics and coding tasks can transfer to socio-cognitive skills like Theory of Mind.

Method: Systematic study of nine advanced LLMs comparing reasoning vs. non-reasoning models on three representative ToM benchmarks, with fine-grained analysis and intervention approaches (Slow-to-Fast adaptive reasoning and Think-to-Match shortcut prevention).

Result: Reasoning models don't consistently outperform non-reasoning models and sometimes perform worse. Key findings: 1) Slow thinking collapses with longer responses hurting performance, 2) Moderate/adaptive reasoning helps, 3) Models rely on option matching shortcuts rather than genuine deduction.

Conclusion: Advancements in LRMs for formal reasoning (math, code) don't fully transfer to Theory of Mind tasks. Achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [9] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: HARPO is a new RL method that balances learning across heterogeneous behavioral tasks and samples by modulating advantages to prevent any single task or sample from dominating optimization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for socially intelligent AI model human behavioral dimensions in isolation, which increases training costs and limits generalization across behavioral settings. While recent reasoning RL methods train unified models across tasks, they don't explicitly address learning across heterogeneous behavioral data.

Method: Heterogeneity-Aware Relative Policy Optimization (HARPO) - an RL method that modulates advantages to ensure no single task or sample carries disproportionate influence during policy optimization, balancing learning across heterogeneous tasks and samples.

Result: Using HARPO, the authors developed Omnisapiens-7B 2.0, which achieves the strongest performance across behavioral tasks with gains up to +16.85% on multitask and +9.37% on held-out settings, while producing more explicit and robust reasoning traces compared to existing behavioral foundation models.

Conclusion: HARPO effectively addresses the gap in learning across heterogeneous behavioral data, enabling the development of superior foundation models for social behavior processing that generalize better across tasks and produce more robust reasoning.

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [10] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR is a reinforcement learning framework for generative recommendation that addresses probability-reward mismatch through value-guided sampling and tree-structured advantage calculation to improve exploration and learning signals.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning generative recommendation models with RL suffers from probability-reward mismatch where conventional likelihood-dominated decoding causes insufficient exploration (pruning high-reward items in low-probability branches) and advantage compression (highly correlated rewards for trajectories sharing high-probability prefixes, yielding weak comparative signals for RL).

Method: V-STAR (Value-guided Sampling and Tree-structured Advantage Reinforcement) has two synergistic components: 1) Value-Guided Efficient Decoding (VED) that identifies decisive nodes and selectively deepens high-potential prefixes to improve exploration efficiency without exhaustive tree search, and 2) Sibling-GRPO which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions.

Result: Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

Conclusion: V-STAR effectively addresses the probability-reward mismatch problem in RL-based generative recommendation by combining value-guided sampling with tree-structured advantage reinforcement, resulting in better exploration and more effective learning signals.

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [11] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: This study investigates perceptions of GenAI in ITEE disciplines, finding interest in programming support but concerns over quality, privacy, and integrity, and proposes a framework for responsible integration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand stakeholder perceptions of GenAI in higher education, particularly within ITEE disciplines, and to address the need for institutions to engage stakeholders, tailor integration, and comply with regulations like the EU AI Act.

Method: A mixed-method approach was used, involving surveys of 61 staff and 37 students at the Faculty of Information Technology and Electrical Engineering (ITEE), University of Oulu.

Result: The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI, alongside concerns over response quality, privacy, and academic integrity. The study identifies high-level requirements and proposes a conceptual framework for responsible GenAI integration.

Conclusion: The study concludes that responsible GenAI integration in higher education requires active stakeholder engagement and tailored approaches that address discipline-specific needs and concerns, alongside ensuring regulatory compliance.

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [12] [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814)
*Xingyi Zhang,Yulei Ye,Kaifeng Huang,Wenhao Li,Xiangfeng Wang*

Main category: cs.AI

TL;DR: ScratchWorld is a new benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch, revealing a reasoning–acting gap despite strong planning capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored area of evaluating AI agents' capabilities to construct programs through Graphical User Interfaces (GUIs) in block-based programming environments like Scratch, which are central to low-code education. There is a need for a benchmark that can rigorously assess multimodal GUI agents on program-by-construction tasks, grounded in educational pedagogy, to better understand and improve AI agents' performance in such environments.

Method: The authors introduce ScratchWorld, a benchmark comprising 83 curated tasks across four problem categories (Create, Debug, Extend, Compute) based on the Use-Modify-Create pedagogical framework. The benchmark uses two complementary interaction modes: primitive mode (fine-grained drag-and-drop) and composite mode (high-level semantic APIs) to disentangle program reasoning from GUI execution. An execution-based evaluation protocol validates functional correctness of constructed Scratch programs through runtime tests in the browser environment.

Result: Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning–acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

Conclusion: ScratchWorld serves as a valuable benchmark for evaluating multimodal GUI agents in Scratch, uncovering a significant gap between reasoning and acting capabilities, which points to ongoing challenges in fine-grained GUI manipulation that need to be addressed in future AI agent development.

Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>


### [13] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC is an adaptive framework that addresses structural resolution mismatch in Knowledge Graph Completion by using cross-modal synergy experts and density-dependent identity anchoring to better integrate entity semantics with heterogeneous topological structures.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods suffer from "structural resolution mismatch" - they fail to reconcile divergent representational demands across varying graph densities, leading to structural noise interference in dense clusters and catastrophic representation collapse in sparse regions.

Method: SynergyKGC advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. It couples a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture.

Result: Systematic evaluations on two public benchmarks validate the superiority of SynergyKGC in significantly boosting KGC hit rates, providing empirical evidence for resilient information integration in non-homogeneous structured data.

Conclusion: SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases, offering a generalized principle for resilient information integration in knowledge graph completion.

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [14] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER enhances RL with CoT supervision using self-evolving rubrics, outperforming outcome-centric RL approaches without human annotation.


<details>
  <summary>Details</summary>
Motivation: Directly rewarding chain-of-thought (CoT) reasoning in LLMs is challenging because training reward models requires heavy human labeling, and static reward models struggle with evolving CoT distributions and reward hacking. There's a need for an autonomous CoT rewarding approach that doesn't require human annotation and can evolve gradually.

Method: The authors propose RLCER (Reinforcement Learning with CoT Supervision via Self-Evolving Rubrics), which enhances outcome-centric RLVR (Reinforcement Learning with Verbal Reinforcement) by rewarding CoTs with self-proposed and self-evolving rubrics. The system autonomously generates and evolves evaluation rubrics for CoT reasoning without human intervention.

Result: Self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Additionally, when these self-proposed rubrics are used as in-prompt hints, they further improve inference-time performance.

Conclusion: RLCER demonstrates that autonomous, self-evolving rubrics can effectively supervise and improve chain-of-thought reasoning in LLMs without human annotation, overcoming limitations of traditional outcome-centric reward approaches and static reward models.

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [15] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: LLMs fail to produce culturally representative adaptations in cooking recipes, showing no correlation between recipe divergence and cultural distance, unlike humans.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for cultural content creation but exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and erasure of culturally specific expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond dominant ones remains a critical challenge.

Method: Study cultural adaptation in LLMs through cooking recipes using the GlobalFusion dataset which pairs human recipes from different countries according to cultural distance. Generate culturally adapted recipes with multiple LLMs using the same country pairs for direct comparison between human and LLM behavior in cross-cultural content creation.

Result: LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. Cultural information is weakly preserved in internal model representations, models inflate novelty by misunderstanding creativity and tradition, and they fail to identify adaptation with associated countries or ground it in culturally salient elements like ingredients.

Conclusion: These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [16] [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)
*Yusong Lin,Haiyang Wang,Shuzhe Wu,Lue Fan,Feiyang Pan,Sanyuan Zhao,Dandan Tu*

Main category: cs.AI

TL;DR: CLI-Gym: A method to generate large-scale environment-intensive tasks for agentic coding by using agents to simulate environment histories and invert healthy states to buggy ones, creating 1,655 tasks and fine-tuning LiberCoder model that achieves +21.1% improvement on Terminal-Bench.


<details>
  <summary>Details</summary>
Motivation: Agentic coding requires interaction with runtime environments (like CLIs) for tasks like dependency resolution and system fixes, but there's a lack of scalable methods to obtain environment-intensive tasks to enhance agents' capabilities.

Method: Based on analogy between Dockerfile and agentic tasks, uses agents to simulate and explore environment histories guided by execution feedback. By tracing histories of healthy environments, inverts state to earlier buggy versions, then derives tasks by packing buggy state with error messages.

Result: Generated 1,655 environment-intensive tasks (largest collection of its kind). Fine-tuned LiberCoder model with curated successful trajectories achieves +21.1% absolute improvement (to 46.1%) on Terminal-Bench, outperforming various strong baselines.

Conclusion: Presents first public pipeline for scalable derivation of environment-intensive tasks, demonstrating effectiveness through large-scale task generation and significant performance improvements on agentic coding benchmarks.

Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

</details>


### [17] [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103)
*Wayne Chi,Yixiong Fang,Arnav Yayavaram,Siddharth Yayavaram,Seth Karten,Qiuhong Anna Wei,Runkun Chen,Alexander Wang,Valerie Chen,Ameet Talwalkar,Chris Donahue*

Main category: cs.AI

TL;DR: GameDevBench is the first benchmark for evaluating multimodal coding agents on game development tasks, featuring 132 complex tasks requiring significant multimodal understanding of game assets like shaders, sprites, and animations.


<details>
  <summary>Details</summary>
Motivation: Multimodal coding agents lag behind text-only coding agents due to lack of evaluation testbeds that combine software development complexity with deep multimodal understanding. Game development provides an ideal testbed requiring navigation of large codebases while manipulating multimodal assets in visual game scenes.

Method: Created GameDevBench with 132 tasks derived from web and video tutorials, requiring significant multimodal understanding. Tasks are complex with solutions requiring over 3x more lines of code and file changes than prior benchmarks. Introduced two simple image and video-based feedback mechanisms to improve agents' multimodal capabilities.

Result: Current agents struggle with game development - best agent solves only 54.5% of tasks. Strong correlation between perceived task difficulty and multimodal complexity: success rates drop from 46.9% on gameplay tasks to 31.6% on 2D graphics tasks. Simple feedback mechanisms consistently improve performance, with Claude Sonnet 4.5 improving from 33.3% to 47.7%.

Conclusion: GameDevBench addresses the gap in multimodal coding agent evaluation and shows current agents' limitations in game development. Simple multimodal feedback mechanisms can significantly improve performance. The benchmark is released publicly to support further research into agentic game development.

Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

</details>


### [18] [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)
*Jiayi Zhou,Yang Sheng,Hantao Lou,Yaodong Yang,Jie Fu*

Main category: cs.AI

TL;DR: A neuro-symbolic framework called (name not specified in abstract) bridges the gap between natural language requirements and formal specifications for LLM-based agents, using LLMs to decompose intent into verifiable constraints and Dafny/Z3 to provide mathematical guarantees instead of probabilistic scores.


<details>
  <summary>Details</summary>
Motivation: As LLM-based agents operate in high-stakes domains, ensuring behavioral safety is crucial. The dominant LLM-as-a-Judge oversight paradigm faces a fundamental dilemma: probabilistic systems can't reliably supervise other probabilistic systems without inheriting their failure modes. Formal verification offers a principled solution, but adoption has been hindered by the bottleneck of translating natural language requirements to formal specifications.

Method: The paper proposes a neuro-symbolic framework with a bidirectional Formal-of-Thought architecture. LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving.

Result: Experiments on 7 agent models across three benchmarks (behavioral safety, multi-domain constraint adherence, and agentic upward deception detection) show: 1) average 16.6% improvement over LLM-as-a-Judge baselines, 2) weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and 3) near-linear safety improvement through iterative refinement.

Conclusion: The proposed framework successfully bridges the gap between natural language requirements and formal specifications, providing mathematical guarantees for LLM-based agent safety instead of probabilistic scores, thereby addressing the fundamental limitation of the LLM-as-a-Judge paradigm.

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: Fine-tuned LLMs can predict functional outcomes after acute ischemic stroke from admission notes alone, achieving performance comparable to structured-data baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored.

Method: Evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry (NYU Langone Get With The Guidelines-Stroke registry, 2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines.

Result: Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines.

Conclusion: Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. This supports the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [20] [Towards Autonomous Mathematics Research](https://arxiv.org/abs/2602.10177)
*Tony Feng,Trieu H. Trinh,Garrett Bingham,Dawsen Hwang,Yuri Chervonyi,Junehyuk Jung,Joonkyung Lee,Carlo Pagano,Sang-hyun Kim,Federico Pasqualotto,Sergei Gukov,Jonathan N. Lee,Junsu Kim,Kaiying Hou,Golnaz Ghiasi,Yi Tay,YaGuang Li,Chenkai Kuang,Yuan Liu,Hanzhao,Lin,Evan Zheran Liu,Nigamaa Nayakanti,Xiaomeng Yang,Heng-tze Cheng,Demis Hassabis,Koray Kavukcuoglu,Quoc V. Le,Thang Luong*

Main category: cs.LG

TL;DR: Aletheia is a math research agent that uses advanced reasoning models and tool use to generate, verify, and revise mathematical solutions, achieving milestones from Olympiad problems to PhD-level exercises and solving open research questions.


<details>
  <summary>Details</summary>
Motivation: While foundational models can solve competition-level math problems, transitioning to professional mathematical research requires navigating vast literature and constructing long-horizon proofs, which current systems struggle with.

Method: Aletheia uses an advanced version of Gemini Deep Think for reasoning, a novel inference-time scaling law that extends beyond Olympiad problems, and intensive tool use to handle the complexities of mathematical research.

Result: Aletheia achieved: (a) a fully AI-generated research paper on arithmetic geometry eigenweights, (b) a human-AI collaborative paper on independent sets bounds, and (c) autonomous solutions to four open questions from Bloom's Erdos Conjectures database (700 problems evaluated).

Conclusion: The work demonstrates significant progress in AI-assisted mathematics research and suggests codifying standard levels to quantify AI autonomy and novelty, with reflections on future human-AI collaboration in mathematics.

Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>


### [21] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: The paper proposes two new kernel-based metrics (Sig-MMD and CSig-MMD) for evaluating probabilistic forecasts that better handle dependencies across time/variables and are more sensitive to tail events.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for probabilistic forecasting lack consensus metrics and have critical flaws: they assume independence across time steps/variables and lack sensitivity to tail events, which are crucial for real-world decision-making in high-stakes domains like finance, epidemiology, and climate science.

Method: The authors propose two kernel-based metrics: signature maximum mean discrepancy (Sig-MMD) and a novel censored Sig-MMD (CSig-MMD). These leverage the signature kernel to capture complex inter-variate and inter-temporal dependencies while being robust to missing data. CSig-MMD specifically introduces a censoring scheme to prioritize tail event prediction while maintaining properness.

Result: The proposed metrics enable more reliable evaluation of direct multi-step forecasting by addressing the limitations of current frameworks - they capture dependencies across time and variables, are sensitive to tail events, and maintain proper scoring rule properties.

Conclusion: The Sig-MMD and CSig-MMD metrics provide a more robust framework for evaluating probabilistic forecasts, facilitating the development of better forecasting algorithms for high-stakes applications where accurate prediction of tail events and handling of dependencies are critical.

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [22] [Versor: A Geometric Sequence Architecture](https://arxiv.org/abs/2602.10195)
*Truong Minh Huy,Edward Hirst*

Main category: cs.LG

TL;DR: Versor introduces a novel sequence architecture using Conformal Geometric Algebra (CGA) to replace traditional nonlinear operations, achieving better performance, interpretability, and efficiency with fewer parameters and linear complexity.


<details>
  <summary>Details</summary>
Motivation: To develop a sequence architecture that can natively represent SE(3)-equivariant relationships without explicit structural encoding, addressing limitations of Transformers and Graph Networks in geometric reasoning tasks.

Method: Embeds states in the Cl_{4,1} manifold and evolves them via geometric transformations (rotors), uses Conformal Geometric Algebra to replace traditional nonlinear operations, and introduces a Recursive Rotor Accumulator for linear complexity.

Result: Outperforms Transformers, Graph Networks, and geometric baselines on chaotic N-body dynamics, topological reasoning, and multimodal benchmarks; achieves orders of magnitude fewer parameters (200× vs Transformers), zero-shot scale generalization (99.3% MCC vs 50.4% for ViT), and O(L) linear complexity.

Conclusion: Versor provides a scalable foundation for geometrically-aware scientific modeling with improved interpretability, efficiency, and generalization capabilities compared to existing architectures.

Abstract: A novel sequence architecture design is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders of magnitude fewer parameters ($200\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (99.3% MCC on topology vs. 50.4% for ViT); and $O(L)$ linear complexity via the novel Recursive Rotor Accumulator. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve up to $78\times$ speedup, providing a scalable foundation for geometrically-aware scientific modeling.

</details>


### [23] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: MVN-Grad is a new Adam-style optimizer that applies momentum after variance normalization, improving stability and performance with theoretical guarantees and empirical gains on vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Standard Adam-type optimizers suffer from cross-time coupling between stale momentum and stochastic normalization, which can lead to instability and suboptimal performance. The authors aim to create a more stable and robust optimizer by decoupling these components and applying momentum after normalization.

Method: MVN-Grad combines variance-based normalization (scaling each coordinate by an exponential moving average of gradient uncertainty) with momentum applied after normalization, eliminating cross-time coupling between stale momentum and the stochastic normalizer present in standard Adam-type updates.

Result: Theoretical analysis shows MVN-Grad has strictly smaller one-step conditional update variance than momentum-then-normalize methods under standard noise assumptions, is robust to outliers (uniformly bounded response to gradient spikes), and avoids sign-type collapse in low-variance regimes. Empirical results on CIFAR-100 image classification and GPT-style language modeling benchmarks demonstrate MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp with smoother training and improved generalization, with no added computational overhead.

Conclusion: MVN-Grad is an effective Adam-style optimizer that improves training stability and generalization by decoupling momentum and normalization, offering both theoretical guarantees and practical benefits across diverse deep learning tasks.

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [24] [Neural Network Quantum Field Theory from Transformer Architectures](https://arxiv.org/abs/2602.10209)
*Dmitry S. Ageev,Yulia A. Ageeva*

Main category: cs.LG

TL;DR: Neural networks can construct Euclidean quantum field theories using transformer attention heads, with non-Gaussian statistics emerging from random parameters that persist at infinite width but become Gaussian when summing many heads.


<details>
  <summary>Details</summary>
Motivation: To establish a connection between neural networks (specifically transformer architectures) and Euclidean scalar quantum field theories, exploring how random network parameters can generate field statistics and how different architectural choices affect the resulting field theory.

Method: Using the NN-QFT framework to define n-point correlators by averaging over random network parameters of transformer attention heads. Analyzing single attention heads with shared random softmax weights, computing two-point functions in attention-weight representation, engineering Euclidean-invariant kernels via random-feature token embeddings, and analyzing connected four-point functions to identify independence-breaking contributions.

Result: For single attention heads, non-Gaussian field statistics persist in the infinite-width limit. The connected four-point function has a finite independence-breaking contribution at infinite width. When summing many independent heads with standard 1/N_h normalization, connected non-Gaussian correlators are suppressed as 1/N_h, yielding a Gaussian NN-QFT in the large-head limit.

Conclusion: Transformer attention heads provide a natural framework for constructing Euclidean scalar quantum field theories, with architectural choices controlling the emergence of Gaussian vs. non-Gaussian statistics, offering a bridge between neural network architectures and quantum field theory.

Abstract: We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\to\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an "independence-breaking" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.

</details>


### [25] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: HybridRAG-Bench is a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge, using recent arXiv literature to avoid data contamination and reward genuine retrieval and reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) struggle with knowledge-intensive questions requiring up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge offers a promising alternative to costly continual pretraining. However, existing benchmarks increasingly overlap with LLM pretraining data, making it difficult to distinguish genuine retrieval and reasoning from parametric recall.

Method: HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection.

Result: The paper introduces HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. Experiments across three domains demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall.

Conclusion: HybridRAG-Bench offers a principled testbed for evaluating hybrid knowledge-augmented reasoning systems, enabling contamination-aware and customizable evaluation as models and knowledge evolve. The code and data are publicly released.

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [26] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: Theoretical analysis of how LoRA rank affects accuracy in fine-tuning tasks, showing closed-form relationships between rank and accuracy for different loss functions.


<details>
  <summary>Details</summary>
Motivation: LoRA achieves good accuracy with rank-1 updates in practice, but theoretical understanding of how accuracy depends on rank remains underexplored. The paper aims to bridge this gap by analyzing LoRA from a dynamical systems perspective.

Method: Using gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy. Deriving gradient flow equations for LoRA and showing their equivalence for simultaneous and sequential parameter updates.

Result: Established closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions through dynamical system equations.

Conclusion: Theoretical framework connects LoRA rank to accuracy with mathematical rigor, providing insights into why low-rank updates work well in practice and offering analytical tools for understanding rank-accuracy trade-offs.

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [27] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: A framework to disentangle semantic variations in diffusion models by analyzing gradients from stochastic outputs, enabling fine-grained control over concepts and measuring concept complexity.


<details>
  <summary>Details</summary>
Motivation: Diffusion models produce varied outputs from a single prompt due to stochastic processes, but users lack control over which specific semantic variations appear. Existing unsupervised methods analyze output features but ignore the underlying generative process.

Method: Collect gradients by backpropagating differences between stochastic realizations of a fixed prompt, then decompose them into meaningful steering directions using Principal Components Analysis or Sparse Autoencoder.

Result: The approach: (1) isolates interpretable, steerable directions for precise control over single concepts; (2) mitigates mode collapse in distilled models by reintroducing lost diversity; (3) establishes a novel estimator for concept complexity based on discovered subspace dimensionality.

Conclusion: The framework provides direct control over semantic variations in diffusion models by analyzing the input embedding space, offering both practical control applications and theoretical insights into concept representation.

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [28] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: Proposes T3-Unlearning, a two-step inference method for machine unlearning in generative models that uses tempering to handle concentrated forget distributions and tilting with a lightweight classifier, improving over classifier guidance approaches.


<details>
  <summary>Details</summary>
Motivation: Classifier guidance can fail for machine unlearning when the forget set represents sharp, concentrated distributions, requiring a more robust approach to effectively remove targeted information from generative models.

Method: T3-Unlearning freezes the base model and applies: (1) tempering the base distribution to flatten high-confidence spikes, (2) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples.

Result: Empirical evaluations on TOFU benchmark show T3-Unlearning improves forget quality and generative utility over existing baselines while training only a fraction of parameters with minimal runtime overhead.

Conclusion: T3-Unlearning provides an effective framework for machine unlearning in generative models, with theoretical guarantees showing tempering is necessary for concentrated distributions, achieving better performance than classifier guidance approaches.

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [29] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: MEL enhances RLVR by adding meta-experience learning from reasoning errors, improving LLM reasoning capabilities through contrastive analysis and parametric knowledge internalization.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR lacks mechanisms for error attribution and experience internalization beyond practice and verification, limiting fine-grained credit assignment and reusable knowledge formation. This creates a meta-learning bottleneck that prevents models from learning from their reasoning errors effectively.

Method: MEL extends RLVR by using LLM's self-verification to conduct contrastive analysis on correct/incorrect reasoning trajectories, identifying precise error bifurcation points, summarizing them into generalizable meta-experience, and internalizing this knowledge into parametric memory through negative log-likelihood minimization.

Result: MEL achieves consistent improvements on benchmarks, yielding 3.92%-4.73% Pass@1 gains across varying model sizes, demonstrating effective knowledge reuse from reasoning errors.

Conclusion: MEL successfully addresses RLVR's meta-learning bottleneck by enabling error attribution and experience internalization, creating a more complete learning cycle analogous to human learning through meta-experience formation and reuse.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [30] [Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents](https://arxiv.org/abs/2602.10226)
*Haochen Wang,Yi Wu,Daryl Chang,Li Wei,Lukasz Heldt*

Main category: cs.LG

TL;DR: A self-evolving system using Google's Gemini LLMs autonomously generates, trains, and deploys complex model improvements for large-scale ML systems like YouTube recommendations, outperforming manual engineering in speed and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of optimizing large-scale machine learning systems, such as recommendation models for global video platforms, which traditionally require extensive manual iterations to test new hypotheses in a massive hyperparameter search space and to design sophisticated optimizers, architectures, and reward functions.

Method: The method involves a self-evolving system with two specialized agents: an Offline Agent (Inner Loop) that generates and trains hypotheses using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed business metrics in live production. These agents, powered by Google's Gemini LLMs, autonomously generate, train, and deploy model changes.

Result: The result is a self-evolving system that successfully discovers novel improvements in optimization algorithms, model architecture, and reward functions, leading to several successful production launches at YouTube, demonstrating superior development velocity and model performance compared to traditional workflows.

Conclusion: The paper concludes that autonomous, LLM-driven evolution systems like the one presented can outperform traditional engineering workflows in both development speed and model performance, as validated by successful production launches at YouTube.

Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.

</details>


### [31] [PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction](https://arxiv.org/abs/2602.10228)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: PRISM: A differentially private synthetic data generation framework that optimizes for specific prediction tasks by selecting features based on available structural knowledge (causal, graphical, or predictive regimes) and allocating privacy budget accordingly.


<details>
  <summary>Details</summary>
Motivation: Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task. This leads to suboptimal prediction performance due to noise accumulation across irrelevant features.

Method: PRISM operates in three regimes based on available structural knowledge: (1) causal regime - targets causal parents of Y for robustness under distribution shift, (2) graphical regime - uses Markov blanket of Y when Bayesian network structure is known, (3) predictive regime - selects features via DP methods without structural claims. The mechanism identifies predictive feature subsets, constructs targeted summary statistics, allocates budget to minimize prediction error bounds, and synthesizes data via graphical-model inference.

Result: Theoretical: End-to-end privacy guarantees and risk bounds. Empirical: Task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC ≈ 0.73 while correlation-based selection collapses to chance (≈ 0.49).

Conclusion: PRISM demonstrates that prediction-aware feature selection and privacy budget allocation in differentially private synthetic data generation significantly improves utility for specific prediction tasks, especially under distribution shift, compared to symmetric noise addition approaches.

Abstract: Differential privacy (DP) provides a mathematical guarantee limiting what an adversary can learn about any individual from released data. However, achieving this protection typically requires adding noise, and noise can accumulate when many statistics are measured. Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task.
  We develop a prediction-centric approach operating in three regimes depending on available structural knowledge. In the causal regime, when the causal parents of $Y$ are known and distribution shift is expected, we target the parents for robustness. In the graphical regime, when a Bayesian network structure is available and the distribution is stable, the Markov blanket of $Y$ provides a sufficient feature set for optimal prediction. In the predictive regime, when no structural knowledge exists, we select features via differentially private methods without claiming to recover causal or graphical structure.
  We formalize this as PRISM, a mechanism that (i) identifies a predictive feature subset according to the appropriate regime, (ii) constructs targeted summary statistics, (iii) allocates budget to minimize an upper bound on prediction error, and (iv) synthesizes data via graphical-model inference. We prove end-to-end privacy guarantees and risk bounds. Empirically, task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC $\approx 0.73$ while correlation-based selection collapses to chance ($\approx 0.49$).

</details>


### [32] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: Frame-level internal tool use enables audio language models to perform temporal grounding directly using their internal audio representations, achieving >50x speedup and robust length generalization compared to token-based methods.


<details>
  <summary>Details</summary>
Motivation: Large audio language models struggle with temporal tasks requiring precise temporal grounding (word alignment, speaker diarization). Standard token-based timestamp generation is computationally expensive and prone to hallucination, especially on audio lengths outside training distribution.

Method: Proposes frame-level internal tool use, training audio LMs to use their own internal audio representations for temporal grounding. Introduces lightweight prediction mechanism with two objectives: binary frame classifier and novel inhomogeneous Poisson process (IHP) loss for modeling temporal event intensity.

Result: Outperforms token-based baselines across word localization, speaker diarization, and event localization tasks. Achieves >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where token-based models collapse.

Conclusion: Frame-level internal tool use provides an efficient and robust alternative to token-based temporal grounding in audio language models, addressing computational and generalization limitations of existing approaches.

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [33] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: Blockwise Advantage Estimation improves GRPO by assigning separate advantages to different text blocks in structured generations, reducing reward interference without needing expensive nested rollouts.


<details>
  <summary>Details</summary>
Motivation: GRPO's single scalar advantage for all tokens in a completion causes objective interference and misattributed credit in structured generations with multiple segments and objectives.

Method: Proposes Blockwise Advantage Estimation family with Outcome-Conditioned Baseline that estimates advantages for later blocks by stratifying samples using prefix-derived intermediate outcomes, avoiding nested rollouts.

Result: On math tasks with uncertainty estimation, the method mitigates reward interference, matches state-of-the-art reward-designed approaches, and preserves test-time gains from confidence-weighted ensembling.

Conclusion: Provides a modular approach for optimizing sequential objectives in structured generations without additional rollouts, scaling naturally to multiple objectives.

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [34] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: A framework for differentially private data synthesis that protects high-risk outlier records by reducing their influence on the generator through risk-based weighting.


<details>
  <summary>Details</summary>
Motivation: Current differential privacy provides worst-case guarantees but empirical attacks (especially membership inference) succeed more often against outliers under moderate privacy budgets with auxiliary information. Some individuals are harder to protect than others in synthetic data release.

Method: Two-stage risk-equalized DP synthesis: 1) Small privacy budget estimates each record's "outlierness" (risk score), 2) DP learning procedure weights each record inversely to its risk score. Uses Gaussian mechanisms where privacy loss is proportional to influence on output.

Result: Proved end-to-end DP guarantees via composition with closed-form per-record bounds. Experiments show: 1) Simulated data with controlled outliers: risk-weighting substantially reduces membership inference success against high-outlierness records, 2) Real-world benchmarks (Breast Cancer, Adult, German Credit): gains are dataset-dependent, highlighting interplay between scorer quality and synthesis pipeline.

Conclusion: Risk-equalized DP synthesis effectively prioritizes protection for high-risk records by reducing their influence on the learned generator, providing better privacy for outliers that need it most while maintaining formal DP guarantees.

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [35] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: A context-aware recommender system uses source code embeddings to model students' programming skills and match them to homework problems, outperforming token-based and graph-based baselines in skill prediction and exercise recommendation.


<details>
  <summary>Details</summary>
Motivation: To improve personalized learning in programming courses by accurately predicting students' skills and recommending exercises that align with their current abilities based on submitted source code.

Method: The system models students' skills using embeddings from their submitted source code across multiple topics, matches these profiles to the skills required by homework problems via cosine similarity, and ranks exercises accordingly. Evaluations compared Jina embeddings against TF-IDF, CodeBERT-cpp, and GraphCodeBERT, and assessed recommendations using real student data.

Result: Jina embeddings outperformed alternative methods in skill prediction, and the system produced more suitable recommendations than baselines based on correctness or solution time, demonstrating the effectiveness of predicted programming skills for problem recommendation.

Conclusion: The proposed recommender system successfully leverages source code embeddings to enhance exercise recommendations in programming courses, offering a robust approach for personalized learning that adapts to students' evolving skills.

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [36] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: GLIMARK extends multiple kernel learning to handle exponential family outcomes, enabling better prediction on diverse data types, as demonstrated on COVID-19 chest X-ray data.


<details>
  <summary>Details</summary>
Motivation: Single kernel methods are limited in capturing complex, heterogeneous data patterns, and traditional multiple kernel learning (MKL) methods are primarily designed for continuous outcomes, not accommodating diverse data types like binary or count data.

Method: The authors propose GLIMARK, which extends multiple kernel learning (MKL) by integrating it with generalized linear models (GLM) to accommodate outcome variables from the exponential family. This involves constructing composite kernels from simpler ones and applying them within a GLM framework.

Result: Empirical results show that GLIMARK effectively recovers or approximates the true data-generating mechanism. Applied to a COVID-19 chest X-ray dataset, it successfully predicts binary ICU escalation outcomes and extracts clinically meaningful features.

Conclusion: GLIMARK provides a flexible and powerful extension of MKL to handle diverse data types through the exponential family, with practical applications in medical prediction tasks.

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [37] [From Classical to Topological Neural Networks Under Uncertainty](https://arxiv.org/abs/2602.10266)
*Sarah Harkins Dayton,Layal Bou Hamdan,Ioannis D. Schizas,David L. Boothe,Vasileios Maroulas*

Main category: cs.LG

TL;DR: This chapter discusses using neural networks, topological data analysis, and Bayesian methods to improve AI robustness, interpretability, and generalization for military applications like image recognition, fraud detection, and link prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to maximize the potential of artificial intelligence in the military domain by addressing challenges in robustness, interpretability, and generalization through topology-aware and uncertainty-aware models.

Method: The chapter employs neural networks, topological data analysis (TDA), topological deep learning, and statistical Bayesian methods to process images, time series, and graphs.

Result: The chapter highlights practical applications in image, video, audio, and time-series recognition, fraud detection, and link prediction for graphical data, demonstrating how these methods enhance AI performance in military contexts.

Conclusion: The chapter concludes that integrating topological and Bayesian methods with neural networks offers a powerful framework for enhancing AI capabilities in military applications, particularly by improving robustness, interpretability, and generalization across diverse data types.

Abstract: This chapter explores neural networks, topological data analysis, and topological deep learning techniques, alongside statistical Bayesian methods, for processing images, time series, and graphs to maximize the potential of artificial intelligence in the military domain. Throughout the chapter, we highlight practical applications spanning image, video, audio, and time-series recognition, fraud detection, and link prediction for graphical data, illustrating how topology-aware and uncertainty-aware models can enhance robustness, interpretability, and generalization.

</details>


### [38] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: The paper introduces Linear-LLM-SCM, a benchmarking framework for evaluating LLMs on linear Gaussian SCM parametrization, finding current LLMs struggle with quantitative causal reasoning due to stochasticity and sensitivity to perturbations.


<details>
  <summary>Details</summary>
Motivation: Large language models have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning—estimating effect sizes that parametrize functional relationships—remains underexplored in continuous domains.

Method: The authors introduce Linear-LLM-SCM, a plug-and-play benchmarking framework that decomposes a DAG into local parent-child sets, prompts an LLM to produce regression-style structural equations per node, aggregates the results, and compares them against ground-truth parameters.

Result: Experiments reveal several challenges: strong stochasticity in results for some models, susceptibility to DAG misspecification via spurious edges, substantial variability in coefficient estimates across settings, and sensitivity to structural and semantic perturbations.

Conclusion: LLMs currently face significant challenges as quantitative causal parameterizers, showing strong stochasticity, sensitivity to structural perturbations, and susceptibility to DAG misspecification, though the open-source framework enables further research in this domain.

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [39] [What Does Preference Learning Recover from Pairwise Comparison Data?](https://arxiv.org/abs/2602.10286)
*Rattana Pukdee,Maria-Florina Balcan,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: The paper analyzes what Bradley-Terry preference learning actually recovers from triplet comparison data when the underlying data violates the BT model assumptions.


<details>
  <summary>Details</summary>
Motivation: Real-world pairwise preference data often violates the assumptions of the Bradley-Terry model, but it's unclear what BT learning actually recovers in such cases.

Method: Formalized preference information through conditional preference distribution (CPRD), analyzed when BT is appropriate for modeling CPRD, and identified factors governing sample efficiency (margin and connectivity).

Result: Provides precise conditions for when BT is appropriate for modeling CPRD and identifies margin and connectivity as key factors governing sample efficiency.

Conclusion: The results offer a data-centric foundation for understanding what preference learning actually recovers from triplet comparison data.

Abstract: Pairwise preference learning is central to machine learning, with recent applications in aligning language models with human preferences. A typical dataset consists of triplets $(x, y^+, y^-)$, where response $y^+$ is preferred over response $y^-$ for context $x$. The Bradley--Terry (BT) model is the predominant approach, modeling preference probabilities as a function of latent score differences. Standard practice assumes data follows this model and learns the latent scores accordingly. However, real data may violate this assumption, and it remains unclear what BT learning recovers in such cases. Starting from triplet comparison data, we formalize the preference information it encodes through the conditional preference distribution (CPRD). We give precise conditions for when BT is appropriate for modeling the CPRD, and identify factors governing sample efficiency -- namely, margin and connectivity. Together, these results offer a data-centric foundation for understanding what preference learning actually recovers.

</details>


### [40] [Configuration-to-Performance Scaling Law with Neural Ansatz](https://arxiv.org/abs/2602.10300)
*Huaqing Zhang,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: A neural scaling law (NCPL) that predicts training performance from full configurations using LLMs, outperforming configuration-agnostic laws and enabling joint hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing scaling laws assume optimal hyperparameters, requiring significant tuning effort and sometimes being impossible due to hardware constraints. There's a need for more predictable performance across broader hyperparameter settings and simpler tuning at scale.

Method: Propose Configuration-to-Performance Scaling Law (CPL) mapping full training configurations to performance. Parameterize with LLMs trained on diverse open-source pretraining logs to create Neural CPL (NCPL).

Result: NCPL achieves 20-40% lower prediction error than Chinchilla law, generalizes to runs using 10x more compute than training data, supports joint tuning of multiple hyperparameters comparably to baselines, and extends to loss-curve prediction.

Conclusion: NCPL provides a powerful, generalizable approach for predicting training performance from full configurations, enabling more practical hyperparameter tuning and better forecasting for large-scale training.

Abstract: Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.

</details>


### [41] [ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data](https://arxiv.org/abs/2602.10303)
*Haoling Wang,Lang Zeng,Tao Sun,Youngjoo Cho,Ying Ding*

Main category: cs.LG

TL;DR: ICODEN is a flexible neural network method using ODEs to model hazard functions for interval-censored survival data, achieving robust prediction in high-dimensional settings without requiring proportional hazards or parametric assumptions.


<details>
  <summary>Details</summary>
Motivation: Predicting time-to-event outcomes with interval-censored data is challenging due to unobserved exact event times. Many existing survival analysis approaches rely on strong model assumptions or cannot handle high-dimensional predictors.

Method: ICODEN is an ordinary differential equation-based neural network that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation.

Result: ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases across simulations with proportional or non-proportional hazards and linear/nonlinear covariate effects. Applications to ADNI and AREDS/AREDS2 data show robust prediction performance for time-to-AD and time-to-late AMD, effectively using hundreds to over 1,000 SNPs and supporting data-driven subgroup identification.

Conclusion: ICODEN is established as a practical, assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings, demonstrating robust performance and enabling data-driven subgroup identification.

Abstract: Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.

</details>


### [42] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: The paper proposes an automated method to learn reward shaping functions for continuous control RL from offline datasets, robust to unobserved confounders, using causal Bellman equations and PBRS framework.


<details>
  <summary>Details</summary>
Motivation: Reward shaping accelerates RL training but designing effective shaping functions for complex continuous control problems remains challenging, especially when datasets contain unobserved confounding variables.

Method: Uses causal Bellman equation to learn a tight upper bound on optimal state values from offline datasets, then applies these as potentials in Potential-Based Reward Shaping (PBRS) framework, tested with Soft-Actor-Critic (SAC).

Result: The method exhibits strong performance guarantees under unobserved confounders on multiple continuous control benchmarks, showing robust performance.

Conclusion: This work represents a solid first step towards confounding-robust continuous control from a causal perspective, with practical implementation available.

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [43] [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)
*Lipai Huang,Kai Yin,Chia-Fu Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: R2RAG-Flood is a training-free retrieval-augmented generation framework that enhances post-storm damage nowcasting by retrieving and conditioning on relevant reasoning trajectories from geospatial neighbors and class prototypes, achieving performance close to supervised baselines without task-specific training.


<details>
  <summary>Details</summary>
Motivation: To enable post-storm property damage nowcasting without requiring task-specific training or fine-tuning of language models, while producing interpretable reasoning trajectories and achieving efficiency comparable to supervised methods.

Method: Builds a reasoning-centric knowledge base from labeled tabular records containing structured predictors, text summaries, and model-generated reasoning trajectories. During inference, uses context-augmented prompts that retrieve relevant reasoning trajectories from geospatial neighbors and class prototypes, enabling LLMs to emulate prior reasoning rather than learn new parameters. Employs a two-stage prediction procedure with damage occurrence determination followed by severity refinement using Property Damage Extent categorization and conditional downgrading.

Result: Achieves 0.613-0.668 overall accuracy and 0.757-0.896 damage class accuracy across seven LLM backbones, approaching the supervised baseline's 0.714 overall accuracy and 0.859 damage class accuracy. Lightweight variants demonstrate substantially higher severity-per-cost efficiency than both supervised baselines and larger LLMs.

Conclusion: R2RAG-Flood provides an effective, training-free approach for post-storm property damage nowcasting that achieves competitive accuracy compared to supervised methods while offering improved efficiency, interpretability through structured rationales, and no requirement for task-specific model training.

Abstract: R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>


### [44] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PUMA (Progressive UnMAsking) is a training method for Masked Diffusion Models that speeds up training by aligning training-time and inference-time masking patterns, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Masked Diffusion Models have high training complexity due to training on exponentially large sets of random masking patterns, creating a train-test mismatch with the structured masks used during inference.

Method: PUMA modifies the forward masking process to align training-time and inference-time masking patterns, focusing optimization on inference-aligned masks and speeding up training.

Result: PUMA speeds up pretraining at the 125M scale by approximately 2.5× and offers complementary advantages on top of common recipes like autoregressive initialization.

Conclusion: PUMA provides an effective solution to the training complexity and train-test mismatch issues in Masked Diffusion Models, enabling faster training while maintaining strong performance.

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [45] [Identifying Evidence-Based Nudges in Biomedical Literature with Large Language Models](https://arxiv.org/abs/2602.10345)
*Jaydeep Chauhan,Mark Seidman,Pezhman Raeisian Parvari,Zhi Zheng,Zina Ben-Miled,Cristina Barboi,Andrew Gonzalez,Malaz Boustani*

Main category: cs.LG

TL;DR: A multi-stage AI system that extracts behavioral nudges from biomedical literature using hybrid filtering and LLM classification, achieving tunable precision-recall trade-offs for evidence synthesis and personalized healthcare.


<details>
  <summary>Details</summary>
Motivation: Behavioral nudges show strong impact on health outcomes, but identifying them from over 8 million PubMed articles is a major bottleneck for evidence-based intervention discovery.

Method: Two-stage pipeline: 1) Hybrid filtering using keywords, TF-IDF, cosine similarity, and nudge-term bonus reduces corpus to ~81k candidates; 2) OpenScholar (quantized LLaMA 3.1 8B) classifies papers and extracts structured fields (nudge type, target behavior) in single pass with JSON schema validation.

Result: Best configuration (Title/Abstract/Intro) achieved 67.0% F1 score and 72.0% recall. High-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating tunable precision-recall trade-off.

Conclusion: The system enables scalable evidence synthesis for nudges, integrates into real-world platforms to ground LLM-generated interventions in peer-reviewed evidence, and demonstrates interpretable domain-specific retrieval pipelines for personalized healthcare.

Abstract: We present a scalable, AI-powered system that identifies and extracts evidence-based behavioral nudges from unstructured biomedical literature. Nudges are subtle, non-coercive interventions that influence behavior without limiting choice, showing strong impact on health outcomes like medication adherence. However, identifying these interventions from PubMed's 8 million+ articles is a bottleneck. Our system uses a novel multi-stage pipeline: first, hybrid filtering (keywords, TF-IDF, cosine similarity, and a "nudge-term bonus") reduces the corpus to about 81,000 candidates. Second, we use OpenScholar (quantized LLaMA 3.1 8B) to classify papers and extract structured fields like nudge type and target behavior in a single pass, validated against a JSON schema.
  We evaluated four configurations on a labeled test set (N=197). The best setup (Title/Abstract/Intro) achieved a 67.0% F1 score and 72.0% recall, ideal for discovery. A high-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating a tunable trade-off for high-trust use cases. This system is being integrated into Agile Nudge+, a real-world platform, to ground LLM-generated interventions in peer-reviewed evidence. This work demonstrates interpretable, domain-specific retrieval pipelines for evidence synthesis and personalized healthcare.

</details>


### [46] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: LITT is a novel Timing-Transformer architecture that enables personalized sequential event analysis by aligning events on a relative timeline, validated on breast cancer EHR data and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Automatically discovering personalized sequential events from large-scale time-series data is crucial for precision medicine, but current AI models (like transformers) are mostly agnostic to event timing and ordering, bypassing potential causal reasoning.

Method: The authors introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual 'relative timeline', enabling event-timing-focused attention and personalized interpretations of clinical trajectories.

Result: LITT's interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict cardiotoxicity-induced heart disease onset timing. It outperforms both benchmark and state-of-the-art survival analysis methods on public datasets.

Conclusion: LITT represents a significant advancement for precision medicine in clinical AI by enabling personalized, interpretable sequential event analysis that accounts for event timing and ordering, outperforming existing methods.

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [47] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: Contrastive learning with Transformers under imbalanced data distributions exhibits three-stage neuron weight evolution, where minority features reduce capacity and hinder feature separation, but pruning can restore performance.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning lacks theoretical understanding under imbalanced data distributions, which are common in real-world applications and can degrade representation quality and induce biased model behavior.

Method: Developed a theoretical framework to analyze training dynamics of contrastive learning with Transformer-based encoders under imbalanced data, examining neuron weight evolution through three distinct stages.

Result: Neuron weights evolve through three stages with different dynamics for majority features, minority features, and noise; minority features reduce representational capacity, increase architectural complexity requirements, and hinder feature separation from noise; pruning restores performance degraded by imbalance and enhances feature separation.

Conclusion: Theoretical analysis reveals fundamental limitations of contrastive learning under data imbalance, while demonstrating that pruning offers both conceptual insights and practical solutions to mitigate these issues, with numerical experiments validating major findings.

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [48] [Affordances Enable Partial World Modeling with LLMs](https://arxiv.org/abs/2602.10390)
*Khimya Khetarpal,Gheorghe Comanici,Jonathan Richens,Jeremy Shar,Fei Xia,Laurent Orseau,Aleksandra Faust,Doina Precup*

Main category: cs.LG

TL;DR: Large language models can be formally shown to contain partial world models informed by affordances, which can be extracted to significantly improve search efficiency in multi-task settings compared to using full world models.


<details>
  <summary>Details</summary>
Motivation: While large pre-trained models contain extensive world knowledge, using them directly for search is inefficient and inaccurate. Partial models focusing on affordance-linked states and actions offer better quality predictions for achieving user intents, but it's unclear if large models can serve as such partial world models.

Method: Formal theoretical analysis proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. Introduces distribution-robust affordances for multi-task settings and shows how partial models can be extracted to improve search efficiency.

Result: Empirical evaluations in tabletop robotics tasks demonstrate that affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

Conclusion: Large models can indeed serve as partial world models informed by affordances, and extracting these partial models significantly improves search efficiency and performance in multi-task settings, providing a formal grounding for using large models as structured world knowledge.

Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents~\citep{khetarpal2020can}. Can we posit large models as partial world models? We provide a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, we introduce distribution-robust affordances and show that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that our affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

</details>


### [49] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: Paper proposes evaluation metrics for model diffing methods, compares LLM-based and SAE-based approaches, finding improved LLM baseline matches SAE performance and yields more abstract behavioral differences.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations only test for designed capabilities, missing unexpected behavioral differences between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation, but there's no systematic comparison of existing approaches nor established evaluation criteria.

Method: The study proposes evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and uses these to systematically compare existing model diffing methods, including LLM-based and SAE-based approaches.

Result: The results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

Conclusion: The paper concludes that an improved LLM-based baseline performs comparably to the SAE-based method for model diffing, while typically surfacing more abstract behavioral differences.

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [50] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: LoRA achieves comparable accuracy to full fine-tuning for organic reaction prediction tasks while better preserving multi-task performance and mitigating catastrophic forgetting, demonstrating practical parameter-efficient adaptation of LLMs for chemistry applications.


<details>
  <summary>Details</summary>
Motivation: Adapting large language models trained on broad organic chemistry to smaller, domain-specific reaction datasets is challenging in chemical and pharmaceutical R&D. Effective specialization requires learning new reaction knowledge while preserving general chemical understanding across related tasks.

Method: The study evaluates Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, they benchmark forward reaction prediction, retrosynthesis, and reagent prediction.

Result: LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both approaches generalize beyond training distributions, producing plausible alternative solvent predictions. C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA.

Conclusion: As LLMs continue to scale, modular, parameter-efficient fine-tuning strategies like LoRA offer practical solutions for flexible deployment in chemistry applications, balancing specialized learning with preservation of general chemical understanding.

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [51] [Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs](https://arxiv.org/abs/2602.10377)
*Luoyang Sun,Jiwen Jiang,Yifeng Ding,Fengfa Li,Yan Song,Haifeng Zhang,Jian Ying,Lei Ren,Kun Zhan,Wei Chen,Yan Xie,Cheng Deng*

Main category: cs.LG

TL;DR: This paper introduces a hardware-software co-design framework for Vision-Language-Action Models (VLAs) that establishes scaling laws to optimize LLM backbones for on-device deployment, balancing accuracy and latency constraints.


<details>
  <summary>Details</summary>
Motivation: On-device VLA deployment requires LLM backbones that balance accuracy with strict inference latency and hardware efficiency constraints, making hardware-software co-design essential but challenging without principled frameworks.

Method: The authors propose a hardware co-design law that models training loss as a function of architectural hyperparameters and characterizes inference latency via roofline modeling. They empirically evaluate 1,942 architectures on NVIDIA Jetson Orin, train 170 selected models on 10B tokens to fit scaling laws, and establish accuracy-latency Pareto frontiers.

Result: The approach reduces architecture selection from months to days and identifies Pareto-optimal designs. At the same latency as Qwen2.5-0.5B on target hardware, their co-designed architecture achieves 19.42% lower perplexity on WikiText-2.

Conclusion: This work presents the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment, enabling efficient architecture selection that balances accuracy and performance constraints for resource-constrained settings.

Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>


### [52] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: AI models excel at advanced math but fail at basic integer addition; errors are mostly due to operand misalignment (tokenization-related) and carrying failures.


<details>
  <summary>Details</summary>
Motivation: Despite AI systems achieving success in advanced mathematics (e.g., winning medals in competitions, proving lemmas), they perform poorly on basic arithmetic like adding two numbers. The authors aim to understand why this gap exists and to systematically characterize the types of errors models make.

Method: The authors conducted a systematic empirical investigation by testing frontier models (Claude Opus 4.1, GPT‑5, Gemini 2.5 Pro) on integer addition tasks with varying digit lengths, analyzed error patterns, and categorized errors into interpretable classes (operand misalignment and failure to carry).

Result: All frontier models showed significantly degraded accuracy for integer addition as digit count increased. Most errors were interpretable: 87.9% of Claude Opus errors, 62.9% of GPT‑5 errors, and 92.4% of Gemini 2.5 Pro errors were attributed to operand misalignment or carrying failures. Misalignment errors were frequently linked to tokenization, while carrying errors appeared as independent random failures.

Conclusion: The study concludes that despite advances in high-level mathematical reasoning, frontier AI models still struggle with basic integer addition due to systematic errors related to tokenization (misalignment) and carrying failures, suggesting that current architectures have inherent limitations in handling elementary arithmetic operations robustly.

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [53] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: Machine learning, especially TabNet, effectively identifies child malnutrition in Nepal using survey data, with key predictors being maternal education, wealth, and child age, offering a scalable screening framework for low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Childhood malnutrition is a major public health issue in Nepal and similar low-resource settings, but conventional case-finding methods are labor-intensive and often unavailable in remote areas. There is a need for scalable, efficient screening tools to identify at-risk children and guide interventions.

Method: Systematic comparison of 16 machine learning and deep learning algorithms, including TabNet, gradient boosting, and traditional models, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was created from stunting, wasting, and underweight status, with performance evaluated using ten metrics, emphasizing F1-score and recall due to class imbalance.

Result: TabNet outperformed other models, including support vector machine and AdaBoost, likely due to its attention-based architecture. Key predictors of malnutrition were maternal education, household wealth index, child age, geographic characteristics, vaccination status, and meal frequency.

Conclusion: The study demonstrates that machine learning, particularly the TabNet model, provides an effective and scalable method for identifying child malnutrition in Nepal, supporting targeted interventions and offering a transferable framework for similar low-resource settings globally.

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [54] [Equivariant Evidential Deep Learning for Interatomic Potentials](https://arxiv.org/abs/2602.10419)
*Zhongyao Wang,Taoyong Cui,Jiawen Zou,Shufei Zhang,Bo Yan,Wanli Ouyang,Weimin Tan,Mao Su*

Main category: cs.LG

TL;DR: A new equivariant evidential deep learning framework (e²IP) for uncertainty quantification in machine learning interatomic potentials that models atomic forces and their uncertainty with rotationally equivariant covariance tensors.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for machine learning interatomic potentials are limited by high computational cost or suboptimal performance. Evidential deep learning offers a single-model alternative but faces challenges when extending from scalar to vector-valued targets like atomic forces, particularly in maintaining statistical consistency under rotational transformations.

Method: The proposed e²IP framework is backbone-agnostic and models atomic forces and their uncertainty jointly by representing uncertainty as a full 3×3 symmetric positive definite covariance tensor that transforms equivariantly under rotations, ensuring statistical self-consistency.

Result: Experiments on diverse molecular benchmarks show that e²IP provides better accuracy-efficiency-reliability balance than non-equivariant evidential baselines and ensemble methods, achieves better data efficiency through fully equivariant architecture, and retains single-model inference efficiency.

Conclusion: The e²IP framework successfully addresses the challenges of extending evidential deep learning to vector-valued atomic forces while maintaining rotational equivariance, offering a practical and efficient solution for uncertainty quantification in machine learning interatomic potentials.

Abstract: Uncertainty quantification (UQ) is critical for assessing the reliability of machine learning interatomic potentials (MLIPs) in molecular dynamics (MD) simulations, identifying extrapolation regimes and enabling uncertainty-aware workflows such as active learning for training dataset construction. Existing UQ approaches for MLIPs are often limited by high computational cost or suboptimal performance. Evidential deep learning (EDL) provides a theoretically grounded single-model alternative that determines both aleatoric and epistemic uncertainty in a single forward pass. However, extending evidential formulations from scalar targets to vector-valued quantities such as atomic forces introduces substantial challenges, particularly in maintaining statistical self-consistency under rotational transformations. To address this, we propose \textit{Equivariant Evidential Deep Learning for Interatomic Potentials} ($\text{e}^2$IP), a backbone-agnostic framework that models atomic forces and their uncertainty jointly by representing uncertainty as a full $3\times3$ symmetric positive definite covariance tensor that transforms equivariantly under rotations. Experiments on diverse molecular benchmarks show that $\text{e}^2$IP provides a stronger accuracy-efficiency-reliability balance than the non-equivariant evidential baseline and the widely used ensemble method. It also achieves better data efficiency through the fully equivariant architecture while retaining single-model inference efficiency.

</details>


### [55] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: LLMs struggle with graph problems due to structural mismatch, so researchers propose encoding graph structure into natural language prompts using human-interpretable color tokens based on WL similarity classes, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Graph problems present fundamental challenges for LLMs because they require reasoning over explicit structure, permutation invariance, and complex relationships, which mismatch with the text-based representations that LLMs excel at processing.

Method: Introduces a human-interpretable structural encoding strategy that computes a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels for graph-to-text translation, injecting graph structure directly into natural language prompts.

Result: Experimental results on multiple algorithmic and predictive graph tasks show considerable improvements by the proposed method on both synthetic and real-world datasets, especially enhancing performance on tasks requiring reasoning over global graph structure.

Conclusion: Semantically meaningful and human-interpretable cues are more effectively processed by LLMs than opaque symbolic encoding for graph problems, and this structural encoding approach enables LLMs to better handle graph tasks by capturing both local and global-range dependencies.

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [56] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: Policy-based RL for generative recommendation suffers from model collapse due to low-quality offline data; DRPO uses distributionally robust optimization with hard filtering to recover high-quality behaviors while discarding noise.


<details>
  <summary>Details</summary>
Motivation: Policy-based RL methods for generative recommendation fail when trained on offline historical logs because low-quality data dominates and causes severe model collapse, with existing methods unable to reconcile variance reduction and noise imitation.

Method: The authors propose Distributionally Robust Policy Optimization (DRPO), which reformulates the objective as an Optimistic Distributionally Robust Optimization problem. They prove that hard filtering is the exact solution to this DRO objective, enabling recovery of high-quality behaviors while discarding divergence-inducing noise.

Result: Extensive experiments show that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks, demonstrating its effectiveness in handling noisy offline data.

Conclusion: DRPO successfully breaks the curse of model collapse in offline RL for recommendation by identifying latent high-quality distributions through distributionally robust optimization with hard filtering, providing a robust solution to the divergence problem in off-policy training.

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [57] [A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring](https://arxiv.org/abs/2602.10432)
*Enzo Nicolas Spotorno,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: Proposes a Dual-Stream Architecture combining unsupervised anomaly detection with physics-based load proxies to monitor vehicle mechanical health on edge devices, distinguishing transient shocks from sustained high loads.


<details>
  <summary>Details</summary>
Motivation: Traditional mileage metrics fail to capture mechanical burden, and unsupervised deep learning models for anomaly detection often conflate statistical stability with mechanical rest, missing critical high-load steady states like hill climbing with heavy payloads that impose significant drivetrain fatigue.

Method: A Dual-Stream Architecture that combines unsupervised deep learning models for anomaly detection with macroscopic physics proxies for cumulative load estimation, leveraging low-frequency sensor data. The architecture is designed for edge-based health monitoring on resource-constrained ECUs, validated on a RISC-V embedded platform.

Result: The proposed Dual-Stream Architecture fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation, generating a multi-dimensional health vector that distinguishes dynamic hazards from sustained mechanical effort. Validated on a RISC-V embedded platform, it demonstrates low computational overhead.

Conclusion: The architecture enables comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring, effectively addressing the blind spot of high-load steady states that traditional methods miss.

Abstract: Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typically transient surface shocks, but often conflate statistical stability with mechanical rest. We identify this as a critical blind spot: high-load steady states, such as hill climbing with heavy payloads, appear statistically normal yet impose significant drivetrain fatigue. To resolve this, we propose a Dual-Stream Architecture that fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation. This approach leverages low-frequency sensor data to generate a multi-dimensional health vector, distinguishing between dynamic hazards and sustained mechanical effort. Validated on a RISC-V embedded platform, the architecture demonstrates low computational overhead, enabling comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring.

</details>


### [58] [Tensor Methods: A Unified and Interpretable Approach for Material Design](https://arxiv.org/abs/2602.10392)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: Tensor completion methods offer interpretable and accurate surrogate models for material design optimization, competing with ML while providing physical insights through tensor factors.


<details>
  <summary>Details</summary>
Motivation: Traditional material design optimization faces exponential search space growth, making exhaustive synthesis/evaluation impossible. Computational methods like FEA are too heavy, while ML surrogate models lack interpretability and perform poorly with non-uniform training data sampling.

Method: Using tensor completion methods as surrogate models for material design optimization. These methods provide interpretable tensor factors as a byproduct of predictions. Specialized tensor methods are studied for better generalization with non-uniform training data sampling.

Result: Classical tensor methods compete with traditional ML in prediction accuracy while providing interpretable tensor factors that can rediscover physical phenomena. Specialized tensor models improve upon baseline ML by up to 5% on aggregate R² and halve error in some out-of-distribution regions with non-uniform sampling.

Conclusion: Tensor completion methods offer an all-in-one solution for interpretable and accurate material design optimization, with tensor factors providing physical insights and better generalization in non-uniform sampling scenarios than traditional ML approaches.

Abstract: When designing new materials, it is often necessary to tailor the material design (with respect to its design parameters) to have some desired properties (e.g. Young's modulus). As the set of design parameters grow, the search space grows exponentially, making the actual synthesis and evaluation of all material combinations virtually impossible. Even using traditional computational methods such as Finite Element Analysis becomes too computationally heavy to search the design space. Recent methods use machine learning (ML) surrogate models to more efficiently determine optimal material designs; unfortunately, these methods often (i) are notoriously difficult to interpret and (ii) under perform when the training data comes from a non-uniform sampling of the design space. We suggest the use of tensor completion methods as an all-in-one approach for interpretability and predictions. We observe classical tensor methods are able to compete with traditional ML in predictions, with the added benefit of their interpretable tensor factors (which are given completely for free, as a result of the prediction). In our experiments, we are able to rediscover physical phenomena via the tensor factors, indicating that our predictions are aligned with the true underlying physics of the problem. This also means these tensor factors could be used by experimentalists to identify potentially novel patterns, given we are able to rediscover existing ones. We also study the effects of both types of surrogate models when we encounter training data from a non-uniform sampling of the design space. We observe more specialized tensor methods that can give better generalization in these non-uniforms sampling scenarios. We find the best generalization comes from a tensor model, which is able to improve upon the baseline ML methods by up to 5% on aggregate $R^2$, and halve the error in some out of distribution regions.

</details>


### [59] [Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering](https://arxiv.org/abs/2602.10437)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.LG

TL;DR: CRL (Control Reinforcement Learning) trains policies to select SAE features for steering language model outputs, providing interpretable intervention logs and enabling new analysis capabilities beyond static feature activation.


<details>
  <summary>Details</summary>
Motivation: Sparse autoencoders (SAEs) can identify which features activate in language models, but cannot determine which features actually change model outputs when amplified. There's a need for methods that go beyond static feature analysis to understand which features are causally relevant for steering model behavior.

Method: Introduces Control Reinforcement Learning (CRL): trains a policy to select SAE features for steering at each token, uses Adaptive Feature Masking to encourage diverse feature discovery while preserving interpretability, and creates interpretable intervention logs showing which features change outputs when amplified.

Result: CRL demonstrates improvements on Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest benchmarks while providing per-token intervention logs. The framework enables new analysis capabilities: branch point tracking, critic trajectory analysis, and layer-wise comparison revealing syntactic features in early layers and semantic features in later layers.

Conclusion: Learned feature steering with CRL establishes a mechanistic interpretability tool that complements static SAE feature analysis with dynamic intervention probes, moving beyond "which features activate" to "which features change model outputs when amplified."

Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature discovery while preserving singlefeature interpretability. The framework yields new analysis capabilities: branch point tracking locates tokens where feature choice determines output correctness; critic trajectory analysis separates policy limitations from value estimation errors; layer-wise comparison reveals syntactic features in early layers and semantic features in later layers. On Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL achieves improvements while providing per-token intervention logs. These results establish learned feature steering as a mechanistic interpretability tool that complements static feature analysis with dynamic intervention probes

</details>


### [60] [Experimental Demonstration of Online Learning-Based Concept Drift Adaptation for Failure Detection in Optical Networks](https://arxiv.org/abs/2602.10401)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,João Pedro,Antonio Napoli,Sasipim Srivallapanondh,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: Online learning approach for concept drift adaptation in optical network failure detection improves performance by 70% over static models with low latency.


<details>
  <summary>Details</summary>
Motivation: Optical network failure detection systems face concept drift issues where network conditions and failure patterns change over time, making static models ineffective.

Method: A novel online learning-based approach that continuously adapts to changing patterns (concept drift) in optical network data for failure detection.

Result: Achieves up to 70% improvement in performance over conventional static models while maintaining low latency requirements.

Conclusion: Online learning is effective for adapting to concept drift in optical network failure detection, significantly outperforming static approaches while meeting real-time requirements.

Abstract: We present a novel online learning-based approach for concept drift adaptation in optical network failure detection, achieving up to a 70% improvement in performance over conventional static models while maintaining low latency.

</details>


### [61] [LakeMLB: Data Lake Machine Learning Benchmark](https://arxiv.org/abs/2602.10441)
*Feiyu Pan,Tianbin Zhang,Aoqian Zhang,Yu Sun,Zheng Wang,Lixing Chen,Li Pan,Jianhua Li*

Main category: cs.LG

TL;DR: LakeMLB is a new benchmark for evaluating ML in data lakes, covering multi-table scenarios (Union/Join) with real-world datasets and integration strategies, released with code and data.


<details>
  <summary>Details</summary>
Motivation: Standardized benchmarks for evaluating machine learning performance in data lake environments are scarce, despite data lakes being foundational platforms for large-scale ML and heterogeneous data storage.

Method: Design of LakeMLB benchmark focusing on common multi-source, multi-table scenarios; provision of three real-world datasets per scenario; support for three integration strategies (pre-training-based, data augmentation-based, feature augmentation-based); extensive experiments with SOTA tabular learning methods.

Result: LakeMLB benchmark introduced with two representative multi-table scenarios (Union and Join), three real-world datasets per scenario covering diverse domains, and extensive experiments with SOTA tabular learning methods, providing performance insights.

Conclusion: LakeMLB benchmark addresses the gap in standardized ML evaluation for data lakes, offering datasets, code, and insights to facilitate rigorous research in data lake ecosystems.

Abstract: Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performance in data lake environments remain scarce. To address this gap, we present LakeMLB (Data Lake Machine Learning Benchmark), designed for the most common multi-source, multi-table scenarios in data lakes. LakeMLB focuses on two representative multi-table scenarios, Union and Join, and provides three real-world datasets for each scenario, covering government open data, finance, Wikipedia, and online marketplaces. The benchmark supports three representative integration strategies: pre-training-based, data augmentation-based, and feature augmentation-based approaches. We conduct extensive experiments with state-of-the-art tabular learning methods, offering insights into their performance under complex data lake scenarios. We release both datasets and code to facilitate rigorous research on machine learning in data lake ecosystems; the benchmark is available at https://github.com/zhengwang100/LakeMLB.

</details>


### [62] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: A theoretical analysis of when random projection preserves influence functions, addressing limitations of Johnson-Lindenstrauss for inverse operations, and providing guarantees for regularized and structured curvature approximations.


<details>
  <summary>Details</summary>
Motivation: Influence functions in modern overparameterized models require computing g^T F^{-1} g' where F is a large curvature matrix. Random projection via sketching is commonly used but lacks theoretical justification for how it behaves under inversion and interacts with regularization and structured approximations.

Method: Develops a unified theory characterizing when projection preserves influence functions, analyzing three scenarios: 1) Unregularized projection requiring injectivity on range(F), 2) Regularized projection with ridge regularization governed by effective dimension, 3) Factorized influence for Kronecker-factored curvatures with decoupled sketches. Also analyzes out-of-range test gradients and leakage terms.

Result: Shows that: 1) Exact preservation requires m ≥ rank(F) for unregularized case, 2) Ridge regularization changes the sketching barrier with guarantees based on effective dimension, 3) Kronecker-factored curvatures maintain guarantees with decoupled sketches despite row correlations, 4) Out-of-range test gradients introduce leakage terms that can be quantified.

Conclusion: The work provides a novel theoretical framework for when projection preserves influence functions, offering principled guidance for choosing sketch sizes in practice and addressing limitations of previous justifications based solely on Johnson-Lindenstrauss.

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [63] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: TaperNorm replaces RMSNorm/LayerNorm, starting as standard normalization then tapering to a fixed affine map, enabling removal of per-token statistics and folding into linear layers for faster inference, while matching baseline performance.


<details>
  <summary>Details</summary>
Motivation: Normalization is widely considered essential for stabilizing Transformer training, but the necessity of sample-dependent normalization within Transformer blocks is questioned, especially for pre-norm Transformers. The authors aim to explore whether per-token statistics are truly needed and to develop a method that can eventually eliminate them while maintaining training stability and performance.

Method: TaperNorm is introduced as a drop-in replacement for RMSNorm/LayerNorm. It behaves like standard normalization early in training, then smoothly transitions to a learned sample-independent linear/affine map via a global gate. The gate starts at 1 during warmup, calibrates scaling via EMAs, then cosine-decays to 0, eliminating per-token statistics. The fixed scalings can be folded into adjacent linear projections. The paper also proposes a fixed-target auxiliary loss to anchor pre-logit residual-stream scale, aiding removal of the final normalization layer.

Result: TaperNorm matches normalized baselines in performance while enabling the removal of per-token statistics. It allows normalization layers to be folded into adjacent linear projections at inference, improving throughput by up to 1.22× in last-token logits mode. The method also supports the potential removal of the final normalization layer with an auxiliary loss.

Conclusion: TaperNorm demonstrates that sample-dependent normalization can be gradually phased out without harming performance, taking a step toward norm-free Transformers. The key role of output normalization is identified as scale anchoring to prevent unbounded logit growth, and an auxiliary loss can provide an alternative anchor.

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [64] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: MIPLIB-NL is a benchmark dataset for evaluating natural language to optimization code translation, created by reverse-engineering real mixed-integer linear programs from MIPLIB 2017 to generate natural language specifications paired with their mathematical formulations.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based optimization modeling evaluation relies on toy-sized or synthetic benchmarks, which fail to capture the complexity of industrial-scale problems with thousands to millions of variables and constraints. There's a lack of benchmarks that align natural-language specifications with real optimization formulations.

Method: The authors developed a structure-aware reverse construction methodology: 1) Recover compact, reusable model structure from flat solver formulations, 2) Reverse-generate natural-language specifications explicitly tied to the recovered structure using a unified model-data separation format, 3) Perform iterative semantic validation through expert review and human-LLM interaction with independent reconstruction checks.

Result: Created MIPLIB-NL with 223 one-to-one reconstructions that preserve the mathematical content of original MIPLIB instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform well on existing benchmarks, exposing failure modes invisible at toy scale.

Conclusion: MIPLIB-NL fills a critical gap by providing a realistic benchmark for evaluating natural language to optimization code translation, revealing the limitations of current methods when faced with industrial-scale complexity and highlighting the need for more robust approaches.

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [65] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: LUCID Attention is a new attention mechanism that uses key-key similarity preconditioning to improve focus on relevant tokens in long contexts without the gradient issues of temperature-based sharpening, achieving significant gains on long-context retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Standard softmax attention diffuses probability mass to irrelevant tokens in long sequences, degrading performance. Attempts to sharpen focus with lower temperature cause vanishing gradient problems that hinder learnability.

Method: LUCID Attention applies a preconditioner derived from exponentiated key-key similarities to attention probabilities. This minimizes overlap between keys in RKHS, allowing queries to focus accurately on important keys while maintaining the same computational complexity as standard attention.

Result: LUCID achieves significant improvements on long-context retrieval tasks: up to 18% improvement on BABILong and 14% improvement on RULER multi-needle performance compared to standard attention, validated on ~1B parameter models evaluated on up to 128K tokens.

Conclusion: LUCID Attention provides an effective architectural modification that addresses softmax attention's limitations in long contexts through preconditioning, enabling better focus without computational overhead or learnability issues associated with temperature adjustment.

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [66] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: LatentRxnFlow models reactions as continuous latent trajectories anchored at the thermodynamic product state, achieving SOTA accuracy while enabling trajectory-level diagnostics and uncertainty awareness without mechanistic supervision.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art models treat reaction prediction as a one-shot mapping from reactants to products, providing limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference.

Method: The approach is built on Conditional Flow Matching, learning time-dependent latent dynamics directly from standard reactant-product pairs without requiring mechanistic annotations or curated intermediate labels.

Result: LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks and, more importantly, its continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics, localization and characterization of failure modes, mitigation of errors via gated inference, and provision of an intrinsic signal of epistemic uncertainty.

Conclusion: LatentRxnFlow offers a promising new paradigm that combines high predictive accuracy with enhanced transparency, diagnosability, and uncertainty awareness, advancing reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [67] [LightGTS-Cov: Covariate-Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.10412)
*Yong Shang,Zhipeng Yao,Ning Jin,Xiangfei Qiu,Hui Zhang,Bin Yang*

Main category: cs.LG

TL;DR: LightGTS-Cov extends LightGTS with a lightweight MLP plug-in to effectively incorporate past and future-known covariates for time series forecasting, achieving superior performance in energy applications.


<details>
  <summary>Details</summary>
Motivation: Existing time series foundation models often ignore exogenous covariates or use simple concatenation, limiting effectiveness in covariate-rich applications like electricity price and renewable energy forecasting.

Method: Extends the ~1M-parameter LightGTS backbone with a ~0.1M-parameter MLP plug-in that integrates time-aligned covariates by residually refining the outputs of the decoding process.

Result: Outperforms LightGTS and other covariate-aware baselines on electricity price and energy generation benchmarks, and demonstrates strong forecasting accuracy and stable operational performance in real-world energy applications.

Conclusion: LightGTS-Cov effectively incorporates covariates while maintaining a lightweight architecture, validating its practical value for real-world industrial forecasting applications.

Abstract: Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.

</details>


### [68] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: ADAlign is an adaptive distribution alignment framework for Graph Domain Adaptation that automatically identifies and aligns relevant discrepancies without manual heuristics, using a novel Neural Spectral Discrepancy metric to capture feature-structure dependencies, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Domain Adaptation methods rely on manually selected graph elements and heuristics to align distributions, which are inflexible and struggle when dominant discrepancies vary across transfer scenarios. There is a need for a more adaptive, scenario-aware approach that can automatically identify and align relevant discrepancies without manual specification.

Method: The method introduces an Adaptive Distribution Alignment framework (ADAlign) that uses a Neural Spectral Discrepancy (NSD) metric. NSD is a parametric distance that leverages neural characteristic functions in the spectral domain to encode feature-structure dependencies of all orders, with a learnable frequency sampler that adaptively emphasizes the most informative spectral components via a minimax paradigm.

Result: Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign outperforms state-of-the-art baselines while achieving efficiency gains with lower memory usage and faster training.

Conclusion: ADAlign provides a flexible, adaptive, and efficient solution for Graph Domain Adaptation by automatically identifying and aligning the most relevant distributional shifts without manual heuristics, achieving superior performance and practical benefits across diverse transfer tasks.

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [69] [Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks](https://arxiv.org/abs/2602.10496)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: Transformer training in overparameterized models collapses to low-dimensional manifolds (3-4D) despite high-dimensional parameter spaces, explaining attention concentration, SGD dynamics, and interpretability limitations.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric structure of learning dynamics in overparameterized transformer models and investigate why these models with high-dimensional parameter spaces exhibit surprisingly simple computational behavior.

Method: Using carefully controlled modular arithmetic tasks to study transformer training trajectories, analyzing dimensional collapse to low-dimensional execution manifolds, and examining SGD dynamics projected onto these subspaces.

Result: Transformer training trajectories rapidly collapse onto low-dimensional execution manifolds (3-4D) despite operating in high-dimensional parameter spaces (d=128). This collapse is robust across random seeds and task difficulties, though manifold orientation varies. This geometric structure explains attention concentration, SGD integrability, and sparse autoencoder limitations.

Conclusion: A unifying geometric framework emerges where most parameters absorb optimization interference while core computation occurs in dramatically reduced subspaces, with implications for interpretability, training curriculum design, and understanding overparameterization in neural networks.

Abstract: We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) stochastic gradient descent (SGD) exhibits approximately integrable dynamics when projected onto the execution subspace, with non-integrability confined to orthogonal staging directions, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.

</details>


### [70] [Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation](https://arxiv.org/abs/2602.10506)
*Wei Chen,Xingyu Guo,Shuang Li,Yan Zhong,Zhao Zhang,Fuzhen Zhuang,Hongrui Liu,Libang Zhang,Guo Ye,Huimei He*

Main category: cs.LG

TL;DR: DiffGDA is a diffusion-based graph domain adaptation method that models domain shifts as a continuous-time generative process using SDEs, outperforming discrete-step approaches on multiple real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing graph domain adaptation methods often use discrete strategies (e.g., intermediate graphs, stepwise alignment) that fail to capture the continuous, nonlinear evolution of graph structures in real-world scenarios, making fixed-step alignment inadequate for approximating actual transformation processes.

Method: The authors propose DiffGDA, which models the domain adaptation process as a continuous-time generative process using stochastic differential equations (SDEs). They introduce a domain-aware network to guide the diffusion trajectory toward the target domain along an optimal adaptation path, jointly modeling structural and semantic transitions.

Result: Extensive experiments on 14 graph transfer tasks across 8 real-world datasets show that DiffGDA consistently outperforms state-of-the-art baselines. Theoretical analysis demonstrates that the diffusion process converges to the optimal solution bridging source and target domains in latent space.

Conclusion: DiffGDA presents a novel diffusion-based approach to graph domain adaptation that effectively models continuous, nonlinear domain shifts, offering a more flexible and theoretically grounded solution than previous discrete-step methods.

Abstract: Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \textbf{DiffGDA}, a \textbf{Diff}usion-based \textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.

</details>


### [71] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: This paper investigates flow matching on binary manifolds, identifies a structural mismatch with velocity-based objectives, formalizes prediction-loss alignment, proves that signal-space alignment eliminates singular weighting, and provides guidelines for robust training on discrete domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the transfer of the flow matching paradigm to binary manifolds for generative modeling of discrete data, addressing a latent structural mismatch that arises when x-prediction is coupled with velocity-based objectives.

Method: The method involves formalizing prediction-loss alignment as a necessary condition for flow matching training, proving that re-aligning the objective to the signal space eliminates singular weighting, and examining design choices specific to binary data.

Result: The results show that aligning the objective to the signal space yields uniformly bounded gradients and enables robust training under uniform timestep sampling without reliance on heuristic schedules.

Conclusion: The paper concludes that signal-space alignment is a key principle for robust flow matching on binary and related discrete domains, providing both theoretical foundations and practical guidelines.

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [72] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: Researchers introduce the first reproducible benchmark for Parkinson's disease prediction using ECoG recordings from rat models and propose a Swap-Adversarial Framework (SAF) that addresses inter-subject variability and domain generalization challenges, achieving superior performance across cross-subject, cross-session, and cross-dataset settings.


<details>
  <summary>Details</summary>
Motivation: ECoG offers better spatial resolution and frequency range than EEG for early PD prediction, but reproducible comparisons have been limited by ethical constraints in human studies and lack of open benchmark datasets.

Method: Proposed Swap-Adversarial Framework (SAF) includes: (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation by randomly swapping channels between subjects, and (3) domain-adversarial training to suppress subject-specific bias and learn task-relevant shared features.

Result: The method consistently outperformed all baselines across cross-subject, cross-session, and cross-dataset settings, showing most significant improvements in highly variable environments. It also achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization from ECoG to EEG data.

Conclusion: The study provides the first reproducible PD prediction benchmark using ECoG data and demonstrates that the proposed SAF effectively addresses inter-subject variability and domain generalization challenges, showing robust performance across diverse experimental settings and strong generalization capabilities.

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [73] [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545)
*Yuxin Ma,Nan Chen,Mateo Díaz,Soufiane Hayou,Dmitriy Kunisky,Soledad Villar*

Main category: cs.LG

TL;DR: The paper introduces a principled approach to model upscaling that maintains equivalence between original and widened models, enabling efficient hyperparameter transfer from smaller to larger models without costly retuning.


<details>
  <summary>Details</summary>
Motivation: Current model upscaling methods for transferring knowledge from smaller to larger models are sensitive to hyperparameters that need tuning at target sizes, which is prohibitively expensive. The common workaround of tuning on smaller models and extrapolating via scaling laws may not be sound with upscaling.

Method: 1) Introduces a general upscaling method inspired by μP and any-dimensional architectures, applicable to broad architectures and optimizers, with theory guaranteeing equivalence between original and widened models. 2) Extends μTransfer theory to create a hyperparameter transfer technique for upscaled models.

Result: The method is empirically demonstrated to be effective on realistic datasets and architectures, providing a principled way to transfer hyperparameters from smaller to larger upscaled models.

Conclusion: The paper presents a theoretically-grounded approach to model upscaling that enables efficient hyperparameter transfer, addressing the practical challenge of training multiple model sizes without costly retuning at each scale.

Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>


### [74] [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)
*Kanghyun Noh,Jinheon Choi,Yulwha Kim*

Main category: cs.LG

TL;DR: QTALE enables seamless integration of token-adaptive layer execution with quantization, preserving accuracy while reducing FLOPs and memory footprint for efficient LLM deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs require substantial computational and memory resources, making efficient deployment challenging. While token-adaptive execution reduces FLOPs and quantization reduces memory footprint, naively combining them causes additional accuracy degradation due to reduced redundancy in token-adaptive models.

Method: QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of execution ratio at inference to reintroduce redundancy when needed.

Result: Experimental results show QTALE enables seamless integration of token-adaptive layer execution with quantization with no noticeable accuracy difference, keeping the gap to quantization-only models below 0.5% on CommonsenseQA benchmarks.

Conclusion: QTALE provides an effective solution for efficient LLM deployment by combining token-adaptive execution for FLOPs reduction and quantization for memory savings while preserving accuracy.

Abstract: Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing layers, and quantization, which lowers memory footprint by reducing weight precision. However, naively integrating these techniques leads to additional accuracy degradation due to reduced redundancy in token-adaptive models. We propose QTALE (Quantization-Robust Token-Adaptive Layer Execution for LLMs), a novel framework that enables seamless integration of token-adaptive execution with quantization while preserving accuracy. Conventional token-adaptive methods reduce redundancy in two ways: (1) by limiting the diversity of training paths explored during fine-tuning, and (2) by lowering the number of parameters actively involved in inference. To overcome these limitations, QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of the execution ratio at inference to reintroduce redundancy when needed. Experimental results show that QTALE enables seamless integration of token-adaptive layer execution with quantization, showing no noticeable accuracy difference, with the gap to quantization-only models kept below 0.5% on CommonsenseQA benchmarks. By combining token-adaptive execution for FLOPs reduction and quantization for memory savings, QTALE provides an effective solution for efficient LLM deployment.

</details>


### [75] [Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss](https://arxiv.org/abs/2602.10553)
*Junichiro Takahashi,Masataka Sato,Satoshi Kodeta,Norihiko Takeda*

Main category: cs.LG

TL;DR: This paper proposes a robust ECG encoder for multimodal pretraining using hospital data, adapting SigLIP with a modified multi-label loss, which improves ECG classification and provides insights into prediction difficulty for different ECG findings.


<details>
  <summary>Details</summary>
Motivation: While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at all. Interpreting ECGs is inherently challenging, and diagnostic accuracy can vary depending on the interpreter's experience. Although echocardiography provides rich diagnostic information, it requires specialized equipment and personnel, limiting its availability.

Method: We employ SigLIP, a CLIP based model with a sigmoid based loss function enabling multi label prediction, and introduce a modified loss function tailored to the multi label nature of ECG data. To further enhance performance, we increase the embedding dimensionality and apply random cropping to mitigate data drift.

Result: Experiments demonstrate that incorporating medical knowledge in the language model and applying the modified loss significantly improve multi label ECG classification. Finally, per label analysis reveals which ECG findings are easier or harder to predict.

Conclusion: Our study provides a foundational framework for developing medical models that utilize ECG data.

Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at all. Interpreting ECGs is inherently challenging, and diagnostic accuracy can vary depending on the interpreter's experience. Although echocardiography provides rich diagnostic information, it requires specialized equipment and personnel, limiting its availability. In this study, we focus on constructing a robust ECG encoder for multimodal pretraining using real world hospital data. We employ SigLIP, a CLIP based model with a sigmoid based loss function enabling multi label prediction, and introduce a modified loss function tailored to the multi label nature of ECG data. Experiments demonstrate that incorporating medical knowledge in the language model and applying the modified loss significantly improve multi label ECG classification. To further enhance performance, we increase the embedding dimensionality and apply random cropping to mitigate data drift. Finally, per label analysis reveals which ECG findings are easier or harder to predict. Our study provides a foundational framework for developing medical models that utilize ECG data.

</details>


### [76] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: A reinforcement learning framework called PiT-PO evolves LLMs into adaptive generators for symbolic regression, enforcing physical validity and reducing redundancy to discover scientifically consistent equations.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based symbolic regression frameworks treat the model as a static generator without updating internal representations based on search feedback, leading to physically inconsistent or mathematically redundant expressions.

Method: Proposes PiT-PO (Physics-informed Token-regularized Policy Optimization), a reinforcement learning framework with dual-constraint mechanism: enforces hierarchical physical validity while applying token-level penalties to suppress redundant structures.

Result: Achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. Also enables small-scale models to outperform closed-source giants.

Conclusion: PiT-PO evolves LLMs into adaptive generators that produce scientifically consistent and structurally parsimonious equations, democratizing access to high-performance scientific discovery.

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [77] [Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity](https://arxiv.org/abs/2602.10585)
*Guangzhi Xiong,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: Neural Additive Experts (NAEs) balance interpretability and accuracy by using a mixture of experts framework with dynamic gating and targeted regularization, outperforming standard GAMs while maintaining clear feature attributions.


<details>
  <summary>Details</summary>
Motivation: The trade-off between interpretability and accuracy is a core challenge in machine learning. Standard Generalized Additive Models (GAMs) are interpretable but limited by their strictly additive nature, which can reduce predictive performance. Adding feature interactions can improve accuracy but may obscure individual feature contributions.

Method: The authors propose Neural Additive Experts (NAEs), a mixture of experts framework that learns multiple specialized networks per feature and uses a dynamic gating mechanism to integrate information across features, relaxing additive constraints. They also introduce targeted regularization to reduce variance among expert predictions.

Result: Theoretical analysis and experiments on synthetic data demonstrate the model's flexibility. Extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations.

Conclusion: NAEs provide a flexible framework that effectively balances interpretability and accuracy, offering a practical solution to the trade-off in machine learning models.

Abstract: The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.

</details>


### [78] [Hierarchical Zero-Order Optimization for Deep Neural Networks](https://arxiv.org/abs/2602.10607)
*Sansheng Cao,Zhengyu Ma,Yonghong Tian*

Main category: cs.LG

TL;DR: HZO optimization reduces query complexity from O(ML²) to O(ML log L) for deep networks while maintaining competitive accuracy with backpropagation.


<details>
  <summary>Details</summary>
Motivation: Zeroth-order optimization has biological plausibility and handles non-differentiable objectives, but its computational complexity has limited application in deep neural networks.

Method: Hierarchical Zeroth-Order (HZO) optimization, a divide-and-conquer strategy that decomposes the depth dimension of the network, challenging the conventional layer-by-layer gradient propagation paradigm.

Result: HZO reduces query complexity from O(ML²) to O(ML log L) for networks of width M and depth L, maintains numerical stability near unitary limit (L_lip ≈ 1), and achieves competitive accuracy on CIFAR-10 and ImageNet compared to backpropagation.

Conclusion: HZO represents a significant advancement in zeroth-order optimization, making it more practical for deep neural networks while preserving its biological plausibility and ability to handle non-differentiable objectives.

Abstract: Zeroth-order (ZO) optimization has long been favored for its biological plausibility and its capacity to handle non-differentiable objectives, yet its computational complexity has historically limited its application in deep neural networks. Challenging the conventional paradigm that gradients propagate layer-by-layer, we propose Hierarchical Zeroth-Order (HZO) optimization, a novel divide-and-conquer strategy that decomposes the depth dimension of the network. We prove that HZO reduces the query complexity from $O(ML^2)$ to $O(ML \log L)$ for a network of width $M$ and depth $L$, representing a significant leap over existing ZO methodologies. Furthermore, we provide a detailed error analysis showing that HZO maintains numerical stability by operating near the unitary limit ($L_{lip} \approx 1$). Extensive evaluations on CIFAR-10 and ImageNet demonstrate that HZO achieves competitive accuracy compared to backpropagation.

</details>


### [79] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: Introduces Chamfer-linkage as a new linkage function for HAC, using Chamfer distance, with O(n^2) time complexity, and shows it outperforms classical linkages in quality across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Classical linkage functions like single-linkage, average-linkage, and Ward's method are widely used in HAC but exhibit high variability and inconsistent clustering quality across real-world datasets, lacking a single explicit global objective, necessitating a more robust alternative.

Method: Proposes Chamfer-linkage, a novel linkage function based on Chamfer distance from machine learning and computer vision, designed to satisfy desirable concept representation properties. Implements it within HAC with theoretical analysis showing O(n^2) time complexity, matching classical linkages.

Result: Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets, as demonstrated experimentally.

Conclusion: Chamfer-linkage serves as a practical drop-in replacement for classical linkage functions, enhancing the toolkit for hierarchical clustering with improved robustness and quality in both theory and practice.

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [80] [Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling](https://arxiv.org/abs/2602.10623)
*Zhibin Duan,Guowei Rong,Zhuo Li,Bo Chen,Mingyuan Zhou,Dandan Guo*

Main category: cs.LG

TL;DR: BNRM is a Bayesian non-negative reward model that uses latent factor analysis to reduce reward hacking and biases, enabling robust, interpretable reward learning for aligning LLMs.


<details>
  <summary>Details</summary>
Motivation: Reward models learned from human preferences are central to aligning large language models via reinforcement learning from human feedback, but they are often vulnerable to reward hacking due to noisy annotations and systematic biases like response length or style.

Method: BNRM integrates non-negative factor analysis into the Bradley-Terry preference model, using a sparse, non-negative latent factor generative process with instance-specific latent variables and global latent factor sparsity, along with an amortized variational inference network conditioned on deep model representations for efficient end-to-end training.

Result: Extensive empirical results show that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.

Conclusion: BNRM provides a scalable, robust, and interpretable framework for reward modeling that significantly mitigates reward hacking and improves generalization, offering a principled solution to key challenges in aligning LLMs via human feedback.

Abstract: Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.

</details>


### [81] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: VESPO is a new method that stabilizes RL training for LLMs by using variational optimization to derive a closed-form reshaping kernel for sequence-level importance weights, enabling stable training even with high staleness and asynchronous execution.


<details>
  <summary>Details</summary>
Motivation: Training stability is a major challenge in RL for LLMs due to policy staleness, asynchronous training, and mismatches between training/inference engines, which cause behavior policy divergence and risk training collapse. Importance sampling helps but suffers from high variance, and existing remedies lack unified theoretical foundations.

Method: VESPO (Variational sEquence-level Soft Policy Optimization) incorporates variance reduction into a variational formulation over proposal distributions, deriving a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization.

Result: Experiments on mathematical reasoning benchmarks show VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models.

Conclusion: VESPO provides a theoretically grounded solution to RL training stability in LLMs, enabling robust training even with significant policy staleness and asynchronous execution while improving performance across different model architectures.

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [82] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: ProtoGLAD is an interpretable unsupervised framework for graph-level anomaly detection that explains anomalies by contrasting them with nearest normal prototype graphs, using a point-set kernel to discover prototypes and clusters, achieving competitive performance with better interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing deep graph-level anomaly detection (GLAD) methods are black-box and lack interpretability; some explanation methods either ignore normal graph references or rely on abstract latent prototypes instead of concrete graphs from the dataset.

Method: The method uses a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifies graphs distant from all discovered normal clusters as anomalies.

Result: Extensive experiments on multiple real-world datasets show that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

Conclusion: ProtoGLAD provides a competitive and interpretable solution for graph-level anomaly detection by using explicit normal prototype graphs for both detection and explanation, balancing performance with human-understandable insights.

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [83] [A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors](https://arxiv.org/abs/2602.10451)
*Jinkyo Han,Bahador Bahmani*

Main category: cs.LG

TL;DR: Physics-informed mixture density networks provide interpretable multimodal modeling for scientific systems with regime switching and non-unique physical mechanisms, embedding physics through component-specific regularization.


<details>
  <summary>Details</summary>
Motivation: Many scientific systems exhibit multimodal behavior from latent regime switching and non-unique physical mechanisms, but learning physically consistent conditional distributions while preserving interpretability remains challenging, especially with limited data.

Method: Develops a physics-informed multimodal conditional modeling framework using mixture density networks (MDNs) with component-specific regularization terms that penalize violations of governing equations or physical laws.

Result: The framework achieves competitive performance compared to conditional flow matching models while offering simpler, more interpretable formulation across bifurcation phenomena, stochastic PDEs, and atomistic-scale shock dynamics.

Conclusion: MDNs provide an effective, interpretable approach for physics-constrained multimodal modeling that naturally accommodates non-uniqueness and stochasticity while preserving physical structure and computational efficiency.

Abstract: Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.

</details>


### [84] [Exploring the impact of adaptive rewiring in Graph Neural Networks](https://arxiv.org/abs/2602.10754)
*Charlotte Cambier van Nooten,Christos Aronis,Yuliya Shapovalova,Lucia Cavallaro*

Main category: cs.LG

TL;DR: This paper explores sparsification methods in Graph Neural Networks to reduce memory and computational costs, applying techniques like Erdős-Rényi sparsification to GCN and GIN models on electrical grid contingency assessment. Results show that tuning sparsity is crucial—moderate sparsity improves generalization, but too much hinders learning, with adaptive rewiring plus early stopping offering a promising approach for scalable GNNs in power grid reliability analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address high memory usage and computational costs in large-scale graph applications by using sparsification methods as a form of regularization in Graph Neural Networks (GNNs), aiming to enhance efficiency for real-world applications such as N-1 contingency assessment in electrical grids, a critical task for ensuring grid reliability.

Method: The study explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs), using techniques from Network Science and Machine Learning, including Erdős-Rényi for model sparsification. It applies these methods to Graph Convolutional Networks (GCN) and Graph Isomorphism Networks (GIN) with different degrees of sparsification and rewiring, tested on three datasets of varying sizes. An adaptive rewiring approach is also examined, particularly when combined with early stopping.

Result: Comparison across sparsification levels shows the potential of combining insights from Network Science and Machine Learning to improve GNN performance and scalability. Experiments highlight the importance of tuning sparsity parameters: while sparsity can improve generalization, excessive sparsity may hinder learning of complex patterns. The adaptive rewiring approach, particularly when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training.

Conclusion: This research contributes to understanding how sparsity can be effectively leveraged in GNNs for critical applications like power grid reliability analysis. The adaptive rewiring approach, especially when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training, highlighting the importance of tuning sparsity parameters to balance efficiency and learning capacity.

Abstract: This paper explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs) to address high memory usage and computational costs in large-scale graph applications. Using techniques from Network Science and Machine Learning, including Erdős-Rényi for model sparsification, we enhance the efficiency of GNNs for real-world applications. We demonstrate our approach on N-1 contingency assessment in electrical grids, a critical task for ensuring grid reliability. We apply our methods to three datasets of varying sizes, exploring Graph Convolutional Networks (GCN) and Graph Isomorphism Networks (GIN) with different degrees of sparsification and rewiring. Comparison across sparsification levels shows the potential of combining insights from both research fields to improve GNN performance and scalability. Our experiments highlight the importance of tuning sparsity parameters: while sparsity can improve generalization, excessive sparsity may hinder learning of complex patterns. Our adaptive rewiring approach, particularly when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training. This research contributes to understanding how sparsity can be effectively leveraged in GNNs for critical applications like power grid reliability analysis.

</details>


### [85] [Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation](https://arxiv.org/abs/2602.10457)
*Brian Hyeongseok Kim,Jacqueline L. Mitchell,Chao Wang*

Main category: cs.LG

TL;DR: This paper introduces a method for constructing counterfactual datasets by altering a few training labels to analyze how label bias influences model predictions, offering a new fairness assessment tool that identifies critical training examples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of interpreting deep neural network inference-time behavior by exploring counterfactual datasets instead of input perturbations, aiming to assess fairness by analyzing the influence of label bias on training and inference.

Method: The method involves heuristically ranking and modifying a bounded number of training labels to construct a counterfactual dataset, retraining the model, and checking if the prediction on a chosen test case changes, with analysis techniques to trace bias propagation.

Result: The approach modifies only a small subset of training labels across over 1100 test cases from 7 fairness datasets, effectively pinpointing critical training examples that drive prediction changes, and reveals connections between training examples and test cases.

Conclusion: The paper concludes that counterfactual datasets provide a novel and interpretable method to probe dataset bias, offering insights into how specific training examples influence test predictions, which can enhance fairness auditing and model transparency.

Abstract: Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.

</details>


### [86] [LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers](https://arxiv.org/abs/2602.10770)
*Bram Van Bolderik,Vlado Menkovski,Sonia Heemstra de Groot,Manil Dev Gomony*

Main category: cs.LG

TL;DR: LOREN is a neural receiver that uses low-rank adapters for code-rate adaptation, achieving comparable performance to fully retrained models while reducing hardware overhead by 65% in area and 15% in power.


<details>
  <summary>Details</summary>
Motivation: Neural network receivers outperform traditional receivers but have high memory/power requirements due to needing separate weight sets for each code rate, limiting their practicality.

Method: LOREN integrates lightweight low-rank adaptation adapters into convolutional layers, freezing a shared base network while training only small adapters per code rate. Uses end-to-end training over 3GPP CDL channels for robustness.

Result: LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. Hardware implementation in 22nm technology shows >65% savings in silicon area and up to 15% power reduction when supporting three code rates.

Conclusion: LOREN enables practical neural network receivers by providing efficient code-rate adaptation with minimal overhead, making neural receivers more viable for real-world deployment.

Abstract: Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propose LOREN, a Low Rank-Based Code-Rate Adaptation Neural Receiver that achieves adaptability with minimal overhead. LOREN integrates lightweight low rank adaptation adapters (LOREN adapters) into convolutional layers, freezing a shared base network while training only small adapters per code rate. An end-to-end training framework over 3GPP CDL channels ensures robustness across realistic wireless environments. LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. The hardware implementation of LOREN in 22nm technology shows more than 65% savings in silicon area and up to 15% power reduction when supporting three code rates.

</details>


### [87] [Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks](https://arxiv.org/abs/2602.10780)
*Enrico Ahlers,Daniel Passon,Yannic Noller,Lars Grunske*

Main category: cs.LG

TL;DR: FIRE is an inference-time backdoor mitigation method that neutralizes triggers by reversing their feature-space directions in a model's latent layers, outperforming existing runtime defenses with low overhead.


<details>
  <summary>Details</summary>
Motivation: Machine learning models are increasingly deployed in real-world systems, making them targets for adversarial attacks like backdoors introduced via poisoned training data. Existing mitigations often require modifying training data, altering the model, or performing expensive input transformations, which are ineffective or inefficient for already deployed models. There is a need for a runtime mitigation approach that can neutralize backdoors without retraining or heavy computational overhead.

Method: FIRE (Feature-space Inference-time REpair) hypothesizes that a backdoor trigger induces structured and repeatable changes in the model's internal feature representations. It treats the trigger as a direction in the latent spaces between layers and applies reverse transformations to neutralize the trigger during inference. The approach manipulates latent representations to move poisoned samples' features along the backdoor directions, effectively correcting the inference mechanism without modifying the model or input data.

Result: FIRE outperforms current runtime mitigation methods across various image benchmarks, attacks, datasets, and network architectures, while maintaining low computational overhead.

Conclusion: FIRE provides an effective and efficient inference-time defense against backdoor attacks by leveraging the internal feature-space structure of the model. It turns the backdoored model against itself to mitigate triggers with minimal computational cost, offering a practical solution for deployed vulnerable models.

Abstract: Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.

</details>


### [88] [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794)
*Benjy Friedmann,Nadav Dym*

Main category: cs.LG

TL;DR: CycFlow replaces diffusion-based heatmap generation with deterministic point transport for TSP, achieving 1000x faster solving with competitive optimality.


<details>
  <summary>Details</summary>
Motivation: Current NCO approaches are dominated by diffusion models that treat TSP as stochastic heatmap generation, which suffers from quadratic computational bottlenecks due to edge scoring.

Method: CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement. The optimal tour is recovered from this 2N-dimensional representation via angular sorting, using data-dependent flow matching instead of iterative edge denoising.

Result: CycFlow accelerates solving speed by up to three orders of magnitude (1000x faster) compared to state-of-the-art diffusion baselines while maintaining competitive optimality gaps.

Conclusion: By replacing stochastic edge denoising with deterministic point transport, CycFlow achieves a paradigm shift that bypasses quadratic computational bottlenecks in favor of linear coordinate dynamics, enabling much faster TSP solving with comparable solution quality.

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.

</details>


### [89] [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847)
*Fanpu Cao,Lu Dai,Jindong Han,Hui Xiong*

Main category: cs.LG

TL;DR: GTR enhances MTSF by capturing long-term periodic patterns without extending input horizon.


<details>
  <summary>Details</summary>
Motivation: Existing models rely on limited historical context, missing global periodic patterns, and extending input windows causes overfitting and high costs.

Method: GTR is a lightweight module that uses adaptive global temporal embeddings, dynamically retrieves global segments, and models local and global dependencies via 2D convolution and residual fusion.

Result: Extensive experiments on six datasets show GTR achieves state-of-the-art performance with minimal overhead.

Conclusion: GTR is an efficient, plug-and-play solution for improving global periodicity modeling in MTSF.

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.

</details>


### [90] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: MVGR-Net is a two-stage framework for ride-hailing forecasting that learns comprehensive geospatial representations from POI and mobility data, then fine-tunes LLMs with prompts to incorporate external events, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Ride-hailing services have transformed urban mobility, making accurate forecasting crucial for optimizing passenger experience and transportation efficiency. However, existing methods struggle with geospatial heterogeneity and high susceptibility to external events.

Method: Two-stage approach: 1) Pretraining stage learns comprehensive geospatial representations by integrating Points-of-Interest (POI) and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. 2) Forecasting stage uses a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events.

Result: Extensive experiments on DiDi's real-world datasets demonstrate state-of-the-art performance in ride-hailing forecasting.

Conclusion: MVGR-Net effectively addresses the challenges of geospatial heterogeneity and external events in ride-hailing forecasting through its multi-view representation learning and LLM fine-tuning approach.

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [91] [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848)
*Luigi Simeone*

Main category: cs.LG

TL;DR: TSFMs show strong zero-shot electricity forecasting performance, maintaining accuracy even with minimal context and outperforming traditional methods like Prophet, though calibration varies significantly across models.


<details>
  <summary>Details</summary>
Motivation: To determine if Time Series Foundation Models (TSFMs) with zero-shot prediction capabilities are suitable for mission-critical applications like electricity demand forecasting, where accuracy, calibration, and robustness directly impact grid operations.

Method: Multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, TinyTimeMixer) against Prophet, SARIMA, and Seasonal Naive baselines using ERCOT hourly load data (2020-2024) on consumer-grade hardware. Evaluation includes context length sensitivity (24-2048h), probabilistic forecast calibration, robustness under distribution shifts (COVID-19, Winter Storm Uri), and prescriptive analytics.

Result: Top TSFMs achieve MASE values near 0.31 at long context lengths (2048h), a 47% reduction over Seasonal Naive baseline. TSFMs maintain stable accuracy even with minimal context (24h), while Prophet fails when fitting window is shorter than seasonality period (MASE > 74). Calibration varies: Chronos-2 produces well-calibrated intervals (95% empirical coverage at 90% nominal), while Moirai-2 and Prophet exhibit overconfidence (~70% coverage).

Conclusion: TSFMs demonstrate practical advantages for electricity forecasting, particularly in handling minimal context and recognizing pre-learned temporal patterns. However, calibration performance varies, requiring careful model selection for mission-critical applications. The benchmark provides guidelines for operational deployment.

Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly affect grid operations--remains an open question. We present a multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, and TinyTimeMixer) alongside Prophet as an industry-standard baseline and two statistical references (SARIMA and Seasonal Naive), using ERCOT hourly load data from 2020 to 2024. All experiments run on consumer-grade hardware (AMD Ryzen 7, 16GB RAM, no GPU). The evaluation spans four axes: (1) context length sensitivity from 24 to 2048 hours, (2) probabilistic forecast calibration, (3) robustness under distribution shifts including COVID-19 lockdowns and Winter Storm Uri, and (4) prescriptive analytics for operational decision support. The top-performing foundation models achieve MASE values near 0.31 at long context lengths (C = 2048h, day-ahead horizon), a 47% reduction over the Seasonal Naive baseline. The inclusion of Prophet exposes a structural advantage of pre-trained models: Prophet fails when the fitting window is shorter than its seasonality period (MASE > 74 at 24-hour context), while TSFMs maintain stable accuracy even with minimal context because they recognise temporal patterns learned during pre-training rather than estimating them from scratch. Calibration varies substantially across models--Chronos-2 produces well-calibrated prediction intervals (95% empirical coverage at 90% nominal level) while both Moirai-2 and Prophet exhibit overconfidence (~70% coverage). We provide practical model selection guidelines and release the complete benchmark framework for reproducibility.

</details>


### [92] [ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents](https://arxiv.org/abs/2602.10863)
*Cong Pang,Xuyu Feng,Yujie Yi,Zixuan Chen,Jiawei Hong,Tiankuo Yao,Nang Yuan,Jiapeng Luo,Lewei Lu,Xin Lou*

Main category: cs.LG

TL;DR: A visual-native search framework using webpage snapshots and information-aware credit assignment to improve reinforcement learning for web information seeking.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for web information seeking suffers from low signal-to-noise feedback due to text parsers discarding layout semantics and sparse outcome rewards that obscure which retrieval actions matter.

Method: Proposes a visual-native search framework representing webpages as visual snapshots, and introduces Information-Aware Credit Assignment (ICA) - a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis, integrated with a GRPO-based training pipeline.

Result: The approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, showing that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck.

Conclusion: Visual-native representation with information-aware credit assignment effectively addresses the credit-assignment problem in open-ended web environments for reinforcement learning agents.

Abstract: Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.

</details>


### [93] [Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving](https://arxiv.org/abs/2602.10512)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: cs.LG

TL;DR: A theoretical analysis showing that hierarchical/cut-aware learners need exponentially less data than flat learners for theorem proving when proofs have reusable structure.


<details>
  <summary>Details</summary>
Motivation: To theoretically explain the gap between worst-case hardness and empirical success of LLM-guided formal theorem proving, and provide justification for subgoal decomposition in agentic theorem provers.

Method: Model tactic proposal as stochastic policy in finite-horizon deterministic MDP, treat state/action spaces as compact metric spaces, analyze problem distributions generated by reference policy with latent-variable model representing proof DAG structure, and derive bounds under top-k search with Tsybakov-type margin conditions.

Result: When cut elimination expands DAG of depth D into cut-free tree of size Ω(Λ^D) while cut-aware hierarchical process has size O(λ^D) with λ≪Λ, flat learners require exponentially more data than hierarchical learners.

Conclusion: Provides principled justification for subgoal decomposition in agentic theorem provers, explaining why hierarchical/cut-aware approaches outperform flat approaches in practice.

Abstract: We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

</details>


### [94] [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: FedPS is a federated data preprocessing framework that uses aggregated statistics for efficient and privacy-preserving feature scaling, encoding, discretization, and missing-value imputation in FL systems.


<details>
  <summary>Details</summary>
Motivation: Preprocessing is critical for model performance in FL, but privacy constraints and communication efficiency challenges are often overlooked in FL research, creating a need for distributed preprocessing solutions.

Method: Based on data-sketching techniques to summarize local datasets, FedPS designs federated algorithms for preprocessing tasks like feature scaling and missing-value imputation, and extends them to horizontal and vertical FL settings.

Result: FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments, enabling effective preprocessing without centralizing raw data.

Conclusion: The FedPS framework addresses key gaps in FL by offering a unified approach to federated data preprocessing, enhancing model performance while maintaining privacy and efficiency.

Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.

</details>


### [95] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: RLTT is a reinforcement learning framework that distributes rewards across full latent reasoning trajectories in LoopLMs, solving credit assignment problems and significantly improving mathematical reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Standard reinforcement learning objectives like GRPO only assign credit to final latent states in LoopLMs, creating a fundamental mismatch with the model's internal multi-step reasoning process, which limits improvement attempts.

Method: RLTT (Reward Latent Thought Trajectories) distributes reward across the full latent reasoning trajectory, providing dense, trajectory-level credit assignment without relying on external verifiers, with negligible overhead compared to GRPO.

Result: RLTT yields substantial improvements over GRPO on mathematical reasoning benchmarks: +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. It also transfers effectively to non-mathematical reasoning despite being trained exclusively on mathematics.

Conclusion: Trajectory-level credit assignment through RLTT is an effective reinforcement learning framework for LoopLMs, solving the credit assignment problem and significantly improving reasoning capabilities across domains.

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [96] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: Proposed model-free RL algorithm for board games achieves more efficient learning than search-based methods like AlphaZero while requiring less computation.


<details>
  <summary>Details</summary>
Motivation: Search-based RL methods like AlphaZero have achieved great success in board games but have significant computational demands that hinder reproducibility. The authors aim to develop a more efficient model-free approach.

Method: Proposed a model-free reinforcement learning algorithm designed specifically for board games. Conducted comprehensive experiments on five diverse board games (Animal Shogi, Gardner Chess, Go, Hex, Othello) and performed extensive ablation studies.

Result: The proposed method achieves more efficient learning than existing methods across all five board game environments. Ablation studies demonstrate the importance of the core techniques used in the method.

Conclusion: The efficient model-free RL algorithm shows potential for board game domains traditionally dominated by search-based methods, offering more accessible and computationally efficient alternatives.

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [97] [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959)
*Feilong Liu*

Main category: cs.LG

TL;DR: RoPE reinterpreted as phase modulation reveals lower bounds (aliasing, stability) and an upper bound (floating-point precision) for base parameter, creating a 'Goldilocks zone' for long-context transformers; validated on LLaMA, Mistral, DeepSeek.


<details>
  <summary>Details</summary>
Motivation: RoPE's behavior at long context lengths is poorly understood, leading to issues like attention collapse and degradation in large language models when scaling context. A principled theoretical framework is needed to predict and prevent these failures.

Method: Reinterpret RoPE as phase modulation on complex oscillators, apply classical signal processing theory to derive lower bounds (aliasing and DC stability) and an upper bound (finite floating-point resolution), and validate with case studies of LLaMA, Mistral, and DeepSeek models.

Result: Derived lower bounds (aliasing and DC stability) and an upper bound (floating-point precision) for RoPE base parameter, defining a feasibility region. Validated that model successes/failures and community retrofits align with bounds; models violating stability bound show attention collapse, and scaling beyond 1M tokens encounters a precision wall.

Conclusion: The analysis establishes a 'Goldilocks zone' for RoPE base parameters, defined by lower bounds (aliasing and DC stability) and an upper bound (floating-point precision), which together determine feasible long-context performance. Models violating these bounds exhibit attention collapse or positional erasure, and scaling beyond ~1M tokens hits a hard precision wall.

Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.

</details>


### [98] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: DAWN addresses two key bottlenecks in residual RL value learning through base-policy transition anchoring and critic normalization, achieving substantial efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Residual RL enables stable online refinement of pretrained policies, but value learning poses unique challenges that remain poorly understood, specifically cold start pathology and structural scale mismatch.

Method: Proposes DAWN (Data-Anchored Warmup and Normalization) with two key solutions: 1) using base-policy transitions as value anchors for implicit warmup, and 2) critic normalization to restore representation sensitivity for discerning value differences.

Result: DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities by addressing the identified bottlenecks.

Conclusion: Simple yet principled solutions suffice to address fundamental bottlenecks in residual RL value learning, enabling more efficient online policy refinement through anchored warmup and normalization techniques.

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [99] [RiemannGL: Riemannian Geometry Changes Graph Deep Learning](https://arxiv.org/abs/2602.10982)
*Li Sun,Qiqi Wan,Suyang Zhou,Zhenhao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: This paper argues that Riemannian geometry should be the foundational framework for graph representation learning, proposing it as a unifying paradigm rather than isolated techniques, and outlines research directions to advance this field.


<details>
  <summary>Details</summary>
Motivation: Graphs have non-Euclidean structures with complex interactions, requiring principled mathematical foundations. Current approaches are limited to narrow manifolds (mainly hyperbolic spaces) and extrinsic formulations, lacking intrinsic manifold structures for graph neural networks.

Method: The paper provides a conceptual framework and research agenda rather than empirical methods. It identifies gaps in existing approaches and proposes a structured research agenda along three dimensions: manifold type, neural architecture, and learning paradigm.

Result: The paper establishes a coherent viewpoint that Riemannian geometry should be the foundational framework for graph learning. It outlines key research directions, open challenges, theoretical foundations, and promising directions for advancing Riemannian graph learning.

Conclusion: Riemannian geometry provides a principled and necessary foundation for graph representation learning. This paper aims to stimulate broader exploration of Riemannian geometry as a unifying paradigm for future graph learning research, moving beyond current limitations of hyperbolic-only approaches and extrinsic formulations.

Abstract: Graphs are ubiquitous, and learning on graphs has become a cornerstone in artificial intelligence and data mining communities. Unlike pixel grids in images or sequential structures in language, graphs exhibit a typical non-Euclidean structure with complex interactions among the objects. This paper argues that Riemannian geometry provides a principled and necessary foundation for graph representation learning, and that Riemannian graph learning should be viewed as a unifying paradigm rather than a collection of isolated techniques. While recent studies have explored the integration of graph learning and Riemannian geometry, most existing approaches are limited to a narrow class of manifolds, particularly hyperbolic spaces, and often adopt extrinsic manifold formulations. We contend that the central mission of Riemannian graph learning is to endow graph neural networks with intrinsic manifold structures, which remains underexplored. To advance this perspective, we identify key conceptual and methodological gaps in existing approaches and outline a structured research agenda along three dimensions: manifold type, neural architecture, and learning paradigm. We further discuss open challenges, theoretical foundations, and promising directions that are critical for unlocking the full potential of Riemannian graph learning. This paper aims to provide a coherent viewpoint and to stimulate broader exploration of Riemannian geometry as a foundational framework for future graph learning research.

</details>


### [100] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: A hybrid architecture for automated EEG monitoring that separates measurement extraction from text generation to guarantee clinical accuracy in reports, achieving 60% fewer false alarms and 50% faster detection.


<details>
  <summary>Details</summary>
Motivation: Clinical EEG recordings exceed LLM context windows, requiring extreme compression that destroys fine-grained temporal precision; LLMs lack inherent time-series comprehension, causing hallucinations of clinically incorrect measurement values.

Method: Hybrid architecture: computes exact clinical values via signal processing before compression, uses a cross-modal bridge for EEG-to-language translation, employs parameter-efficient fine-tuning with constrained decoding around frozen slots, and applies multirate sampling to maintain long-range context with event-level precision.

Result: Evaluation on TUH and CHB-MIT datasets shows 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision; first system guaranteeing clinical measurement accuracy in automated EEG reports.

Conclusion: The approach effectively overcomes limitations of LLMs in EEG analysis by ensuring accurate measurement extraction, leading to reliable automated reporting with improved performance.

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [101] [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008)
*Ammar Ali,Baher Mohammad,Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: ROCKET is a training-free model compression method that formulates layer-wise compression as a knapsack problem and uses single-step sparse matrix factorization, achieving state-of-the-art compression performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current model compression methods often require extensive training, iterative optimization, or fine-tuning, which can be computationally expensive. There's a need for efficient training-free compression that maintains high performance while significantly reducing model size.

Method: ROCKET uses two key innovations: 1) Formulates layer-wise compression allocation as a multi-choice knapsack problem to select optimal compression levels per layer while minimizing reconstruction error within target size constraints. 2) Introduces single-step sparse matrix factorization inspired by dictionary learning, which sparsifies weights based on activation-weight sensitivity and updates dictionaries via closed-form least squares, bypassing iterative optimization and backpropagation.

Result: ROCKET outperforms existing compression methods (factorization, structured-sparsification, dynamic compression) at 20-50% compression rates. It retains over 90% of original model performance at 30% compression without fine-tuning. With light fine-tuning (30M tokens), compressing Qwen3-14B to 8B parameters yields performance nearly matching original Qwen3-8B.

Conclusion: ROCKET provides an effective training-free compression approach that achieves state-of-the-art performance through intelligent layer-wise compression allocation and efficient single-step factorization, offering a practical solution for model compression with minimal computational overhead.

Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.

</details>


### [102] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: A method for offline safe imitation learning from demonstrations without safety/reward labels, using non-preferred trajectories to infer safety constraints.


<details>
  <summary>Details</summary>
Motivation: Real-world domains require safe policies but have two challenges: 1) online learning is risky, and 2) specifying accurate safety costs is difficult. However, collecting non-preferred (unsafe) trajectories is often feasible.

Method: Formulate as Constrained MDP (CMDP), derive lower bound on reward objective, learn cost model estimating likelihood of non-preferred behavior from offline demonstrations, propose OSIL algorithm.

Result: Empirical results show OSIL learns safer policies satisfying cost constraints without degrading reward performance, outperforming several baselines.

Conclusion: OSIL enables learning safe and reward-maximizing policies purely from offline demonstrations with non-preferred trajectories, addressing practical constraints of real-world safe imitation learning.

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [103] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: Activation-based data attribution traces model behavior changes to training data, identifies harmful emergent behaviors like distractor-triggered compliance, and enables effective mitigation through data filtering or label switching, outperforming existing methods in cost and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a method that can trace and attribute specific, often harmful, behavioral changes in post-trained language models back to responsible training datapoints, enabling the identification and mitigation of emergent safety issues that arise from contaminated preference data.

Method: The method involves computing activation-difference vectors for test prompts and preference pairs, ranking training datapoints by cosine similarity to these vectors, and causally validating attributions by retraining models with modified data. It also uses clustering on behavior-datapoint similarity matrices for unsupervised discovery of emergent behaviors.

Result: The method successfully identified 'distractor-triggered compliance,' a harmful behavior in OLMo 2's DPO training, where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduced this behavior by 63%, and switching their labels achieved a 78% reduction. It outperformed gradient-based attribution and LLM-judge baselines while being over 10 times cheaper.

Conclusion: The study concludes that activation-based data attribution is a highly effective, efficient, and practical method for identifying and mitigating harmful emergent behaviors in post-trained language models, outperforming existing approaches and providing a realistic benchmark for safety research.

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [104] [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565)
*Abhijeet Vyas,Brian Bullins*

Main category: cs.LG

TL;DR: Online min-max optimization with new performance measures beyond convex-concave settings, proposing algorithms for static duality gap and dynamic saddle point regret under various conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations of static Nash equilibrium in online min-max optimization, particularly its incompatibility with individual regrets even in strongly convex-strongly concave settings. The authors aim to develop better performance measures and algorithms for online min-max problems.

Method: Proposes static duality gap (SDual-Gap_T) inspired by online convex optimization framework, and introduces dynamic saddle point regret (DSP-Reg_T). Uses reduction to classic OCO problems to develop algorithms achieving bounds under various conditions including strong convexity-strong concavity, min-max exponential concavity, and two-sided Polyak-Łojasiewicz condition.

Result: Develops algorithms that achieve bounds for SDual-Gap_T and DSP-Reg_T under different function classes. Establishes a class of functions satisfying min-max exponential concavity that captures a two-player portfolio selection variant. Derives bounds under two-sided PL condition for dynamic regret compatible with individual regrets.

Conclusion: The paper introduces new performance measures for online min-max optimization and provides algorithmic solutions with theoretical guarantees under various conditions, expanding the scope beyond traditional convex-concave settings.

Abstract: We propose and study an online version of min-max optimization based on cumulative saddle points under a variety of performance measures beyond convex-concave settings. After first observing the incompatibility of (static) Nash equilibrium (SNE-Reg$_T$) with individual regrets even for strongly convex-strongly concave functions, we propose an alternate \emph{static} duality gap (SDual-Gap$_T$) inspired by the online convex optimization (OCO) framework. We provide algorithms that, using a reduction to classic OCO problems, achieve bounds for SDual-Gap$_T$~and a novel \emph{dynamic} saddle point regret (DSP-Reg$_T$), which we suggest naturally represents a min-max version of the dynamic regret in OCO. We derive our bounds for SDual-Gap$_T$~and DSP-Reg$_T$~under strong convexity-strong concavity and a min-max notion of exponential concavity (min-max EC), and in addition we establish a class of functions satisfying min-max EC~that captures a two-player variant of the classic portfolio selection problem. Finally, for a dynamic notion of regret compatible with individual regrets, we derive bounds under a two-sided Polyak-Łojasiewicz (PL) condition.

</details>


### [105] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: GRASP is a feature selection framework for medical prediction that combines SHAP-based group importance scoring with group L21 regularization to identify compact, non-redundant, and stable feature sets while maintaining predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Feature selection in medical prediction faces challenges with existing methods like LASSO, which often lack robustness and interpretability. There's a need for methods that can identify compact, non-redundant feature sets while maintaining predictive performance.

Method: GRASP couples Shapley value driven attribution (SHAP) with group L21 regularization. It first distills group-level importance scores from a pretrained tree model using SHAP, then enforces structured sparsity through group L21 regularized logistic regression to obtain stable and interpretable feature selections.

Result: Extensive comparisons show GRASP consistently delivers comparable or superior predictive accuracy compared to LASSO, SHAP, and deep learning methods, while identifying fewer, less redundant, and more stable features.

Conclusion: GRASP provides a novel framework for feature selection in medical prediction that addresses limitations of existing methods by combining interpretable SHAP attribution with structured regularization, yielding robust, compact, and interpretable feature sets.

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [106] [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568)
*Lev McKinney,Anvith Thudi,Juhan Bae,Tara Rezaei,Nicolas Papernot,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: K-FADE is a new LLM unlearning method that uses K-FAC approximations to compute efficient Gauss-Newton steps, effectively suppressing unwanted outputs while minimizing performance degradation on desired data.


<details>
  <summary>Details</summary>
Motivation: Standard LLM training can produce undesirable outputs, and existing unlearning methods that remove specific data often degrade model performance on other distributions where the model's behavior should be retained.

Method: The approach uses the forget set to compute a few uphill Gauss-Newton steps with parametric Hessian approximations (specifically K-FAC) to transform output constraints on the retain set into weight constraints, enabling minimal changes to desired behavior.

Result: K-FADE effectively suppresses outputs from the forget set while approximating retraining results without the forget set, with less alteration to retain set outputs than previous methods, and allows unlearning updates to be reapplied after further training.

Conclusion: K-FADE provides a state-of-the-art, conceptually simple approach to LLM unlearning that maintains model performance on desired distributions while efficiently removing unwanted behaviors.

Abstract: Standard large language model training can create models that produce outputs their trainer deems unacceptable in deployment. The probability of these outputs can be reduced using methods such as LLM unlearning. However, unlearning a set of data (called the forget set) can degrade model performance on other distributions where the trainer wants to retain the model's behavior. To improve this trade-off, we demonstrate that using the forget set to compute only a few uphill Gauss-Newton steps provides a conceptually simple, state-of-the-art unlearning approach for LLMs. While Gauss-Newton steps adapt Newton's method to non-linear models, it is non-trivial to efficiently and accurately compute such steps for LLMs. Hence, our approach crucially relies on parametric Hessian approximations such as Kronecker-Factored Approximate Curvature (K-FAC). We call this combined approach K-FADE (K-FAC for Distribution Erasure). Our evaluation on the WMDP and ToFU benchmarks demonstrates that K-FADE suppresses outputs from the forget set and approximates, in output space, the results of retraining without the forget set. Critically, our method does this while altering the outputs on the retain set less than previous methods. This is because K-FADE transforms a constraint on the model's outputs across the entire retain set into a constraint on the model's weights, allowing the algorithm to minimally change the model's behavior on the retain set at each step. Moreover, the unlearning updates computed by K-FADE can be reapplied later if the model undergoes further training, allowing unlearning to be cheaply maintained.

</details>


### [107] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: This paper introduces a flexible $f$-divergence approach for offline RL that adaptively constrains learning objectives based on dataset characteristics, balancing RL optimization with behavior policy constraints to improve performance on challenging datasets with limited exploration and diverse policies.


<details>
  <summary>Details</summary>
Motivation: Practical offline RL datasets often contain limited exploration and diverse behavior policies with varying expertise levels, which can impair value estimation and lead to overly conservative constraints. The authors aim to develop a method that balances the RL objective with adaptive behavior policy constraints to improve learning from such challenging datasets.

Method: The authors derive a connection between $f$-divergence and Bellman residual constraints through a generalized Linear Programming formulation for RL and convex conjugates. They introduce a flexible $f$-divergence formulation to adaptively constrain learning objectives based on the offline dataset's characteristics.

Result: Experiments on MuJoCo, Fetch, and AdroitHand environments demonstrate the correctness of the proposed Linear Programming formulation and show that the flexible $f$-divergence improves performance when applied to compatible constrained optimization algorithms.

Conclusion: The paper concludes that the proposed Linear Programming formulation and flexible $f$-divergence approach effectively balance the RL objective with behavior policy constraints, enabling improved performance in offline RL when learning from challenging datasets with limited diversity and exploration.

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [108] [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](https://arxiv.org/abs/2602.11090)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: A cross-regularized uncertainty framework learns adaptive uncertainty controls during training for neural PDE surrogates, improving calibration and focusing uncertainty in high-error areas without per-regime tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for calibrated uncertainty in neural PDE surrogates deployed in data-limited or partially observed regimes, where downstream decisions depend on reliable uncertainty estimates. Existing approaches (ensemble replication, fixed stochastic noise, post hoc calibration) are limited, so a method that learns adaptive uncertainty controls during training is proposed.

Method: The method involves a cross-regularized training framework where the predictor is optimized on a training split for fit, while low-dimensional uncertainty controls are optimized on a separate regularization split to reduce train-test mismatch. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components like spectral modes. The approach is instantiated using Fourier Neural Operators and evaluated on APEBench sweeps over observed fraction and training-set size.

Result: Across APEBench sweeps over observed fraction and training-set size, the learned predictive distributions are better calibrated on held-out splits, and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.

Conclusion: The paper concludes that the proposed cross-regularized uncertainty framework effectively learns adaptive uncertainty controls during training, leading to better calibrated predictive distributions and uncertainty fields that concentrate in high-error regions. This approach provides a practical and flexible method for obtaining reliable uncertainty estimates in neural PDE surrogates without requiring per-regime tuning.

Abstract: Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.

</details>


### [109] [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: Proposes a control-driven clipping strategy that adapts differential privacy clipping thresholds using a lightweight spectral diagnostic from model parameters, avoiding per-example gradient computations and maintaining privacy without extra cost.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive clipping methods for differentially private training often rely on per-example gradient norm statistics, which adds computational overhead and is sensitive to datasets and architectures. The clipping threshold is critical but hard to set optimally: too small causes optimization bias, too large degrades accuracy with noise.

Method: Uses a control-driven clipping strategy that adapts the threshold via a lightweight weight-only spectral diagnostic. At periodic probe steps, analyzes a designated weight matrix via spectral decomposition to estimate a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain.

Result: The method produces threshold updates that are post-processing of parameters from privacy-preserving training, so they don't increase privacy loss beyond the underlying DP optimizer under standard composition accounting. This avoids computational overhead from per-example gradient statistics.

Conclusion: The proposed control-driven clipping strategy provides an efficient, privacy-preserving way to adapt clipping thresholds in differentially private training, addressing the limitations of existing adaptive methods while maintaining training stability and accuracy.

Abstract: Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.

</details>


### [110] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: Weight decay during pretraining boosts model plasticity, improving downstream fine-tuning performance despite possibly worse base model metrics.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter optimization and scaling laws typically focus on base model validation loss, neglecting downstream adaptability. This study aims to explore pretraining from the perspective of model plasticity.

Method: Systematic experiments are conducted to examine the role of weight decay, a key regularization parameter, in pretraining and its effects on model plasticity.

Result: Models trained with larger weight decay values show higher plasticity, leading to better performance after fine-tuning on downstream tasks. This includes findings such as linearly separable representations, regularized attention matrices, and reduced overfitting.

Conclusion: The work highlights the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and reveals the complex role of weight decay in shaping model behavior.

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [111] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: TRACE is a framework that decomposes risk change under covariate shift into four interpretable factors, providing a computable diagnostic and a deployment gate score for safe model replacement.


<details>
  <summary>Details</summary>
Motivation: When a source-trained model is replaced by a new model trained on shifted data, its performance on the source domain can change unpredictably. This makes it difficult to understand why performance changes and to decide when it is safe to deploy a new model without extensive labeling.

Method: TRACE decomposes the absolute risk change into an interpretable upper bound with four terms: two generalization gaps (estimated on held-out data), a model change penalty (controlled by average output distance between models on target sample), and a covariate shift penalty (estimated via a model sensitivity factor from high-quantile input gradients and a data-shift measure using feature-space Optimal Transport by default, with a robust alternative using Maximum Mean Discrepancy).

Result: The TRACE framework provides a valid and interpretable decomposition of the absolute risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty. Experiments in linear regression, synthetic, and vision benchmarks show that the TRACE bound correctly captures the scaling of the true risk difference with shift magnitude, maintains a strong monotonic relationship with true performance degradation, and yields a deployment gate score that correlates strongly with the absolute risk change and achieves high AUROC/AUPRC for gating decisions.

Conclusion: TRACE offers a powerful diagnostic tool for understanding performance changes under covariate shift, enabling safe and label-efficient model replacement via a deployment gate score that reliably predicts risk degradation.

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [112] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: The paper introduces GENIUS, a benchmark for evaluating Generative Fluid Intelligence (GFI) in Unified Multimodal Models, focusing on pattern induction, constraint execution, and contextual adaptation, revealing significant deficits due to poor context comprehension.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Unified Multimodal Models (UMMs) primarily assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This overlooks Generative Fluid Intelligence (GFI)—the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. The authors aim to rigorously evaluate this under-explored capability.

Method: The authors formalize GFI as a synthesis of three primitives: Inducing Implicit Patterns, Executing Ad-hoc Constraints, and Adapting to Contextual Knowledge. They introduce GENIUS, a benchmark suite to evaluate these primitives, and conduct a systematic evaluation of 12 representative models. A diagnostic analysis disentangles failure modes, attributing deficits to context comprehension issues.

Result: The systematic evaluation reveals significant performance deficits in GFI tasks across 12 representative models. Diagnostic analysis shows these failures stem from limited context comprehension rather than insufficient intrinsic generative capability.

Conclusion: The paper concludes that GENIUS establishes a rigorous standard for assessing Generative Fluid Intelligence (GFI) in UMMs, highlighting significant performance deficits due to limited context comprehension rather than insufficient generative capability. It proposes a training-free attention intervention strategy to bridge this gap, guiding the field toward dynamic, general-purpose reasoning beyond mere knowledge utilization.

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


### [113] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: RI-FedAvg is a novel federated learning algorithm that uses a Roughness Index regularization to mitigate client drift in non-IID settings, achieving better accuracy and faster convergence than existing methods.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in non-IID settings due to client drift, which impairs convergence and model performance. Existing methods struggle to handle heterogeneous data distributions across clients effectively.

Method: Proposes RI-FedAvg, which incorporates a Roughness Index-based regularization term into the local objective. The RI quantifies the roughness of high-dimensional loss functions and adaptively penalizes updates based on local loss landscape fluctuations to mitigate client drift.

Result: Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 show RI-FedAvg outperforms state-of-the-art baselines (FedAvg, FedProx, FedDyn, SCAFFOLD, DP-FedAvg) in non-IID scenarios, achieving higher accuracy and faster convergence. Theoretical convergence analysis proves it converges to a stationary point under standard assumptions.

Conclusion: RI-FedAvg effectively mitigates client drift in federated learning for non-IID data, enhancing robustness and efficiency in practical heterogeneous environments. The Roughness Index regularization provides a promising approach for handling data heterogeneity in FL.

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [114] [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602)
*Yutao Chen,Jasmine Bayrooti,Steven Morad*

Main category: cs.LG

TL;DR: A new optimization method (nGEM) for mixture density networks that leverages information geometry and EM framework to achieve faster convergence and better performance than standard NLL training.


<details>
  <summary>Details</summary>
Motivation: Standard training of mixture density networks using negative log-likelihood (NLL) suffers from slow convergence and mode collapse problems, limiting their effectiveness for representing continuous multimodal conditional densities.

Method: The authors interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, revealing connections to natural gradient descent. They then derive the natural gradient expectation maximization (nGEM) objective to improve optimization.

Result: Empirically, nGEM achieves up to 10× faster convergence with almost zero computational overhead, and scales well to high-dimensional data where standard NLL training fails.

Conclusion: Integrating information geometry into the training of mixture density networks through the nGEM framework provides significant optimization improvements, addressing the limitations of standard NLL training while maintaining computational efficiency.

Abstract: Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode collapse. In this work, we improve the optimization of mixture density networks by integrating their information geometry. Specifically, we interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, which reveals surprising theoretical connections to natural gradient descent. We then exploit such connections to derive the natural gradient expectation maximization (nGEM) objective. We show that empirically nGEM achieves up to 10$\times$ faster convergence while adding almost zerocomputational overhead, and scales well to high-dimensional data where NLL otherwise fails.

</details>


### [115] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: dnaHNet is a tokenizer-free genomic model with adaptive chunking that achieves better efficiency and biological accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Genomic foundation models face a fundamental tradeoff: standard fixed-vocabulary tokenizers fragment biologically meaningful motifs (codons, regulatory elements), while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts.

Method: dnaHNet uses a tokenizer-free autoregressive model with a differentiable dynamic chunking mechanism that segments and models genomic sequences end-to-end, adaptively compressing raw nucleotides into latent tokens to balance compression with predictive accuracy.

Result: Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency, achieves >3× inference speedup over Transformers, and shows superior zero-shot performance in predicting protein variant fitness and gene essentiality while automatically discovering hierarchical biological structures without supervision.

Conclusion: dnaHNet establishes a scalable, interpretable framework for next-generation genomic modeling that overcomes the fundamental tradeoff between biological coherence and computational efficiency.

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [116] [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611)
*Nicolás Becerra-Zuniga,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: PINNs' accuracy is fundamentally limited by data-PDE inconsistencies, creating a "consistency barrier" that prevents further improvement even with more training.


<details>
  <summary>Details</summary>
Motivation: While PINNs are popular for PDE modeling with scarce data, real-world data often contains inconsistencies with governing equations due to noise, discretization errors, or modeling assumptions. The impact of these inconsistencies on PINN accuracy is not well understood.

Method: Systematic analysis using 1D viscous Burgers equation with manufactured analytical solution to isolate data fidelity effects. PINNs trained with datasets of varying numerical accuracy and perfectly consistent analytical data to quantify consistency barriers.

Result: PINNs can partially mitigate low-fidelity data using PDE residuals, but training saturates at error levels dictated by data inconsistency. With high-fidelity numerical data, PINN solutions match those from analytical data, effectively removing consistency barriers.

Conclusion: Data quality fundamentally limits PINN accuracy via consistency barriers. Understanding this interplay between data fidelity and physics enforcement provides crucial guidance for building reliable physics-informed surrogate models.

Abstract: Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>


### [117] [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614)
*Nusaibah Farrukh,Malavika Pradeep,Akshay Sasi,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: Feature-based machine learning with Catch-22 features outperforms deep learning for cognitive load classification using EEG and pupillometry, with pupillometry alone competing effectively with EEG.


<details>
  <summary>Details</summary>
Motivation: Current cognitive load assessment methods using physiological signals like EEG and pupillometry lack comparative analysis and practical integration for lightweight, wearable monitoring solutions. EEG is resource-intensive while pupillometry is portable, but existing deep learning approaches have limited interpretability and high computational costs.

Method: Used OpenNeuro 'Digit Span Task' dataset to investigate cognitive load classification from EEG and pupillometry. Integrated feature-based and model-driven approaches, specifically using Catch-22 features with classical machine learning models instead of deep learning.

Result: Feature-based approaches with Catch-22 features and classical ML outperformed deep learning in both binary and multiclass tasks. Pupillometry alone can compete with EEG for cognitive load detection, providing a portable, practical alternative.

Conclusion: Pupillometry combined with interpretable models and SHAP feature analysis offers physiologically meaningful insights and challenges the assumption that EEG is necessary for cognitive load detection, supporting development of wearable, affordable cognitive monitoring systems.

Abstract: Cognitive load, the mental effort required during working memory, is central to neuroscience, psychology, and human-computer interaction. Accurate assessment is vital for adaptive learning, clinical monitoring, and brain-computer interfaces. Physiological signals such as pupillometry and electroencephalography are established biomarkers of cognitive load, but their comparative utility and practical integration as lightweight, wearable monitoring solutions remain underexplored. EEG provides high temporal resolution of neural activity. Although non-invasive, it is technologically demanding and limited in wearability and cost due to its resource-intensive nature, whereas pupillometry is non-invasive, portable, and scalable. Existing studies often rely on deep learning models with limited interpretability and substantial computational expense. This study integrates feature-based and model-driven approaches to advance time-series analysis. Using the OpenNeuro 'Digit Span Task' dataset, this study investigates cognitive load classification from EEG and pupillometry. Feature-based approaches using Catch-22 features and classical machine learning models outperform deep learning in both binary and multiclass tasks. The findings demonstrate that pupillometry alone can compete with EEG, serving as a portable and practical proxy for real-world applications. These results challenge the assumption that EEG is necessary for load detection, showing that pupil dynamics combined with interpretable models and SHAP based feature analysis provide physiologically meaningful insights. This work supports the development of wearable, affordable cognitive monitoring systems for neuropsychiatry, education, and healthcare.

</details>


### [118] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: Privacy audits using established attacks show generative AI models for hospital time series have limited risk of privacy leakage when trained on large datasets, but differential privacy would reduce utility without clear benefit.


<details>
  <summary>Details</summary>
Motivation: Sharing real medical data for ML training is problematic due to privacy concerns, and synthetic data via generative AI is seen as a solution. However, generative models don't inherently guarantee privacy, and existing privacy mechanisms (like differential privacy) have limitations for time series models.

Method: Conducted privacy audits using established privacy attacks on state-of-the-art hospital time series generative models trained on MIMIC-IV. Also used eICU dataset to attack synthetic data generators trained on MIMIC-IV.

Result: Established privacy attacks are ineffective against generated clinical time series when synthetic data generators are trained on sufficiently large datasets. Differential privacy mechanisms wouldn't improve privacy but would decrease utility for ML tasks.

Conclusion: Current generative models for hospital time series, when trained on large datasets, show limited privacy risks from established attacks. Privacy-preserving techniques like DP may be unnecessary and counterproductive, though ongoing privacy audits remain essential.

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [119] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: CG-BGs combine coarse-grained modeling with Boltzmann Generators to enable scalable, unbiased sampling of molecular systems by using learned potentials of mean force for reweighting.


<details>
  <summary>Details</summary>
Motivation: Sampling equilibrium molecular configurations from Boltzmann distribution is challenging. Boltzmann Generators (BGs) have scalability limitations, while coarse-grained models lack proper reweighting for asymptotically correct statistics.

Method: Propose Coarse-Grained Boltzmann Generators (CG-BGs) that operate in coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples from a flow-based model. The PMF is efficiently learned via force matching from rapidly converged data.

Result: CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for unbiased sampling of larger molecular systems.

Conclusion: CG-BGs provide a principled framework that unifies scalable reduced-order modeling with exact importance sampling, addressing the longstanding challenge of Boltzmann distribution sampling for larger molecular systems.

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [120] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: A framework for evaluating temporal preservation in synthetic longitudinal patient data using metrics across marginal, covariance, individual-level, and measurement structures.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of synthetic longitudinal patient data often focuses on marginal-level resemblance, which can mask important distortions in temporal dependencies and individual trajectories. There's a need for comprehensive metrics to assess how well synthetic data preserves temporal characteristics.

Method: Proposes a set of metrics categorized into four aspects: marginal structure (overall distributions), covariance structure (relationships between variables), individual-level structure (patient trajectories), and measurement structure (timing patterns). Analyzes how factors like data quality, measurement frequency, and preprocessing affect temporal preservation.

Result: Strong marginal-level resemblance can conceal distortions in covariance and disruptions in individual trajectories. Temporal preservation is influenced by original data quality, measurement frequency, and preprocessing strategies. Sparse or irregular measurement times limit learning of temporal dependencies. No single metric adequately captures temporal preservation - multidimensional evaluation is needed.

Conclusion: The proposed metrics provide a comprehensive framework for evaluating temporal preservation in synthetic longitudinal patient data, enabling better assessment and improvement of generative models and supporting the creation of more realistic synthetic data.

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [121] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: Domain knowledge guided BO uses physical insight to transform coordinates, decoupling parameters and aligning active subspaces with search axes for robust high-dimensional optimization.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian Optimization struggles with high-dimensional problems where parameters are tightly coupled and objective landscapes are highly asymmetric with sparse rewards, making needle-in-a-haystack scenarios challenging even for advanced methods like TuRBO.

Method: Leverages physical insight to fundamentally simplify search by transforming coordinates to decouple input features and align active subspaces with primary search axes, combined with reverse annealing exploration strategy.

Result: Demonstrated on challenging 12-dimensional, 6-crystal Split-and-Delay optical system where conventional approaches (standard BO, TuRBO, multi-objective BO) consistently failed. The approach reliably converges to global optimum with significantly accelerated search.

Conclusion: Coordinate transformation guided by physical insight provides generalizable paradigm for converting high-dimensional coupled optimization problems into simpler representations, enabling rapid robust automated tuning for complex scientific instruments.

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [122] [Reducing Estimation Uncertainty Using Normalizing Flows and Stratification](https://arxiv.org/abs/2602.10706)
*Paweł Lorek,Rafał Topolnicki,Tomasz Trzciński,Maciej Zięba,Aleksandra Krystecka*

Main category: cs.LG

TL;DR: A flow-based model with stratified sampling reduces estimation uncertainty for expectation of functions of random variables, outperforming Monte Carlo and Gaussian mixture methods across high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods for estimating expectations of functions of random variables rely on parametric distribution assumptions (like Gaussian or mixed Gaussian), which can lead to significant estimation uncertainty when these assumptions don't hold.

Method: Proposes a flow-based model integrated with stratified sampling that uses a parametrized neural network to flexibly model unknown data distributions, overcoming limitations of parametric assumptions.

Result: The model shows marked reduction in estimation uncertainty across multiple datasets, including high-dimensional ones (30 and 128 dimensions), outperforming crude Monte Carlo estimators and Gaussian mixture models.

Conclusion: The flow-based approach with stratified sampling provides a flexible, effective alternative to traditional parametric methods for expectation estimation, particularly valuable when distribution assumptions are uncertain.

Abstract: Estimating the expectation of a real-valued function of a random variable from sample data is a critical aspect of statistical analysis, with far-reaching implications in various applications. Current methodologies typically assume (semi-)parametric distributions such as Gaussian or mixed Gaussian, leading to significant estimation uncertainty if these assumptions do not hold. We propose a flow-based model, integrated with stratified sampling, that leverages a parametrized neural network to offer greater flexibility in modeling unknown data distributions, thereby mitigating this limitation. Our model shows a marked reduction in estimation uncertainty across multiple datasets, including high-dimensional (30 and 128) ones, outperforming crude Monte Carlo estimators and Gaussian mixture models. Reproducible code is available at https://github.com/rnoxy/flowstrat.

</details>


### [123] [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718)
*Yifan Zhang,Zunhai Su,Shuhao Hu,Rui Yang,Wei Wu,Yulei Qian,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: SnapMLA is an FP8 MLA decoding framework that optimizes long-context efficiency through hardware-aware algorithm-kernel co-optimization, achieving up to 1.91x throughput improvement with minimal performance degradation.


<details>
  <summary>Details</summary>
Motivation: FP8 attention shows promise but faces challenges in DeepSeek MLA decoding: numerical heterogeneity from decoupled positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and lack of optimized system-level support.

Method: Three hardware-aware algorithm-kernel co-optimization techniques: 1) RoPE-Aware Per-Token KV Quantization (maintains RoPE in high precision, uses per-token granularity), 2) Quantized PV Computation Pipeline Reconstruction (resolves quantization scale misalignment), 3) End-to-End Dataflow Optimization (efficient data read-write workflow with specialized kernels).

Result: SnapMLA achieves up to 1.91x improvement in throughput with negligible performance degradation in challenging long-context tasks including mathematical reasoning and code generation benchmarks.

Conclusion: SnapMLA successfully addresses FP8 integration challenges in MLA decoding through hardware-aware optimizations, delivering significant throughput gains while maintaining accuracy for long-context tasks.

Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>


### [124] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: A new horizon-aware algorithm (CURE-UCB) for Rising Multi-Armed Bandits that outperforms horizon-agnostic strategies by explicitly incorporating the time budget T into decision-making.


<details>
  <summary>Details</summary>
Motivation: The Rising Multi-Armed Bandit (RMAB) framework models real-world scenarios where rewards increase with repeated plays (e.g., robotics, hyperparameter tuning). However, optimal strategies in RMAB dramatically shift based on the available time budget T, making horizon awareness crucial but underexplored.

Method: Proposes CUmulative Reward Estimation UCB (CURE-UCB), a novel algorithm that explicitly integrates the horizon T into the decision-making process for RMAB problems.

Result: Theoretical analysis establishes a new regret upper bound, proving that CURE-UCB strictly outperforms horizon-agnostic strategies in structured environments like "linear-then-flat" instances. Extensive experiments demonstrate significant superiority over baseline methods.

Conclusion: Horizon awareness is crucial for RMAB problems, and the proposed CURE-UCB algorithm effectively leverages knowledge of the time budget T to achieve superior performance compared to horizon-agnostic approaches.

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [125] [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743)
*Vaisakh Shaj,Cameron Barker,Aidan Scannell,Andras Szecsenyi,Elliot J. Crowley,Amos Storkey*

Main category: cs.LG

TL;DR: KLA (Kalman Linear Attention) reframes sequence modeling through Bayesian filters, enabling parallel probabilistic inference with explicit uncertainty tracking while matching or outperforming modern SSMs and GLAs on language tasks.


<details>
  <summary>Details</summary>
Motivation: State-space language models like Mamba and GLA offer efficient linear complexity but lack expressivity and robust state-tracking for complex reasoning. Classical Bayesian filters provide principled state estimation and uncertainty tracking but are typically sequential.

Method: Reparameterize the Kalman filter in information form to enable associative scan computation for parallel training. Build on this to create KLA (Kalman Linear Attention) layer - a neural sequence-modeling primitive that performs time-parallel probabilistic inference with explicit belief-state uncertainty.

Result: KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining computational advantages. On language modeling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.

Conclusion: KLA successfully bridges the gap between efficient sequence modeling and robust probabilistic reasoning, providing a powerful alternative to existing SSMs and GLAs with better expressivity and uncertainty tracking capabilities.

Abstract: State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.

</details>


### [126] [Predicting integers from continuous parameters](https://arxiv.org/abs/2602.10751)
*Bas Maat,Peter Bloem*

Main category: cs.LG

TL;DR: The paper proposes modeling integer-valued labels directly with discrete distributions rather than treating them as continuous, introducing novel distributions like Bitwise and discrete Laplace that outperform traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Many real-world prediction tasks involve integer labels (e.g., up-vote counts, available bicycles), but standard regression treats them as continuous, changing the underlying distribution. Direct discrete modeling could better capture integer properties while maintaining compatibility with neural network training via backpropagation.

Method: The authors investigate various discrete distributions whose parameters can be predicted from features, including both existing and novel options. They focus on distributions with continuous parameters to enable gradient-based learning in neural networks, testing approaches like Bitwise (binary decomposition with Bernoulli distributions) and discrete Laplace distributions.

Result: Experiments across tabular learning, sequential prediction, and image generation tasks show that two distributions perform best overall: Bitwise (representing integers in bits with Bernoulli distributions on each bit) and a discrete analogue of the Laplace distribution (with exponentially decaying tails around a continuous mean).

Conclusion: Directly modeling integer labels with appropriate discrete distributions, particularly Bitwise and discrete Laplace distributions, provides better performance than traditional continuous regression approaches while maintaining compatibility with neural network training through continuous parameterization.

Abstract: We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.

</details>


### [127] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: A threshold-based watermarking scheme for federated learning that requires coalitions of at least t clients to verify model provenance, preventing individual clients from removing watermarks while maintaining scalability.


<details>
  <summary>Details</summary>
Motivation: In federated learning, clients jointly train models without sharing raw data, but need mechanisms to prove model provenance. Existing watermarking approaches either don't scale well with many clients (watermarks dilute) or give individual clients too much power (ability to verify and potentially remove watermarks).

Method: The method involves secret-sharing the watermark key τ so that coalitions of at least t clients are needed to reconstruct it, and verification can be performed without revealing τ in the clear. The protocol is instantiated in the white-box setting and evaluated on image classification tasks.

Result: The proposed (t,K)-threshold watermarking remains detectable at scale (K=128) with minimal accuracy loss and stays above the detection threshold (z ≥ 4) under attacks including adaptive fine-tuning using up to 20% of training data.

Conclusion: The paper concludes that (t,K)-threshold watermarking is a scalable and robust solution for multi-client federated learning, enabling secure provenance verification while protecting against collusion and removal attacks.

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [128] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: Semi-Supervised Cross-Domain Imitation Learning (SS-CDIL) method that uses minimal target expert demonstrations plus unlabeled imperfect trajectories to achieve stable, data-efficient policy learning across domains.


<details>
  <summary>Details</summary>
Motivation: Cross-domain imitation learning is valuable when expert data collection is costly, but existing supervised methods rely on proxy tasks and explicit alignment, while unsupervised methods align distributions without paired data but are often unstable.

Method: Introduces SS-CDIL setting and algorithm with theoretical justification. Uses offline data with few target expert demos and unlabeled imperfect trajectories. Proposes novel cross-domain loss function for learning inter-domain state-action mappings and adaptive weight function to balance source and target knowledge.

Result: Experiments on MuJoCo and Robosuite show consistent gains over baselines, demonstrating stable and data-efficient policy learning with minimal supervision.

Conclusion: The proposed SS-CDIL approach effectively addresses domain discrepancy and achieves superior performance compared to existing methods, making cross-domain imitation learning more practical with limited supervision.

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [129] [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796)
*Jie Jiang,Ke Cheng,Xin Xu,Mengyang Pang,Tianhao Lu,Jiaheng Li,Yue Liu,Yuan Wang,Jun Zhang,Huan Yu,Zhouchen Lin*

Main category: cs.LG

TL;DR: PRISM is a novel architecture that bridges the efficiency of linear models with the expressivity of Transformers by capturing multi-step iterative refinement in a parallelizable form, achieving high throughput without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental tension in generative sequence modeling: Transformers are expressive but computationally heavy, while efficient linear sequence models are limited to shallow, single-step updates. Existing efficient architectures face theoretical limitations, and parallelizable methods like Test-Time Training break hardware parallelism due to state-dependent gradients.

Method: Proposes PRISM with a solver-inspired inductive bias that captures multi-step refinement in parallelizable form. Uses Write-Forget Decoupling to isolate non-linearity within the injection operator. Employs a two-stage proxy architecture: short-convolution anchors initial residual using local history energy, while a learned predictor estimates refinement updates directly from input. This distills iterative correction patterns into a parallelizable feedforward operator.

Result: Theoretically achieves Rank-L accumulation, expanding beyond the single-step Rank-1 bottleneck. Empirically achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.

Conclusion: PRISM successfully resolves the tension between expressivity and efficiency by enabling parallelizable multi-step refinement, making it a promising approach for efficient generative sequence modeling.

Abstract: Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training (TTT) break hardware parallelism due to state-dependent gradients. We propose PRISM (Parallel Residual Iterative Sequence Model) to resolve this tension. PRISM introduces a solver-inspired inductive bias that captures key structural properties of multi-step refinement in a parallelizable form. We employ a Write-Forget Decoupling strategy that isolates non-linearity within the injection operator. To bypass the serial dependency of explicit solvers, PRISM utilizes a two-stage proxy architecture: a short-convolution anchors the initial residual using local history energy, while a learned predictor estimates the refinement updates directly from the input. This design distills structural patterns associated with iterative correction into a parallelizable feedforward operator. Theoretically, we prove that this formulation achieves Rank-$L$ accumulation, structurally expanding the update manifold beyond the single-step Rank-$1$ bottleneck. Empirically, it achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.

</details>


### [130] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: RePO (Rephrasing Policy Optimization) is a new RL method that helps LLMs learn from hard domain-specific samples while maintaining training stability by rephrasing off-policy knowledge into on-policy trajectories.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning LLMs with domain-specific data have limitations: SFT degrades model generality, on-policy RL fails to handle hard samples, and off-policy RL suffers from training instability due to distribution shifts.

Method: RePO prompts the policy model to first comprehend off-policy knowledge, then rephrase it into trajectories matching its own stylistic and parametric distribution. It dynamically replaces low-reward rollouts with these rephrased high-quality trajectories while preserving on-policy training dynamics.

Result: Experiments on several benchmarks show RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

Conclusion: RePO successfully reconciles effective off-policy knowledge absorption with the stability of on-policy RL, offering a promising approach for domain-specific LLM alignment.

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [131] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: ASC is a new differentially private algorithm for worst-case group optimization that adaptively adjusts per-group sampling and clipping, achieving better worst-case accuracy and uniform privacy without hurting average accuracy.


<details>
  <summary>Details</summary>
Motivation: Models trained by minimizing average loss often fail to perform well on minority or hard-to-learn groups, and existing methods that optimize for worst-case groups create unequal privacy guarantees in differentially private settings, weakening privacy for minorities.

Method: ASC (Adaptively Sampled and Clipped Worst-case Group Optimization) adaptively controls both the sampling rate and the clipping threshold per group, allowing harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups.

Result: ASC achieves lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy compared to prior work.

Conclusion: ASC provides an effective differentially private algorithm for worst-case group optimization that balances group performance and uniform privacy protection, improving both fairness and privacy in model training.

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [132] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: SimuScene is the first systematic study training and evaluating LLMs on simulating physical scenarios via code across 5 physics domains, showing current models struggle (21.5% pass rate) but can be improved with reinforcement learning using visual rewards.


<details>
  <summary>Details</summary>
Motivation: While LLMs have been studied for math, coding, and scientific reasoning, their ability to accurately represent and simulate physical scenarios via code remains underexplored, creating a gap in understanding LLM capabilities for physics-based simulation.

Method: Built an automatic data collection pipeline with human verification to create SimuScene dataset (7,659 physical scenarios, 334 human-verified test examples). Evaluated 10 contemporary LLMs, then introduced a reinforcement learning pipeline with visual rewards using a vision-language model as judge to train textual models.

Result: Even the strongest LLM achieved only 21.5% pass rate on the test set, demonstrating the difficulty of physical simulation via code. However, training with their data improved physical simulation capabilities while substantially enhancing general code generation performance.

Conclusion: SimuScene provides a systematic benchmark for evaluating LLMs on physical scenario simulation via code, revealing current limitations but also showing that targeted training with visual rewards can significantly improve both physics simulation and general code generation abilities.

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [133] [Automated Model Design using Gated Neuron Selection in Telecom](https://arxiv.org/abs/2602.10854)
*Adam Orucu,Marcus Medhage,Farnaz Moradi,Andreas Johnsson,Sarunas Girdzijauskas*

Main category: cs.LG

TL;DR: TabGNS is a gradient-based Neural Architecture Search method for tabular data that automates neural network design for telecom applications, achieving better performance with much smaller models and faster search times.


<details>
  <summary>Details</summary>
Motivation: Deep learning adoption in telecom faces challenges in designing compact neural networks for resource-constrained environments, requiring automation to create efficient models.

Method: TabGNS (Tabular Gated Neuron Selection) - a novel gradient-based Neural Architecture Search method specifically designed for tabular data in telecommunications networks.

Result: TabGNS improves prediction performance while reducing architecture size by 51-82% and search time by up to 36x compared to state-of-the-art tabular NAS methods.

Conclusion: TabGNS enables automated neural network design throughout the model lifecycle, accelerating ML deployment in telecommunications networks.

Abstract: The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network architectures for these applications remains challenging and time-consuming, particularly when targeting compact models suitable for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tailored for tabular data in telecommunications networks. We evaluate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction performance while reducing the architecture size by 51-82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks.

</details>


### [134] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: The paper shows that learning a multivariate CDF with uniform ε-approximation using only one-bit feedback has sample complexity that's nearly dimension-invariant, with dimension n only appearing in logarithmic terms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the multivariate DKW inequality from the "full feedback" setting to the "bandit feedback" setting where only minimal one-bit observations are available. This has practical applications in scenarios like learning fixed-price mechanisms in small markets where only limited feedback is obtainable.

Method: The study examines learning an n-dimensional cumulative distribution function (CDF) within error ε > 0 using observations restricted to minimal one-bit feedback. The approach involves analyzing sample complexity for achieving uniform approximation over arbitrary fine grids.

Result: Main result shows near-dimensional-invariance in sample complexity: uniform ε-approximation can be achieved with sample complexity (1/ε³) × log(1/ε)^(O(n)), where dimensionality n only affects logarithmic terms. This yields tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets like bilateral trade settings.

Conclusion: The paper establishes that learning multivariate CDFs with bandit feedback has surprisingly favorable sample complexity properties, with dimension primarily affecting only logarithmic factors rather than polynomial scaling. This provides theoretical foundations for practical applications in mechanism design with limited feedback.

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [135] [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905)
*Deyi Kong,Zaiwei Chen,Shuzhong Zhang,Shancong Mou*

Main category: cs.LG

TL;DR: NHGD is a bilevel optimization method that uses empirical Fisher info as Hessian inverse surrogate, enabling parallel updates for reduced compute time.


<details>
  <summary>Details</summary>
Motivation: To solve computational bottleneck in hypergradient estimation (Hessian inverse computation) in bilevel optimization problems.

Method: Exploit statistical structure of inner problem, use empirical Fisher info matrix as consistent Hessian surrogate, enable synchronous parallel optimize-and-approximate framework reusing gradients.

Result: Achieves high-probability error bounds and sample complexity matching state-of-the-art, with significant reduction in computational time; empirical tests show scalability and effectiveness in large-scale ML.

Conclusion: NHGD offers an efficient, scalable solution for bilevel optimization, balancing theoretical guarantees with practical computational savings.

Abstract: In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.

</details>


### [136] [Tuning the burn-in phase in training recurrent neural networks improves their performance](https://arxiv.org/abs/2602.10911)
*Julian D. Schiller,Malte Heinrich,Victor G. Lopez,Matthias A. Müller*

Main category: cs.LG

TL;DR: Truncated BPTT with optimized burn-in phase reduces RNN training errors by over 60% in some cases, as shown by theoretical bounds and experiments on time series tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational and memory challenges of training RNNs with standard BPTT on long sequences by exploring truncated BPTT, and to understand the impact of the burn-in phase on training performance.

Method: The method involves establishing theoretical bounds on accuracy and performance loss when using truncated BPTT over subsequences instead of full sequences, and validating these bounds through experiments on standard benchmarks in system identification and time series forecasting.

Result: The results show that the burn-in phase significantly influences training, with proper tuning reducing prediction error by over 60% on training and test data in some cases, confirming the theoretical bounds.

Conclusion: The study concludes that the burn-in phase is a critical tuning parameter in truncated BPTT for RNN training, with proper adjustment leading to substantial performance gains, as validated by theoretical bounds and experimental results.

Abstract: Training recurrent neural networks (RNNs) with standard backpropagation through time (BPTT) can be challenging, especially in the presence of long input sequences. A practical alternative to reduce computational and memory overhead is to perform BPTT repeatedly over shorter segments of the training data set, corresponding to truncated BPTT. In this paper, we examine the training of RNNs when using such a truncated learning approach for time series tasks. Specifically, we establish theoretical bounds on the accuracy and performance loss when optimizing over subsequences instead of the full data sequence. This reveals that the burn-in phase of the RNN is an important tuning knob in its training, with significant impact on the performance guarantees. We validate our theoretical results through experiments on standard benchmarks from the fields of system identification and time series forecasting. In all experiments, we observe a strong influence of the burn-in phase on the training process, and proper tuning can lead to a reduction of the prediction error on the training and test data of more than 60% in some cases.

</details>


### [137] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: FlexDOME algorithm achieves near-constant strong constraint violation with sublinear strong regret and last-iterate convergence in safe online RL.


<details>
  <summary>Details</summary>
Motivation: Existing primal-dual methods for safe online RL in CMDPs either incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations, creating a need for better algorithms.

Method: The FlexDOME algorithm incorporates time-varying safety margins and regularization terms into a primal-dual framework, using a novel term-wise asymptotic dominance strategy to schedule the safety margin.

Result: FlexDOME provably achieves near-constant Õ(1) strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence, as validated by experiments.

Conclusion: FlexDOME successfully achieves near-constant strong constraint violation with sublinear strong regret and non-asymptotic last-iterate convergence, addressing critical limitations in safe online RL.

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [138] [Spatial-Morphological Modeling for Multi-Attribute Imputation of Urban Blocks](https://arxiv.org/abs/2602.10923)
*Vasilii Starikov,Ruslan Kozliak,Georgii Kontsevik,Sergey Mityagin*

Main category: cs.LG

TL;DR: A spatial-morphological imputer tool combines morphological clustering with neighborhood methods to reconstruct missing FSI/GSI values at city block level, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Accurate reconstruction of missing morphological indicators (FSI and GSI) is crucial for urban planning and data-driven analysis, but existing methods may not fully capture both global morphological patterns and local spatial context.

Method: Develops a spatial-morphological (SM) imputer tool that combines data-driven morphological clustering (inspired by SpaceMatrix framework) with neighborhood-based methods (inverse distance weighting or spatial k-nearest neighbors) to reconstruct missing values at city block level.

Result: While SM alone captures meaningful morphological structure, its combination with IDW or sKNN methods provides superior performance compared to existing state-of-the-art models. Composite methods demonstrate complementary advantages of combining morphological and spatial approaches.

Conclusion: The spatial-morphological imputer effectively combines global morphological patterns as priors with local spatial information for context-dependent interpolation, offering a robust solution for reconstructing missing urban morphological indicators.

Abstract: Accurate reconstruction of missing morphological indicators of a city is crucial for urban planning and data-driven analysis. This study presents the spatial-morphological (SM) imputer tool, which combines data-driven morphological clustering with neighborhood-based methods to reconstruct missing values of the floor space index (FSI) and ground space index (GSI) at the city block level, inspired by the SpaceMatrix framework. This approach combines city-scale morphological patterns as global priors with local spatial information for context-dependent interpolation. The evaluation shows that while SM alone captures meaningful morphological structure, its combination with inverse distance weighting (IDW) or spatial k-nearest neighbor (sKNN) methods provides superior performance compared to existing SOTA models. Composite methods demonstrate the complementary advantages of combining morphological and spatial approaches.

</details>


### [139] [CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control](https://arxiv.org/abs/2602.10933)
*Riccardo Barbano,Alexander Denker,Zeljko Kereta,Runchang Li,Francisco Vargas*

Main category: cs.LG

TL;DR: We propose a cooperative stochastic optimal control framework for compositional generation with multiple pre-trained diffusion models, treating them as agents steered toward a shared output objective.


<details>
  <summary>Details</summary>
Motivation: Existing methods for composing generative models treat it as algebraic operations on probability densities, requiring explicit target distributions, which is often not feasible in practice.

Method: We formulate compositional generation as a cooperative stochastic optimal control problem, where pre-trained diffusion models are interacting agents whose diffusion trajectories are jointly controlled to optimize a shared objective on aggregated outputs.

Result: The framework is validated on conditional MNIST generation and compared against a baseline using per-step gradient guidance, showing its effectiveness.

Conclusion: This paradigm offers a novel, practical approach to controlling multiple pre-trained models without relying on explicit density knowledge, opening up new possibilities in generative tasks.

Abstract: Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

</details>


### [140] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: This paper analyzes bias in temporal attention mechanisms, showing they suffer from a diagonal attention sink, and proposes effective regularization methods to address it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate whether temporal attention mechanisms suffer from a similar bias as observed in causal attention and temporal convolutions, where over-squashing creates a bias toward the first tokens. The goal is to analyze and address potential information degeneration in spatio-temporal models.

Method: The authors derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer and theoretically analyze how off-diagonal attention scores depend on sequence length. They then propose regularization methods to address the diagonal attention sink.

Result: The theoretical analysis shows that temporal attention matrices suffer from a diagonal attention sink, where attention scores become biased toward diagonal entries. The proposed regularization methods are experimentally demonstrated to be effective in mitigating this bias.

Conclusion: The paper concludes that temporal attention mechanisms suffer from a diagonal attention sink bias, but that the proposed regularization methods effectively mitigate this issue and improve model performance.

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [141] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: MoEEdit is a routing-stable knowledge editing framework for sparse Mixture-of-Experts LLMs, using null-space projections to prevent routing shifts and improve efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing methods are designed for dense architectures and are not applicable to sparse Mixture-of-Experts models, leading to computational inefficiency and routing instability.

Method: MoEEdit reparameterizes expert updates via per-expert null-space projections to keep router inputs invariant, suppressing routing shifts, and solves the block-structured optimization with a block coordinate descent solver.

Result: MoEEdit achieves state-of-the-art efficacy and generalization, maintains high specificity and routing stability, and offers superior compute and memory efficiency.

Conclusion: MoEEdit provides a robust, efficient, and scalable framework for knowledge editing in sparse MoE LLMs, establishing routing-stable interventions as essential for reliable model updates.

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [142] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: The paper proposes HCW-GLB-OMD, an algorithm for heteroskedastic generalized linear bandits with adversarial corruptions, achieving near-optimal regret with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address heteroskedastic generalized linear bandits with adversarial corruptions, which generalizes many important bandit settings (heteroskedastic linear bandits, logistic/Poisson bandits) but lacks robust and efficient algorithms.

Method: HCW-GLB-OMD combines online mirror descent (OMD) for efficient estimation with Hessian-based confidence weights to achieve corruption robustness, requiring only O(1) space and time per iteration.

Result: Under self-concordance assumption, achieves regret bound of Õ(d√∑g(τ_t)μ̇_{t,*} + d²g_maxκ + dκC), nearly matching lower bound of Ω̃(d√∑g(τ_t)μ̇_{t,*} + dC), achieving instance-wise minimax optimality up to κ-factor.

Conclusion: The algorithm provides a unified, computationally efficient solution for heteroskedastic GLBs with adversarial corruptions, achieving near-optimal performance across various problem instances.

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [143] [Sample Efficient Generative Molecular Optimization with Joint Self-Improvement](https://arxiv.org/abs/2602.10984)
*Serra Korkmaz,Adam Izdebski,Jonathan Pirnay,Rasmus Møller-Larsen,Michal Kmicikiewicz,Pankhil Gawade,Dominik G. Grimm,Ewa Szczurek*

Main category: cs.LG

TL;DR: Joint Self-Improvement is a novel approach for sample-efficient molecular optimization that combines a joint generative-predictive model with self-improving sampling to address distribution shift and efficiently generate optimized molecules.


<details>
  <summary>Details</summary>
Motivation: Generative molecular optimization faces two key challenges: 1) sample efficiency is crucial since optimized candidates are rare and expensive to evaluate, and 2) surrogate models suffer from distribution shift as optimization drives candidates increasingly out-of-distribution from the training data.

Method: The proposed Joint Self-Improvement approach features two key components: (i) a joint generative-predictive model that aligns the generator with the surrogate to alleviate distribution shift, and (ii) a self-improving sampling scheme that biases the generative part using the predictive component to efficiently generate optimized molecules at inference time.

Result: Experiments across offline and online molecular optimization benchmarks demonstrate that Joint Self-Improvement outperforms state-of-the-art methods under limited evaluation budgets.

Conclusion: Joint Self-Improvement effectively addresses the challenges of sample efficiency and distribution shift in molecular optimization by combining joint modeling with self-improving sampling, making it a promising approach for practical molecular design under constrained evaluation budgets.

Abstract: Generative molecular optimization aims to design molecules with properties surpassing those of existing compounds. However, such candidates are rare and expensive to evaluate, yielding sample efficiency essential. Additionally, surrogate models introduced to predict molecule evaluations, suffer from distribution shift as optimization drives candidates increasingly out-of-distribution. To address these challenges, we introduce Joint Self-Improvement, which benefits from (i) a joint generative-predictive model and (ii) a self-improving sampling scheme. The former aligns the generator with the surrogate, alleviating distribution shift, while the latter biases the generative part of the joint model using the predictive one to efficiently generate optimized molecules at inference-time. Experiments across offline and online molecular optimization benchmarks demonstrate that Joint Self-Improvement outperforms state-of-the-art methods under limited evaluation budgets.

</details>


### [144] [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)
*Abhishek Vijaya Kumar,Bhaskar Kataria,Byungsoo Oh,Emaad Manzoor,Rachee Singh*

Main category: cs.LG

TL;DR: TVCACHE is a stateful tool-value cache for LLM agent post-training that reduces idle GPU time by caching tool outputs, achieving up to 70% cache hit rates and 6.9X speedup in tool call execution without degrading training rewards.


<details>
  <summary>Details</summary>
Motivation: During RL post-training of LLM agents, external tool calls take seconds to minutes, leaving GPUs idle and inflating training time and cost. While many tool invocations repeat across parallel rollouts, naive caching is incorrect because tool outputs depend on environment state from prior agent interactions.

Method: TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups. A cache hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state.

Result: On three diverse workloads (terminal-based tasks, SQL generation, and video understanding), TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

Conclusion: TVCACHE effectively addresses the idle GPU problem in LLM agent post-training by providing a stateful caching mechanism that safely reuses tool outputs, significantly reducing training time and cost while maintaining training effectiveness.

Abstract: In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>


### [145] [When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging](https://arxiv.org/abs/2602.11020)
*Rui Ma*

Main category: cs.LG

TL;DR: This paper explores multi-view learning and adversarial robustness for financial time series prediction using image representations, finding that late fusion with dual encoders works best for clean performance, while adversarial robustness depends on attack type and view constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve next-day direction prediction in financial markets using same-source multi-view learning (price/volume charts and technical indicators) while also studying adversarial robustness of these models against test-time perturbations.

Method: The method constructs two aligned views from rolling windows: OHLCV-rendered price/volume charts and technical-indicator matrices. Uses leakage-resistant time-block splits with embargo for evaluation. Applies ex-post minimum-movement filters to reduce label ambiguity. Tests early fusion (channel stacking) vs late fusion (dual encoders with fusion head) with cross-view consistency regularization. Evaluates adversarial robustness using FGSM and PGD attacks under view-constrained and joint attack scenarios.

Result: Results show strong dependence on label-noise regime with non-monotonic data-noise trade-off. Late fusion with dual encoders provides dominant clean-performance gains, while early fusion can exhibit negative transfer. Models show severe vulnerability to tiny adversarial budgets with strong view asymmetry. Late fusion improves robustness under view-constrained attacks, but joint attacks remain challenging and cause substantial worst-case degradation.

Conclusion: The study demonstrates that multi-view learning for financial prediction requires careful handling of label noise and view fusion strategies. Late fusion with dual encoders is most effective for clean performance, and while it improves robustness against view-constrained attacks, joint adversarial attacks remain a significant vulnerability that needs further research.

Abstract: We study same-source multi-view learning and adversarial robustness for next-day direction prediction with financial image representations. On Shanghai Gold Exchange (SGE) spot gold data (2005-2025), we construct two window-aligned views from each rolling window: an OHLCV-rendered price/volume chart and a technical-indicator matrix. To ensure reliable evaluation, we adopt leakage-resistant time-block splits with embargo and use Matthews correlation coefficient (MCC). We find that results depend strongly on the label-noise regime: we apply an ex-post minimum-movement filter that discards samples with realized next-day absolute return below tau to define evaluation subsets with reduced near-zero label ambiguity. This induces a non-monotonic data-noise trade-off that can reveal predictive signal but eventually increases variance as sample size shrinks; the filter is used for offline benchmark construction rather than an inference-time decision rule. In the stabilized subsets, fusion is regime dependent: early fusion by channel stacking can exhibit negative transfer, whereas late fusion with dual encoders and a fusion head provides the dominant clean-performance gains; cross-view consistency regularization has secondary, backbone-dependent effects. We further evaluate test-time L-infinity perturbations using FGSM and PGD under two threat scenarios: view-constrained attacks that perturb one view and joint attacks that perturb both. We observe severe vulnerability at tiny budgets with strong view asymmetry. Late fusion consistently improves robustness under view-constrained attacks, but joint attacks remain challenging and can still cause substantial worst-case degradation.

</details>


### [146] [Learning Page Order in Shuffled WOO Releases](https://arxiv.org/abs/2602.11040)
*Efe Kahraman,Giulio Tosato*

Main category: cs.LG

TL;DR: Analysis of page ordering on shuffled Dutch FOI documents using embeddings, comparing methods like pointer networks and transformers, with best performance on documents up to 15 pages but failures in long document generalization and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To address document page ordering in heterogeneous WOO documents (Dutch freedom of information releases), where semantic signals are unreliable, by evaluating various machine learning methods for effective reordering.

Method: Use page embeddings and compare five methods: pointer networks, seq2seq transformers, and specialized pairwise ranking models, with evaluation metrics like Kendall's tau and ablation studies on positional encodings.

Result: Best approach successfully reorders documents up to 15 pages (Kendall's tau: 0.95 for 2-5 pages to 0.72 for 15 pages). Failures include seq2seq transformers' poor generalization on long documents (tau drops to 0.014) and curriculum learning underperforming by 39%. Attention analysis shows different ordering strategies for short vs. long documents, and model specialization improves tau by +0.21 on longer documents.

Conclusion: Page ordering in heterogeneous documents is feasible for short documents but challenging for long ones due to factors like positional encoding issues and differing strategies. Model specialization helps, but seq2seq transformers and curriculum learning need further optimization for long document tasks.

Abstract: We investigate document page ordering on 5,461 shuffled WOO documents (Dutch freedom of information releases) using page embeddings. These documents are heterogeneous collections such as emails, legal texts, and spreadsheets compiled into single PDFs, where semantic ordering signals are unreliable. We compare five methods, including pointer networks, seq2seq transformers, and specialized pairwise ranking models. The best performing approach successfully reorders documents up to 15 pages, with Kendall's tau ranging from 0.95 for short documents (2-5 pages) to 0.72 for 15 page documents. We observe two unexpected failures: seq2seq transformers fail to generalize on long documents (Kendall's tau drops from 0.918 on 2-5 pages to 0.014 on 21-25 pages), and curriculum learning underperforms direct training by 39% on long documents. Ablation studies suggest learned positional encodings are one contributing factor to seq2seq failure, though the degradation persists across all encoding variants, indicating multiple interacting causes. Attention pattern analysis reveals that short and long documents require fundamentally different ordering strategies, explaining why curriculum learning fails. Model specialization achieves substantial improvements on longer documents (+0.21 tau).

</details>


### [147] [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)
*Xinyu Yuan,Yan Qiao,Zonghui Wang,Wenzhi Chen*

Main category: cs.LG

TL;DR: Pram is an ML-based method using multimodal language models to solve multi-commodity flow problems, achieving near-optimal solutions with significantly faster runtime and strong robustness.


<details>
  <summary>Details</summary>
Motivation: Existing optimization engines struggle to balance optimality and tractability for rapidly expanding allocation systems. There's a need for methods that can address this trade-off dilemma for service providers.

Method: Pram divides the MCF problem into local subproblems solved by MLM-powered agents, then ensures global consistency via multi-agent reinforcement learning. The method learns to perform gradient descent in context.

Result: On real-world datasets and public topologies, Pram achieves performance comparable to linear programming solvers (very close to optimal), with 1-2 orders of magnitude faster runtimes and strong robustness (<10% degradation under failures).

Conclusion: Pram provides a practical, scalable solution for future networks that is objective-agnostic, seamlessly integrates with existing systems, and demonstrates MLMs' generalization ability to unforeseen events.

Abstract: The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

</details>


### [148] [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)
*Jialin Liu,Zhaorui Zhang,Ray C. C. Cheung*

Main category: cs.LG

TL;DR: MoToRec uses discrete semantic tokenization via a sparsely-regularized RQ-VAE to generate disentangled representations, improving cold-start recommendation performance.


<details>
  <summary>Details</summary>
Motivation: To address the performance impairment in Graph Neural Network (GNN) based recommender systems due to data sparsity and the item cold-start problem, particularly for new items with limited or no interaction history, by leveraging multimodal content more effectively.

Method: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec) framework with a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) for generating compositional semantic codes of discrete, interpretable tokens, enhanced by adaptive rarity amplification and a hierarchical multi-source graph encoder.

Result: Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios.

Conclusion: Discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge in recommendation systems.

Abstract: Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.

</details>


### [149] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: Synthetic motion data can improve HAR model generalization when mixed with real data or scaled sufficiently, but motion-capture pretraining offers limited gains due to domain mismatch with wearable signals.


<details>
  <summary>Details</summary>
Motivation: Real-world human motion data for wearable-based Human Activity Recognition (HAR) is scarce and expensive to collect at scale, while synthetic data generation offers a promising alternative for pretraining models.

Method: Pretrain motion time-series models on synthetic data generated from motion-capture representations, then evaluate transfer performance across diverse downstream HAR tasks.

Result: Synthetic pretraining improves generalization when mixed with real data or sufficiently scaled. Large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals.

Conclusion: Synthetic motion data has potential for transferable HAR representations but faces sim-to-real challenges; mixing synthetic with real data or scaling synthetic data are effective strategies, while motion-capture pretraining alone is limited by domain mismatch.

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [150] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: B3IT is a black-box change detection method for LLMs that uses border inputs (where multiple tokens compete for top output) to achieve high performance at 30x lower cost than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for remote change detection in LLMs are either too expensive for large-scale deployment or require access to model weights/log probabilities (white/grey-box access). The goal is to achieve low-cost, strict black-box operation using only output tokens.

Method: The method uses Border Inputs - specific inputs where there are multiple output top tokens competing. Statistical analysis shows these inputs enable powerful change detection based on the model's Jacobian and Fisher information. The Black-Box Border Input Tracking (B3IT) scheme is proposed based on this insight.

Result: Extensive experiments show border inputs are easily found for non-reasoning endpoints, and B3IT achieves performance comparable to the best grey-box approaches. It reduces costs by 30x compared to existing methods while operating in a strict black-box setting.

Conclusion: B3IT provides an effective, low-cost solution for black-box change detection in LLMs by leveraging border inputs, overcoming limitations of existing expensive or access-restricted methods.

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [151] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: MerLin is an open-source framework for systematic benchmarking and empirical exploration in photonic and hybrid quantum machine learning, integrating quantum circuit simulation into PyTorch/scikit-learn workflows and reproducing 18 state-of-the-art works to establish a shared experimental baseline.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for systematic and empirical exploration in quantum machine learning (QML) to identify practical benefits of quantum models in the near term. Current research often focuses on isolated algorithmic proposals, lacking a unified framework for benchmarking across models, datasets, and hardware constraints. This gap hinders the identification of quantum advantages and the reproducibility of results in photonic and hybrid QML.

Method: The methodology involves developing MerLin as a discovery engine for photonic and hybrid QML. It integrates strong simulation of linear optical circuits into standard machine learning workflows (PyTorch and scikit-learn), enabling differentiable training. The framework is designed around systematic benchmarking and reproducibility. The authors reproduced eighteen existing QML works as modular experiments, ensuring they are reusable and extensible. Hardware-aware features are implemented to facilitate tests on quantum hardware and co-design exploration.

Result: The authors introduced MerLin, an open-source framework that integrates optimized strong simulation of linear optical circuits into PyTorch and scikit-learn workflows, enabling end-to-end differentiable training of quantum layers. They reproduced eighteen state-of-the-art photonic and hybrid QML works, spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments, establishing a shared experimental baseline consistent with empirical benchmarking methodologies in AI. The framework includes hardware-aware features for testing on available quantum hardware and exploration beyond current capabilities.

Conclusion: MerLin provides a practical, open-source framework for systematic benchmarking and empirical exploration in photonic and hybrid QML. By embedding quantum models within established machine learning ecosystems, it enables practitioners to leverage existing tooling for ablation studies, cross-modality comparisons, and hybrid classical-quantum workflows. The framework's hardware-aware design positions it as a future-proof co-design tool linking algorithms, benchmarks, and hardware, facilitating the identification of quantum advantages and enhancing reproducibility in QML research.

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [152] [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097)
*David A. Barajas-Solano*

Main category: cs.LG

TL;DR: The paper analyzes PINNs for IBVPs from a statistical learning perspective, reformulating parameter estimation as minimizing KL divergence between residual distributions, identifying it as a singular learning problem, and using singular learning theory to study parameter estimates and predictive uncertainty.


<details>
  <summary>Details</summary>
Motivation: To understand the training and performance of physics-informed neural networks (PINNs) for initial and boundary value problems (IBVP) from a statistical learning perspective, providing a theoretical foundation for PINN parameter estimation and performance analysis.

Method: Reformulating PINN parameter estimation as a statistical learning problem, treating physics penalties as infinite indirect data sources, and minimizing Kullback-Leibler divergence between true and PINN residual distributions. Using singular learning theory tools, specifically the Local Learning Coefficient, to analyze PINN parameter estimates via stochastic optimization for a heat equation IBVP.

Result: Physics penalties are reinterpreted as infinite indirect data sources rather than regularization terms. PINN learning is shown to be a singular learning problem, and the Local Learning Coefficient helps analyze parameter estimates for a heat equation IBVP, with implications for predictive uncertainty quantification and extrapolation capacity.

Conclusion: The analysis demonstrates that physics-informed learning with PINNs is a singular learning problem, and the Local Learning Coefficient from singular learning theory provides insights into parameter estimation, predictive uncertainty quantification, and extrapolation capacity of PINNs.

Abstract: We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.

</details>


### [153] [From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123)
*Genmao Zhuang,Amir Barati Farimani*

Main category: cs.LG

TL;DR: MKNA is a language-driven system that autonomously translates scientific queries into actions for materials discovery, identifying high-performance ceramics like new Be-C-rich compounds.


<details>
  <summary>Details</summary>
Motivation: Traditional materials discovery relies on expert intuition and expensive simulations, which can be slow and inefficient, requiring automation to accelerate the process.

Method: The Materials Knowledge Navigation Agent (MKNA) uses natural language to perform database retrieval, property prediction, structure generation, and stability evaluation, while extracting quantitative thresholds and design motifs from literature.

Result: MKNA applied to high-Debye-temperature ceramics found a screening criterion (Theta_D > 800 K), rediscovered known materials like diamond and SiC, and proposed novel, stable Be-C-rich compounds in the 1500-1700 K range.

Conclusion: MKNA successfully automates materials exploration by generating stable candidates and reconstructing interpretable design rules, offering a general platform for language-guided discovery in various technologies.

Abstract: Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

</details>


### [154] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: Generative methods for offline multi-objective optimization underperform evolutionary alternatives on metrics like generational distance due to offline-frontier shift, revealing offline MOO as a distribution-shift-limited problem.


<details>
  <summary>Details</summary>
Motivation: Recent generative approaches (including diffusion models) show strong performance in offline multi-objective optimization under hypervolume metric, but their behavior under other established MOO metrics is less understood and potentially problematic.

Method: Analyze generative methods vs. evolutionary alternatives across multiple MOO metrics, identify failure mode related to offline-frontier shift (displacement of offline dataset from Pareto front), and examine out-of-distribution sampling requirements via integral probability metric.

Result: Generative methods systematically underperform evolutionary alternatives on metrics like generational distance, remain conservatively close to offline objective distribution, and fail to overcome offline-frontier shift limitation.

Conclusion: Offline MOO is a distribution-shift-limited problem; overcoming offline-frontier shift requires out-of-distribution sampling in objective space; generative methods' conservatism explains their failure; this provides diagnostic lens for understanding when/why generative optimization methods fail.

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [155] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: Asymmetric prompt weighting in RL for LLMs, favoring low-success prompts, accelerates training especially in from-scratch scenarios where informative responses are rare.


<details>
  <summary>Details</summary>
Motivation: Current RL algorithms (GRPO, DAPO, RLOO) focus on ambiguous prompts with intermediate success probability, but may not optimize learning when dealing with very easy or very hard prompts. The paper explores whether asymmetric weighting that favors low-success prompts could improve training efficiency.

Method: The paper proposes asymmetric prompt weighting schemes that assign higher weights to prompts with low or zero empirical success probability. It analyzes this approach in both from-scratch RL (like R1-Zero) and post-SFT RL scenarios, and provides theoretical characterization of optimal prompt weights that minimize time to reach target accuracy under fixed update budget.

Result: Asymmetric weighting particularly benefits from-scratch RL where training traverses a wide accuracy range, but has less impact in post-SFT RL where models already start at high accuracy. Theoretical analysis shows that in low-success regimes where informative responses are rare, optimal weights become asymmetric, upweighting low success probabilities to accelerate effective-time convergence.

Conclusion: Asymmetric prompt weighting is a valuable strategy for RL training of LLMs, especially in from-scratch scenarios, as it accelerates convergence by focusing on low-success prompts when informative responses are scarce and response cost dominates.

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [156] [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130)
*Maximilian Plattner,Fabian Paischer,Johannes Brandstetter,Arturs Berzins*

Main category: cs.LG

TL;DR: Identifies Meltdown, a catastrophic fragmentation failure in 3D diffusion transformers for surface completion, localizes it via activation-patching, introduces a spectral entropy proxy, and proposes PowerRemap for stabilization.


<details>
  <summary>Details</summary>
Motivation: To address a catastrophic failure mode (Meltdown) in 3D diffusion transformers for surface completion, where arbitrarily small on-surface perturbations to input point clouds cause output fragmentation into disconnected pieces.

Method: Using activation-patching from mechanistic interpretability to localize Meltdown to a single early denoising cross-attention activation; analyzing its singular-value spectrum as a scalar proxy; introducing PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning.

Result: Meltdown persists across state-of-the-art architectures, datasets, and denoising strategies; the spectral entropy of the identified activation serves as a proxy for fragmentation; PowerRemap counters the failure with stabilization rates up to 98.3%.

Conclusion: PowerRemap effectively counters Meltdown with stabilization rates up to 98.3%, demonstrating that mechanistic analysis of diffusion models can successfully guide the development of test-time controls to stabilize model behavior.

Abstract: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

</details>


### [157] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: A training-free, token-level early stopping method for diffusion language models that freezes tokens as they converge, cutting computational steps without losing quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency in diffusion language models, where many tokens stabilize long before the final denoising step, leading to unnecessary computational overhead.

Method: The method introduces a training-free, token-level early stopping approach that uses lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens have converged, allowing per-token freezing before the final denoising step.

Result: The approach yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing total diffusion steps while preserving generation quality, achieving state-of-the-art efficiency gains across mathematical reasoning, general QA, and scientific understanding benchmarks.

Conclusion: The paper concludes that the proposed training-free, token-level early stopping method effectively reduces the number of diffusion steps required for text generation without compromising quality, achieving state-of-the-art efficiency across diverse benchmarks.

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [158] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: TabICLv2 is a new state-of-the-art foundation model for tabular data that outperforms previous methods through improved synthetic data generation, architectural innovations, and optimized pretraining protocols.


<details>
  <summary>Details</summary>
Motivation: Recent tabular foundation models like TabPFNv2 and TabICL have shown that in-context learning can surpass gradient-boosted trees on tabular data benchmarks, demonstrating the potential for foundation models in this domain.

Method: The model is built on three pillars: (1) novel synthetic data generation engine for high pretraining diversity, (2) architectural innovations including a new scalable softmax in attention for better generalization to larger datasets, and (3) optimized pretraining protocols replacing AdamW with the Muon optimizer.

Result: TabICLv2 without any tuning surpasses the performance of RealTabPFN-2.5 (which is hyperparameter-tuned, ensembled, and fine-tuned on real data) on TabArena and TALENT benchmarks. It generalizes effectively to million-scale datasets under 50GB GPU memory and is faster than RealTabPFN-2.5.

Conclusion: TabICLv2 represents a significant advancement in tabular foundation models, achieving state-of-the-art performance through systematic improvements in data generation, architecture, and training protocols, with the authors committing to open research by releasing code and model weights.

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


### [159] [Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151)
*Sedigheh Eslami,Maksim Gaiduk,Markus Krimmel,Louis Milliken,Bo Wang,Denis Bykov*

Main category: cs.LG

TL;DR: PPLX-embed is a family of multilingual embedding models using diffusion-pretrained language model backbones with multi-stage contrastive learning, achieving competitive performance on multiple retrieval benchmarks and strong real-world search performance.


<details>
  <summary>Details</summary>
Motivation: To develop effective multilingual embedding models for web-scale retrieval that can capture comprehensive bidirectional context and preserve global context across long documents, addressing the needs of real-world, large-scale search scenarios.

Method: Uses multi-stage contrastive learning on diffusion-pretrained language model backbones, leveraging bidirectional attention from diffusion-based pretraining. Employs mean pooling and late chunking strategy to preserve global context. Two model types: pplx-embed-v1 for standard retrieval and pplx-embed-context-v1 for contextualized embeddings with global document context.

Result: pplx-embed-v1 achieves competitive performance on MTEB (Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet benchmarks. pplx-embed-context-v1 sets new records on the ConTEB benchmark. Both models demonstrate strong performance on internal evaluation suite for real-world, large-scale search over tens of millions of documents.

Conclusion: The pplx-embed models effectively address web-scale retrieval needs by leveraging diffusion-pretrained backbones and innovative context preservation techniques, validating their effectiveness in production environments where retrieval quality and efficiency are critical at scale.

Abstract: In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [160] [AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles](https://arxiv.org/abs/2602.10429)
*Wenkai Fan,Shurui Zhang,Xiaolong Wang,Haowei Yang,Tsz Wai Chan,Xingyan Chen,Junquan Bi,Zirui Zhou,Jia Liu,Kani Chen*

Main category: cs.MA

TL;DR: AIvilization v0 is a large-scale artificial society with LLM agents in a resource-constrained economy, featuring hierarchical planning, adaptive memory, and human steering to balance long-term goals with reactive adaptation.


<details>
  <summary>Details</summary>
Motivation: To create a sustainable artificial society that maintains long-horizon autonomy while remaining executable under rapidly changing environments, addressing the tension between stable goals and reactive correctness.

Method: A unified LLM-agent architecture with: (1) hierarchical branch-thinking planner for goal decomposition and validation; (2) adaptive agent profile with dual-process memory separating short-term execution from long-term consolidation; (3) human-in-the-loop steering interface for objective injection. Environment includes survival costs, multi-tier production, AMM-based pricing, and education-occupation system.

Result: The system produces stable markets reproducing stylized facts (heavy-tailed returns, volatility clustering) and structured wealth stratification driven by education/access constraints. Full architecture outperforms simplified planners in multi-objective, long-horizon settings, supporting delayed investment and sustained exploration.

Conclusion: AIvilization v0 demonstrates that coupling hierarchical planning, adaptive memory, and human steering enables robust long-horizon autonomy in artificial societies, with implications for understanding economic emergence and agent coordination in complex environments.

Abstract: AIvilization v0 is a publicly deployed large-scale artificial society that couples a resource-constrained sandbox economy with a unified LLM-agent architecture, aiming to sustain long-horizon autonomy while remaining executable under rapidly changing environment. To mitigate the tension between goal stability and reactive correctness, we introduce (i) a hierarchical branch-thinking planner that decomposes life goals into parallel objective branches and uses simulation-guided validation plus tiered re-planning to ensure feasibility; (ii) an adaptive agent profile with dual-process memory that separates short-term execution traces from long-term semantic consolidation, enabling persistent yet evolving identity; and (iii) a human-in-the-loop steering interface that injects long-horizon objectives and short commands at appropriate abstraction levels, with effects propagated through memory rather than brittle prompt overrides. The environment integrates physiological survival costs, non-substitutable multi-tier production, an AMM-based price mechanism, and a gated education-occupation system. Using high-frequency transactions from the platforms mature phase, we find stable markets that reproduce key stylized facts (heavy-tailed returns and volatility clustering) and produce structured wealth stratification driven by education and access constraints. Ablations show simplified planners can match performance on narrow tasks, while the full architecture is more robust under multi-objective, long-horizon settings, supporting delayed investment and sustained exploration.

</details>


### [161] [An Ontology-driven Dynamic Knowledge Base for Uninhabited Ground Vehicles](https://arxiv.org/abs/2602.10555)
*Hsan Sandar Win,Andrew Walters,Cheng-Chew Lim,Daniel Webber,Seth Leslie,Tan Doan*

Main category: cs.MA

TL;DR: The paper introduces Dynamic Contextual Mission Data (DCMD) to create an ontology-driven dynamic knowledge base for UGVs, enabling real-time contextual updates to enhance situation awareness, autonomy, and mission agility in complex environments.


<details>
  <summary>Details</summary>
Motivation: UGVs rely heavily on a priori information loaded pre-mission, but unexpected events during missions can cause identification ambiguities and require excessive user intervention. The authors aim to enhance UGVs' autonomy, situation awareness, and agility in complex, dynamic environments by enabling dynamic, contextual updates to their knowledge base.

Method: The authors designed an ontology-driven dynamic knowledge base for UGVs, incorporating near real-time information acquisition and analysis to provide in-mission, on-platform DCMD updates. They implemented this system on a team of four UGVs and tested it in a laboratory-based surveillance mission.

Result: The implementation on four UGVs during a surveillance mission demonstrated that the ontology-driven dynamic representation of the operational environment was machine actionable. It successfully generated contextual information, supported timely mission execution, and directly improved situation awareness.

Conclusion: The paper concludes that an ontology-driven dynamic knowledge base with DCMD is a viable and effective approach to enhance UGVs' operational capabilities at the tactical edge. The system's ability to provide machine-actionable, contextual updates in near real-time directly contributes to improved situation awareness and mission success, demonstrating its practical utility in complex, dynamic environments.

Abstract: In this paper, the concept of Dynamic Contextual Mission Data (DCMD) is introduced to develop an ontology-driven dynamic knowledge base for Uninhabited Ground Vehicles (UGVs) at the tactical edge. The dynamic knowledge base with DCMD is added to the UGVs to: support enhanced situation awareness; improve autonomous decision making; and facilitate agility within complex and dynamic environments. As UGVs are heavily reliant on the a priori information added pre-mission, unexpected occurrences during a mission can cause identification ambiguities and require increased levels of user input. Updating this a priori information with contextual information can help UGVs realise their full potential. To address this, the dynamic knowledge base was designed using an ontology-driven representation, supported by near real-time information acquisition and analysis, to provide in-mission on-platform DCMD updates. This was implemented on a team of four UGVs that executed a laboratory based surveillance mission. The results showed that the ontology-driven dynamic representation of the UGV operational environment was machine actionable, producing contextual information to support a successful and timely mission, and contributed directly to the situation awareness.

</details>


### [162] [Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging](https://arxiv.org/abs/2602.10685)
*Alejandro Mendoza Barrionuevo,Samuel Yanes Luis,Daniel Gutiérrez Reina,Sergio L. Toral Marín*

Main category: cs.MA

TL;DR: This paper proposes a systematic set of general-purpose cooperation metrics for heterogeneous multi-agent systems operating under partial observability and temporal role dependency, validated in a destructive foraging scenario with autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Most previous studies focus primarily on algorithmic performance with respect to task completion, lacking comprehensive metrics to characterize cooperation aspects like coordination, dependency, fairness, and sensitivity in heterogeneous multi-agent systems operating under partial observability and temporal role dependencies.

Method: The authors propose a systematic set of transferable cooperation metrics structured into three categories: primary metrics, inter-team metrics, and intra-team metrics. These metrics are validated in a realistic destructive foraging scenario involving two specialized teams (search and destruction) with sequential dependencies, using both learning-based algorithms and classical heuristic paradigms.

Result: The paper presents a comprehensive suite of cooperation metrics that provide multilevel characterization of cooperation, covering efficiency, coordination, dependency between teams and agents, fairness, and sensitivity. The metrics have been validated in a heterogeneous multi-agent foraging scenario with autonomous vehicles.

Conclusion: The proposed cooperation metrics offer a systematic framework for analyzing cooperation in heterogeneous multi-agent systems beyond just task completion efficiency, enabling better characterization of coordination, dependencies, fairness, and sensitivity in complex multi-agent domains similar to foraging.

Abstract: This work addresses the problem of analyzing cooperation in heterogeneous multi-agent systems which operate under partial observability and temporal role dependency, framed within a destructive multi-agent foraging setting. Unlike most previous studies, which focus primarily on algorithmic performance with respect to task completion, this article proposes a systematic set of general-purpose cooperation metrics aimed at characterizing not only efficiency, but also coordination and dependency between teams and agents, fairness, and sensitivity. These metrics are designed to be transferable to different multi-agent sequential domains similar to foraging. The proposed suite of metrics is structured into three main categories that jointly provide a multilevel characterization of cooperation: primary metrics, inter-team metrics, and intra-team metrics. They have been validated in a realistic destructive foraging scenario inspired by dynamic aquatic surface cleaning using heterogeneous autonomous vehicles. It involves two specialized teams with sequential dependencies: one focused on the search of resources, and another on their destruction. Several representative approaches have been evaluated, covering both learning-based algorithms and classical heuristic paradigms.

</details>


### [163] [The emergence of numerical representations in communicating artificial agents](https://arxiv.org/abs/2602.10996)
*Daniela Mihai,Lucas Weber,Francesca Franzon*

Main category: cs.MA

TL;DR: Communication pressure alone enables neural agents to develop precise number representations, but these emergent codes lack compositionality and fail to generalize to unseen numbers.


<details>
  <summary>Details</summary>
Motivation: To investigate whether communication pressure alone can lead to the emergence of numerical representations in artificial agents, and whether these emergent codes resemble human numeral systems.

Method: Two neural network agents play a referential game communicating numerosities using either discrete tokens (symbolic) or continuous sketches (iconic). No pre-defined numeric concepts are provided.

Result: Agents achieve high in-distribution accuracy with precise symbol-meaning mappings, but the emergent codes are non-compositional: they fail to generalize to unseen numerosities, reusing symbols for highest trained numerosity (discrete) or collapsing extrapolated values (continuous).

Conclusion: Communication pressure suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalization abilities.

Abstract: Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question. We study two neural network-based agents that must communicate numerosities in a referential game using either discrete tokens or continuous sketches, thus exploring both symbolic and iconic representations. Without any pre-defined numeric concepts, the agents achieve high in-distribution communication accuracy in both communication channels and converge on high-precision symbol-meaning mappings. However, the emergent code is non-compositional: the agents fail to derive systematic messages for unseen numerosities, typically reusing the symbol of the highest trained numerosity (discrete), or collapsing extrapolated values onto a single sketch (continuous). We conclude that the communication pressure alone suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalisation abilities.

</details>


### [164] [Learning to Compose for Cross-domain Agentic Workflow Generation](https://arxiv.org/abs/2602.11114)
*Jialiang Wang,Shengxiang Xu,Hanmo Liu,Jiachuan Wang,Yuyu Luo,Shimin Di,Min-Ling Zhang,Lei Chen*

Main category: cs.MA

TL;DR: This paper presents a method for automatically generating agentic workflows using a decompose-recompose-decide approach that learns reusable capabilities across domains, enabling single-pass workflow generation that outperforms iterative refinement methods.


<details>
  <summary>Details</summary>
Motivation: Current agentic workflow generation systems struggle with domain shift and rely on iterative refinement, which is costly, slow, and yields unstable domain-specific behavior. There's a need for a more efficient and generalizable approach to workflow generation.

Method: The method internalizes a decompose-recompose-decide mechanism into an LLM: (1) Learn a compact set of reusable workflow capabilities across domains, (2) Map each input task to a sparse composition over these learned bases to generate task-specific workflows in one pass, (3) Attribute success/failure to counterfactual contributions from capabilities to understand which ones drive success.

Result: The single-pass generator outperforms state-of-the-art refinement baselines that require 20 iterations across multi-domain, cross-domain, and unseen-domain evaluations, while substantially reducing generation latency and cost.

Conclusion: The decompose-recompose-decide approach enables efficient and generalizable workflow generation by learning reusable capabilities and their effects, providing a scalable solution for cross-domain agentic workflow generation that reduces iteration costs and improves stability.

Abstract: Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.

</details>
