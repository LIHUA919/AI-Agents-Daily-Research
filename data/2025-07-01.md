<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: PokéAI is a multi-agent LLM framework for autonomously playing Pokémon Red, featuring Planning, Execution, and Critique agents. It achieves an 80.8% win rate in battles and shows a link between linguistic ability and strategic reasoning.


<details>
  <summary>Details</summary>
Motivation: To create an autonomous system for playing Pokémon Red using specialized LLM agents, exploring the intersection of language models and strategic gameplay.

Method: Three agents (Planning, Execution, Critique) work in a closed-loop system: Planning generates tasks, Execution performs them, and Critique evaluates outcomes. A battle module was tested for performance.

Result: The battle AI achieved an 80.8% win rate, close to human performance. LLM Arena scores correlated with battle success, and unique playstyles emerged for each model.

Conclusion: PokéAI demonstrates the potential of LLMs in strategic gaming, linking linguistic skills to gameplay performance and showcasing unique agent behaviors.

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [2] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: MAPF-GPT-DDG, a decentralized suboptimal MAPF solver, improves upon MAPF-GPT by fine-tuning with centralized expert data and a delta-data generation mechanism, achieving superior scalability and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and scalability of learning-based MAPF solvers for real-world applications like logistics and search-and-rescue.

Method: Fine-tunes a pre-trained MAPF model using centralized expert data and a novel delta-data generation mechanism.

Result: Outperforms existing learning-based MAPF solvers, including MAPF-GPT, and handles up to 1 million agents.

Conclusion: MAPF-GPT-DDG sets a new benchmark for scalability and solution quality in MAPF domains.

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [3] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: The paper explores combining natural language and drag-and-drop interfaces for robot task specification, using an LLM-based pipeline to generate human-like action sequences. Larger models outperform smaller ones, but smaller models still perform adequately.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between intuitive natural language programming and precise drag-and-drop interfaces for robot task specification.

Method: An LLM-based pipeline converts natural language input into human-like action sequences, compared to hand-specified sequences.

Result: Larger models generate more human-like sequences, but smaller models also perform satisfactorily.

Conclusion: Combining natural language and drag-and-drop approaches is feasible, with LLMs effectively bridging the gap.

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [4] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax is a domain-specific language for board games that compiles into hardware-accelerated code, combining generality with speed for AI research.


<details>
  <summary>Details</summary>
Motivation: To accelerate games research (e.g., RL, cognitive science) by enabling rapid simulation and flexible representation.

Method: Developed Ludax, a framework that compiles game descriptions into hardware-accelerated code, integrating with deep learning pipelines.

Result: Ludax provides speed benchmarking, RL agent training demonstrations, and is open-source.

Conclusion: Ludax bridges game description languages and hardware acceleration, offering a tool to advance AI research in games.

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [5] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: URSA is a scientific agent ecosystem using LLMs to accelerate research tasks through modular agents and tools, including physics simulations.


<details>
  <summary>Details</summary>
Motivation: LLMs' advanced capabilities overlap with human scientists' skills, offering potential to revolutionize research by addressing bottlenecks.

Method: URSA employs modular agents and tools, integrating advanced physics simulation codes, to tackle diverse scientific problems.

Result: The architecture and examples demonstrate URSA's potential to enhance research efficiency and impact.

Conclusion: URSA exemplifies how agentic AI can transform scientific research by leveraging LLMs and modular tools.

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [6] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: The paper advocates for designing and evaluating machine learning explanations with specific practical goals in mind, using a statistical decision theory framework.


<details>
  <summary>Details</summary>
Motivation: Current explainable ML methods lack consideration of how explanations are practically used, leading to potential misuse or ambiguity.

Method: Proposes a functionally-grounded approach using statistical decision theory to formalize explanation goals and evaluate their impact.

Result: Demonstrates the approach's applicability in diverse use cases (e.g., clinical decision support) and quantifies the potential performance boost from explanations.

Conclusion: Evaluation should combine theoretical and empirical perspectives, with clear definitions to ensure explanations are tailored to concrete use cases.

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [7] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: The paper proposes a method to assess AI trustworthiness by combining ethical guidelines with algorithmic processes like PageRank and TrustRank, aiming for a balanced, quantitative approach.


<details>
  <summary>Details</summary>
Motivation: AI's societal impact and complexity necessitate trustworthy systems, but current tools either lack quantification (guidelines) or a holistic view (technological tools).

Method: Introduces an assessment framework merging ethical components of Trustworthy AI with algorithmic processes (PageRank, TrustRank) to minimize subjectivity.

Result: The approach provides quantitative insights into AI trustworthiness while incorporating theoretical guidelines, achieving a holistic assessment.

Conclusion: Combining ethical and algorithmic methods offers a balanced way to evaluate AI trustworthiness, addressing gaps in existing tools.

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [8] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: ReasonBridge transfers reasoning skills from closed-source to open-source LLMs using hierarchical distillation, a small curated dataset (Reason1K), and sparse adapters, improving performance by up to 23%.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between closed-source and open-source LLMs in complex reasoning tasks.

Method: Hierarchical knowledge distillation, tailored dataset (Reason1K), sparse reasoning-focused adapters, and guided inference interventions.

Result: Open-source models improved by up to 23%, with Qwen2.5-14B outperforming Claude-Sonnet3.5 on MATH500 and matching it on AIME.

Conclusion: ReasonBridge efficiently enhances reasoning in open-source models, narrowing the gap with closed-source counterparts.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [9] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: The paper discusses the potential of AI in enterprises, focusing on AI-driven agents to enhance decision-making productivity. It proposes six tenets for successful AI adoption in enterprises, advocating a shift from AI-centric to user-centric AI paradigms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in current AI applications for enterprise decision-making, emphasizing the need for user-centric AI solutions to meet persistent enterprise needs.

Method: The method involves analyzing the limitations of the current AI-centric user paradigm and proposing six tenets for agentic success in enterprises, supported by market mechanisms for AI platforms.

Result: The result is a framework of six tenets that guide the design and delivery of AI agents to better serve enterprise decision-making needs, promoting user-centric AI.

Conclusion: The conclusion advocates for a shift to user-centric AI in enterprises, with six principles to align AI agents with enterprise decision-making goals, leveraging market mechanisms for effective implementation.

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [10] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto is a lightweight MoE model combining GRU and FFNN experts for diverse reasoning tasks, achieving specialization and interpretability while matching homogeneous baselines.


<details>
  <summary>Details</summary>
Motivation: Current MoE models lack representational diversity due to identical inductive biases, limiting efficiency and specialization for varied inputs.

Method: Hecto uses a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism.

Result: Hecto matches or nears homogeneous baselines in performance, shows clear expert specialization, and improves with larger batch sizes.

Conclusion: Hecto sets a new benchmark for conditional computation, offering specialized reasoning in low-resource settings through architectural diversity.

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [11] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: The paper explores using self-play (Critic-Discernment Game) to enhance LLMs' reasoning comprehension without supervision, showing improved performance in tasks like math and error correction.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' reasoning abilities, they lack true comprehension. This work aims to improve their rationality in reasoning processes autonomously.

Method: A Critic-Discernment Game (CDG) is designed where a prover solves problems and faces critiques (helpful or misleading), refining its reasoning.

Result: CDG training significantly boosts LLMs' ability to comprehend reasoning in tasks like math, error detection, and long-chain reasoning.

Conclusion: Self-play via CDG effectively enhances LLMs' reasoning comprehension without human supervision, demonstrating broad applicability.

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [12] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE is a multimodal reasoning benchmark designed to test MLLMs' ability to handle complex, step-by-step reasoning across modalities, revealing significant limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on complex multimodal reasoning, limiting understanding of MLLMs' capabilities in such tasks.

Method: MARBLE includes two tasks, M-Portal and M-Cube, requiring multistep planning under spatial, visual, and physical constraints.

Result: Current MLLMs perform poorly, with near-random accuracy on M-Portal and 0% on M-Cube, highlighting reasoning and perception bottlenecks.

Conclusion: MARBLE exposes MLLMs' limitations, aiming to inspire advancements in multimodal reasoning and planning.

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [13] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA is the first open-source, speech-native assistant for multi-turn dialogue with tool use, outperforming open-weight systems on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in open-source systems for speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning.

Method: Combines ASR, TTS, and LLMs in a cascaded pipeline, supports tools like calendar booking, web search, and email, with modular design for easy integration.

Result: Scores 92.75% on OpenBookQA, 4.39 on AlpacaEval, and achieves 90% task success in human evaluations.

Conclusion: AURA is a competitive, open-source solution for complex, goal-driven speech tasks.

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [14] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: The paper introduces a five-stage evolutionary framework for AI development, comparing it to human cognitive technologies. It predicts future stages and emphasizes reflexive feedback loops in AI's evolution.


<details>
  <summary>Details</summary>
Motivation: To systematically explain AI's past and future development, providing a prescriptive model for researchers and developers.

Method: Proposes a "Geometry of Cognition" framework, analyzing AI's progression through epochs mirroring human cognitive advancements.

Result: Identifies current "Metalinguistic Moment" and predicts future stages leading to provably aligned AI.

Conclusion: The framework offers actionable strategies for next-gen AI systems, completing a trilogy on AI's economic, cognitive, and methodological aspects.

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [15] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: LLMs like ChatGPT 4o and o1-mini simulate risky decision-making but show more risk-averse behavior than humans, with o1-mini closer to human responses. Performance varies by language, with Chinese prompts less accurate than English.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' reliability in simulating complex risky decision-making, comparing model outputs to human responses.

Method: Used lottery-based tasks with transportation survey data from Sydney, Dhaka, Hong Kong, and Nanjing. Analyzed risk preferences via CRRA framework with demographic inputs for ChatGPT 4o and o1-mini.

Result: Both models were more risk-averse than humans; o1-mini aligned better. Chinese prompts showed larger deviations from actual responses than English.

Conclusion: LLMs show promise but have limitations in replicating human risk behavior, especially in linguistic and cultural contexts.

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [16] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: This dissertation explores the societal impact of AI foundation models, focusing on their capabilities, risks, and governance, aiming for better societal outcomes through research and policy.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of AI foundation models brings both promise and harm, necessitating a deeper understanding of their societal implications.

Method: The study is organized around three themes: conceptual framing (capabilities, risks, supply chain), empirical insights (transparency via evaluations and indexes), and actionable policy recommendations.

Result: The dissertation provides a framework for understanding and governing foundation models, advancing evidence-based AI policy.

Conclusion: By bridging research and policy, the work aims to improve societal outcomes in the AI era through better governance.

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [17] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: The paper evaluates the relational reasoning capabilities of three LLMs (DeepSeek-R1, DeepSeek-V3, GPT-4o) using benchmark tasks. DeepSeek-R1 performs best but struggles with complexity due to token limits and incomplete outputs. Future work should explore multimodal reasoning and failure analysis.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the deep relational reasoning abilities of state-of-the-art LLMs, identifying strengths and limitations.

Method: Benchmark tasks in family tree and general graph reasoning were used to evaluate the models' logical deduction and relational inference.

Result: DeepSeek-R1 achieved the highest F1-scores but all models faltered with increased complexity. Analysis revealed planning strategies but also incoherent reasoning.

Conclusion: The study highlights the need for deeper scrutiny of LLMs' reasoning dynamics and suggests future directions like multimodal reasoning and systematic failure analysis.

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [18] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: The paper introduces a semantic-aware relational message passing method for Knowledge Graph Completion (KGC), using a Top-K neighbor selection strategy and multi-head attention to improve prediction accuracy by focusing on relevant contextual information.


<details>
  <summary>Details</summary>
Motivation: Traditional node-based message passing in KGC introduces noise and suffers from information dilution or over-smoothing by indiscriminately aggregating neighboring edge data.

Method: Proposes a semantic-aware Top-K neighbor selection strategy to evaluate and select relevant edges, followed by a multi-head attention aggregator to fuse information.

Result: The method outperforms existing approaches on established benchmarks by mitigating irrelevant information interference.

Conclusion: The framework effectively leverages edge structure and features, capturing contextual information for improved link prediction in KGC.

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [19] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: The paper introduces 'rises' in concept lattices to measure distributivity, showing their link to classical distributivity notions and analyzing real-world data.


<details>
  <summary>Details</summary>
Motivation: To quantify distributivity in Formal Concept Analysis (FCA) lattices, where no standardized measure exists.

Method: Introduces 'rises' to assess distributivity, linking them to meet- and join-distributivity.

Result: Concept lattices from real-world data are highly join-distributive but less meet-distributive.

Conclusion: Rises effectively measure distributivity, revealing distinct distributivity patterns in real-world lattices.

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [20] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL is a lightweight text2sql pipeline for financial statements, combining large and small language models to handle complex queries. It achieves 61.33% accuracy with fast response times, outperforming GPT-4o-mini.


<details>
  <summary>Details</summary>
Motivation: Text2sql struggles with domain-specific queries in finance due to varying database designs and reporting standards. FinStat2SQL addresses this for Vietnamese enterprises.

Method: Uses a multi-agent setup with large and small language models for entity extraction, SQL generation, and self-correction. Evaluated on a synthetic QA dataset.

Result: A fine-tuned 7B model achieves 61.33% accuracy with sub-4-second response times, outperforming GPT-4o-mini.

Conclusion: FinStat2SQL provides a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible in Vietnam.

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [21] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: The paper investigates how LLMs balance self-interest and collective well-being in multi-agent systems, revealing distinct behavioral patterns and challenges in cooperation, especially for reasoning-focused models.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' cooperation and social mechanisms is crucial for their safe and aligned deployment as autonomous agents.

Method: The study adapts a public goods game with institutional choice to observe LLMs' behavior in social dilemmas over repeated interactions.

Result: Four behavioral patterns emerged: sustained cooperation, fluctuating engagement, declining cooperation, and rigid strategies. Reasoning LLMs struggled with cooperation, while traditional ones excelled.

Conclusion: Enhancing reasoning capabilities in LLMs doesn't guarantee cooperation, offering insights for deploying collaborative LLM agents.

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [22] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim introduces generative agents with adaptive learning for urban mobility simulation, outperforming rule-based systems by capturing human-like behavioral diversity and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based urban mobility simulations lack the complexity and adaptability of human decision-making, prompting the need for AI-driven generative agents.

Method: GATSim combines an urban mobility foundation model with agent cognitive systems, featuring memory, tool usage, and lifelong learning for realistic travel behaviors.

Result: Generative agents produce believable travel behaviors, match human annotators in mobility scenarios, and generate realistic traffic patterns.

Conclusion: GATSim demonstrates the potential of generative agents in urban mobility simulations, offering adaptable and human-like behavioral modeling.

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [23] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: HonestVQA introduces a self-supervised honesty calibration framework for DocVQA, improving accuracy and ethical alignment by quantifying uncertainty and aligning model confidence with correctness.


<details>
  <summary>Details</summary>
Motivation: Current DocVQA systems lack ethical responsiveness, often producing overconfident answers to ambiguous questions, posing risks in ethically accountable domains.

Method: HonestVQA uses uncertainty quantification, weighted loss functions for confidence alignment, and contrastive learning for ethical response behavior. It introduces H-Score and ECI metrics for evaluation.

Result: HonestVQA improves accuracy by up to 4.3% and F1 by 4.3%, reduces overconfidence, and achieves strong generalization (78.9% accuracy, 76.1% F1-score).

Conclusion: HonestVQA effectively addresses ethical and performance gaps in DocVQA, demonstrating improved accuracy, reduced overconfidence, and strong generalization.

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [24] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: The paper proposes a CBT-based system using NLP models (BERT, RoBERTa, T5, PEGASUS, mT5) to analyze social media content for cognitive distortions and negative emotions, aiding psychotherapists in early intervention.


<details>
  <summary>Details</summary>
Motivation: The gap in methodologies for analyzing cognitive pathways in online environments motivates the development of a system to detect and address mental health issues from social media data.

Method: The system employs NLP models (BERT, RoBERTa for sentiment analysis; T5, PEGASUS for summarization; mT5 for translation) to categorize and analyze textual and visual content for negative emotions and cognitive distortions.

Result: The system not only identifies negative thoughts but also predicts additional mental health disorders (e.g., phobias, eating disorders), enabling comprehensive intervention strategies.

Conclusion: The proposed system enhances early detection and treatment of psychological issues, providing psychotherapists with a powerful tool for online mental health interventions.

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [25] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: A hybrid model combining AlexNet and LSTM improves electricity price forecasting accuracy by addressing limitations of traditional methods and standalone RNN/ANN models.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and standalone models like RNN and ANN fail to accurately forecast electricity prices due to insufficient analysis of external variables and sequential data.

Method: The hybrid model integrates AlexNet for feature extraction and LSTM for learning sequential patterns, using data like demand, temperature, sunlight, and rain, with preprocessing techniques like minimum-maximum scaling and time windows.

Result: The hybrid model achieves 97.08% accuracy, outperforming standalone RNN (96.64%) and ANN (96.63%).

Conclusion: The hybrid AlexNet-LSTM model significantly enhances electricity price prediction accuracy by leveraging external variables and sequential learning.

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [26] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: GPTZero detects AI-generated text well but struggles with false positives for human-written essays.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reliability of GPTZero in detecting AI-generated text across different essay lengths.

Method: Tested GPTZero on 28 AI-generated and 50 human-written essays of varying lengths (short, medium, long).

Result: GPTZero accurately detected AI-generated essays (91-100% AI) but had false positives for human-written ones.

Conclusion: Educators should be cautious when using AI detection tools like GPTZero due to limited reliability for human-authored texts.

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [27] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor, a fine-tuned LLM, converts chemical procedures into structured actions using a sequential data framework and multi-round review, achieving 10% better performance than baselines.


<details>
  <summary>Details</summary>
Motivation: Automating chemical procedure extraction is challenging due to ambiguous language and costly human annotation. ChemActor addresses this by leveraging LLMs.

Method: Uses a sequential LLM-generated data framework with a data selection module and multi-round review metric to generate machine-executable actions.

Result: Outperforms baseline models by 10% in R2D and D2A tasks.

Conclusion: ChemActor demonstrates state-of-the-art performance in automating chemical procedure extraction, validated by extensive experiments.

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [28] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: Coordination Transformers (CooT) is a new framework for multi-agent coordination that adapts to unseen partners using interaction histories, outperforming baselines in tasks like Overcooked.


<details>
  <summary>Details</summary>
Motivation: Addressing poor generalization and high training costs in existing multi-agent coordination methods.

Method: Uses in-context coordination with interaction histories to predict partner actions, trained on diverse agent pairs.

Result: Outperforms baselines in unseen partner tasks; human evaluations confirm its effectiveness.

Conclusion: CooT is robust, flexible, and context-sensitive, offering a promising solution for multi-agent coordination.

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [29] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces MMReason, a benchmark for evaluating long-chain reasoning in Multimodal Large Language Models (MLLMs), addressing gaps in existing benchmarks by focusing on difficulty, diversity, and robust assessment of reasoning steps.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM benchmarks lack precision in evaluating long-chain reasoning due to insufficient difficulty, diversity, and susceptibility to shortcuts like guessing or memorization.

Method: MMReason curates diverse, challenging questions from six disciplines and multiple difficulty levels, reformulates them into open-ended formats, and uses multi-model voting to eliminate shortcuts. It includes step-by-step solutions and a ternary scoring mechanism for reliable assessment.

Result: The benchmark evaluates popular MLLMs, providing insights into their reasoning capabilities.

Conclusion: MMReason aims to advance MLLM reasoning research by offering a robust evaluation tool.

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [30] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: Multi-agent LLM systems improve resistance to jailbreaking attacks but introduce trade-offs like higher false positives and computational costs.


<details>
  <summary>Details</summary>
Motivation: Address concerns about jailbreaking attacks on LLMs by exploring multi-agent systems as a defence.

Method: Evaluate three jailbreaking strategies (AutoDefense, BetterDan, JB) using single-agent vs. multi-agent setups.

Result: Multi-agent systems reduce false negatives but vary in effectiveness by attack type and increase false positives and overhead.

Conclusion: Highlights limitations of current defences and suggests improving alignment robustness in future LLMs.

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [31] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: An automated method using a Language Model (LM) fine-tunes RL agent reward weights based on user-defined behavioral goals, improving performance without manual engineering.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in RL deployment, especially the need for expert-designed reward functions and their inefficiency when game mechanics change.

Method: An LM iteratively updates reward weights using behavioral goals and performance statistics, enabling self-correction and alignment.

Result: LM-guided tuning improved agent success rates from 9% to 74% in one iteration, reaching 80% success and competitive lap times vs. expert tuning.

Conclusion: The LM-based approach effectively automates reward tuning, reducing reliance on experts and adapting to changes efficiently.

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [32] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: HASD addresses slide-level domain shift in pathology AI by integrating hierarchical adaptation and prototype selection, improving performance in clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Domain shift in pathology AI is problematic due to center-specific conditions, and current methods fail to capture global WSI features needed for clinical use.

Method: HASD uses a hierarchical framework with domain-level alignment, slide-level geometric invariance, and patch-level attention consistency, plus a prototype selection mechanism.

Result: Achieved 4.1% AUROC improvement in HER2 grading and 3.9% C-index gain in survival prediction.

Conclusion: HASD offers a practical, efficient slide-level domain adaptation solution for pathology institutions.

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [33] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: The paper proposes 'Agent for Science' (Agent4S) as a revolutionary paradigm using LLM-driven agents to automate research workflows, surpassing the limitations of current AI for Science (AI4S).


<details>
  <summary>Details</summary>
Motivation: Current AI4S is inefficient in addressing core research inefficiencies, prompting the need for a transformative approach like Agent4S.

Method: Introduces a five-level classification for Agent4S, detailing a roadmap from task automation to fully autonomous 'AI Scientists.'

Result: The framework positions Agent4S as the next revolutionary step in scientific discovery.

Conclusion: Agent4S represents the true Fifth Scientific Paradigm, enabling fully autonomous and collaborative research workflows.

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [34] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: The paper proposes a new interdisciplinary approach called 'data control' to enhance AI safety by combining AI advancements with control theory and system analysis.


<details>
  <summary>Details</summary>
Motivation: AI's rapid advancement lacks safety assurance, especially in safety-critical systems, prompting the need for a robust interdisciplinary safety framework.

Method: The paper introduces 'data control,' leveraging control theory and system analysis to improve AI safety through a top-down, interdisciplinary approach.

Result: A generic foundation for safety analysis and assurance is outlined, adaptable for specific AI systems and future innovations.

Conclusion: The 'data control' perspective aims to bridge AI and control theory, fostering safer AI engineering through interdisciplinary collaboration.

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [35] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: Attestable Audits use Trusted Execution Environments to verify AI model compliance while protecting sensitive data, addressing challenges in AI governance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack verifiable results and confidentiality for model IP and datasets, raising concerns in AI governance.

Method: Proposes Attestable Audits, leveraging Trusted Execution Environments to ensure verifiable interactions with compliant AI models.

Result: A prototype demonstrates feasibility on typical audit benchmarks against Llama-3.1, ensuring data protection even without mutual trust.

Conclusion: Attestable Audits provide a scalable, verifiable solution for AI model compliance audits while safeguarding sensitive data.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [36] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL is a new logical framework for querying and verifying Bayesian networks, enabling versatile reasoning and what-if scenarios without manual model changes.


<details>
  <summary>Details</summary>
Motivation: To provide a structured language for specifying and querying Bayesian networks, enhancing reasoning about causal and evidence-based relationships.

Method: Developed BayesL, a logical framework that allows creation of queries over Bayesian networks.

Result: Facilitates versatile reasoning and comprehensive what-if scenario evaluations.

Conclusion: BayesL simplifies querying and verifying Bayesian networks, improving efficiency and flexibility in reasoning.

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [37] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: The paper explores using Graph Neural Networks (GNNs) to rank word equations in solving processes, improving solver performance by leveraging a novel graph-based representation and training with minimum unsatisfiable subsets (MUSes).


<details>
  <summary>Details</summary>
Motivation: The performance of solvers for word equations depends on the processing order, motivating the use of GNNs for ranking to optimize solving efficiency.

Method: A graph-based representation for word equations is introduced, and three approaches for ranking equations using GNNs are proposed, trained with MUSes.

Result: The framework outperforms state-of-the-art solvers in benchmarks with variables appearing at most once per equation.

Conclusion: GNN-based ranking enhances word equation solving, particularly for specific benchmark cases.

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [38] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: The paper discusses the security risks of autonomous AI agents powered by large language models (LLMs) and proposes a framework (R2A2) to mitigate these risks.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous AI agents introduces novel security risks beyond conventional systems, necessitating a systematic approach to address vulnerabilities.

Method: The survey examines agent autonomy foundations, identifies vulnerabilities, and reviews defense strategies. It introduces the R2A2 framework for risk-aware decision-making.

Result: The paper identifies key risks (e.g., memory poisoning, tool misuse) and proposes R2A2 to enhance safety through proactive measures.

Conclusion: The R2A2 framework offers a principled approach to mitigate security risks in autonomous AI agents, balancing autonomy and safety.

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [39] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: The paper highlights the limitations of current AI systems in deductive reasoning and advocates for a shift from statistical learning to exact learning to achieve reliable reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite advances in AI, systems falter on deductive reasoning tasks, making them unfit for artificial general intelligence.

Method: Proposes a shift from statistical learning to exact learning, demanding correctness on all inputs.

Result: Current AI systems are unsound in deductive reasoning due to statistical learning approaches.

Conclusion: Exact learning is essential and possible for achieving reliable deductive reasoning in AI.

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [40] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: The paper evaluates LLMs' abilities to solve stochastic modeling problems in Operations Research, showing their proficiency comparable to human experts, though further work is needed for reliable automation.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored capabilities of LLMs in solving stochastic modeling problems in Operations Research.

Method: Manually curated graduate-level problems and SimOpt library were used to test LLMs' problem-solving and decision-making under uncertainty.

Result: State-of-the-art LLMs perform on par with human experts in classroom and practical settings.

Conclusion: LLMs show potential for assisting OR researchers and automating OR tasks, but more work is needed for reliable real-world application.

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [41] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: The paper introduces 'industrial brain,' a framework combining neuro networks and symbolic reasoning to predict and plan resilience in industrial chains, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with resilience prediction in chaotic, real-world industrial chain scenarios, necessitating a more robust solution.

Method: Proposes 'industrial brain,' integrating higher-order neuro networks and CT-OODA symbolic reasoning to model and predict resilience from observational data.

Result: Industrial brain improves resilience prediction accuracy by up to 10.8% over GoT/OlaGPT and 11.03% over spectral dimension reduction, generalizing well to unseen data.

Conclusion: The industrial brain framework effectively addresses gaps in resilience prediction and planning for industrial chains, demonstrating superior performance and robustness.

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [42] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: The paper outlines risk-management practices for general-purpose AI (GPAI) and foundation models, targeting developers to mitigate adverse risks while aligning with standards like NIST AI RMF and ISO/IEC 23894.


<details>
  <summary>Details</summary>
Motivation: To address the dual nature of GPAI/foundation models, which offer benefits but also pose significant risks, requiring tailored risk-management approaches.

Method: Proposes risk-management controls and practices, adapting existing frameworks (NIST AI RMF and ISO/IEC 23894) to the unique challenges of GPAI/foundation models.

Result: Provides actionable guidance for developers to identify, analyze, and mitigate risks associated with GPAI/foundation models.

Conclusion: The document serves as a practical resource for developers to manage risks while leveraging the capabilities of GPAI/foundation models.

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [43] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: AI-based framework using RAG pipelines (Zephyr-7B-beta and DeepSeek R1-7B) to analyze refugee child mental health data, with DeepSeek R1-7B outperforming in accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the psychological trauma of displaced children by leveraging AI to process unstructured health data for better policy and practice.

Method: Comparison of two RAG pipelines (Zephyr-7B-beta and DeepSeek R1-7B) on humanitarian datasets to minimize hallucination risks.

Result: DeepSeek R1-7B achieved superior accuracy (0.91) in answer relevance compared to Zephyr-7B-beta.

Conclusion: The study offers a scalable AI strategy to aid policymakers and practitioners in improving mental health support for displaced children.

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [44] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: The paper introduces a category theory-based method to address non-Markovian dynamics in decision-making, proving equivalence between MDP and NMDP categories and using HAS for precise control of state dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess decision algorithms' ability to handle non-Markovian dynamics, limiting progress in systems like RL.

Method: Developed a generalized methodology using category theory, defining MDP and NMDP categories and proving their equivalence. Introduced HAS to control state dependencies.

Result: The method effectively represents non-Markovian dynamics, enabling rigorous evaluation of decision algorithms.

Conclusion: The approach provides a novel theoretical foundation and practical tool for addressing non-Markovian dynamics in decision-making.

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [45] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL is a self-play framework for training language models through zero-sum games, eliminating human supervision and enabling transferable reasoning skills.


<details>
  <summary>Details</summary>
Motivation: To develop reasoning in language models without relying on human-curated data or domain-specific rewards.

Method: Uses self-play in multi-turn, zero-sum games with role-conditioned advantage estimation (RAE) for stable multi-agent training.

Result: Achieves 8.6% improvement in math and 8.4% in general reasoning, outperforming supervised fine-tuning. Multi-game training further enhances performance.

Conclusion: Zero-sum games naturally foster transferable reasoning, offering a scalable and autonomous approach for model development.

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: A novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework is introduced to enhance CPS security against sophisticated cyber threats, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Increasing connectivity of Cyber-Physical Systems (CPS) makes them vulnerable to adaptive and zero-day attacks, which traditional security methods fail to address.

Method: HAMARL uses a hierarchical structure with local agents for subsystem security and a global coordinator for system-wide defense, plus an adversarial training loop to simulate evolving threats.

Result: Experiments on an industrial IoT testbed show HAMARL improves attack detection accuracy, reduces response times, and ensures operational continuity.

Conclusion: Combining hierarchical multi-agent coordination with adversarial training enhances CPS resilience and security.

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [47] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: The paper introduces a TDW loss-incorporated Latent Factorization of Tensors (TDWLFT) model to improve missing spatiotemporal traffic data imputation by reducing outlier sensitivity.


<details>
  <summary>Details</summary>
Motivation: Incomplete or corrupted traffic data due to communication failures and sensor malfunctions hinder ITS performance, and existing LFT models are vulnerable to outliers.

Method: The proposed TDWLFT model uses a threshold distance weighted (TDW) loss function to assign differentiated weights to samples, reducing outlier impact.

Result: Experiments on traffic speed datasets show TDWLFT outperforms state-of-the-art methods in accuracy and efficiency.

Conclusion: The TDWLFT model effectively addresses outlier sensitivity in spatiotemporal traffic data imputation, enhancing ITS performance.

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [48] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: The paper explores reproducing knowledge-based structured thinking in deep learning models using feature-grounded embeddings to align representations with domain-specific conceptual features.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human-like reasoning (using prior knowledge and conceptual categories) and deep learning models by grounding embeddings in interpretable features.

Method: Introduces a feature-grounded embedding approach to align shareable representations with domain-specific conceptual features.

Result: The proposed method aims to create interpretable and structured embeddings that mimic human reasoning.

Conclusion: The approach could enhance deep learning models by incorporating human-like structured thinking through feature-based embeddings.

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [49] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: RL-Net, a neuro-symbolic rule learning neural network, balances performance (93.03% F1) and interpretability in radar-based hand gesture recognition, outperforming transparent (MIRA) and explainable black-box (XentricAI) models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretability (rule-based models) and performance (deep neural networks) in hand gesture recognition.

Method: RL-Net learns interpretable rule lists via neural optimization, benchmarked against MIRA and XentricAI for accuracy, interpretability, and user adaptability.

Result: RL-Net achieves strong performance while reducing rule complexity, though challenges like rule pruning and hierarchy bias are noted.

Conclusion: RL-Net is a practical middle ground for interpretable HGR, with potential for edge-deployable sensing systems.

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [50] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: The study introduces a cohort of 18 PASC patients with text time series features using Llama-3.1-70B-Instruct and proposes an Active Attention Network to predict clinical risk and progression events, aiming to improve patient care.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of PASC progression events (e.g., hospitalization, reinfection) is crucial for patient management and resource allocation, but traditional models fail to capture nuanced progression.

Method: Uses a cohort of 18 PASC patients with text time series features (Llama-3.1-70B-Instruct) and clinical expert annotations. Proposes an Active Attention Network integrating human expertise and active learning.

Result: Aims to enhance clinical risk prediction accuracy and identify progression events with fewer annotations.

Conclusion: The approach seeks to improve patient care and decision-making for SARS-CoV-2 patients by leveraging advanced modeling and expert input.

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [51] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Main category: cs.LG

TL;DR: EAGLE is a deep learning framework for cancer survival prediction, addressing limitations of existing methods with attention-based fusion, dimensionality reduction, interpretability, and adaptability across cancer types.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches for cancer survival prediction lack efficient fusion strategies, computational scalability, and interpretability, hindering clinical adoption.

Method: EAGLE uses dynamic cross-modal attention, massive dimensionality reduction, three attribution methods for interpretability, and a unified pipeline for adaptability.

Result: EAGLE achieved high predictive performance, identified risk groups with significant survival differences, and provided patient-level insights into modality contributions.

Conclusion: EAGLE bridges AI capabilities and clinical needs, offering scalable, interpretable, and accurate survival prediction for diverse cancers.

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [52] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: A multi-task Vision Transformer (ViT) architecture (1EMD) is proposed for multi-variable climate downscaling, outperforming single-variable models in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: GCMs lack regional detail, and RCMs are computationally expensive. Single-variable deep learning models miss cross-variable interactions.

Method: A ViT with shared encoder and variable-specific decoders jointly predicts temperature, wind speed, and geopotential height from GCM inputs.

Result: The multi-variable model shows positive knowledge transfer and outperforms single-variable baselines in accuracy and efficiency.

Conclusion: Multi-variable modeling is effective for high-resolution climate downscaling, offering better performance and computational efficiency.

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [53] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: A two-neural-network pipeline improves time series stabilization, achieving 3x better temperature control than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Stabilizing time series processes is critical in industries, and machine learning can enhance efficiency and quality.

Method: Uses a pipeline with two neural networks: an oracle predictor and an optimizer, replacing point-wise optimization with neural network training.

Result: Achieves 3x better stability in temperature control compared to ordinary solvers.

Conclusion: The proposed neural network pipeline effectively stabilizes time series processes with significant improvements.

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [54] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: The paper introduces a task-agnostic contrastive pretraining approach for Relational Deep Learning (RDL) to learn transferable representations from relational databases, outperforming task-specific models.


<details>
  <summary>Details</summary>
Motivation: Existing RDL models require task-specific supervised learning, limiting scalability and reuse. The paper aims to address this by proposing a pretraining method for database-wide representation learning.

Method: The approach uses three contrastive objectives (row-level, link-level, context-level) to capture relational data's structural and semantic heterogeneity. It employs a modular RDL architecture and efficient sampling for heterogeneous databases.

Result: Preliminary results show fine-tuning pretrained models outperforms training from scratch, validating the method's effectiveness for transferable representations.

Conclusion: The proposed task-agnostic pretraining approach enhances RDL scalability and reuse, demonstrating promise for learning transferable relational data representations.

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [55] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: The paper explores how untrained deep neural policies influence exploration in RL, showing they produce non-trivial state-visitation distributions and correlated actions, offering insights into inductive biases for exploration.


<details>
  <summary>Details</summary>
Motivation: Understanding how policy architectures implicitly shape exploration in RL, especially in sparse or adversarial reward settings.

Method: Theoretical and empirical analysis using infinite-width networks and continuous-time limits to study untrained policies in a toy model.

Result: Untrained policies generate correlated actions and non-trivial state-visitation distributions, revealing implicit exploration behaviors.

Conclusion: The work provides a framework for using policy initialization to study and design exploration behaviors in early RL training.

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [56] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Main category: cs.LG

TL;DR: The paper interprets RLHF and DPO as mutual information maximization methods, linking them to contrastive learning, and proposes MIO to improve reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between RLHF/DPO and mutual information maximization, addressing limitations in incentivizing reasoning in LLMs.

Method: Reinterprets RLHF and DPO through mutual information, replaces the DV/MINE bound with the Jensen-Shannon estimator, and introduces MIO.

Result: MIO mitigates late-stage decline in chosen-likelihood and outperforms DPO in reasoning and mathematical benchmarks.

Conclusion: MIO offers a competitive alternative to RLHF and DPO, enhancing reasoning in LLMs.

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [57] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Main category: cs.LG

TL;DR: FGSM in adversarial fine-tuning is stable and efficient, reducing training time by 4x with minimal robustness loss compared to PGD.


<details>
  <summary>Details</summary>
Motivation: Adversarial training is computationally expensive; FGSM offers a faster alternative for robust transfer learning.

Method: Revisits FGSM for adversarial fine-tuning, testing stability and performance across datasets.

Result: FGSM is stable, avoids catastrophic overfitting, and loses only 0.39%-1.39% robustness vs. PGD.

Conclusion: FGSM is a highly efficient and effective alternative to PGD in robust transfer learning.

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [58] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Main category: cs.LG

TL;DR: The paper reviews mixed-variable input challenges in simulation-based problems and proposes a unified framework for hierarchical, conditional, and heterogeneous domains. It introduces meta and partially-decreed variables, design space graphs, and integrates surrogate models for efficient optimization, demonstrated via a green aircraft case study.


<details>
  <summary>Details</summary>
Motivation: Address challenges in representing and optimizing mixed-variable inputs with hierarchical, conditional, or tree-structured domains, common in complex simulations.

Method: Proposes a unified framework with meta and partially-decreed variables, design space graphs, and surrogate models using hierarchical kernels and distances. Implemented in SMT 2.0.

Result: The framework effectively models and optimizes complex hierarchical domains, demonstrated in Bayesian optimization for green aircraft design.

Conclusion: The proposed framework generalizes existing approaches, enabling efficient modeling and optimization of complex mixed-variable domains.

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [59] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: The paper introduces H-VAW-D, a hierarchical algorithm for online regression with unconstrained quadratic loss in RKHS, achieving optimal dynamic regret via random feature approximation and adaptive learning.


<details>
  <summary>Details</summary>
Motivation: To extend the DVAW forecaster to non-parametric domains and achieve optimal dynamic regret in RKHS by learning discount factors and random features adaptively.

Method: Proposes H-VAW-D, combining DVAW with random feature approximation, learning discount factors and feature numbers hierarchically. Computational complexity is O(T ln T).

Result: Achieves expected dynamic regret of O(T^{2/3}P_T^{1/3} + sqrt(T) ln T), where P_T is the functional path length.

Conclusion: H-VAW-D efficiently adapts to non-parametric settings, balancing computational cost and regret performance.

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [60] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: The study investigates whether improvements in mathematical reasoning in large language models (LLMs) come from major changes in transformer layers or minor adjustments. Layer-wise ablation experiments reveal that mathematical reasoning relies on specific, persistent layer structures, unlike non-mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the source of improved mathematical reasoning in LLMs post-training (instruction tuning, reinforcement learning, knowledge distillation) and whether it stems from significant layer changes or minor adjustments.

Method: Systematic layer-wise ablation experiments on base and post-trained LLM variants (instruction-tuned, knowledge-distilled, reinforcement learning) tested on mathematical reasoning benchmarks.

Result: Mathematical reasoning depends on specific layer structures, with removal causing up to 80% accuracy drops. Non-mathematical tasks lack such critical layers. Critical layers coincide with major representational transformations.

Conclusion: Mathematical reasoning requires specialized layers emerging during pre-training, while non-reasoning tasks do not. The critical layers are where major representational changes occur.

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [61] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Main category: cs.LG

TL;DR: BayPOD-AL is an active learning framework using Bayesian POD to efficiently learn reduced-order models, reducing computational costs and improving data efficiency.


<details>
  <summary>Details</summary>
Motivation: Machine Learning surrogates require large datasets, limiting real-world applicability. BayPOD-AL addresses this by optimizing data usage.

Method: BayPOD-AL combines Bayesian POD with active learning to select informative data points, reducing training dataset size.

Result: Tested on temperature prediction, BayPOD-AL outperforms other methods in data efficiency and computational cost reduction.

Conclusion: BayPOD-AL is effective, generalizable, and efficient for learning reduced-order models with limited data.

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [62] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Main category: cs.LG

TL;DR: A method to learn stochastic multiscale models from data, improving predictive accuracy over traditional simulations.


<details>
  <summary>Details</summary>
Motivation: Addressing computational challenges in simulating dynamical systems with wide-ranging scales by avoiding fine-scale discretization.

Method: Proposes learning stochastic differential equations from data using a coarse mesh and auxiliary state for unresolved scales, employing forward-solver-free variational inference.

Result: Learned models outperform direct numerical simulation and closure-type models in predictive accuracy.

Conclusion: The approach offers a data-driven, efficient alternative to traditional multiscale modeling.

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [63] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: DistShap is a parallel algorithm for efficiently computing Shapley value-based explanations for GNN predictions by distributing computations across multiple GPUs, scaling to models with millions of features.


<details>
  <summary>Details</summary>
Motivation: Explaining GNN predictions is crucial but computationally expensive, especially for large graphs with millions of edges or features.

Method: DistShap distributes Shapley value computations by sampling subgraphs, parallelizing GNN inference, and solving a distributed least squares problem to determine edge importance.

Result: DistShap outperforms existing methods in accuracy and scales to GNNs with millions of features using up to 128 GPUs.

Conclusion: DistShap provides an efficient and scalable solution for explaining GNN predictions, addressing computational challenges in large-scale applications.

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [64] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: The paper addresses semantic collapsing in generative personalization, where learned visual concepts drift from their textual meanings, simplifying outputs. A training-free method adjusts embeddings to mitigate this.


<details>
  <summary>Details</summary>
Motivation: Semantic collapsing reduces the richness of multi-concept prompts, leading to oversimplified outputs. The goal is to preserve intended concepts in generated images.

Method: A training-free approach adjusts the magnitude and direction of pre-trained embeddings during inference to prevent semantic drift.

Result: The method improves text-image alignment across various personalization techniques, maintaining semantic richness.

Conclusion: The proposed embedding adjustment effectively mitigates semantic collapsing without additional training, enhancing generative personalization.

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [65] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: The paper introduces the Residual Matrix Transformer (RMT), replacing the transformer's residual stream with an outer product memory matrix, offering improved performance, efficiency, and scalability.


<details>
  <summary>Details</summary>
Motivation: To enhance the transformer's residual stream mechanism for better efficiency and performance by leveraging an outer product memory matrix.

Method: Replace the transformer's residual stream with an outer product memory matrix, creating the RMT model.

Result: RMT outperforms the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41% fewer training tokens, while also improving downstream performance.

Conclusion: The RMT model provides a more efficient and scalable alternative to traditional transformers, with theoretical and empirical advantages.

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [66] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: cs.LG

TL;DR: FairMarket-RL combines LLMs and RL to ensure fairness in P2P trading, achieving high fairness scores and equitable outcomes in decentralized energy systems.


<details>
  <summary>Details</summary>
Motivation: Existing P2P trading lacks robust fairness frameworks, necessitating a scalable, adaptive solution.

Method: Uses LLMs as fairness critics with FTB and FBS metrics, integrated into RL rewards via IPPO training.

Result: Achieves >90% buyer demand fulfillment, fair seller margins, and FTB/FBS scores >0.80.

Conclusion: FairMarket-RL provides a scalable, equity-driven solution for decentralized trading.

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [67] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Main category: cs.LG

TL;DR: The paper introduces a unified framework to analyze symmetries in neural network loss landscapes, enabling discovery of low- or zero-loss paths between independently trained models like Vision Transformers and GPT-2.


<details>
  <summary>Details</summary>
Motivation: Understanding the geometry of neural network loss landscapes is crucial for generalization and optimization, but symmetries like neuron permutations obscure this. Prior work focused narrowly on permutations, missing richer symmetries in modern architectures.

Method: The authors propose a framework covering four symmetry classes: permutations, semi-permutations, orthogonal transformations, and invertible maps. This generalizes previous approaches and captures broader reparameterizations.

Result: The framework successfully identifies low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models, revealing deeper loss landscape structure.

Conclusion: The work highlights the importance of symmetry-aware analysis for understanding model space geometry and advances the study of neural network loss landscapes.

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [68] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: BEST-Route is a novel LLM query routing framework that dynamically selects models and response counts to balance cost and quality, reducing costs by up to 60% with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: LLMs are expensive to deploy at scale, and existing routing methods often overuse large models due to insufficient quality from small models.

Method: BEST-Route leverages multiple responses from small models to enhance quality while remaining cost-effective, dynamically adjusting based on query difficulty and quality thresholds.

Result: Experiments show cost reductions of up to 60% with less than 1% performance drop.

Conclusion: BEST-Route effectively balances cost and quality in LLM query routing, offering significant savings without compromising performance.

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [69] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Main category: cs.LG

TL;DR: The paper introduces RTC-GTNLN, a novel Robust Tensor Completion model using a non-convex tensor rank surrogate (L1-L2 norm) to address missing values and noise in spatiotemporal traffic data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Spatiotemporal traffic data often suffers from missing values and noise due to sensor and communication issues, requiring robust recovery methods for reliable downstream applications.

Method: Proposes the RTC-GTNLN model, integrating a gradient tensor L1-L2 norm for low-rank representation and feature fusion to exploit global low-rankness and local consistency without trade-off parameters.

Result: Experiments on real-world traffic datasets show RTC-GTNLN outperforms state-of-the-art methods in handling missing data and noise.

Conclusion: RTC-GTNLN effectively addresses dual degradation in traffic data, offering superior recovery performance without compromising model accuracy.

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [70] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Main category: cs.LG

TL;DR: The paper introduces an INT8 quantized training method using the Forward-Forward algorithm, achieving faster training, energy savings, and reduced memory usage on edge devices.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's inefficiencies in time and energy consumption limit its use in resource-constrained edge devices, prompting exploration of alternatives like the Forward-Forward algorithm.

Method: The paper combines INT8 quantization with the Forward-Forward algorithm and introduces a "look-ahead" scheme to stabilize gradient quantization and improve accuracy.

Result: Experiments show 4.6% faster training, 8.3% energy savings, 27.0% reduced memory usage, and competitive accuracy on NVIDIA Jetson Orin Nano.

Conclusion: The proposed method is efficient for edge devices, balancing speed, energy, memory, and accuracy.

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [71] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: Score-based diffusion models generate samples and enable zero-shot conditioning for data fusion, applied here to super-resolution of high-dimensional dynamical systems using sparse sensor data.


<details>
  <summary>Details</summary>
Motivation: To leverage score-based diffusion models for updating learned distributions with real-time data, enabling accurate super-resolution of complex systems like atmospheric datasets.

Method: Uses score-based diffusion modeling to learn a score function and reverse a noising process, applying it to super-resolution tasks with sparse, multimodal data.

Result: Accurate recovery of high-dimensional states from low-fidelity measurements and balanced influence of multiple data modalities in reconstructions.

Conclusion: Score-based diffusion models effectively fuse and update data for super-resolution, demonstrating potential for real-time applications in complex systems.

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [72] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: The paper proposes a geometric framework using Riemannian geometry to define and analyze fingerprints of generative models, improving model attribution and distinguishing synthetic from human data.


<details>
  <summary>Details</summary>
Motivation: The need for reliable model authentication and differentiation between synthetic and human-generated data due to IP protection, accountability, and the threat of model collapse.

Method: A geometric approach defining artifacts and fingerprints of GMs using Riemannian geometry, replacing Euclidean metrics with geodesic distances and Riemannian center of mass.

Result: The method effectively distinguishes various GMs across datasets, resolutions, architectures, and modalities, improving attribution and generalization.

Conclusion: The proposed framework offers a principled and practical solution for understanding and utilizing generative model fingerprints.

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [73] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA integrates MC-Dropout into LoRA for task-specific uncertainty quantification, offering reliable confidence estimates for decision-making.


<details>
  <summary>Details</summary>
Motivation: General-purpose transformer uncertainty methods lack task-specific guardrails, limiting their utility in downstream workflows.

Method: BayesLoRA combines MC-Dropout with LoRA adapters to quantify uncertainty, leveraging amplified variance outside fine-tuning distributions.

Result: The framework provides reliable confidence estimates, enabling agents to introspect and modulate behavior under uncertainty.

Conclusion: BayesLoRA is a tailored solution for uncertainty quantification in agentic decision-making, outperforming general-purpose methods.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [74] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Main category: cs.LG

TL;DR: A novel dataset on global migration flows (1990-present) uses deep learning to estimate patterns, outperforming traditional methods with higher resolution and uncertainty bounds.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, high-resolution dataset on global migration flows and stocks, addressing gaps in existing data and methods.

Method: Deep recurrent neural networks trained on 18 covariates (geographic, economic, cultural, etc.) with ensemble learning and uncertainty propagation.

Result: Outperforms traditional methods in accuracy and temporal resolution, with confidence bounds for estimates.

Conclusion: The open-source dataset and model offer a valuable resource for migration studies, highlighting regions needing more data.

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [75] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Main category: cs.LG

TL;DR: xLSTMAD introduces the first anomaly detection method using xLSTM, excelling in accuracy on multivariate time series data.


<details>
  <summary>Details</summary>
Motivation: No prior work explored xLSTM for anomaly detection, prompting the development of xLSTMAD.

Method: xLSTMAD uses an encoder-decoder xLSTM architecture with forecasting (xLSTMAD-F) and reconstruction (xLSTMAD-R) variants, evaluated with MSE and SoftDTW loss functions.

Result: xLSTMAD outperforms 23 baselines on the TSB-AD-M benchmark, achieving state-of-the-art accuracy.

Conclusion: xLSTMAD demonstrates xLSTM's potential for anomaly detection, opening new research avenues.

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [76] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Main category: cs.LG

TL;DR: Quantum Neural Networks (QNNs) are explored for wind turbine power output prediction, showing competitive or slightly better performance than classical methods, with insights on dataset size and circuit complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate the applicability of QNNs in predicting wind turbine power output, addressing the need for efficient machine learning in renewable energy systems.

Method: Six QNN configurations using the Z Feature Map for data encoding and varying ansatz structures were tested via cross-validation and hold-out dataset experiments.

Result: QNNs achieved competitive or marginally better predictive performance than classical methods, with performance and simulation time influenced by dataset size and circuit complexity.

Conclusion: QNNs show promise for energy applications, offering valuable insights for integrating quantum machine learning in renewable energy research.

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [77] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Main category: cs.LG

TL;DR: The paper proposes an automatic approach (Auto-SLE) to improve the accuracy of learning large Bayesian networks (BNs) by combining multiple BN structure learning algorithms into an ensemble (SLE).


<details>
  <summary>Details</summary>
Motivation: Learning large BNs is challenging due to unstable accuracy in divide-and-conquer methods.

Method: Introduces SLE and Auto-SLE to automate and optimize ensemble learning, integrating it into a divide-and-conquer framework.

Result: Achieves 30%–225% accuracy improvement on datasets with 10,000 variables and generalizes well to larger datasets (e.g., 30,000 variables).

Conclusion: Auto-SLE shows significant potential for scalable and accurate BN structure learning.

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [78] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: P$^2$U is a method for efficient model distribution by transmitting low-bit precision models and updates, balancing accuracy, bandwidth, and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient model distribution in bandwidth-constrained environments like federated learning and IoT.

Method: Progressive Precision Update (P$^2$U) transmits low-bit precision models with updates for the difference to high-precision models.

Result: P$^2$U achieves better tradeoffs in accuracy, bandwidth, and latency, even with aggressive quantization (e.g., 4-bit).

Conclusion: P$^2$U is a practical solution for scalable model distribution in low-resource settings, compatible with existing compression techniques.

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [79] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Main category: cs.LG

TL;DR: A novel sparse autoregression framework is proposed for interpretable periodicity quantification in time series data, using ℓ0-norm sparsity and mixed-integer optimization (MIO). A subspace pursuit strategy accelerates MIO, and a two-stage scheme scales it for multidimensional data. Applications include human mobility and climate pattern analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability in time series autoregression for capturing auto-correlations and temporal patterns like periodicity and seasonality.

Method: Proposes a sparse autoregression framework with ℓ0-norm sparsity, reformulates it as MIO, and introduces a subspace pursuit strategy for acceleration. For multidimensional data, a two-stage optimization scheme is developed.

Result: The MIO solver is accelerated without losing solution quality. Applications reveal periodicities in ridesharing data and dynamic climate patterns, including El Nino.

Conclusion: The framework effectively quantifies periodicity and scales to large problems, demonstrating practical utility in real-world time series analysis.

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [80] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Main category: cs.LG

TL;DR: MAGNET, a missing-modality-aware graph neural network, addresses challenges in multimodal biological data by fusing partial modalities efficiently and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Handling missing modalities in multimodal biological data is challenging due to diverse missing patterns and scalability issues with current methods.

Method: MAGNET uses a patient-modality multi-head attention mechanism to fuse lower-dimensional embeddings and constructs a patient graph for predictions, adapting to missing-pattern variability.

Result: Experiments on multiomics datasets show MAGNET outperforms state-of-the-art fusion methods in cancer classification.

Conclusion: MAGNET provides an effective solution for missing-modality challenges, with linear complexity and adaptability to real-world missing patterns.

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [81] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Main category: cs.LG

TL;DR: A novel method for generating time series using generative AI, conditioned on natural language descriptions, is proposed, demonstrating potential applications in forecasting, data augmentation, and more.


<details>
  <summary>Details</summary>
Motivation: Time series generative AI is underdeveloped despite its importance in fields like finance and climate. This research aims to bridge the gap by leveraging natural language descriptions.

Method: A diffusion model combined with a language model is used to generate time series from text descriptions.

Result: The method successfully generates time series from natural language, with applications in custom forecasting, data augmentation, and transfer learning. A new dataset of 63,010 time series-description pairs is also introduced.

Conclusion: The proposed method advances time series generative AI, offering practical applications and a valuable public dataset for future research.

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [82] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: A parallel computation architecture is proposed to handle high-dimensional data by decomposing it into dimension-independent structures, enabling efficient distributed processing and integration of advanced analysis methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with high-dimensional data due to computational challenges, and current tools lack support for advanced mathematical statistics.

Method: A parallel computation framework based on space completeness, decomposing high-dimensional data for distributed processing.

Result: The framework supports seamless integration of data mining and parallel-optimized machine learning, applicable to diverse data types like medical and natural images.

Conclusion: The proposed architecture addresses the dimensionality curse, offering a unified system for advanced scientific computations.

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [83] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Main category: cs.LG

TL;DR: Infinite Sampling reduces memory overhead in GRPO for LLMs by decoupling group size from GPU memory, using micro sampling groups, continuous sampling, and a length-aware scheduler.


<details>
  <summary>Details</summary>
Motivation: Addressing the high memory overhead in GRPO due to generating and storing multiple responses per prompt, which limits scalability.

Method: Proposes Infinite Sampling with micro sampling groups, continuous sampling, and a length-aware scheduler combining token-conditioned prediction and two-stage planning.

Result: Reduces peak memory by 50% and improves throughput by 25% compared to naive methods, maintaining performance.

Conclusion: Infinite Sampling enables efficient and stable GRPO training under GPU memory constraints.

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [84] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Main category: cs.LG

TL;DR: The paper presents a machine learning-based approach for anomaly detection in connected autonomous vehicles (CAVs), using stacked LSTM and Random Forest models to identify abnormal driving patterns with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and reliability in CAVs by detecting anomalies caused by sensor malfunctions, cyber-attacks, or environmental disruptions.

Method: Simulated vehicle behavior to generate a dataset of typical and atypical interactions, then applied stacked LSTM for temporal dependencies and Random Forest for ensemble-based anomaly detection.

Result: Random Forest achieved R2 of 0.9830 and MAE of 5.746, while stacked LSTM had R2 of 0.9998 and MAE of 82.425, showing high accuracy in anomaly detection.

Conclusion: The models effectively predict vehicle trajectories and detect anomalies, proving their utility for CAV safety.

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [85] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: Kernel Outlier Detection (KOD) is a new method for high-dimensional anomaly detection, overcoming limitations like distributional assumptions and hyperparameter tuning. It uses kernel transformation and projection pursuit with novel ensemble techniques.


<details>
  <summary>Details</summary>
Motivation: Address challenges in outlier detection for high-dimensional data, overcoming limitations of existing methods like dependence on assumptions or hard-to-tune hyperparameters.

Method: Combines kernel transformation with projection pursuit, introducing a new ensemble of search directions and a novel way to combine results.

Result: Effective on three small datasets with challenging structures and four large benchmark datasets.

Conclusion: KOD offers a flexible, lightweight solution for high-dimensional outlier detection.

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [86] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Main category: cs.LG

TL;DR: A reinforcement learning (RL)-based method optimizes microgrid energy management, outperforming rule-based and existing RL benchmarks using a digital twin for realistic simulation.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy sources (RESs) requires new decentralized energy management approaches, with microgrids (MGs) offering localized control solutions.

Method: Proposes an RL agent that learns optimal energy trading and storage policies using historical data, simulated via a digital twin incorporating degradation factors.

Result: Validated with real-world data from Italy, the RL-based strategy outperforms rule-based methods and existing RL benchmarks.

Conclusion: The RL-based approach provides a robust solution for intelligent microgrid management.

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [87] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: The paper introduces the Barycentric Weight Layer (BWLer) to address precision limitations in Physics-informed neural networks (PINNs) for solving PDEs, achieving near-machine-precision accuracy in some cases.


<details>
  <summary>Details</summary>
Motivation: Current PINNs fall short of machine-precision accuracy required for scientific tasks, prompting investigation into whether the issue stems from PDE ill-conditioning or MLP architecture.

Method: The BWLer, a barycentric polynomial interpolation layer, is introduced to replace or augment MLPs in PINNs, separating solution representation from PDE derivative calculations.

Result: BWLer significantly improves accuracy (up to 1800x better RMSE) and achieves near-machine-precision for certain PDEs, outperforming standard PINNs.

Conclusion: BWLer offers a practical way to combine PINNs' flexibility with the precision of classical spectral solvers.

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [88] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: The paper introduces ternary language models (TriLMs) to address memory bottlenecks in LLM inference, proposing quantization-aware training, novel packing schemes, and a GPU kernel (TriRun) for efficiency.


<details>
  <summary>Details</summary>
Motivation: Memory bandwidth and capacity limitations in modern GPUs create bottlenecks for LLM inference, prompting the need for more efficient models.

Method: Quantization-aware training for TriLMs, scaling law analysis, and development of 2-bit/1.6-bit packing schemes and the TriRun GPU kernel.

Result: TriLMs show better scalability with data than parameters, and TriRun accelerates inference by up to 5x. Spectra-1.1 suite is released for community use.

Conclusion: The work provides efficient LLM solutions, with open resources for further research and deployment.

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [89] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: A feature-wise mixing framework is introduced to mitigate bias in ML models, achieving significant bias reduction and improved predictive performance without explicit bias attribute identification.


<details>
  <summary>Details</summary>
Motivation: Existing bias mitigation strategies limit scalability and generalizability, prompting the need for a more efficient solution.

Method: Feature-wise mixing redistributes feature representations across datasets, evaluated using bias-sensitive loss functions and cross-validation.

Result: Average bias reduction of 43.35% and significant MSE decrease across classifiers, outperforming SMOTE oversampling.

Conclusion: Feature-wise mixing is effective and efficient, with potential for real-world applications requiring accurate predictions.

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [90] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Main category: cs.LG

TL;DR: The paper analyzes RL policy robustness by classifying parameters as fragile, robust, or antifragile under internal (synaptic filtering) and external (adversarial attacks) stresses. It validates the framework on PPO-trained agents in Mujoco environments, identifying antifragile parameters that improve performance under stress.


<details>
  <summary>Details</summary>
Motivation: To systematically understand and improve RL policy robustness by analyzing parameter responses to internal and external stresses, inspired by synaptic plasticity.

Method: Uses synaptic filtering for internal stress and adversarial attacks for external stress to classify parameters. Defines scores to quantify fragility, robustness, or antifragility. Validates on PPO-trained agents in Mujoco.

Result: Identifies antifragile parameters that enhance policy performance under stress, showing potential for targeted filtering techniques to improve RL adaptability.

Conclusion: The framework lays groundwork for designing robust and antifragile RL systems, with implications for future advancements in RL policy adaptability.

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [91] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: A novel framework enhances Quanvolutional Neural Networks (QuNNs) by introducing trainable layers and residual learning to address gradient optimization challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional quanvolutional layers are static and lack adaptability, limiting QuNN performance.

Method: Proposes Residual Quanvolutional Neural Networks (ResQuNNs) with residual blocks for better gradient flow.

Result: Improved training performance and efficient gradient access across layers.

Conclusion: Strategic placement of residual blocks maximizes QuNN performance, advancing quantum deep learning.

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [92] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: The paper addresses the challenge of effective knowledge transfer from large-scale pretrained Vision Transformers (ViTs) to smaller models by proposing mutual information-aware optimization and MLP block reweighting.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of knowledge distillation drops when transferring from large-scale pretrained models, especially for small or imbalanced datasets.

Method: Proposes mutual information-aware optimization during finetuning and reweights MLP blocks to address mutual information loss.

Result: Enables small student models to benefit effectively from strong pretrained models.

Conclusion: The method improves knowledge transfer from large-scale pretrained ViTs to smaller models, especially in challenging datasets.

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [93] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: Double-Diffusion, a novel diffusion probabilistic model, combines physics-guided forecasting with stochasticity for air quality prediction, outperforming other models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Air quality prediction is complex due to spatio-temporal dynamics and uncertainty. Existing models struggle to balance certainty and uncertainty.

Method: Proposes Double-Diffusion, a physics-guided diffusion model with a new denoiser architecture and sampling strategy from image restoration.

Result: Ranks first in evaluations, reduces inference time by 30-50%, and improves CRPS by 3-12%.

Conclusion: Double-Diffusion effectively balances physics and stochasticity, setting a new benchmark for air quality prediction.

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [94] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Main category: cs.LG

TL;DR: The paper evaluates how well LLMs like GPT-4 align with human psychological concepts using standardized questionnaires and similarity analysis, showing GPT-4 outperforms others.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs accurately internalize human psychological constructs, given their ability to produce human-like text.

Method: Used 43 standardized psychological questionnaires to evaluate LLMs via pairwise similarity analysis and hierarchical clustering.

Result: GPT-4 achieved 66.2% classification accuracy, outperforming GPT-3.5 (55.9%) and BERT (48.1%). Its semantic similarity correlated with human responses.

Conclusion: Modern LLMs can approximate human psychological constructs, providing a framework to evaluate alignment and improve AI interpretability.

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [95] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: The paper introduces Meta-Causal Graphs as world models to handle shifting causal mechanisms, and a Causality-Seeking Agent to identify and refine these graphs through curiosity-driven exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional world models assume fixed causal rules, but real-world observations often show drifting causal mechanisms due to narrow observational windows. This work addresses the challenge of modeling such shifts.

Method: The authors propose Meta-Causal Graphs, composed of causal subgraphs triggered by latent meta states. A Causality-Seeking Agent identifies meta states, discovers causal relationships via curiosity-driven interventions, and refines the graph iteratively.

Result: Experiments on synthetic tasks and a robot arm manipulation task show the method effectively captures causal dynamics shifts and generalizes to unseen contexts.

Conclusion: The Meta-Causal Graph framework robustly models shifting causal mechanisms, enabling better generalization in dynamic environments.

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [96] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: Forget-MI is a novel machine unlearning method for multimodal medical data, improving privacy by effectively removing sensitive data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation in AI, especially in healthcare, is critical due to reliance on sensitive patient data. Existing methods struggle with removing data from multimodal architectures.

Method: Forget-MI uses loss functions and perturbation techniques to unlearn unimodal and joint representations of forgotten data while preserving knowledge from remaining data.

Result: The method reduces MIA by 0.202, decreases AUC and F1 scores on the forget set by 0.221 and 0.305, and matches test set performance of the retrained model.

Conclusion: Forget-MI outperforms existing approaches in privacy preservation and performance retention, making it suitable for healthcare applications.

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [97] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: The paper introduces the maneuverRecognition Python package for automating driving maneuver recognition using time series classification, addressing data preprocessing, model training, and evaluation.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety, reduce accidents, and support eco-friendly driving by improving maneuver recognition in vehicle telematics.

Method: Developed a Python package (maneuverRecognition) with functions for preprocessing, modeling, and evaluation, including a customizable LSTM-based network.

Result: Demonstrated the package's effectiveness using real driving data from smartphone sensors of three individuals.

Conclusion: The maneuverRecognition package provides a practical solution for efficient maneuver recognition in vehicle telematics.

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [98] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [99] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Main category: cs.LG

TL;DR: The paper proposes SynCheck, a quality-guided synthetic data utilization scheme, to address unpredictable synthetic data quality in wireless sensing tasks.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data in wireless sensing lacks predictable quality, leading to performance degradation despite its potential benefits.

Method: The authors introduce metrics (affinity and diversity) to quantify synthetic data quality and propose SynCheck to refine data quality during training.

Result: SynCheck outperforms quality-oblivious methods, achieving a 4.3% performance improvement even when others degrade performance by 13.4%.

Conclusion: SynCheck effectively mitigates synthetic data quality issues, enhancing wireless sensing task performance.

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [100] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Main category: cs.LG

TL;DR: GAMA is an attribution method for generative models like LSTMs, enabling interpretability without negative data, validated on synthetic and antibody-antigen datasets.


<details>
  <summary>Details</summary>
Motivation: Generative models lack interpretability, especially in biological contexts where negative data is scarce or unreliable.

Method: Developed GAMA, an attribution method based on Integrated Gradients, tested on synthetic and experimental antibody-antigen data.

Result: GAMA successfully recovers biologically relevant features and validates generative sequence designs without negative data.

Conclusion: GAMA enhances interpretability of generative models in therapeutic design, addressing the gap in biological sequence analysis.

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [101] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan,Victor Chepoi,Emmanuel Esposito,Maximilian Thiessen*

Main category: cs.LG

TL;DR: The paper explores monophonic halfspaces in graphs, introduces a decomposition theorem, and provides efficient learning algorithms, contrasting with NP-hard geodesic halfspaces.


<details>
  <summary>Details</summary>
Motivation: To address open questions about graph-based convexity and halfspaces, particularly monophonic halfspaces, and their applications in machine learning.

Method: Uses a $2$-satisfiability based decomposition theorem to represent monophonic halfspaces as disjoint vertex subsets, enabling efficient learning algorithms.

Result: Achieves polynomial-time empirical risk minimization, efficient sample compression, and proper learning with linear error rate in the PAC setting.

Conclusion: Monophonic halfspaces are efficiently learnable, unlike geodesic halfspaces, resolving open questions and advancing graph-based learning.

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [102] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li,Muhao Guo,Marija Ilic,Yang Weng,Guangchun Ruan*

Main category: cs.LG

TL;DR: The paper proposes a meta-representation framework, M2oE2, using hypernetworks and Mixture-of-Experts to dynamically adapt load forecasting models to external factors, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate residential load forecasting is crucial for power system reliability amid increasing renewable integration and demand-side flexibility. Existing models inadequately handle external factors like weather and pricing.

Method: The authors introduce a hypernetwork-based framework where external data dynamically adapts the forecasting model. A Mixture-of-Experts mechanism is integrated for selective expert activation and input filtering.

Result: M2oE2 outperforms state-of-the-art methods in accuracy and robustness across diverse load datasets, with minimal additional overhead.

Conclusion: The proposed M2oE2 framework effectively leverages external data as meta-knowledge, enhancing load forecasting performance and reliability.

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [103] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Main category: cs.LG

TL;DR: Proposes a reference model-based federated learning method for optimal fine-tuning, addressing catastrophic forgetting and improving model performance while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Federated learning ensures privacy but struggles with model performance and diverse user needs. This work aims to optimize models efficiently.

Method: Uses Bayesian parameter-efficient transfer learning with an optimal proximal term and a reference model to prevent catastrophic forgetting.

Result: Achieves high model performance and low computing cost.

Conclusion: The method effectively balances privacy, performance, and efficiency in federated learning.

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [104] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: The paper introduces SGKI, a kernel-based method for image inpainting and super-resolution, providing pixel estimates with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for reliable missing pixel estimation in images, including uncertainty measures, using RKHS assumptions.

Method: SGKI extends kernel interpolation, leveraging RKHS (especially band-limited functions) and Schur complements for efficient confidence band computation.

Result: SGKI successfully estimates missing pixels and provides non-asymptotic confidence bands, validated on synthetic and benchmark datasets.

Conclusion: SGKI offers a robust, uncertainty-aware solution for image inpainting and super-resolution, with potential for vector-valued extensions.

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [105] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: MGLUs (Masked Gated Linear Units) improve memory efficiency and speed over GLUs by using shared weight matrices and binary masks, achieving significant performance gains in LLMs.


<details>
  <summary>Details</summary>
Motivation: GLUs in LLMs require high memory reads due to separate weight matrices, creating a bottleneck. MGLUs aim to reduce this overhead.

Method: Introduces MoEG architecture with shared weight matrices and binary masks, and FlashMGLU, an optimized kernel for efficiency.

Result: MGLUs achieve up to 19.7× speed-up, 47% memory efficiency, and 34% faster inference than GLUs, with SwiMGLU matching or surpassing SwiGLU accuracy.

Conclusion: MGLUs offer a memory-efficient, faster alternative to GLUs in LLMs without compromising accuracy.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [106] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li,Zhu Qiyuan,Jiacheng Wang,Wei Li,Hao Gu,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: Sub-MoE is a compression framework for Mixture of Experts (MoE) models, addressing parameter conflicts via subspace expert merging, outperforming existing methods with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: MoE models face memory and deployment challenges due to massive parameters, and existing merging methods struggle with parameter conflicts from expert specialization.

Method: Sub-MoE uses joint SVD for expert weight concatenation, adaptive clustering, and subspace merging to align and fuse experts in a shared subspace.

Result: Sub-MoE maintains 96%|86% of original performance with 25%|50% expert reduction on Mixtral-8x7B in zero-shot benchmarks.

Conclusion: Sub-MoE effectively compresses MoE models by resolving parameter conflicts and enabling efficient expert merging, with potential for further optimization.

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [107] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen,Constanza Fierro,Anders Søgaard*

Main category: cs.LG

TL;DR: The paper addresses the unpredictability of reasoning time in models with hidden chains of thought, proposing methods to predict and display this time to improve user experience.


<details>
  <summary>Details</summary>
Motivation: Users lack insight into how long models spend reasoning, causing frustration, especially as models handle longer tasks asynchronously.

Method: Introduces and evaluates online and offline methods to predict model "thinking time," akin to a "progress bar for reasoning."

Result: Proposes practical solutions for predicting reasoning time, enhancing user interaction.

Conclusion: Highlights the importance of predictability in reasoning models and suggests future research directions.

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [108] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du,Yulin Wang,Shiji Song,Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces BAPE, a method to explicitly model posterior probabilities for Bayes classifier learning, addressing gradient imbalance in long-tailed data and improving generalization without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Deep learning's implicit posterior estimation fails in long-tailed data, leading to gradient imbalance and suboptimal decisions.

Method: BAPE explicitly models posterior probabilities using point estimation, bypassing gradient descent, and includes a distribution adjustment technique for test data adaptation.

Result: BAPE significantly improves generalization on long-tailed datasets (CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, iNaturalist).

Conclusion: BAPE ensures Bayes optimal decisions, outperforms implicit methods, and is orthogonal to existing long-tailed learning approaches.

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [109] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: The paper critiques the focus on isolated, counterintuitive deep learning phenomena, arguing they lack real-world relevance but can still refine broader theories.


<details>
  <summary>Details</summary>
Motivation: To challenge the efficiency of studying isolated deep learning phenomena and advocate for aligning research with broader field progress.

Method: Analyzes prominent examples of deep learning phenomena and their research outcomes, proposing practical recommendations.

Result: Finds little evidence of real-world applicability for these phenomena but acknowledges their value in refining general theories.

Conclusion: Recommends shifting focus from isolated puzzles to broader principles to ensure pragmatic progress in deep learning.

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [110] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang,WenZhe Li,Fei Chen,Yongjie Xu,Chang Yu,Zhen Lei,Stan Z. Li*

Main category: cs.LG

TL;DR: HDTree is a diffusion-based method for modeling hierarchical single-cell data, outperforming traditional and VAE-based approaches in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional and VAE-based methods for hierarchical single-cell data analysis are limited by computational cost, performance, and stability. HDTree aims to address these challenges.

Method: HDTree uses a unified hierarchical codebook and quantized diffusion processes to model tree node transitions, eliminating branch-specific modules.

Result: HDTree outperforms existing methods in accuracy and performance on general-purpose and single-cell datasets.

Conclusion: HDTree provides a robust tool for hierarchical lineage analysis, improving cellular differentiation modeling and offering biological insights.

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [111] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: VALID-Mol improves LLM-generated molecular designs by integrating chemical validation, increasing valid structure generation from 3% to 83%.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs producing chemically invalid or impractical structures in molecular design for drug discovery.

Method: Combines prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM.

Result: Achieves 83% valid chemical structures and up to 17-fold predicted improvements in target affinity.

Conclusion: Offers a generalizable methodology for scientifically-constrained LLM applications with quantifiable reliability improvements.

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [112] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita,Julienne LaChance*

Main category: cs.LG

TL;DR: The paper highlights biases and instability in data valuation methods, showing how pre-processing and algorithmic choices impact fairness and reliability, and proposes the DValCards framework for transparency.


<details>
  <summary>Details</summary>
Motivation: To address the biases and instability in data valuation methods used in ML, which can lead to unfair compensation and ethical issues in data markets.

Method: Analyzed 9 tabular classification datasets and 6 data valuation methods to demonstrate biases caused by pre-processing, subsampling, and undervaluation of underrepresented groups.

Result: Found that data valuation metrics are highly sensitive to algorithmic choices, exacerbate class imbalance, and undervalue underrepresented data.

Conclusion: Advocates for transparency via the DValCards framework to mitigate misuse and promote trust in responsible ML systems.

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [113] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc,Michal K. Grzeszczyk,Linglong Qian,Nassim Oufattole,Jeff Rasley,Arkadiusz Sitek*

Main category: cs.LG

TL;DR: FTS is a federated framework for training generative models on distributed EHR data, using tokenized patient timelines and federated learning to ensure privacy and scalability.


<details>
  <summary>Details</summary>
Motivation: To enable generative modeling of EHR data across institutions without sharing raw data, ensuring privacy while maintaining model performance.

Method: Tokenize patient histories as PHTs, train local autoregressive transformers, aggregate weights centrally, and synthesize data for a global model.

Result: Models trained on synthetic data perform comparably to those trained on real data in five clinical prediction tasks.

Conclusion: FTS provides privacy, scalability, and extensibility for healthcare applications like counterfactual inference and synthetic trials.

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [114] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier,Sujai Hiremath,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: The paper addresses the challenge of causal discovery in bivariate data with unmeasured mediators, proposing a new method (BiDD) that outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: Standard additive noise models (ANMs) fail when unobserved mediators corrupt causal relationships, and prior solutions are brittle in practice.

Method: The paper introduces Bivariate Denoising Diffusion (BiDD), which uses a novel independence test statistic during noising and denoising processes to infer causality.

Result: BiDD shows consistent performance in synthetic and real-world data, excelling in mediator-corrupted settings and maintaining robustness in mediator-free cases.

Conclusion: BiDD is a promising solution for causal discovery in the presence of unmeasured mediators, offering theoretical guarantees and practical advantages.

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [115] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Main category: cs.LG

TL;DR: The paper proposes a neurosymbolic approach to enhance LLMs by integrating logic-based reasoning (e.g., Prolog) to improve logical reasoning, decision-making, and interpretability, demonstrating success in multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs lack effectiveness in domains requiring strict logical reasoning and interpretability, prompting the need for a hybrid solution.

Method: Augment LLMs with logic-based reasoning modules (Prolog predicates) and composable toolsets to decompose queries and orchestrate solutions.

Result: Improved precision, coverage, and documentation in multi-step reasoning tasks, as shown on the DABStep benchmark.

Conclusion: Combining LLMs with modular logic reasoning enhances reliability, interpretability, and scalability for trustworthy AI agents.

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [116] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

Main category: cs.LG

TL;DR: A new tool, BenchMake, is introduced to transform open scientific datasets into benchmarks for machine learning, using non-negative matrix factorization to isolate challenging cases and create statistically significant test sets.


<details>
  <summary>Details</summary>
Motivation: The rarity of benchmark datasets in computational science hinders the evaluation of new methods. BenchMake addresses this by leveraging openly available datasets.

Method: BenchMake employs non-negative matrix factorization to identify edge cases on the convex hull and partitions data to maximize divergence and statistical significance across various modalities.

Result: BenchMake's splits are compared to established and random splits using ten public benchmark sets, demonstrating its effectiveness across diverse scientific domains.

Conclusion: BenchMake provides a scalable solution for creating benchmarks from open datasets, enhancing the evaluation of computational methods.

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [117] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros,Hossein Sharifi-Noghabi,Gabriel L. Oliveira,Saghar Irandoust*

Main category: cs.LG

TL;DR: PETSA is a parameter-efficient test-time adaptation method for time series forecasting, using small calibration modules and a specialized loss function to improve adaptability without full model updates.


<details>
  <summary>Details</summary>
Motivation: Real-world time series are non-stationary, degrading pre-trained models' performance. Existing TTA methods update the full model, increasing costs.

Method: PETSA updates only small calibration modules using low-rank adapters and dynamic gating. It employs a specialized loss with robust, frequency-domain, and patch-wise structural terms.

Result: PETSA improves adaptability of forecasting backbones with fewer parameters, achieving competitive or better performance on benchmark datasets.

Conclusion: PETSA offers an efficient and effective solution for test-time adaptation in time series forecasting.

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [118] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Main category: cs.LG

TL;DR: The paper introduces a User-Based Sequencing (UBS) method with a Transformer Encoder for insider threat detection, achieving high accuracy and low error rates.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture sequential dependencies in user behavior, limiting their effectiveness in detecting subtle insider threats.

Method: Proposes UBS to structure temporal sequences from the CERT dataset, using a Transformer Encoder for modeling and reconstruction errors as anomaly scores, evaluated with unsupervised outlier detection algorithms.

Result: Achieves 96.61% accuracy, 99.43% recall, and low false rates, outperforming traditional methods.

Conclusion: Sequential user modeling and advanced anomaly detection significantly improve insider threat detection.

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [119] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara,Gautam Siddharth Kashyap,Nipun Joshi,Arpita Soni*

Main category: cs.LG

TL;DR: DisasterNet-LLM, a specialized LLM, outperforms existing models in disaster classification by integrating multimodal data with advanced techniques like cross-modal attention and adaptive transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to effectively integrate multimodal data (images, weather records, textual reports) for disaster management, necessitating a more robust solution.

Method: DisasterNet-LLM uses advanced pretraining, cross-modal attention mechanisms, and adaptive transformers for comprehensive disaster analysis.

Result: Achieves 89.5% accuracy, 88.0% F1 score, 0.92 AUC, and 0.88 BERTScore in multimodal disaster classification tasks.

Conclusion: DisasterNet-LLM is a superior solution for disaster classification, offering high accuracy and robust performance in integrating diverse data types.

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [120] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Main category: cs.LG

TL;DR: TripleAD is a mutual distillation-based triple-channel framework for graph anomaly detection, addressing attribute, structural, and mixed anomalies separately to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised methods struggle with detecting both attribute and structural anomalies due to interference, leading to suboptimal results.

Method: TripleAD uses three modules: multiscale attribute estimation, link-enhanced structure estimation, and attribute-mixed curvature, with mutual distillation for collaboration.

Result: Experiments show TripleAD outperforms baselines in detecting anomalies.

Conclusion: TripleAD effectively mitigates interference between anomaly types and enhances detection performance.

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [121] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: SMART is a lightweight, data-efficient recalibration method for neural networks that uses the logit gap to improve calibration while minimizing bias and variance.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are overconfident, posing risks in safety-critical scenarios. Current calibration methods struggle with bias-variance trade-offs.

Method: SMART scales logits based on the logit gap (margin between top two logits) and uses a soft-binned ECE objective for stable updates with limited data.

Result: SMART achieves state-of-the-art calibration performance with fewer parameters, even on limited data.

Conclusion: SMART provides a robust, efficient solution for uncertainty quantification in neural networks.

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [122] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: FedWSQ improves federated learning by combining weight standardization and distribution-aware non-uniform quantization, enhancing robustness and reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in FL due to data heterogeneity and communication constraints.

Method: Integrates weight standardization (WS) to filter biased updates and distribution-aware non-uniform quantization (DANUQ) to minimize quantization errors.

Result: FedWSQ reduces communication overhead and maintains high model accuracy, outperforming existing methods in challenging FL settings.

Conclusion: FedWSQ is a robust and efficient FL framework, effective in diverse and extreme FL scenarios.

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [123] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper analyzes Quasi-Hyperbolic Momentum (QHM) in stochastic nonconvex optimization, showing that increasing batch size, without decaying learning rates, improves convergence for neural networks.


<details>
  <summary>Details</summary>
Motivation: Despite momentum methods' widespread use in deep learning, their theoretical justification in stochastic nonconvex settings is limited. QHM is studied to generalize momentum-based algorithms.

Method: The paper provides asymptotic and non-asymptotic convergence results for mini-batch QHM with increasing batch size, comparing it to decaying learning rates.

Result: Asymptotic convergence requires decaying learning rates or increasing batch sizes. Increasing batch size without decaying learning rates is more effective for non-asymptotic convergence. Experiments confirm benefits for neural networks.

Conclusion: Mini-batch QHM with increasing batch size is a viable strategy for training neural networks, avoiding the drawbacks of decaying learning rates.

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [124] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng,Qianxiao Li,Ting Lin,Zuowei Shen*

Main category: cs.LG

TL;DR: The paper explores the universal approximation property (UAP) in transformer architectures, introducing a unified framework that extends prior work on residual networks to attention-based models. It identifies token distinguishability as key for UAP and provides a general sufficient condition, simplifying verification. The framework is applied to prove UAP for various attention mechanisms, generalizing prior results and enabling novel architecture designs with UAP guarantees.


<details>
  <summary>Details</summary>
Motivation: To extend the theoretical understanding of UAP to transformer architectures, bridging gaps in prior work and providing a unified framework for analyzing attention-based models.

Method: The study introduces a general sufficient condition for UAP in transformers, leveraging analyticity assumptions on attention layers. It uses a non-constructive approach to verify UAP for diverse attention mechanisms, including kernel-based and sparse variants.

Result: The framework successfully proves UAP for various transformer architectures, generalizing prior results and covering new cases. It also enables the design of novel architectures with inherent UAP guarantees.

Conclusion: The work provides a principled foundation for understanding and designing transformer architectures with UAP, offering insights into token distinguishability and functional symmetries.

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [125] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul,Uriel Singer,Itai Gat,Yaron Lipman*

Main category: cs.LG

TL;DR: Transition Matching (TM) unifies diffusion/flow models and continuous autoregressive generation, introducing three variants (DTM, ARTM, FHTM) that improve performance and flexibility in media generation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in diffusion/flow models and unify text/media generation, TM offers a novel paradigm for expressive, flexible generation.

Method: TM decomposes tasks into Markov transitions, enabling non-deterministic kernels and arbitrary supervision. Variants include DTM (generalizes flow matching), ARTM (partial causality), and FHTM (full causality).

Result: DTM achieves state-of-the-art image quality and text adherence. FHTM matches/surpasses flow-based methods in text-to-image tasks.

Conclusion: TM advances generative modeling by unifying paradigms and introducing flexible, high-performing variants.

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [126] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Main category: cs.LG

TL;DR: The paper introduces A2P, a framework for Anomaly Prediction (AP) using Anomaly-Aware Forecasting and Synthetic Anomaly Prompting to predict future anomalies accurately.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to predict specific future anomalies, focusing only on immediate ones. The paper aims to address this gap.

Method: Proposes A2P with two components: Anomaly-Aware Forecasting (AAF) to learn anomaly relationships and Synthetic Anomaly Prompting (SAP) with a learnable Anomaly Prompt Pool (APP) for robust anomaly detection.

Result: A2P outperforms state-of-the-art methods in predicting future anomalies, validated on multiple real-world datasets.

Conclusion: A2P effectively addresses the AP task, offering precise future anomaly predictions with its novel framework.

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [127] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao,Bing Yang,Cai Yu*

Main category: cs.LG

TL;DR: The paper proposes a Nonlinear Low-rank Representation model with CNNs (NLR) to impute missing Water Quality Data (WQD), outperforming traditional methods by capturing temporal and nonlinear features.


<details>
  <summary>Details</summary>
Motivation: Water quality monitoring systems face challenges with missing data due to sensor failures, leading to high-dimensional, sparse data. Traditional methods fail to capture dynamics and deep features.

Method: The NLR model uses CNNs to fuse temporal features and extract nonlinear interactions, enabling deep fusion of multidimensional information.

Result: Experiments on three real datasets show NLR significantly improves imputation accuracy over state-of-the-art models.

Conclusion: NLR provides an effective solution for handling WQD in dynamic environments, enhancing data integrity for decision-making.

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [128] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa,Sara M. Kapoor,Theo Simon Sorg*

Main category: cs.LG

TL;DR: A study explores how a Transformer model learns modular exponentiation, revealing specialized circuits and grokking-like dynamics.


<details>
  <summary>Details</summary>
Motivation: Modular exponentiation is key in cryptography but lacks mechanistic understanding in neural models.

Method: A 4-layer Transformer is trained, analyzed using PCA, activation patching, and reciprocal operand training.

Result: Reciprocal training boosts performance, with sudden generalization. A subgraph of attention heads achieves full performance.

Conclusion: Transformers learn modular arithmetic via specialized circuits, aiding interpretability and efficiency.

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [129] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg,Martin Iglesias Goyanes,Friso Kingma,Andreu Mora,Leandro von Werra,Thomas Wolf*

Main category: cs.LG

TL;DR: DABstep is a new benchmark for evaluating AI agents on multi-step data analysis tasks, featuring 450 real-world challenges from financial analytics. It tests capabilities like data manipulation and contextual reasoning, with automatic scoring. Leading LLMs perform poorly, with only 14.55% accuracy on hard tasks.


<details>
  <summary>Details</summary>
Motivation: To create a realistic benchmark for evaluating AI agents on complex, multi-step data analysis tasks, addressing gaps in existing benchmarks.

Method: DABstep includes 450 real-world tasks requiring code-based data processing and contextual reasoning. It uses a factoid-style answer format with automatic correctness checks.

Result: Leading LLM-based agents perform poorly, with the best achieving only 14.55% accuracy on the hardest tasks.

Conclusion: DABstep provides a valuable tool for advancing research in autonomous data analysis, highlighting current limitations of AI agents.

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [130] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski,Matthew Tivnan,Yuang Wang,Siyeop Yoon,Pengfei Jin,Dufan Wu,Quanzheng Li,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: SDBs integrate known linear measurement systems into matrix-valued SDEs, improving performance in linear inverse problems and robustness under system misspecification.


<details>
  <summary>Details</summary>
Motivation: Address the gap in supervised bridge methods by incorporating structural information (measurement models) often overlooked.

Method: Introduce System embedded Diffusion Bridge Models (SDBs), embedding linear measurement systems into matrix-valued SDE coefficients.

Result: Consistent improvements in diverse linear inverse problems and robust generalization under system misspecification.

Conclusion: SDBs offer a promising solution for real-world inverse problems by leveraging structural information effectively.

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [131] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li,Linwei Tao,Haohui Lu,Minjing Dong,Junbin Gao,Chang Xu*

Main category: cs.LG

TL;DR: WATS is a post-hoc calibration framework for GNNs that uses graph wavelet features to improve confidence estimates, outperforming existing methods in calibration accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: GNNs often misalign confidence estimates with actual correctness, limiting their use in safety-critical applications. Existing methods rely on coarse statistics, ignoring fine-grained graph topology.

Method: Proposes WATS, which assigns node-specific temperatures using heat-kernel graph wavelet features, avoiding retraining or neighbor data access.

Result: WATS achieves the lowest ECE, outperforming baselines by up to 42.3% and reducing calibration variance by 17.24%. It scales efficiently across diverse graphs.

Conclusion: WATS effectively addresses GNN calibration issues, offering improved accuracy and scalability without requiring model changes.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [132] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper analyzes watermark radioactivity in diffusion and autoregressive models, proposes a new method for autoregressive models, and demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of radioactive watermarking methods for image autoregressive models (IARs) and the failure of existing methods in diffusion models (DMs).

Method: Proposes a watermarking method for IARs inspired by techniques in large language models (LLMs), focusing on radioactivity.

Result: The method effectively preserves radioactivity in IARs, enabling robust provenance tracking.

Conclusion: The proposed watermarking method for IARs successfully addresses the limitations of existing methods, ensuring traceability and preventing misuse.

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [133] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: SAEs are less effective for known concepts but powerful for discovering unknown ones, clarifying their utility in interpretability, fairness, and social sciences.


<details>
  <summary>Details</summary>
Motivation: Reconcile competing narratives about SAEs by distinguishing their effectiveness for known vs. unknown concepts.

Method: Conceptual distinction between SAEs' roles in acting on known concepts vs. discovering unknown ones.

Result: SAEs are useful for interpretability, fairness, and social sciences, despite skepticism.

Conclusion: SAEs have distinct applications in discovering unknown concepts, clarifying their value in specific domains.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [134] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao,Steve McLaughlin,Yoann Altmann*

Main category: cs.LG

TL;DR: A gradient-free message-passing framework for training spiking neural networks (SNNs) using Expectation-Propagation, enabling learning of marginal distributions and faster convergence than gradient-based methods.


<details>
  <summary>Details</summary>
Motivation: To unify training methods for SNNs, handling both discrete and continuous weights, and marginalizing nuisance parameters efficiently.

Method: Proposes a gradient-free Expectation-Propagation framework for training SNNs, learning marginal distributions and handling nuisance parameters.

Result: Faster convergence in practice compared to gradient-based methods, with successful classification and regression results.

Conclusion: The framework offers efficient training for deep Bayesian networks, paving the way for new methods in SNN training.

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [135] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: The paper introduces a method to reorder decoder input tokens in Transformers to improve learning of arithmetic tasks by identifying learning-friendly sequences through hierarchical reordering.


<details>
  <summary>Details</summary>
Motivation: The order of intermediate steps in the chain of thought critically impacts reasoning difficulty in Transformers, prompting the need for optimized token sequences.

Method: A pipeline trains a Transformer on mixed-order sequences, identifies benign orders via early loss drops, and uses a hierarchical two-stage approach for efficient reordering.

Result: The method successfully identifies learning-friendly orders from billions of candidates, even recovering the reverse-digit order for multiplication.

Conclusion: Optimizing token order in Transformers enhances learning efficiency, particularly for order-sensitive arithmetic tasks.

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [136] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: A novel model-driven trace clustering method optimizes stochastic process models using entropic relevance, improving interpretability and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: High variability in process discovery leads to complex models; existing clustering techniques often ignore stochasticity, limiting their ability to capture real-world dynamics.

Method: Uses entropic relevance, a stochastic conformance metric, to assign traces based on structural alignment and likelihood, optimizing stochastic models per cluster.

Result: Computationally efficient, scales linearly, and produces clearer control-flow patterns, outperforming alternatives in representing process behavior.

Conclusion: The method enhances interpretability and performance, demonstrating the importance of stochasticity in clustering.

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [137] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Main category: cs.LG

TL;DR: A reinforcement learning (RL) strategy using Proximal Policy Optimisation (PPO) is proposed to synchronize resin flow fronts in composites manufacturing, improving process control and product quality.


<details>
  <summary>Details</summary>
Motivation: Ensuring uniform resin impregnation in processes like resin infusion (RI) and resin transfer moulding (RTM) is crucial to prevent defects like porosities and dry spots, which affect structural integrity.

Method: The study employs RL, specifically PPO, to manage resin flow dynamics in a partially observable environment, using simulations with two resin inlets and one outlet.

Result: The RL approach effectively achieves accurate flow convergence, demonstrating its potential for enhancing process control.

Conclusion: The RL-based strategy shows promise for improving resin flow management in composites manufacturing, leading to better product quality.

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [138] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Main category: cs.LG

TL;DR: ADReFT is a novel repair method for Autonomous Driving Systems (ADSs) that uses offline learning and adaptive actions to improve safety, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: Existing online repair solutions for ADSs lack generalizability and adaptability, often being overly conservative and ineffective.

Method: ADReFT employs a transformer-based model with State Monitor and Decision Adapter heads, pretrained with supervised learning and fine-tuned with reinforcement learning.

Result: ADReFT achieves better repair performance by generating precise and contextually appropriate repair decisions.

Conclusion: ADReFT effectively addresses the limitations of current ADS repair methods, enhancing safety and reliability.

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [139] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu,Parjanya Prajakta Prashant,Alex Cloninger,Babak Salimi*

Main category: cs.LG

TL;DR: KAIROS is a scalable, model-agnostic framework for data valuation, outperforming existing methods in accuracy and efficiency by using MMD-based influence scores.


<details>
  <summary>Details</summary>
Motivation: Existing data valuation methods are biased, costly, or inaccurate, limiting their practical use in regulatory compliance and AI asset valuation.

Method: KAIROS assigns distributional influence scores using Maximum Mean Discrepancy (MMD) between training data and a clean reference set, avoiding retraining and providing closed-form solutions.

Result: KAIROS achieves up to 50x speedup, outperforms baselines in accuracy and runtime, and handles noise, mislabeling, and poisoning effectively.

Conclusion: KAIROS offers a scalable, efficient, and accurate solution for data valuation, with theoretical guarantees and practical benefits.

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [140] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi,Francesco Ferrini,Antonio Longa,Bruno Lepri,Andrea Passerini,Manfred Jaeger*

Main category: cs.LG

TL;DR: The paper studies the expressive power of Graph Neural Networks (GNNs) for link representation, introducing a framework to compare methods and proposing a synthetic benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing research on GNN expressiveness focuses on graph-level representations, leaving a gap in understanding link-level expressiveness.

Method: The authors introduce the $k_\phi$-$k_\rho$-$m$ framework to unify and compare link models, derive a hierarchy of methods, and propose a synthetic benchmark for evaluation.

Result: Expressive models outperform simpler ones in high-symmetry scenarios, though they may underperform on standard benchmarks.

Conclusion: The study highlights the importance of dataset-aware model selection and provides tools for analyzing future GNN architectures for link representation.

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [141] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi,Matteo Forasassi,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: The paper addresses performance degradation in deep predictive coding networks by introducing methods to balance errors and improve weight updates, achieving results comparable to backpropagation.


<details>
  <summary>Details</summary>
Motivation: Performance drops in deep predictive coding networks due to imbalanced errors and ineffective guidance in deeper layers.

Method: Proposes precision-weighting for latent variables and a novel weight update mechanism to address error accumulation.

Result: Improved test accuracy in networks with over seven layers, matching backpropagation performance.

Conclusion: Better understanding of the relaxation phase is key for scaling equilibrium propagation in complex tasks.

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [142] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter,Lukas Niehaus,Julius Schöning*

Main category: cs.LG

TL;DR: The paper evaluates mixed training strategies for ANNs using hybrid datasets to bridge the domain gap between synthetic and real data, analyzing their generalizability and robustness across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: The disparity between synthetic and real data causes poor ANN performance in real-world scenarios, prompting the need for systematic evaluation of mixed training strategies.

Method: The study analyzes two mixing strategies on three architectures and three hybrid datasets, varying synthetic-to-real data proportions.

Result: The findings offer insights into optimizing synthetic data use in ANN training to improve robustness and efficacy.

Conclusion: The study contributes to better understanding and application of synthetic data in ANN training by evaluating mixed strategies.

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [143] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.LG

TL;DR: A novel adaptive anomaly detection framework for sequential RFS observations, distinguishing normal from anomalous data by detecting deviations from expected statistical behavior.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of monitoring sequential RFS observations and accurately identifying anomalies by learning normal behavior online and adapting to behavioral shifts.

Method: Introduces a Power Discounting Posteriors (PD) class for RFS-based posterior distributions, enabling dynamic adaptation and anomaly detection via a predictive posterior density function.

Result: Demonstrated effectiveness through extensive qualitative and quantitative simulation experiments.

Conclusion: The proposed framework successfully adapts to behavioral shifts and accurately identifies anomalies in sequential RFS observations.

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [144] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang,Shangding Gu*

Main category: cs.LG

TL;DR: Selecting uniformly distributed data improves training efficiency and performance in LLMs by increasing pairwise distance, proven theoretically and validated experimentally.


<details>
  <summary>Details</summary>
Motivation: To identify general principles of data selection beyond quality and diversity that enhance model performance, especially for complex tasks with limited prior knowledge.

Method: Theoretical analysis links uniform data distribution to larger minimum pairwise distance ($h_{\min}$), proving its impact on training dynamics and approximation error. A convergence framework for GD beyond NTK is introduced.

Result: Maximizing pairwise distance accelerates training and improves performance in LLMs across diverse datasets, as shown in comprehensive experiments.

Conclusion: Uniform data distribution is a general and effective principle for data selection, enhancing both training efficiency and model performance.

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [145] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

Main category: cs.LG

TL;DR: The paper revisits SGD with AdaGrad-type preconditioning, unifying convergence analysis for adaptive methods and proving acceleration with Nesterov momentum.


<details>
  <summary>Details</summary>
Motivation: To provide a unified theoretical framework for SGD with adaptive preconditioning and explore the benefits of combining preconditioning with momentum.

Method: Develops a convergence analysis under anisotropic smoothness and noise, linking Scion and DASGO, and tests acceleration with Nesterov momentum.

Result: Recovers state-of-the-art convergence results, connects Scion and DASGO theoretically, and shows accelerated convergence for AdaGrad-type methods with momentum.

Conclusion: AdaGrad-type methods can benefit from both preconditioning and momentum, explaining Adam's practical efficiency.

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [146] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: A novel SSL approach integrates clustering assumptions with annotated data, simplifying training and improving performance over supervised baselines, while also complementing other SSL methods.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods rely heavily on complex consistency regularization or entropy minimization, prompting a simpler, clustering-based alternative.

Method: Extends a differentiable clustering module, using annotated data to guide cluster centroids for an end-to-end trainable deep SSL model.

Result: Outperforms supervised-only baselines and enhances other SSL methods when combined.

Conclusion: The proposed clustering-based SSL approach is effective, simple, and compatible with existing methods.

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [147] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: The paper introduces EFPI, a method for recognizing football formations and assigning player positions using static templates and cost minimization from tracking data.


<details>
  <summary>Details</summary>
Motivation: Understanding team formations and player positioning is vital for tactical analysis in football.

Method: EFPI uses linear sum assignment to match players to template formations by minimizing distance costs, scaling player positions to template dimensions, and optionally incorporating a stability parameter.

Result: The method works effectively on individual frames and extends to larger game segments, with open-source availability.

Conclusion: EFPI provides a flexible and accurate approach for formation recognition and player position assignment in football.

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [148] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss,Till Aust,Heiko Hamann*

Main category: cs.LG

TL;DR: The paper explores using plants as natural sensors for environmental monitoring, integrating them with wearable devices and AutoML for high-accuracy data analysis.


<details>
  <summary>Details</summary>
Motivation: To leverage plants' natural sensing capabilities for scalable, sustainable environmental monitoring and precision agriculture.

Method: Equipped *Hedera helix* with PhytoNode to record electrophysiological activity outdoors, analyzed data using AutoML and manual tuning.

Result: Achieved 95% macro F1 scores in binary classification tasks; AutoML outperformed manual tuning.

Conclusion: Demonstrates a scalable, plant-integrated biohybrid system for real-world environmental monitoring.

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [149] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna,Lesley Davidson,Soumya Banerjee,Abhishek Dasgupta,Laurence Kenney,Vikranth Harthikote Nagaraja*

Main category: cs.LG

TL;DR: An AI-powered framework translates complex medical documents into marginalized languages, enabling underserved populations to access healthcare information in real time.


<details>
  <summary>Details</summary>
Motivation: Millions in Africa face healthcare barriers due to language and literacy gaps, especially with donated prosthetic devices lacking accessible documentation.

Method: Uses a Retrieval-Augmented Generation (RAG) pipeline for semantic understanding and NLP models for multilingual translation and question-answering.

Result: The system provides localized, real-time answers in native languages, improving accessibility to medical information.

Conclusion: This framework bridges the gap in healthcare accessibility, empowering patients and clinicians with informed decision-making tools.

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [150] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood,Misko Dzamba,Xiang Fu,Meng Gao,Muhammed Shuaibi,Luis Barroso-Luque,Kareem Abdelmaqsoud,Vahe Gharakhanyan,John R. Kitchin,Daniel S. Levine,Kyle Michel,Anuroop Sriram,Taco Cohen,Abhishek Das,Ammar Rizvi,Sushree Jagriti Sahoo,Zachary W. Ulissi,C. Lawrence Zitnick*

Main category: cs.LG

TL;DR: Meta FAIR introduces Universal Models for Atoms (UMA), a family of models trained on 500M atomic structures, achieving high speed, accuracy, and generalization across chemical domains without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The need for fast and accurate atomic property computation in chemistry and materials science applications like drug discovery and energy storage.

Method: UMA models use a novel 'mixture of linear experts' architecture, scaling model capacity with dataset size, and are trained on diverse chemical domains.

Result: UMA models perform comparably or better than specialized models across multiple domains without fine-tuning.

Conclusion: UMA's release aims to accelerate computational workflows and advance AI capabilities in atomic simulations.

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [151] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen,Vassilis Kekatos,Ming Jin*

Main category: cs.LG

TL;DR: The paper proposes a convex training framework for neural networks to ensure certified robustness via Lipschitz constraints, using semidefinite relaxation and a randomized subspace method for scalability.


<details>
  <summary>Details</summary>
Motivation: Certified robustness is essential for safety-critical applications, but existing Lipschitz-constrained training methods suffer from non-convexity and poor scalability.

Method: The framework reparameterizes the NN using loop transformation for convex admissibility and employs a randomized subspace LMI approach to decompose global constraints into scalable layerwise ones.

Result: Empirical results on MNIST, CIFAR-10, and ImageNet show competitive accuracy, improved Lipschitz bounds, and better runtime.

Conclusion: The proposed method offers a scalable and certifiable solution for training robust neural networks.

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [152] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: LLM-based agents enable universal interoperability, disrupting closed platforms by making data exchange cheap and unavoidable, but require frameworks to address risks.


<details>
  <summary>Details</summary>
Motivation: The dominance of closed, proprietary platforms limits data exchange and user freedom. LLM-based agents can disrupt this by enabling seamless interoperability.

Method: Proposes using LLM-based agents to automatically translate data formats and interact with human-designed interfaces, reducing interoperability costs.

Result: Universal interoperability undermines monopolistic behaviors and promotes data portability but introduces security risks and technical debt.

Conclusion: The ML community should embrace universal interoperability while developing frameworks to mitigate risks, restoring user freedom and competitive markets.

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [153] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

Main category: cs.LG

TL;DR: Derivation of Jacobian and Hessian matrices for Kullback-Leibler divergence between multivariate Gaussians using differentials.


<details>
  <summary>Details</summary>
Motivation: To provide a clear, didactic explanation of the derivations for Jacobian and Hessian matrices in this context, inspired by existing literature.

Method: Uses first and second-order differentials, referencing theory from cited works (Magnus, 1999; Minka).

Result: Detailed derivations and summaries of results for the Jacobian and Hessian matrices.

Conclusion: The document successfully breaks down complex derivations into understandable parts, making it accessible for learning.

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [154] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: TTA-VLM is a benchmark for evaluating test-time adaptation (TTA) methods on vision-language models (VLMs), addressing limitations like inconsistent evaluations and lack of comprehensive metrics. It shows limited gains from current TTA methods and their poor collaboration with fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: Current TTA research lacks fair comparisons due to duplicated results, limited metrics, and inconsistent settings, obscuring practical strengths and weaknesses.

Method: TTA-VLM implements 8 episodic and 7 online TTA methods in a unified framework, evaluating them across 15 datasets and extending beyond CLIP to SigLIP. It includes metrics like robustness, calibration, and stability.

Result: Findings include limited gains from existing TTA methods, poor collaboration with fine-tuning methods, and trade-offs between accuracy and model trustworthiness.

Conclusion: TTA-VLM provides a fair, comprehensive benchmark to encourage development of more reliable and generalizable TTA strategies.

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [155] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang,Xingyu Xu,Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [156] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: A training-free sampling algorithm accelerates diffusion models without retraining, requiring fewer score function evaluations for accurate data distribution approximation.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of diffusion models by reducing the computational cost of sampling without additional training.

Method: Proposes a principled, training-free algorithm using high-order ODE solvers, Lagrange interpolation, and successive refinement to approximate the probability flow ODE.

Result: Achieves provable acceleration with fewer score evaluations, applicable to a broad class of distributions without smoothness or log-concavity assumptions.

Conclusion: The algorithm robustly accelerates diffusion models, gracefully handling inexact score estimates and outperforming prior methods.

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [157] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: A multimodal contrastive learning framework enhances time series forecasting by aligning visual and textual representations derived from numerical sequences, outperforming unimodal and cross-modal baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal numerical inputs struggle to capture high-level semantic patterns, and existing text-based methods lack perceptual intuition like visual interpretation.

Method: Proposes a framework transforming raw time series into structured visual and textual perspectives, aligning them via contrastive learning, and introducing a variate selection module for informative variable identification.

Result: Outperforms unimodal and cross-modal baselines on fifteen short-term and six long-term forecasting benchmarks.

Conclusion: Multimodal alignment effectively enhances time series forecasting by capturing richer and complementary representations.

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [158] [Cooperation as Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS](https://arxiv.org/abs/2506.22876)
*Shayak Nandi,Fernanda M. Eliott*

Main category: cs.MA

TL;DR: The paper addresses misalignment in multi-agent systems (MAS) by identifying conceptual design issues like semantic ambiguity and normative projection. It introduces the Misalignment Mosaic, a framework to diagnose meaning-level misalignment, focusing on cooperation vs. coordination ambiguity.


<details>
  <summary>Details</summary>
Motivation: Misalignment in MAS often stems from upstream design issues like semantic ambiguity and moral overreading, which are overlooked as technical failures.

Method: The paper uses the Rabbit-Duck illusion to illustrate perspective-dependent behavior readings and introduces the Misalignment Mosaic, a diagnostic framework with four components: Terminological Inconsistency, Concept-to-Code Decay, Morality as Cooperation, and Interpretive Ambiguity.

Result: The Misalignment Mosaic helps researchers identify misalignment arising from language, framing, and design assumptions, not just policy or reward structures.

Conclusion: The framework generalizes to other overloaded concepts in MAS, offering a way to diagnose meaning as a source of misalignment rather than defining terms rigidly.

Abstract: Misalignment in multi-agent systems (MAS) is often treated as a technical
failure; yet many such failures originate upstream, during the conceptual
design phase, where semantic ambiguity and normative projection take place.
This paper identifies a foundational source of interpretive misalignment in
MAS: the systemic conflation of cooperation and coordination, and the moral
overreading that follows. Using the Rabbit-Duck illusion, we illustrate how
perspective-dependent readings of agent behavior can create epistemic
instability. To address this, we introduce the Misalignment Mosaic, a
diagnostic framework for diagnosing meaning-level misalignment in MAS. It
comprises four components: 1. Terminological Inconsistency, 2. Concept-to-Code
Decay, 3. Morality as Cooperation, and 4. Interpretive Ambiguity. The Mosaic
enables researchers to examine how misalignment arises not only through policy
or reward structures but also through language, framing, and design
assumptions. While this paper focuses on the specific ambiguity between
coordination and cooperation, the Mosaic generalizes to other overloaded
concepts in MAS, such as alignment, autonomy, and trust. Rather than define
cooperation once and for all, we offer a framework to diagnose meaning itself
as a source of misalignment.

</details>
