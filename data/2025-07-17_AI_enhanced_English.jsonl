{"id": "2507.11744", "categories": ["cs.MA", "cs.GT", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.11744", "abs": "https://arxiv.org/abs/2507.11744", "authors": ["Marcin Kowalik", "Przemys\u0142aw Stok\u0142osa", "Mateusz Grabowski", "Janusz Starzyk", "Pawe\u0142 Raif"], "title": "A Cellular Automata Approach to Donation Game", "comment": "16 pages, 12 figures", "summary": "The donation game is a well-established framework for studying the emergence\nand evolution of cooperation in multi-agent systems. The cooperative behavior\ncan be influenced by the environmental noise in partially observable settings\nand by the decision-making strategies of agents, which may incorporate not only\nreputation but also traits such as generosity and forgiveness. Traditional\nsimulations often assume fully random interactions, where cooperation is tested\nbetween randomly selected agent pairs. In this paper, we investigate\ncooperation dynamics using the concept of Stephen Wolfram's one-dimensional\nbinary cellular automata. This approach allows us to explore how cooperation\nevolves when interactions are limited to neighboring agents. We define binary\ncellular automata rules that conform to the donation game mechanics.\nAdditionally, we introduce models of perceptual and action noise, along with a\nmutation matrix governing the probabilistic evolution of agent strategies. Our\nempirical results demonstrate that cooperation is significantly affected by\nagents' mobility and their spatial locality on the game board. These findings\nhighlight the importance of distinguishing between entirely random multi-agent\nsystems and those in which agents are more likely to interact with their\nnearest neighbors.", "AI": {"tldr": "The paper explores cooperation dynamics in the donation game using binary cellular automata, focusing on neighbor-limited interactions and noise models.", "motivation": "To study how cooperation evolves when interactions are spatially localized, unlike traditional random interaction models.", "method": "Uses binary cellular automata rules aligned with donation game mechanics, incorporating perceptual/action noise and strategy mutation.", "result": "Cooperation is significantly influenced by agent mobility and spatial locality, differing from random interaction models.", "conclusion": "Spatial interactions and noise models critically impact cooperation, emphasizing the need to distinguish between random and localized interaction systems."}}
{"id": "2507.11906", "categories": ["cs.MA", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11906", "abs": "https://arxiv.org/abs/2507.11906", "authors": ["Tadahiro Taniguchi", "Masatoshi Nagano", "Haruumi Omoto", "Yoshiki Hayashi"], "title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models", "comment": null, "summary": "Collective human activities like using an Ouija board (or Kokkuri-san) often\nproduce emergent, coherent linguistic outputs unintended by any single\nparticipant. While psychological explanations such as the ideomotor effect\nexist, a computational understanding of how decentralized, implicit linguistic\nknowledge fuses through shared physical interaction remains elusive. We\nintroduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this\nphenomenon as collective Langevin dynamics sampling from implicitly fused\nlanguage models. Each participant is represented as an agent associated with an\nenergy landscape derived from an internal language model reflecting linguistic\npriors, and agents exert stochastic forces based on local energy gradients. We\ntheoretically prove that the collective motion of the shared pointer\n(planchette) corresponds to Langevin MCMC sampling from the sum of individual\nenergy landscapes, representing fused collective knowledge. Simulations\nvalidate that CoCre-Sam dynamics effectively fuse different models and generate\nmeaningful character sequences, while ablation studies confirm the essential\nroles of collective interaction and stochasticity. Altogether, CoCre-Sam\nprovides a novel computational mechanism linking individual implicit knowledge,\nembodied collective action, and emergent linguistic phenomena, grounding these\ncomplex interactions in the principles of probabilistic sampling.", "AI": {"tldr": "CoCre-Sam models collective linguistic outputs (e.g., Ouija board) as Langevin dynamics sampling from fused language models, showing how decentralized knowledge combines through shared interaction.", "motivation": "To computationally understand how decentralized, implicit linguistic knowledge fuses through shared physical interaction, beyond psychological explanations like the ideomotor effect.", "method": "Introduces CoCre-Sam, a framework where participants are agents with energy landscapes from internal language models. Collective pointer motion is modeled as Langevin MCMC sampling from summed energy landscapes.", "result": "Simulations show CoCre-Sam fuses models and generates meaningful sequences, with collective interaction and stochasticity being essential.", "conclusion": "CoCre-Sam links individual implicit knowledge, collective action, and emergent linguistic phenomena through probabilistic sampling."}}
{"id": "2507.12400", "categories": ["cs.MA", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.12400", "abs": "https://arxiv.org/abs/2507.12400", "authors": ["Noble Harasha", "Cristina Gava", "Nancy Lynch", "Claudia Contini", "Frederik Mallmann-Trenn"], "title": "Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment", "comment": null, "summary": "Deploying motile nanosized particles, also known as ``nanobots'', in the\nhuman body promises to improve selectivity in drug delivery and reduce side\neffects. We consider a swarm of nanobots locating a single cancerous region and\ntreating it by releasing an onboard payload of drugs at the site. At nanoscale,\nthe computation, communication, sensing, and locomotion capabilities of\nindividual agents are extremely limited, noisy, and/or nonexistent.\n  We present a general model to formally describe the individual and collective\nbehavior of agents in a colloidal environment, such as the bloodstream, for\ncancer detection and treatment by nanobots. This includes a feasible and\nprecise model of agent locomotion, inspired by actual nanoparticles that, in\nthe presence of an external chemical gradient, move towards areas of higher\nconcentration by means of self-propulsion. We present two variants of our\ngeneral model: The first assumes an endogenous chemical gradient that is fixed\nover time and centered at the targeted cancer site; the second is a more\nspeculative and dynamic variant in which agents themselves create and amplify a\nchemical gradient centered at the cancer site. In both settings, agents can\nsense the gradient and ascend it noisily, locating the cancer site more quickly\nthan via simple Brownian motion.\n  For the first variant of the model, we present simulation results to show the\nbehavior of agents under our locomotion model, as well as {analytical results}\nto bound the time it takes for the agents to reach the cancer site. For the\nsecond variant, simulation results highlight the collective benefit in having\nagents issue their own chemical signal. While arguably more speculative in its\nagent capability assumptions, this variant shows a significant improvement in\nruntime performance over the first variant, resulting from its chemical signal\namplification mechanism.", "AI": {"tldr": "The paper models nanobot swarms for cancer treatment, focusing on locomotion in a chemical gradient. Two variants are explored: one with a fixed gradient and another where nanobots amplify the gradient, showing improved performance.", "motivation": "To enhance drug delivery precision and reduce side effects by using nanobots to locate and treat cancerous regions, despite their limited individual capabilities.", "method": "A general model for nanobot behavior in colloidal environments, with two variants: fixed chemical gradient and dynamic gradient amplified by nanobots. Simulations and analytical results validate performance.", "result": "The fixed gradient variant shows bounded time for cancer site location, while the dynamic variant improves runtime performance through collective signal amplification.", "conclusion": "Nanobots can effectively locate and treat cancer sites using chemical gradients, with dynamic amplification offering superior performance."}}
{"id": "2507.11660", "categories": ["cs.LG", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11660", "abs": "https://arxiv.org/abs/2507.11660", "authors": ["Joao F. Rocha", "Ke Xu", "Xingzhi Sun", "Ananya Krishna", "Dhananjay Bhaskar", "Blanche Mongeon", "Morgan Craig", "Mark Gerstein", "Smita Krishnaswamy"], "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics", "comment": null, "summary": "The advent of single-cell technology has significantly improved our\nunderstanding of cellular states and subpopulations in various tissues under\nnormal and diseased conditions by employing data-driven approaches such as\nclustering and trajectory inference. However, these methods consider cells as\nindependent data points of population distributions. With spatial\ntranscriptomics, we can represent cellular organization, along with dynamic\ncell-cell interactions that lead to changes in cell state. Still, key\ncomputational advances are necessary to enable the data-driven learning of such\ncomplex interactive cellular dynamics. While agent-based modeling (ABM)\nprovides a powerful framework, traditional approaches rely on handcrafted rules\nderived from domain knowledge rather than data-driven approaches. To address\nthis, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)\nintegrating ABM with deep learning to model intercellular communication, and\nits effect on the intracellular gene regulatory network. Using graph ODE\nnetworks (GDEs) with shared weights per cell type, our approach represents\ngenes as vertices and interactions as directed edges, dynamically learning\ntheir strengths through a designed attention mechanism. Trained to match\ncontinuous trajectories of simulated as well as inferred trajectories from\nspatial transcriptomics data, the model captures both intercellular and\nintracellular interactions, enabling a more adaptive and accurate\nrepresentation of cellular dynamics.", "AI": {"tldr": "STAGED integrates agent-based modeling with deep learning to model cellular dynamics, capturing intercellular and intracellular interactions using graph ODE networks.", "motivation": "Current methods treat cells as independent points, missing dynamic interactions. Spatial transcriptomics reveals cellular organization, but computational advances are needed for data-driven learning of interactive dynamics.", "method": "STAGED combines agent-based modeling with deep learning, using graph ODE networks with shared weights per cell type and an attention mechanism to dynamically learn interaction strengths.", "result": "The model accurately captures intercellular and intracellular interactions, matching simulated and inferred trajectories from spatial transcriptomics data.", "conclusion": "STAGED provides a more adaptive and accurate representation of cellular dynamics by integrating data-driven approaches with agent-based modeling."}}
{"id": "2507.11547", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11547", "abs": "https://arxiv.org/abs/2507.11547", "authors": ["Yingxue Zhao", "Qianyi Chen", "Haoran Li", "Haosu Zhou", "Hamid Reza Attar", "Tobias Pfaff", "Tailin Wu", "Nan Li"], "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming", "comment": null, "summary": "In recent years, various artificial intelligence-based surrogate models have\nbeen proposed to provide rapid manufacturability predictions of material\nforming processes. However, traditional AI-based surrogate models, typically\nbuilt with scalar or image-based neural networks, are limited in their ability\nto capture complex 3D spatial relationships and to operate in a\npermutation-invariant manner. To overcome these issues, emerging graph-based\nsurrogate models are developed using graph neural networks. This study\ndeveloped a new graph neural network surrogate model named Recurrent U\nNet-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate\npredictions of sheet material deformation fields across multiple forming\ntimesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model\ntemporal dynamics and a U-Net inspired graph-based downsample/upsample\nmechanism to handle spatial long-range dependencies. A novel 'node-to-surface'\ncontact representation method was proposed, offering significant improvements\nin computational efficiency for large-scale contact interactions. The RUGNN\nmodel was validated using a cold forming case study and a more complex hot\nforming case study using aluminium alloys. Results demonstrate that the RUGNN\nmodel provides accurate deformation predictions closely matching ground truth\nFE simulations and outperforming several baseline GNN architectures. Model\ntuning was also performed to identify suitable hyperparameters, training\nstrategies, and input feature representations. These results demonstrate that\nRUGNN is a reliable approach to support sheet material forming design by\nenabling accurate manufacturability predictions.", "AI": {"tldr": "A new graph neural network (RUGNN) is proposed for accurate 3D deformation predictions in material forming, outperforming traditional AI models and baseline GNNs.", "motivation": "Traditional AI surrogate models struggle with 3D spatial relationships and permutation invariance, prompting the need for advanced graph-based solutions.", "method": "Developed RUGNN with GRUs for temporal dynamics and U-Net-inspired spatial handling, plus a novel 'node-to-surface' contact method for efficiency.", "result": "RUGNN accurately predicts deformation fields in cold and hot forming, matching ground truth FE simulations and outperforming baseline GNNs.", "conclusion": "RUGNN is a reliable tool for manufacturability predictions in sheet material forming, validated by case studies and hyperparameter tuning."}}
{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "AI": {"tldr": "The paper explores AI's role in shifting human-nature dynamics from dominance to interdependence, showcasing AI's application in ecological design and restoration.", "motivation": "To investigate if AI can mediate a shift from human dominance over nature to a relationship of interdependence, and how this can be achieved through ecological design.", "method": "Case studies of AI applications in ecological design, including data analysis, image recognition, and ecological restoration, plus a prototype for AI-assisted water remediation using reinforcement learning and phytoremediation.", "result": "AI expands creative methods and reframes ecological design theory and practice, linking scientific insight, artistic practice, and environmental stewardship.", "conclusion": "AI offers a promising pathway for sustainable, technology-enabled ecosystems, providing a roadmap for future research in this field."}}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "MLLMs show promise as verifiers for agent behavior but suffer from agreement bias. Self-Grounded Verification (SGV) improves accuracy by leveraging MLLMs' own sampling mechanisms.", "motivation": "Extending verifiers to domains without clear success criteria (e.g., computer use) is challenging. MLLMs offer potential due to their world knowledge and reasoning skills.", "method": "Proposes Self-Grounded Verification (SGV), a two-step method: eliciting broad priors from MLLMs, then evaluating trajectories conditioned on these priors.", "result": "SGV improves MLLM verifiers by up to 20 points in accuracy and failure detection, achieving state-of-the-art performance on benchmarks.", "conclusion": "SGV effectively addresses agreement bias in MLLMs, enhancing their utility as verifiers in diverse domains."}}
{"id": "2507.11570", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11570", "abs": "https://arxiv.org/abs/2507.11570", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "comment": null, "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "AI": {"tldr": "SurgeryLSTM, a masked BiLSTM with attention, outperforms traditional ML models in predicting LOS in elective spine surgery, offering higher accuracy (R2=0.86) and interpretability.", "motivation": "To improve LOS prediction in elective spine surgery by leveraging temporal modeling and enhancing interpretability for clinical adoption.", "method": "Compared traditional ML models (linear regression, random forest, SVM, XGBoost) with SurgeryLSTM (masked BiLSTM with attention) using EHR data. Evaluated performance via R2 and explainable AI.", "result": "SurgeryLSTM achieved the highest accuracy (R2=0.86). Attention mechanism identified key predictors (bone disorder, chronic kidney disease, lumbar fusion) and influential temporal segments.", "conclusion": "SurgeryLSTM is an effective, interpretable AI solution for LOS prediction, supporting its integration into clinical workflows for better discharge planning and patient care."}}
{"id": "2507.11633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11633", "abs": "https://arxiv.org/abs/2507.11633", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "comment": "8 pages, ICML MAS workshop", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience.", "AI": {"tldr": "A modular harness design for LLM agents improves performance in diverse gaming environments by integrating perception, memory, and reasoning components.", "motivation": "To enable a single LLM or VLM backbone to handle varied multi-turn gaming scenarios without domain-specific adjustments.", "method": "The framework uses perception, memory, and reasoning modules, tested on classic and modern game suites to analyze performance impacts.", "result": "The harness consistently outperforms baselines, with memory excelling in long-horizon puzzles and perception in vision-noisy arcades.", "conclusion": "The modular design effectively advances general-purpose agents, leveraging the familiarity of games for broader applicability."}}
{"id": "2507.11574", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11574", "abs": "https://arxiv.org/abs/2507.11574", "authors": ["Kazuma Kobayashi", "Shailesh Garg", "Farid Ahmed", "Souvik Chakraborty", "Syed Bahauddin Alam"], "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators", "comment": null, "summary": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe\ndeployment of deep learning in real-time virtual sensing, particularly in\nhigh-stakes domains where sparse, noisy, or non-collocated sensor data are the\nnorm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework\nthat transforms neural operator-based virtual sensing with calibrated,\ndistribution-free prediction intervals. By unifying Monte Carlo dropout with\nsplit conformal prediction in a single DeepONet architecture, CMCO achieves\nspatially resolved uncertainty estimates without retraining, ensembling, or\ncustom loss design. Our method addresses a longstanding challenge: how to endow\noperator learning with efficient and reliable UQ across heterogeneous domains.\nThrough rigorous evaluation on three distinct applications: turbulent flow,\nelastoplastic deformation, and global cosmic radiation dose estimation-CMCO\nconsistently attains near-nominal empirical coverage, even in settings with\nstrong spatial gradients and proxy-based sensing. This breakthrough offers a\ngeneral-purpose, plug-and-play UQ solution for neural operators, unlocking\nreal-time, trustworthy inference in digital twins, sensor fusion, and\nsafety-critical monitoring. By bridging theory and deployment with minimal\ncomputational overhead, CMCO establishes a new foundation for scalable,\ngeneralizable, and uncertainty-aware scientific machine learning.", "AI": {"tldr": "CMCO introduces a framework for robust uncertainty quantification in deep learning for virtual sensing, combining Monte Carlo dropout and conformal prediction to provide calibrated, distribution-free prediction intervals without retraining or custom loss design.", "motivation": "The need for reliable uncertainty quantification in high-stakes domains with sparse or noisy sensor data drives the development of CMCO.", "method": "CMCO unifies Monte Carlo dropout and split conformal prediction in a DeepONet architecture, enabling spatially resolved uncertainty estimates.", "result": "CMCO achieves near-nominal empirical coverage across turbulent flow, elastoplastic deformation, and cosmic radiation dose estimation, even in challenging settings.", "conclusion": "CMCO offers a plug-and-play UQ solution for neural operators, enabling trustworthy real-time inference in safety-critical applications."}}
{"id": "2507.11589", "categories": ["cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.11589", "abs": "https://arxiv.org/abs/2507.11589", "authors": ["Sandeep Suresh Cranganore", "Andrei Bodnar", "Arturs Berzins", "Johannes Brandstetter"], "title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "comment": "63 pages, 22 figures, 10 Tables, Github:\n  https://github.com/AndreiB137/EinFields", "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\n\\emph{metric}, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural\nTensor Fields} with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields", "AI": {"tldr": "Einstein Fields is a neural representation for compressing 4D numerical relativity simulations into compact neural network weights, enabling efficient modeling of spacetime geometry and physical quantities via automatic differentiation.", "motivation": "To address the computational intensity of numerical relativity simulations by leveraging neural networks for compact, efficient, and accurate modeling of spacetime dynamics.", "method": "Uses Neural Tensor Fields to encode spacetime geometry, naturally capturing dynamics as a byproduct, and employs automatic differentiation for deriving physical quantities.", "result": "Demonstrates potential in continuum modeling, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use across canonical test beds.", "conclusion": "Einstein Fields offer a scalable and expressive approach to numerical relativity, supported by an open-source JAX-based library."}}
{"id": "2507.11733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11733", "abs": "https://arxiv.org/abs/2507.11733", "authors": ["Srikanth Vemula"], "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "comment": null, "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes.", "AI": {"tldr": "ClarifAI enhances AI transparency using Case-Based Reasoning and ontologies for better decision-making.", "motivation": "To improve AI interpretability for stakeholders in high-stake applications.", "method": "Combines Case-Based Reasoning (CBR) and ontology-driven approaches for detailed explanations.", "result": "ClarifAI improves AI interpretability across sectors and critical processes.", "conclusion": "ClarifAI advances AI transparency, enabling deployment in critical decision-making."}}
{"id": "2507.11590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11590", "abs": "https://arxiv.org/abs/2507.11590", "authors": ["Raju Challagundla", "Mohsen Dorodchi", "Pu Wang", "Minwoo Lee"], "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques", "comment": null, "summary": "As privacy regulations become more stringent and access to real-world data\nbecomes increasingly constrained, synthetic data generation has emerged as a\nvital solution, especially for tabular datasets, which are central to domains\nlike finance, healthcare and the social sciences. This survey presents a\ncomprehensive and focused review of recent advances in synthetic tabular data\ngeneration, emphasizing methods that preserve complex feature relationships,\nmaintain statistical fidelity, and satisfy privacy requirements. A key\ncontribution of this work is the introduction of a novel taxonomy based on\npractical generation objectives, including intended downstream applications,\nprivacy guarantees, and data utility, directly informing methodological design\nand evaluation strategies. Therefore, this review prioritizes the actionable\ngoals that drive synthetic data creation, including conditional generation and\nrisk-sensitive modeling. Additionally, the survey proposes a benchmark\nframework to align technical innovation with real-world demands. By bridging\ntheoretical foundations with practical deployment, this work serves as both a\nroadmap for future research and a guide for implementing synthetic tabular data\nin privacy-critical environments.", "AI": {"tldr": "A survey on synthetic tabular data generation, focusing on preserving feature relationships, statistical fidelity, and privacy, with a novel taxonomy and benchmark framework.", "motivation": "Addressing the need for synthetic data due to strict privacy regulations and limited access to real-world data, especially in finance, healthcare, and social sciences.", "method": "Comprehensive review of recent advances, introducing a taxonomy based on generation objectives (downstream applications, privacy, utility) and proposing a benchmark framework.", "result": "A roadmap for future research and practical deployment of synthetic tabular data in privacy-critical environments.", "conclusion": "The survey bridges theory and practice, guiding methodological design and evaluation for synthetic data generation."}}
{"id": "2507.11737", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11737", "abs": "https://arxiv.org/abs/2507.11737", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "comment": null, "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them.", "AI": {"tldr": "The paper introduces DP-Bench, a benchmark for dynamic programming (DP) problems, and DPLM, a specialized 7B-parameter model. It uses DualReflect, a synthetic data generation pipeline, to overcome data scarcity and achieve strong performance.", "motivation": "Automating DP model formulation is challenging due to stochastic transitions and limited training data. Existing LLMs are not directly applicable, necessitating a specialized approach.", "method": "Developed DP-Bench for evaluation and DPLM, a specialized model. Introduced DualReflect, a pipeline combining forward and backward synthetic data generation.", "result": "DPLM matches state-of-the-art LLMs and outperforms them on hard problems. DualReflect's backward generation excels in low-data regimes, while forward generation adds diversity at scale.", "conclusion": "Combining forward and backward generation in DualReflect is key for effective DP problem-solving, balancing correctness and diversity."}}
{"id": "2507.11620", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11620", "abs": "https://arxiv.org/abs/2507.11620", "authors": ["Steven Dillmann", "Juan Rafael Mart\u00ednez-Galarza"], "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "AI": {"tldr": "The paper introduces tensor representations and sparse autoencoders for analyzing irregular event time series, demonstrating effectiveness in X-ray astronomy.", "motivation": "Event time series are unstructured and irregular, making pattern extraction challenging with conventional methods.", "method": "Proposes 2D/3D tensor representations and sparse autoencoders to learn meaningful latent embeddings.", "result": "Successfully captures temporal/spectral signatures and classifies X-ray transients in real-world data.", "conclusion": "The framework is flexible, scalable, and generalizable for analyzing event time series across domains."}}
{"id": "2507.11787", "categories": ["cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11787", "abs": "https://arxiv.org/abs/2507.11787", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions.", "AI": {"tldr": "A survey on Swarm Intelligence (SI) algorithms for document search based on semantic similarity, highlighting their effectiveness and suggesting future research.", "motivation": "Swarm Intelligence is popular in AI for solving optimization problems, and this survey explores its application in semantic document search.", "method": "Review of latest developments in SI algorithms for semantic similarity-based document search.", "result": "Identifies effectiveness of SI in solving such problems and current advancements.", "conclusion": "Recommends future research directions for improving SI applications in semantic document search."}}
{"id": "2507.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11639", "abs": "https://arxiv.org/abs/2507.11639", "authors": ["Fouad Oubari", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Deep Generative Methods and Tire Architecture Design", "comment": null, "summary": "As deep generative models proliferate across the AI landscape, industrial\npractitioners still face critical yet unanswered questions about which deep\ngenerative models best suit complex manufacturing design tasks. This work\naddresses this question through a complete study of five representative models\n(Variational Autoencoder, Generative Adversarial Network, multimodal\nVariational Autoencoder, Denoising Diffusion Probabilistic Model, and\nMultinomial Diffusion Model) on industrial tire architecture generation. Our\nevaluation spans three key industrial scenarios: (i) unconditional generation\nof complete multi-component designs, (ii) component-conditioned generation\n(reconstructing architectures from partial observations), and (iii)\ndimension-constrained generation (creating designs that satisfy specific\ndimensional requirements). To enable discrete diffusion models to handle\nconditional scenarios, we introduce categorical inpainting, a mask-aware\nreverse diffusion process that preserves known labels without requiring\nadditional training. Our evaluation employs geometry-aware metrics specifically\ncalibrated for industrial requirements, quantifying spatial coherence,\ncomponent interaction, structural connectivity, and perceptual fidelity. Our\nfindings reveal that diffusion models achieve the strongest overall\nperformance; a masking-trained VAE nonetheless outperforms the multimodal\nvariant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics,\nand within the diffusion family MDM leads in-distribution whereas DDPM\ngeneralises better to out-of-distribution dimensional constraints.", "AI": {"tldr": "The paper evaluates five deep generative models for industrial tire design, finding diffusion models perform best overall, with specific strengths in conditional and constrained scenarios.", "motivation": "Addressing the lack of clarity on which deep generative models are best for complex manufacturing design tasks, particularly in tire architecture generation.", "method": "Comparative study of five models (VAE, GAN, MMVAE, DDPM, MDM) across three industrial scenarios: unconditional generation, component-conditioned generation, and dimension-constrained generation. Introduces categorical inpainting for conditional scenarios.", "result": "Diffusion models excel overall; masking-trained VAE outperforms MMVAE in component-conditioned tasks, and MDM leads in-distribution while DDPM generalizes better to out-of-distribution constraints.", "conclusion": "Diffusion models are most effective for industrial tire design, with specific models suited to different scenarios, highlighting the importance of tailored evaluation metrics."}}
{"id": "2507.11916", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "AI": {"tldr": "The paper introduces a method for batching GPU computations in depth-first search (DFS), proposing cost-bounded DFS (CB-DFS) to leverage CPU and GPU parallelism. It extends algorithms like Batch IDA* and Batch BTS, maintaining optimality. Evaluations on Rubik's Cube and sliding tile puzzles show efficient GPU batching in DFS.", "motivation": "GPU advancements enable parallel processing, but few search algorithms exploit GPUs. The paper aims to address this gap by batching GPU computations in DFS.", "method": "Proposes CB-DFS, a cost-bounded DFS method, extending algorithms like Batch IDA* and Batch BTS. Builds on Asynchronous Parallel IDA* (AIDA*) while ensuring optimality.", "result": "Demonstrates efficient GPU batching in DFS on 3x3 Rubik's Cube and 4x4 sliding tile puzzles. Analyzes hyperparameters, heuristic size, and hardware impact.", "conclusion": "The method successfully integrates GPU parallelism into DFS, enhancing performance while preserving optimality."}}
{"id": "2507.11645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11645", "abs": "https://arxiv.org/abs/2507.11645", "authors": ["Ahmed Salah", "David Yevick"], "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation", "comment": "15 pages, 11 figures", "summary": "Grokking refers to delayed generalization in which the increase in test\naccuracy of a neural network occurs appreciably after the improvement in\ntraining accuracy This paper introduces several practical metrics including\nvariance under dropout, robustness, embedding similarity, and sparsity\nmeasures, that can forecast grokking behavior. Specifically, the resilience of\nneural networks to noise during inference is estimated from a Dropout\nRobustness Curve (DRC) obtained from the variation of the accuracy with the\ndropout rate as the model transitions from memorization to generalization. The\nvariance of the test accuracy under stochastic dropout across training\ncheckpoints further exhibits a local maximum during the grokking. Additionally,\nthe percentage of inactive neurons decreases during generalization, while the\nembeddings tend to a bimodal distribution independent of initialization that\ncorrelates with the observed cosine similarity patterns and dataset symmetries.\nThese metrics additionally provide valuable insight into the origin and\nbehaviour of grokking.", "AI": {"tldr": "The paper introduces metrics like dropout robustness, embedding similarity, and sparsity to predict grokking\u2014delayed generalization in neural networks. These metrics reveal patterns during the transition from memorization to generalization.", "motivation": "To understand and forecast grokking behavior in neural networks, which involves delayed generalization after training accuracy improves.", "method": "Proposes metrics such as Dropout Robustness Curve (DRC), variance under dropout, embedding similarity, and sparsity measures to analyze grokking.", "result": "Metrics like DRC and inactive neuron percentage show patterns during grokking, with embeddings tending to a bimodal distribution. These correlate with dataset symmetries.", "conclusion": "The introduced metrics effectively forecast grokking and provide insights into its behavior and origins."}}
{"id": "2507.11988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11988", "abs": "https://arxiv.org/abs/2507.11988", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "comment": "14 pages, 1 figures,", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "AI": {"tldr": "Aime is a dynamic multi-agent framework that improves adaptability and robustness in MAS by replacing rigid planning with reactive planning, dynamic actor instantiation, and centralized progress management.", "motivation": "Current MAS with LLMs are limited by rigid planning, static capabilities, and inefficient communication, hindering adaptability in dynamic environments.", "method": "Aime introduces a Dynamic Planner, Actor Factory for on-demand agent creation, and a Progress Management Module for system-wide state awareness.", "result": "Aime outperforms specialized state-of-the-art agents in benchmarks like GAIA, SWE-bench, and WebVoyager.", "conclusion": "Aime provides a more resilient and effective foundation for multi-agent collaboration."}}
{"id": "2507.11649", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11649", "abs": "https://arxiv.org/abs/2507.11649", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "AI": {"tldr": "Proposes a privacy-preserving FL evaluation protocol using Zero-Knowledge Proofs (ZKPs) to verify local loss thresholds without revealing raw data.", "motivation": "Addresses privacy leakage risks in FL evaluation by sharing performance metrics.", "method": "Uses ZKPs for verifiable evaluation, with self-contained modules for FL simulation, ZKP circuit design, and testing on MNIST and HAR datasets.", "result": "Evaluated computational overhead, communication cost, and verifiability for CNN (MNIST) and MLP (HAR) models.", "conclusion": "The protocol ensures privacy-preserving and verifiable FL evaluation without external APIs."}}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "AI": {"tldr": "Bio-inspired UAV navigation uses optic flow, mimicking honeybees. A Reinforcement Learning agent trained with optic flow focuses on flow discontinuities and large magnitudes, resembling insect behavior.", "motivation": "Bio-inspired design leverages biological systems' efficient navigation despite limited sensory input, aiming to improve UAV navigation.", "method": "Train a Reinforcement Learning agent using optic flow for tunnel navigation, analyzing attention patterns.", "result": "Agents focus on optic flow discontinuities and large magnitudes, resembling insect behavior, suggesting a viable UAV control strategy.", "conclusion": "The findings support bio-inspired optic flow as a simple, effective strategy for UAV navigation, replicating insect-like behavior."}}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "AI": {"tldr": "TPE-MARL is a topology-enhanced MARL method for CAVs in mixed traffic, compressing state info and improving decision-making via QMIX, visit counts, and mutual info. It outperforms in efficiency, safety, and rationality.", "motivation": "Addressing the exploration-exploitation trade-off in MARL for CAVs, exacerbated by high-dimensional state-action spaces in dynamic traffic.", "method": "Constructs a game topology tensor to compress traffic state info and uses QMIX with visit counts and agent mutual info for MARL.", "result": "TPE-MARL balances exploration-exploitation, excelling in traffic efficiency, safety, and decision smoothness, even matching human rationality.", "conclusion": "TPE-MARL effectively optimizes CAV decision-making in mixed traffic, demonstrating superior performance and practical applicability."}}
{"id": "2507.11688", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11688", "abs": "https://arxiv.org/abs/2507.11688", "authors": ["Travis Pence", "Daisuke Yamada", "Vikas Singh"], "title": "Composing Linear Layers from Irreducibles", "comment": "27 Pages, 13 Tables, 8 Figures", "summary": "Contemporary large models often exhibit behaviors suggesting the presence of\nlow-level primitives that compose into modules with richer functionality, but\nthese fundamental building blocks remain poorly understood. We investigate this\ncompositional structure in linear layers by asking: can we identify/synthesize\nlinear transformations from a minimal set of geometric primitives? Using\nClifford algebra, we show that linear layers can be expressed as compositions\nof bivectors -- geometric objects encoding oriented planes -- and introduce a\ndifferentiable algorithm that decomposes them into products of rotors. This\nconstruction uses only O(log^2 d) parameters, versus O(d^2) required by dense\nmatrices. Applied to the key, query, and value projections in LLM attention\nlayers, our rotor-based layers match the performance of strong baselines such\nas block-Hadamard and low-rank approximations. Our findings provide an\nalgebraic perspective on how these geometric primitives can compose into\nhigher-level functions within deep models.", "AI": {"tldr": "The paper explores the compositional structure of linear layers in large models using Clifford algebra, identifying bivectors as fundamental primitives and introducing a rotor-based decomposition method with fewer parameters.", "motivation": "To understand and synthesize the fundamental building blocks (geometric primitives) of linear transformations in large models, which remain poorly understood.", "method": "Uses Clifford algebra to express linear layers as compositions of bivectors and introduces a differentiable algorithm to decompose them into rotors, reducing parameter count from O(d^2) to O(log^2 d).", "result": "Rotor-based layers match the performance of strong baselines like block-Hadamard and low-rank approximations in LLM attention layers.", "conclusion": "The study provides an algebraic perspective on how geometric primitives compose into higher-level functions in deep models, offering a parameter-efficient alternative to dense matrices."}}
{"id": "2507.12186", "categories": ["cs.AI", "I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12186", "abs": "https://arxiv.org/abs/2507.12186", "authors": ["Edward Kim", "Hanna Kurniawati"], "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "AI": {"tldr": "A novel online POMDP solver, Partially Observable Reference Policy Programming, improves performance by sampling deeply and ensuring gradual policy updates, with bounded performance loss based on average sampling errors.", "motivation": "Address the challenge of online POMDP planning in dynamically evolving environments, where traditional methods suffer from high sampling sparsity and performance loss.", "method": "Proposes an anytime online approximate POMDP solver that samples future histories deeply and enforces gradual policy updates, with theoretical guarantees on bounded performance loss.", "result": "Empirical tests on large-scale problems, including a helicopter emergency scenario, show the solver outperforms current online benchmarks.", "conclusion": "The solver is effective for complex, dynamic environments, offering improved performance and theoretical robustness."}}
{"id": "2507.11690", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11690", "abs": "https://arxiv.org/abs/2507.11690", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "comment": "10 pages, 9 additional pages for Appendix", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "AI": {"tldr": "The paper analyzes how coreset selection methods impact dataset biases and model robustness, revealing nuanced interactions and limited guarantees for bias reduction.", "motivation": "To understand if and how dataset reduction methods like coreset selection affect biases in datasets and downstream model robustness.", "method": "Comprehensive analysis using ten spurious correlation benchmarks, five score metrics, and five data selection policies across various coreset sizes.", "result": "Embedding-based scores reduce bias risk compared to learning dynamics. Difficult samples may lower bias but don't ensure robustness.", "conclusion": "Coreset selection methods can influence bias but lack reliability in guaranteeing robust models."}}
{"id": "2507.12207", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.12207", "abs": "https://arxiv.org/abs/2507.12207", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "comment": "ICML 2025 CO-Build Workshop Poster", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "AI": {"tldr": "BuildEvo uses LLMs to design interpretable energy prediction heuristics, outperforming benchmarks with improved generalization and transparency.", "motivation": "Traditional heuristics lack precision, and advanced models are opaque and neglect physical principles, necessitating a better solution.", "method": "BuildEvo employs an evolutionary process with LLMs to create and refine heuristics using physical insights from building data.", "result": "BuildEvo achieves state-of-the-art performance on benchmarks, with better generalization and transparent logic.", "conclusion": "The framework advances automated design of robust, physically grounded heuristics for trustworthy energy system models."}}
{"id": "2507.11702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11702", "abs": "https://arxiv.org/abs/2507.11702", "authors": ["Hein de Wilde", "Ali Mohammed Mansoor Alsahag", "Pierre Blanchet"], "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption", "comment": null, "summary": "Railroad traffic disruption as a result of leaf-fall cost the UK rail\nindustry over 300 million per year and measures to mitigate such disruptions\nare employed on a large scale, with 1.67 million kilometers of track being\ntreated in the UK in 2021 alone. Therefore, the ability to anticipate the\ntiming of leaf-fall would offer substantial benefits for rail network\noperators, enabling the efficient scheduling of such mitigation measures.\nHowever, current methodologies for predicting leaf-fall exhibit considerable\nlimitations in terms of scalability and reliability. This study endeavors to\ndevise a prediction system that leverages specialized prediction methods and\nthe latest satellite data sources to generate both scalable and reliable\ninsights into leaf-fall timings. An LSTM network trained on ground-truth\nleaf-falling data combined with multispectral and meteorological satellite data\ndemonstrated a root-mean-square error of 6.32 days for predicting the start of\nleaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which\nimproves upon previous work on the topic, offers promising opportunities for\nthe optimization of leaf mitigation measures in the railway industry and the\nimprovement of our understanding of complex ecological systems.", "AI": {"tldr": "A study proposes an LSTM-based model using satellite and ground data to predict leaf-fall timings for UK railroads, reducing prediction errors and aiding mitigation efforts.", "motivation": "Leaf-fall disrupts UK rail traffic, costing \u00a3300M yearly. Current prediction methods lack scalability and reliability, necessitating a better solution.", "method": "An LSTM network trained on ground-truth leaf-fall data, multispectral, and meteorological satellite data.", "result": "The model achieved RMSE of 6.32 days for start and 9.31 days for end of leaf-fall, outperforming prior methods.", "conclusion": "The system improves leaf-fall prediction, aiding rail operators in scheduling mitigation and advancing ecological understanding."}}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "AI": {"tldr": "The paper explores LLMs' spatial strategic reasoning in Chinese Chess (Xiangqi), proposing a tailored training framework and Xiangqi-R1 model, which outperforms general-purpose LLMs.", "motivation": "To address the gap in LLMs' spatial strategic reasoning, using Xiangqi as a complex testbed due to its intricate rules and spatial demands.", "method": "A multi-stage training framework: fine-tuning for legal moves, adding strategic annotations, and reinforcement learning with GRPO for stable reasoning.", "result": "Xiangqi-R1 improves move legality by 18% and analysis accuracy by 22% over general-purpose LLMs.", "conclusion": "The approach shows promise for developing general strategic intelligence in spatially complex domains."}}
{"id": "2507.11706", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11706", "abs": "https://arxiv.org/abs/2507.11706", "authors": ["Taira Tsuchiya", "Shinji Ito", "Haipeng Luo"], "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs", "comment": "40 pages", "summary": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting.", "AI": {"tldr": "The paper introduces preference-based MDPs (PbMDPs) with adversarial preferences and Borda scores, establishes regret lower bounds, and proposes algorithms achieving sublinear regret.", "motivation": "To address the gap in understanding episodic MDPs with adversarial preferences, where learners observe preferences rather than numerical losses, focusing on Borda scores for reward functions.", "method": "The work derives regret lower bounds for PbMDPs and proposes two algorithms: a global optimization approach and a policy optimization method, both achieving sublinear regret.", "result": "A regret lower bound of \u03a9((H\u00b2SK)\u00b9/\u00b3T\u00b2/\u00b3) is established, and algorithms achieve regret bounds of \u00d5((H\u00b2S\u00b2K)\u00b9/\u00b3T\u00b2/\u00b3) and \u00d5((H\u2076SK\u2075)\u00b9/\u00b3T\u00b2/\u00b3) under known and unknown transitions.", "conclusion": "The paper provides theoretical foundations and practical algorithms for PbMDPs with adversarial preferences, demonstrating sublinear regret and addressing computational challenges."}}
{"id": "2507.11710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11710", "abs": "https://arxiv.org/abs/2507.11710", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links", "comment": "18 pages, 7 figures, preprint", "summary": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link\nprediction (LP) task. However, these models often rely on all dataset samples\nbeing drawn from the same distribution. In addition, graph generative models\n(GGMs) show a pronounced ability to generate novel output graphs. Despite this,\nGGM applications remain largely limited to domain-specific tasks. To bridge\nthis gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)\nstructurally-conditioned graph generation, and (2) adversarial co-training\nbetween an auto-encoder and GNN. As such, FLEX ensures structural-alignment\nbetween sample distributions to enhance link-prediction performance in\nout-of-distribution (OOD) scenarios. Notably, FLEX does not require expert\nknowledge to function in different OOD scenarios. Numerous experiments are\nconducted in synthetic and real-world OOD settings to demonstrate FLEX's\nperformance-enhancing ability, with further analysis for understanding the\neffects of graph data augmentation on link structures. The source code is\navailable here: https://github.com/revolins/FlexOOD.", "AI": {"tldr": "FLEX is a GGM framework combining structurally-conditioned graph generation and adversarial co-training to improve GNN link-prediction in OOD scenarios without expert knowledge.", "motivation": "Current GNNs for link prediction assume uniform data distribution, while GGMs are underutilized. FLEX bridges this gap for OOD scenarios.", "method": "FLEX uses structurally-conditioned graph generation and adversarial co-training between an auto-encoder and GNN to align sample distributions.", "result": "FLEX enhances link-prediction performance in OOD settings, validated by experiments in synthetic and real-world scenarios.", "conclusion": "FLEX effectively improves GNN performance in OOD scenarios without requiring expert knowledge, demonstrating the value of graph data augmentation."}}
{"id": "2507.11729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11729", "abs": "https://arxiv.org/abs/2507.11729", "authors": ["Amirhossein Ahmadi", "Hamidreza Zareipour", "Henry Leung"], "title": "Globalization for Scalable Short-term Load Forecasting", "comment": "63 pages with 22 figures", "summary": "Forecasting load in power transmission networks is essential across various\nhierarchical levels, from the system level down to individual points of\ndelivery (PoD). While intuitive and locally accurate, traditional local\nforecasting models (LFMs) face significant limitations, particularly in\nhandling generalizability, overfitting, data drift, and the cold start problem.\nThese methods also struggle with scalability, becoming computationally\nexpensive and less efficient as the network's size and data volume grow. In\ncontrast, global forecasting models (GFMs) offer a new approach to enhance\nprediction generalizability, scalability, accuracy, and robustness through\nglobalization and cross-learning. This paper investigates global load\nforecasting in the presence of data drifts, highlighting the impact of\ndifferent modeling techniques and data heterogeneity. We explore\nfeature-transforming and target-transforming models, demonstrating how\nglobalization, data heterogeneity, and data drift affect each differently. In\naddition, we examine the role of globalization in peak load forecasting and its\npotential for hierarchical forecasting. To address data heterogeneity and the\nbalance between globality and locality, we propose separate time series\nclustering (TSC) methods, introducing model-based TSC for feature-transforming\nmodels and new weighted instance-based TSC for target-transforming models.\nThrough extensive experiments on a real-world dataset of Alberta's electricity\nload, we demonstrate that global target-transforming models consistently\noutperform their local counterparts, especially when enriched with global\nfeatures and clustering techniques. In contrast, global feature-transforming\nmodels face challenges in balancing local and global dynamics, often requiring\nTSC to manage data heterogeneity effectively.", "AI": {"tldr": "The paper explores global forecasting models (GFMs) for load forecasting in power networks, addressing limitations of local models (LFMs) like generalizability and scalability. It examines feature- and target-transforming models, proposing clustering methods to handle data heterogeneity, with GFMs showing superior performance.", "motivation": "Traditional local forecasting models (LFMs) struggle with generalizability, scalability, and data drift. GFMs offer a promising alternative, but their effectiveness under data heterogeneity and drift needs investigation.", "method": "The study compares feature-transforming and target-transforming GFMs, proposing time series clustering (TSC) methods to manage data heterogeneity. Experiments use Alberta's electricity load data.", "result": "Global target-transforming models outperform local ones, especially with global features and clustering. Feature-transforming models require TSC to balance local and global dynamics.", "conclusion": "GFMs, particularly target-transforming ones, enhance load forecasting accuracy and robustness, with clustering techniques mitigating data heterogeneity challenges."}}
{"id": "2507.11732", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11732", "abs": "https://arxiv.org/abs/2507.11732", "authors": ["Shiyu Chen", "Cencheng Shen", "Youngser Park", "Carey E. Priebe"], "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning", "comment": null, "summary": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide\nrange of node-level graph learning tasks. However, their performance is often\nconstrained by reliance on random or minimally informed initial feature\nrepresentations, which can lead to slow convergence and suboptimal solutions.\nIn this paper, we leverage a statistically grounded method, one-hot graph\nencoder embedding (GEE), to generate high-quality initial node features that\nenhance the end-to-end training of GNNs. We refer to this integrated framework\nas the GEE-powered GNN (GG), and demonstrate its effectiveness through\nextensive simulations and real-world experiments across both unsupervised and\nsupervised settings. In node clustering, GG consistently achieves\nstate-of-the-art performance, ranking first across all evaluated real-world\ndatasets, while exhibiting faster convergence compared to the standard GNN. For\nnode classification, we further propose an enhanced variant, GG-C, which\nconcatenates the outputs of GG and GEE and outperforms competing baselines.\nThese results confirm the importance of principled, structure-aware feature\ninitialization in realizing the full potential of GNNs.", "AI": {"tldr": "The paper introduces a GEE-powered GNN (GG) framework, using one-hot graph encoder embedding (GEE) for better initial node features, improving GNN performance in node clustering and classification.", "motivation": "GNNs often suffer from suboptimal performance due to poor initial feature representations. This work aims to enhance GNNs by using a statistically grounded method (GEE) for feature initialization.", "method": "The proposed GG framework integrates GEE to generate high-quality initial node features. An enhanced variant, GG-C, concatenates GG and GEE outputs for node classification.", "result": "GG achieves state-of-the-art performance in node clustering and faster convergence. GG-C outperforms baselines in node classification.", "conclusion": "Principled, structure-aware feature initialization (via GEE) is crucial for maximizing GNN potential, as demonstrated by GG and GG-C."}}
{"id": "2507.11739", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.11739", "abs": "https://arxiv.org/abs/2507.11739", "authors": ["Urban Fasel"], "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction", "comment": null, "summary": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for\ndiscovering nonlinear dynamical system models from data. Quantifying\nuncertainty in SINDy models is essential for assessing their reliability,\nparticularly in safety-critical applications. While various uncertainty\nquantification methods exist for SINDy, including Bayesian and ensemble\napproaches, this work explores the integration of Conformal Prediction, a\nframework that can provide valid prediction intervals with coverage guarantees\nbased on minimal assumptions like data exchangeability. We introduce three\napplications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)\nquantifying uncertainty in time series prediction, (2) model selection based on\nlibrary feature importance, and (3) quantifying the uncertainty of identified\nmodel coefficients using feature conformal prediction. We demonstrate the three\napplications on stochastic predator-prey dynamics and several chaotic dynamical\nsystems. We show that conformal prediction methods integrated with E-SINDy can\nreliably achieve desired target coverage for time series forecasting,\neffectively quantify feature importance, and produce more robust uncertainty\nintervals for model coefficients, even under non-Gaussian noise, compared to\nstandard E-SINDy coefficient estimates.", "AI": {"tldr": "The paper explores integrating Conformal Prediction with Ensemble-SINDy (E-SINDy) to quantify uncertainty in SINDy models, demonstrating improved reliability in time series forecasting, feature importance, and model coefficients.", "motivation": "Quantifying uncertainty in SINDy models is crucial for reliability, especially in safety-critical applications. Existing methods lack robustness under non-Gaussian noise.", "method": "Integrates Conformal Prediction with E-SINDy for three applications: time series prediction uncertainty, model selection via feature importance, and coefficient uncertainty quantification.", "result": "Conformal Prediction with E-SINDy achieves desired coverage in forecasting, robustly quantifies feature importance, and provides better uncertainty intervals for coefficients under non-Gaussian noise.", "conclusion": "The integration of Conformal Prediction with E-SINDy enhances uncertainty quantification in SINDy models, offering more reliable and robust results."}}
{"id": "2507.11757", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11757", "abs": "https://arxiv.org/abs/2507.11757", "authors": ["Yuehua Song", "Yong Gao"], "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction", "comment": null, "summary": "Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.", "AI": {"tldr": "A novel GNN-based framework, Graph-in-Graph (GiG), integrates transductive and inductive learning to improve drug-target interaction prediction by leveraging molecular and network-level features.", "motivation": "Current machine learning approaches struggle to effectively integrate diverse drug and target features for DTI prediction.", "method": "The GiG model represents drug and target molecular structures as meta-nodes in a DTI graph, combining transductive and inductive learning.", "result": "GiG outperforms existing methods across all evaluation metrics on a specialized benchmark.", "conclusion": "Integrating different learning paradigms and interaction data enhances DTI prediction accuracy."}}
{"id": "2507.11759", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11759", "abs": "https://arxiv.org/abs/2507.11759", "authors": ["Alexandra Volokhova", "L\u00e9na N\u00e9hale Ezzine", "Piotr Gai\u0144ski", "Luca Scimeca", "Emmanuel Bengio", "Prudencio Tossou", "Yoshua Bengio", "Alex Hernandez-Garcia"], "title": "Torsional-GFN: a conditional conformation generator for small molecules", "comment": "The two first authors are Alexandra Volokhova and L\\'ena N\\'ehale\n  Ezzine, with equal contribution", "summary": "Generating stable molecular conformations is crucial in several drug\ndiscovery applications, such as estimating the binding affinity of a molecule\nto a target. Recently, generative machine learning methods have emerged as a\npromising, more efficient method than molecular dynamics for sampling of\nconformations from the Boltzmann distribution. In this paper, we introduce\nTorsional-GFN, a conditional GFlowNet specifically designed to sample\nconformations of molecules proportionally to their Boltzmann distribution,\nusing only a reward function as training signal. Conditioned on a molecular\ngraph and its local structure (bond lengths and angles), Torsional-GFN samples\nrotations of its torsion angles. Our results demonstrate that Torsional-GFN is\nable to sample conformations approximately proportional to the Boltzmann\ndistribution for multiple molecules with a single model, and allows for\nzero-shot generalization to unseen bond lengths and angles coming from the MD\nsimulations for such molecules. Our work presents a promising avenue for\nscaling the proposed approach to larger molecular systems, achieving zero-shot\ngeneralization to unseen molecules, and including the generation of the local\nstructure into the GFlowNet model.", "AI": {"tldr": "Torsional-GFN, a conditional GFlowNet, efficiently samples molecular conformations proportional to the Boltzmann distribution, enabling zero-shot generalization to unseen molecular structures.", "motivation": "Generating stable molecular conformations is essential for drug discovery, particularly for estimating binding affinity. Traditional methods like molecular dynamics are inefficient, motivating the use of generative machine learning.", "method": "Torsional-GFN conditions on molecular graphs and local structures (bond lengths, angles) to sample torsion angle rotations, using only a reward function for training.", "result": "The model successfully samples conformations proportional to the Boltzmann distribution for multiple molecules, with zero-shot generalization to unseen bond lengths and angles from MD simulations.", "conclusion": "Torsional-GFN offers a scalable approach for larger molecular systems, potential zero-shot generalization to new molecules, and future integration of local structure generation."}}
{"id": "2507.11771", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11771", "abs": "https://arxiv.org/abs/2507.11771", "authors": ["Sheikh Abdur Raheem Ali", "Justin Xu", "Ivory Yang", "Jasmine Xinze Li", "Ayse Arslan", "Clark Benham"], "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "comment": null, "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.", "AI": {"tldr": "The paper investigates the effectiveness of Contrastive Activation Addition (CAA) across different scales of Llama 2 models, finding it works best in early-mid layers but loses efficacy with larger models. Negative steering is more impactful than positive.", "motivation": "To understand how alignment techniques like CAA perform as LLMs scale in size and complexity, given uncertainty about their efficacy.", "method": "Applied CAA to Llama 2 models (7B, 13B, 70B) by manipulating the residual stream using contrastive pairs (e.g., hate to love) and evaluated refusal behavior.", "result": "CAA is most effective in early-mid layers, less effective with larger models, and negative steering has stronger effects than positive.", "conclusion": "CAA's effectiveness varies with model scale and layer placement, with negative steering being more influential, suggesting limitations for scaling alignment techniques."}}
{"id": "2507.11776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11776", "abs": "https://arxiv.org/abs/2507.11776", "authors": ["Merel Kampere", "Ali Mohammed Mansoor Alsahag"], "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network", "comment": null, "summary": "The Dutch railway network is one of the busiest in the world, with delays\nbeing a prominent concern for the principal passenger railway operator NS. This\nresearch addresses a gap in delay prediction studies within the Dutch railway\nnetwork by employing an XGBoost Classifier with a focus on topological\nfeatures. Current research predominantly emphasizes short-term predictions and\nneglects the broader network-wide patterns essential for mitigating ripple\neffects. This research implements and improves an existing methodology,\noriginally designed to forecast the evolution of the fast-changing US air\nnetwork, to predict delays in the Dutch Railways. By integrating Node\nCentrality Measures and comparing multiple classifiers like RandomForest,\nDecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is\nto predict delayed trajectories. However, the results reveal limited\nperformance, especially in non-simultaneous testing scenarios, suggesting the\nnecessity for more context-specific adaptations. Regardless, this research\ncontributes to the understanding of transportation network evaluation and\nproposes future directions for developing more robust predictive models for\ndelays.", "AI": {"tldr": "The study uses an XGBoost Classifier to predict delays in the Dutch railway network, focusing on topological features and comparing multiple classifiers, but results show limited performance.", "motivation": "Addresses a gap in delay prediction studies for the Dutch railway network, emphasizing broader network-wide patterns to mitigate ripple effects.", "method": "Employs an XGBoost Classifier with Node Centrality Measures and compares it with RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression.", "result": "Limited performance, especially in non-simultaneous testing, indicating a need for context-specific adaptations.", "conclusion": "Contributes to transportation network evaluation and suggests future directions for more robust delay prediction models."}}
{"id": "2507.11807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11807", "abs": "https://arxiv.org/abs/2507.11807", "authors": ["Ruofan Hu", "Dongyu Zhang", "Huayi Zhang", "Elke Rundensteiner"], "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "comment": "KDD 2025, 12 pages, 7 figures", "summary": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "AI": {"tldr": "The paper introduces CLID-MU, a meta-learning method for noisy label scenarios that doesn't rely on clean labeled data, leveraging cross-layer data structure consistency to guide training.", "motivation": "Existing meta-learning methods for noisy labels require clean labeled meta-datasets, which are hard to obtain. This work addresses the challenge of noisy label learning without such dependencies.", "method": "CLID-MU uses cross-layer information divergence to evaluate model performance by aligning data structures between the last hidden and final layers, bypassing the need for labels.", "result": "CLID-MU outperforms state-of-the-art methods on benchmark datasets with synthetic and real-world noise.", "conclusion": "CLID-MU effectively addresses noisy label learning without clean labeled data, demonstrating superior performance."}}
{"id": "2507.11789", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11789", "abs": "https://arxiv.org/abs/2507.11789", "authors": ["Alessandro Palma", "Sergei Rybakov", "Leon Hetzel", "Stephan G\u00fcnnemann", "Fabian J. Theis"], "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "comment": "31 pages, 14 figures", "summary": "Latent space interpolations are a powerful tool for navigating deep\ngenerative models in applied settings. An example is single-cell RNA\nsequencing, where existing methods model cellular state transitions as latent\nspace interpolations with variational autoencoders, often assuming linear\nshifts and Euclidean geometry. However, unless explicitly enforced, linear\ninterpolations in the latent space may not correspond to geodesic paths on the\ndata manifold, limiting methods that assume Euclidean geometry in the data\nrepresentations. We introduce FlatVI, a novel training framework that\nregularises the latent manifold of discrete-likelihood variational autoencoders\ntowards Euclidean geometry, specifically tailored for modelling single-cell\ncount data. By encouraging straight lines in the latent space to approximate\ngeodesic interpolations on the decoded single-cell manifold, FlatVI enhances\ncompatibility with downstream approaches that assume Euclidean latent geometry.\nExperiments on synthetic data support the theoretical soundness of our\napproach, while applications to time-resolved single-cell RNA sequencing data\ndemonstrate improved trajectory reconstruction and manifold interpolation.", "AI": {"tldr": "FlatVI is a training framework for variational autoencoders that enforces Euclidean geometry in latent space, improving trajectory reconstruction in single-cell RNA sequencing data.", "motivation": "Existing methods assume linear shifts and Euclidean geometry in latent space, which may not align with geodesic paths on the data manifold, limiting accuracy.", "method": "FlatVI regularizes the latent manifold of variational autoencoders to encourage Euclidean geometry, ensuring straight lines in latent space approximate geodesic interpolations.", "result": "Experiments show improved trajectory reconstruction and manifold interpolation in synthetic and single-cell RNA sequencing data.", "conclusion": "FlatVI enhances compatibility with downstream methods by enforcing Euclidean latent geometry, improving accuracy in modeling cellular state transitions."}}
{"id": "2507.11821", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen is an automated framework for generating MNIST-style datasets tailored to specific domains, combining CLIP-based semantics, reinforcement learning, and human feedback for efficient and accurate categorization.", "motivation": "Standard datasets like MNIST are too generic for domain-specific tasks, and creating custom datasets is often impractical due to time, legal, or scope constraints.", "method": "MNIST-Gen uses hierarchical semantic categorization, CLIP-based understanding, reinforcement learning, and human feedback to generate datasets. It offers multiple processing modes (individual review, smart batch, fast batch) and models transformations as composable morphisms.", "result": "MNIST-Gen achieved 85% automatic categorization accuracy and 80% time savings, demonstrated by generating Tree-MNIST and Food-MNIST datasets.", "conclusion": "MNIST-Gen provides a practical solution for creating domain-specific datasets efficiently, addressing limitations of standard benchmarks."}}
{"id": "2507.11948", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11948", "abs": "https://arxiv.org/abs/2507.11948", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "comment": null, "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "AI": {"tldr": "Kevin, a model trained with multi-turn RL, improves CUDA kernel generation, boosting correctness from 56% to 82% and speedup from 0.53x to 1.10x over PyTorch Eager.", "motivation": "GPU kernel writing is iterative and critical for AI efficiency, making it suitable for RL due to verifiable rewards like correctness and speedup.", "method": "Developed a multi-turn RL recipe to address challenges like learning from long trajectories and reward attribution across turns.", "result": "Kevin outperforms its base model (QwQ-32B) and frontier models (o4-mini), achieving higher correctness and speedup.", "conclusion": "Multi-turn RL is effective for kernel optimization, with serial refinement proving more beneficial than parallel sampling."}}
{"id": "2507.11818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11818", "abs": "https://arxiv.org/abs/2507.11818", "authors": ["Andrei Rekesh", "Miruna Cretu", "Dmytro Shevchuk", "Vignesh Ram Somnath", "Pietro Li\u00f2", "Robert A. Batey", "Mike Tyers", "Micha\u0142 Koziarski", "Cheng-Hao Liu"], "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "comment": null, "summary": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "AI": {"tldr": "SynCoGen is a framework for synthesizable 3D molecule generation, combining masked graph diffusion and flow matching, achieving state-of-the-art results in molecule and conformer generation.", "motivation": "The challenge of synthesizability in generative small molecule design, especially in 3D representations, limits geometry-based conditional generation.", "method": "SynCoGen uses simultaneous masked graph diffusion and flow matching to sample from joint distributions of building blocks, reactions, and atomic coordinates, trained on the SynSpace dataset.", "result": "Achieves top performance in molecule and conformer generation and competitive results in zero-shot molecular linker design for drug discovery.", "conclusion": "SynCoGen provides a foundation for non-autoregressive molecular generation, enabling applications like analog expansion and lead optimization."}}
{"id": "2507.11975", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11975", "abs": "https://arxiv.org/abs/2507.11975", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "comment": "25 pages, 5 figures, 4 tables", "summary": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "AI": {"tldr": "The paper proposes integrating simultaneous training and pruning in RL, specifically for OFENet-enhanced RL, using stochastic optimization and variational Bernoulli distributions to prune inactive units, achieving efficient RL agents with minimal performance loss.", "motivation": "To address the computational and memory complexity of scaling deep neural networks in RL by applying pruning methods, which are underexplored in RL compared to supervised learning.", "method": "Combines training and pruning in RL by formulating a stochastic optimization problem over network weights and variational Bernoulli distributions for pruning. Uses cost-aware regularization tailored to OFENet architectures.", "result": "Demonstrates significant pruning of OFENets with minimal performance loss on MuJoCo benchmarks, showing pruned networks outperform smaller networks trained from scratch.", "conclusion": "Pruning during training in RL produces more efficient and higher-performing agents, validating the proposed approach."}}
{"id": "2507.11997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11997", "abs": "https://arxiv.org/abs/2507.11997", "authors": ["Tairan Huang", "Yili Wang"], "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "comment": null, "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "AI": {"tldr": "MLED is a multi-level LLM-enhanced framework for graph fraud detection, integrating textual and graph data to improve fraudster identification.", "motivation": "Existing methods ignore raw textual information and struggle with multimodal fusion of text and graph data.", "method": "MLED uses LLMs to extract textual knowledge and integrates it with graph structures via type-level and relation-level enhancers.", "result": "MLED outperforms existing methods on four real-world datasets.", "conclusion": "MLED is a generalized, effective framework for enhancing graph fraud detection with LLMs."}}
{"id": "2507.11836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11836", "abs": "https://arxiv.org/abs/2507.11836", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "comment": null, "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "AI": {"tldr": "HyperEvent reframes dynamic link prediction as hyper-event recognition, capturing structural cohesion of causally related events, and outperforms state-of-the-art methods in accuracy and efficiency.", "motivation": "Existing methods fail to capture structural cohesion of composite hyper-events, limiting their effectiveness in dynamic link prediction.", "method": "HyperEvent dynamically constructs association sequences using event correlation vectors to quantify dependencies and predict hyper-events.", "result": "HyperEvent outperforms state-of-the-art methods on 4/5 datasets and achieves a 6.95% improvement in Mean Reciprocal Rank on the Flight dataset with 10.17% training time.", "conclusion": "HyperEvent is a scalable, accurate framework for dynamic link prediction, validated by superior performance and efficiency on large-scale graphs."}}
{"id": "2507.12011", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12011", "abs": "https://arxiv.org/abs/2507.12011", "authors": ["Yao Lu", "Hongyu Gao", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "comment": null, "summary": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "AI": {"tldr": "DUSE is a data expansion framework for AMR that uses uncertainty scoring and active learning to address data scarcity, outperforming baselines and showing cross-architecture generalization.", "motivation": "Deep neural networks for AMR require large labeled datasets, but target domain data is often scarce. Manual collection or augmentation are costly or insufficient.", "method": "DUSE employs an uncertainty scoring function to filter useful samples from AMR datasets and refines the scorer using active learning.", "result": "DUSE outperforms 8 coreset selection baselines in balanced and imbalanced settings and generalizes well to unseen models.", "conclusion": "DUSE effectively tackles data scarcity in AMR by dynamically expanding samples and demonstrates strong performance and generalization."}}
{"id": "2507.11839", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11839", "abs": "https://arxiv.org/abs/2507.11839", "authors": ["Chengyue Gong", "Xinshi Chen", "Yuxuan Zhang", "Yuxuan Song", "Hao Zhou", "Wenzhi Xiao"], "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "comment": null, "summary": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "AI": {"tldr": "Protenix-Mini is a lightweight, optimized model for protein structure prediction, reducing computational overhead while maintaining high accuracy.", "motivation": "The need for efficient biomolecular structure prediction in real-world applications drives the development of a lightweight model without significant accuracy loss.", "method": "Key modifications include replacing the Multi-step AF3 sampler with a few-step ODE sampler, pruning redundant Transformer blocks, and substituting the MSA module with an ESM module.", "result": "Protenix-Mini achieves high-fidelity predictions with only a 1-5% performance drop compared to the full-scale model.", "conclusion": "Protenix-Mini is ideal for resource-limited applications requiring accurate protein structure prediction."}}
{"id": "2507.12145", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12145", "abs": "https://arxiv.org/abs/2507.12145", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "title": "PRISM: Distributed Inference for Foundation Models at Edge", "comment": null, "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "AI": {"tldr": "PRISM is a communication-efficient and compute-aware strategy for deploying foundation models on edge devices, reducing communication and computation overhead with minimal accuracy loss.", "motivation": "Foundation models (FMs) face challenges in edge deployment due to high communication and computation demands. PRISM addresses these issues for practical edge use.", "method": "PRISM uses Segment Means for feature approximation, restructures self-attention to avoid redundant computations, and introduces partition-aware causal masking for autoregressive models.", "result": "PRISM reduces communication overhead by up to 99.2% and computation by 51.24% for BERT, with minor accuracy degradation.", "conclusion": "PRISM provides a scalable and efficient solution for deploying foundation models in resource-constrained edge environments."}}
{"id": "2507.11847", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11847", "abs": "https://arxiv.org/abs/2507.11847", "authors": ["Yu-Jie Zhang", "Sheng-An Xu", "Peng Zhao", "Masashi Sugiyama"], "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "comment": null, "summary": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "AI": {"tldr": "A jointly efficient algorithm for generalized linear bandits (GLB) achieves near-optimal regret with low computational cost.", "motivation": "GLBs model diverse reward distributions but face trade-offs between computational and statistical efficiency. Existing methods compromise one for the other.", "method": "Proposes an algorithm using a tight confidence set for online mirror descent (OMD) estimator, leveraging mix loss analysis for efficiency.", "result": "The method achieves nearly optimal regret with constant-time updates, matching statistical efficiency of maximum likelihood estimation.", "conclusion": "The algorithm balances computational and statistical efficiency, offering a practical solution for GLB problems."}}
{"id": "2507.12196", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12196", "abs": "https://arxiv.org/abs/2507.12196", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "title": "Selective Quantization Tuning for ONNX Models", "comment": "5 pages, 3 figures, 2 tables", "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "AI": {"tldr": "TuneQn is a suite for selective quantization of ONNX models, optimizing performance and size across hardware, reducing accuracy loss by up to 54.14% and model size by 72.9%.", "motivation": "Fully quantized models often suffer from accuracy loss and deployment challenges on low-end hardware, necessitating selective quantization.", "method": "TuneQn selectively quantizes ONNX models, deploys them on CPUs/GPUs, profiles performance, and uses multi-objective optimization to identify optimal models.", "result": "TuneQn achieved up to 54.14% less accuracy loss and 72.9% model size reduction compared to fully quantized and original models, respectively.", "conclusion": "TuneQn effectively balances accuracy and model size through selective quantization, making it practical for deployment on diverse hardware."}}
{"id": "2507.11855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11855", "abs": "https://arxiv.org/abs/2507.11855", "authors": ["Davin Hill", "Brian L. Hill", "Aria Masoomi", "Vijay S. Nori", "Robert E. Tillman", "Jennifer Dy"], "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "comment": null, "summary": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "AI": {"tldr": "OrdShap is a novel attribution method that disentangles feature value and position effects in sequential deep learning models, providing deeper insights into model behavior.", "motivation": "Existing feature attribution methods assume fixed feature ordering, conflating feature values and their positions, limiting interpretability.", "method": "Introduces OrdShap, which quantifies how predictions change with feature position permutations, leveraging game-theoretic principles.", "result": "Empirical results show OrdShap effectively captures feature value and position attributions across health, NLP, and synthetic datasets.", "conclusion": "OrdShap offers a theoretically grounded and practical solution for position-sensitive attribution in sequential models."}}
{"id": "2507.12262", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12262", "abs": "https://arxiv.org/abs/2507.12262", "authors": ["Zachary James", "Joseph Guinness"], "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "comment": null, "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "AI": {"tldr": "A framework using nonstationary kernels with neural network-parameterized kernels improves Gaussian process regression, outperforming stationary and hierarchical models in accuracy and log-score.", "motivation": "Stationary kernels in Gaussian processes limit expressiveness and may not suit many datasets. Nonstationary kernels, parameterized by neural networks, offer flexibility and better performance.", "method": "Proposes a framework where nonstationary kernel parameters vary across feature space, modeled by a neural network. Joint training of the neural network and Gaussian process is done using the chain rule for derivatives.", "result": "Outperforms stationary and hierarchical models in accuracy and log-score on machine learning datasets. Demonstrates ability to recover nonstationary parameters in spatial datasets.", "conclusion": "The method is flexible, scalable, and adaptable to various nonstationary kernels, providing improved performance and interpretability."}}
{"id": "2507.11865", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11865", "abs": "https://arxiv.org/abs/2507.11865", "authors": ["Hanwen Dai", "Chang Gao", "Fang He", "Congyuan Ji", "Yanni Yang"], "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "comment": null, "summary": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "AI": {"tldr": "The paper proposes a policy-improved deep deterministic policy gradient (pi-DDPG) framework to dynamically manage drivers' acceptance of Discount Express services, addressing challenges like high stochasticity and limited historical data.", "motivation": "To mitigate market fragmentation and improve matching efficiency while managing reduced profit margins for ride-hailing platforms.", "method": "Develops a pi-DDPG framework with a refiner module, convolutional LSTM, and prioritized experience replay to optimize driver acceptance behavior.", "result": "Numerical experiments show pi-DDPG improves learning efficiency and reduces early-stage training losses.", "conclusion": "The pi-DDPG framework effectively manages driver acceptance behavior, enhancing platform performance under uncertain conditions."}}
{"id": "2507.12305", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12305", "abs": "https://arxiv.org/abs/2507.12305", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "comment": "ICCV 2025", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "AI": {"tldr": "A novel prompt-based method for online continual learning (OCL) is proposed, addressing data privacy and catastrophic forgetting. It outperforms SOTAs with fewer parameters and moderate efficiency.", "motivation": "Data privacy constraints in OCL complicate catastrophic forgetting. Existing methods either replay data (violating privacy) or use growing parameters (reducing throughput).", "method": "The method includes: (1) a lightweight prompt generator, (2) trainable scaler-shifter, (3) PTM generalization preservation, and (4) hard-soft updates.", "result": "Achieves higher performance on CIFAR100, ImageNet-R, ImageNet-A, and CUB datasets with fewer parameters and moderate efficiency.", "conclusion": "The proposed method balances performance and efficiency, offering a practical solution for OCL. Code is available on GitHub."}}
{"id": "2507.11901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11901", "abs": "https://arxiv.org/abs/2507.11901", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Imbalanced Regression Pipeline Recommendation", "comment": null, "summary": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "AI": {"tldr": "Meta-IR is a meta-learning framework for imbalanced regression, recommending optimal resampling and learning model pipelines per task, outperforming AutoML and baselines.", "motivation": "Addressing the challenge of imbalanced regression tasks, where rare target values complicate model performance, by avoiding exhaustive testing of resampling and learning model combinations.", "method": "Proposes Meta-IR with Independent and Chained formulations. Meta-classifiers map meta-features to best pipelines, with Chained showing superior performance by modeling relationships between resampling and learning models.", "result": "Meta-IR outperformed AutoML and 42 baseline configurations, with the Chained formulation proving more effective.", "conclusion": "Meta-IR provides an efficient, zero-shot solution for imbalanced regression, leveraging meta-learning to recommend optimal pipelines without extensive testing."}}
{"id": "2507.12314", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12314", "abs": "https://arxiv.org/abs/2507.12314", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Zhenfang Liu", "Qing Sheng", "Jie Xiao", "Jungang Lou"], "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "comment": null, "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "AI": {"tldr": "The paper introduces Thought Purity (TP), a defense mechanism against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs), addressing security vulnerabilities while maintaining performance.", "motivation": "LRMs like Deepseek-R1 are vulnerable to adversarial attacks (e.g., CoTA) that exploit Chain-of-Thought (CoT) processes, compromising reasoning and task performance.", "method": "Proposes TP with three components: safety-optimized data processing, reinforcement learning-enhanced rules, and adaptive monitoring metrics.", "result": "TP effectively defends against CoTA, balancing security and functionality in LRMs.", "conclusion": "TP is the first comprehensive defense against CoTA, enhancing security in next-gen AI architectures."}}
{"id": "2507.11902", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11902", "abs": "https://arxiv.org/abs/2507.11902", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "comment": null, "summary": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "AI": {"tldr": "The paper conducts an experimental study on imbalanced regression tasks, proposing a taxonomy and evaluating balancing and predictive models with user-focused metrics.", "motivation": "Imbalanced data is common in real-world scenarios, but most research focuses on classification, leaving regression tasks understudied.", "method": "The study involves extensive experiments with various balancing and predictive models, using tailored metrics for evaluation. A taxonomy for imbalanced regression is proposed based on regression model, learning process, and evaluation metrics.", "result": "The study provides insights into the effectiveness of balancing strategies in regression, highlighting their benefits for model learning.", "conclusion": "The work advances understanding of imbalanced regression, offering practical guidance and directions for future research."}}
{"id": "2507.12412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12412", "abs": "https://arxiv.org/abs/2507.12412", "authors": ["Dzung Dinh", "Boqi Chen", "Marc Niethammer", "Junier Oliva"], "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "comment": null, "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "AI": {"tldr": "NOCTA is a method for cost-effective feature acquisition in dynamic prediction tasks, outperforming baselines with its non-greedy approach.", "motivation": "Resource constraints in critical applications like healthcare require efficient feature acquisition, balancing information gain with cost and risk.", "method": "NOCTA includes two estimators: NOCTA-NP (non-parametric, nearest neighbors) and NOCTA-P (parametric, utility prediction).", "result": "NOCTA variants outperform existing baselines on synthetic and real-world medical datasets.", "conclusion": "NOCTA effectively addresses the challenge of dynamic feature acquisition with cost-awareness."}}
{"id": "2507.11926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11926", "abs": "https://arxiv.org/abs/2507.11926", "authors": ["Max Hopkins", "Sihan Liu", "Christopher Ye", "Yuichi Yoshida"], "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "comment": "67 pages", "summary": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "AI": {"tldr": "The paper addresses the challenge of replicable learning in reinforcement learning (RL), showing that exploration in RL is not inherently more expensive than batch learning. It presents an algorithm with near-optimal sample efficiency.", "motivation": "The motivation stems from the replicability crisis in empirical science and machine learning, particularly the gap in understanding replicable learning in control settings like RL compared to batch settings.", "method": "The authors develop a replicable RL algorithm for low-horizon tabular MDPs, achieving sample efficiency of $\tilde{O}(S^2A)$. They also provide matching lower bounds to demonstrate near-optimality.", "result": "The main result is a replicable RL algorithm requiring $\tilde{O}(S^2A)$ samples, bridging the gap between generative and episodic settings. Lower bounds confirm the algorithm's near-optimality.", "conclusion": "The work resolves the question of whether replicable exploration is inherently more expensive, showing it is not, and provides a near-optimal solution for replicable RL."}}
{"id": "2507.12419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12419", "abs": "https://arxiv.org/abs/2507.12419", "authors": ["Andrea Perin", "Giacomo Lagomarsini", "Claudio Gallicchio", "Giuseppe Nuti"], "title": "Mixture of Raytraced Experts", "comment": "Preliminary version (pre-submission)", "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "AI": {"tldr": "A Mixture of Raytraced Experts (MoE) architecture dynamically selects expert sequences, enabling variable computation depth and width, improving accuracy and reducing training epochs by 10-40%.", "motivation": "Existing MoE architectures use fixed computation per sample, limiting flexibility and efficiency. This work aims to enhance accuracy and training speed by dynamically selecting expert sequences.", "method": "The model trains by iteratively sampling experts, unfolding sequences like Recurrent Neural Networks, eliminating the need for load-balancing mechanisms.", "result": "Preliminary experiments show 10-40% fewer training epochs with comparable/higher accuracy.", "conclusion": "The approach opens new research directions for faster, more expressive MoE models, with code available for further exploration."}}
{"id": "2507.11928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11928", "abs": "https://arxiv.org/abs/2507.11928", "authors": ["Abhishek Sriram", "Neal Tuffy"], "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "comment": "This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)", "summary": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "AI": {"tldr": "A machine learning framework reduces RF power amplifier simulation needs by 65% while maintaining high accuracy, using MaxMin Latin Hypercube Sampling and CatBoost.", "motivation": "To minimize simulation time and resources in RF power amplifier design without sacrificing accuracy.", "method": "Combines MaxMin Latin Hypercube Sampling with CatBoost to strategically select 35% of critical simulation points, predicting performance across the design space.", "result": "Achieves 65% simulation reduction with \u00b10.3 to \u00b10.4 dBm accuracy, 0.901 average R\u00b2, and 58.24% to 77.78% time savings.", "conclusion": "The framework enables rapid, accurate RF power amplifier design iterations, meeting production standards efficiently."}}
{"id": "2507.12002", "categories": ["cs.LG", "I.2.0; J.4"], "pdf": "https://arxiv.org/pdf/2507.12002", "abs": "https://arxiv.org/abs/2507.12002", "authors": ["Alice Zhang", "Callihan Bertley", "Dawei Liang", "Edison Thomaz"], "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "comment": null, "summary": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "AI": {"tldr": "A computational approach using smartwatch data (audio and inertial) to detect in-person conversations, achieving 82.0% and 77.2% F1-scores in lab and semi-naturalistic settings.", "motivation": "Social interactions are vital for human behavior, and detecting conversations is foundational. Current methods struggle in acoustically-challenging scenarios.", "method": "Leveraged audio and inertial data from smartwatches, tested with 11 lab and 24 semi-naturalistic participants. Evaluated ML/DL models with 3 fusion methods.", "result": "Achieved 82.0% (lab) and 77.2% (semi-naturalistic) macro F1-scores, showing benefits of multimodal data fusion.", "conclusion": "Multimodal sensing (audio + inertial) improves conversation detection, especially in challenging environments."}}
{"id": "2507.12041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12041", "abs": "https://arxiv.org/abs/2507.12041", "authors": ["Anmol Kagrecha", "Henrik Marklund", "Potsawee Manakul", "Richard Zeckhauser", "Benjamin Van Roy"], "title": "Granular feedback merits sophisticated aggregation", "comment": "31 pages, 8 figures", "summary": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "AI": {"tldr": "The paper explores how feedback granularity affects the accuracy of predicting population feedback distributions, showing sophisticated methods outperform regularized averaging for granular feedback.", "motivation": "To improve predictions of population feedback distributions when limited by small sample sizes, especially as feedback granularity increases.", "method": "Compare regularized averaging with more sophisticated methods for combining feedback, testing on binary and five-point feedback scales.", "result": "Sophisticated methods significantly outperform regularized averaging with granular feedback (e.g., five-point scale), requiring fewer individuals for the same accuracy.", "conclusion": "Feedback granularity determines the effectiveness of sophisticated methods, with greater granularity enabling substantial improvements over simple averaging."}}
{"id": "2507.12043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12043", "abs": "https://arxiv.org/abs/2507.12043", "authors": ["Wen Wen", "Tieliang Gong", "Yunjiao Zhang", "Zeyu Gao", "Weizhan Zhang", "Yong-Jin Liu"], "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "comment": null, "summary": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "AI": {"tldr": "The paper presents a theoretical framework for replay-based continual learning, deriving information-theoretic bounds to explain generalization behavior and mitigate catastrophic forgetting.", "motivation": "To address the limited theoretical understanding of generalization in replay-based continual learning methods, despite their empirical success.", "method": "Establishes a unified theoretical framework with information-theoretic bounds, analyzing the interaction between memory buffers and current tasks. Uses stochastic gradient Langevin dynamics (SGLD) as an example.", "result": "Derived bounds show that limited exemplars of previous tasks improve generalization and mitigate forgetting. Prediction-based bounds offer tighter, tractable upper bounds.", "conclusion": "The framework effectively captures generalization dynamics in replay-based continual learning, validated by experiments."}}
{"id": "2507.12053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12053", "abs": "https://arxiv.org/abs/2507.12053", "authors": ["Seanglidet Yean", "Jiazu Zhou", "Bu-Sung Lee", "Markus Schl\u00e4pfer"], "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "comment": "International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)", "summary": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "AI": {"tldr": "A data-driven method using cGANs to generate urban mobility flows, incorporating dynamic factors like land use and population density, outperforming traditional models.", "motivation": "Urban planners need accurate mobility simulations for sustainable development, but existing models lack adaptability to evolving urban factors.", "method": "Uses conditional generative adversarial networks (cGANs) to blend historical data with dynamic parameters like land use and region sizes.", "result": "Demonstrated effectiveness with Singapore mobile phone data, showing superior performance over static or historical-data-dependent models.", "conclusion": "The approach offers a scalable, adaptable solution for urban mobility simulation without extensive calibration, aiding future urban planning."}}
{"id": "2507.12070", "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.12070", "abs": "https://arxiv.org/abs/2507.12070", "authors": ["George Bird"], "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "comment": "36 pages, 31 figures", "summary": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "AI": {"tldr": "A novel method for assessing representational alignment reveals that algebraic symmetries in network primitives predict task-agnostic structure. Activation functions' algebraic properties influence whether representations discretize or remain continuous, impacting interpretability and performance.", "motivation": "To understand how functional form choices in neural networks introduce unintended inductive biases, leading to task-independent structural artifacts in representations.", "method": "Developed a tool based on Spotlight Resonance to analyze representational alignment, focusing on activation functions' algebraic symmetries through an ablation study.", "result": "Discrete activation functions lead to discretized representations, while continuous ones maintain continuity. Quantization correlates with increased reconstruction error.", "conclusion": "Functional form choices causally influence representation structure, offering insights into emergent interpretability and potential performance trade-offs."}}
{"id": "2507.12094", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12094", "abs": "https://arxiv.org/abs/2507.12094", "authors": ["Yiding Feng", "Wei Tang"], "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "comment": null, "summary": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "AI": {"tldr": "The paper introduces the 'informativeness gap' to compare miscalibrated predictors, generalizing existing notions like U-Calibration and Blackwell informativeness. It provides a dual characterization and a relaxed EMD-based measure, proving its soundness, completeness, and sample efficiency.", "motivation": "Decision-makers often face multiple miscalibrated models; the goal is to determine which is more useful for downstream tasks.", "method": "Introduces the informativeness gap, a framework comparing predictors, and a dual characterization leading to a relaxed EMD-based measure.", "result": "The measure is sound, complete, and sample-efficient, with novel combinatorial insights for calibrated predictors.", "conclusion": "The informativeness gap generalizes prior work and offers a practical, theoretically grounded tool for comparing predictors."}}
{"id": "2507.12127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12127", "abs": "https://arxiv.org/abs/2507.12127", "authors": ["Ngoc Duy Pham", "Thusitha Dayaratne", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "comment": null, "summary": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "AI": {"tldr": "The paper proposes a semi-supervised Federated Learning (FL) approach for spectrum sensing (FLSS) to address labeled data scarcity and introduces a novel defense against data poisoning attacks, achieving high accuracy and robustness.", "motivation": "The rapid growth of wireless devices exacerbates spectrum scarcity, and centralized ML-based DSA systems face privacy and regulatory challenges. FL offers a promising alternative but lacks labeled data and is vulnerable to attacks.", "method": "A semi-supervised FL approach combined with energy detection trains models on unlabeled data. A novel defense mechanism, inspired by vaccination, counters data poisoning attacks without majority-based assumptions.", "result": "FLSS achieves near-perfect accuracy on unlabeled datasets and maintains robustness against data poisoning attacks, even with malicious participants.", "conclusion": "The proposed FLSS approach effectively addresses labeled data scarcity and security vulnerabilities, making it a viable solution for dynamic spectrum allocation."}}
{"id": "2507.12133", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12133", "abs": "https://arxiv.org/abs/2507.12133", "authors": ["Hanwen Liu", "Yuhe Huang", "Yifeng Gong", "Yanjie Zhai", "Jiaxuan Lu"], "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "comment": null, "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "AI": {"tldr": "HyDRA is a hybrid RF architecture combining VMD, CNNs, Transformers, and Mamba for secure wireless device recognition, excelling in both closed-set and open-set classification with real-time efficiency.", "motivation": "Enhancing wireless security through non-cryptographic RFFI by leveraging hardware-induced signal distortions for device recognition.", "method": "Integrates optimized VMD for preprocessing and a novel architecture fusing CNNs, Transformers, and Mamba for classification, supporting both closed-set and open-set tasks.", "result": "Achieves SOTA accuracy in closed-set scenarios and robust open-set performance, with millisecond-level inference on NVIDIA Jetson Xavier NX.", "conclusion": "HyDRA provides a practical, efficient solution for real-time wireless authentication in diverse environments."}}
{"id": "2507.12142", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "RiemannLoRA improves LoRA by treating LoRA matrices as a smooth manifold, addressing overparametrization and initialization challenges, enhancing convergence and performance.", "motivation": "Challenges in LoRA include optimal initialization and overparametrization in low-rank matrix factorization, which hinder efficiency and performance.", "method": "Proposes RiemannLoRA, treating LoRA matrices as a smooth manifold to remove overparametrization and determine optimal initialization via fastest loss decrease direction.", "result": "RiemannLoRA outperforms standard LoRA and its variants in convergence speed and final performance on LLMs and diffusion models.", "conclusion": "RiemannLoRA provides a unified, efficient solution to LoRA's challenges, improving both training dynamics and model performance."}}
{"id": "2507.12144", "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "pdf": "https://arxiv.org/pdf/2507.12144", "abs": "https://arxiv.org/abs/2507.12144", "authors": ["Boris Bonev", "Thorsten Kurth", "Ankur Mahesh", "Mauro Bisson", "Jean Kossaifi", "Karthik Kashinath", "Anima Anandkumar", "William D. Collins", "Michael S. Pritchard", "Alexander Keller"], "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "comment": null, "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "AI": {"tldr": "FourCastNet 3 improves global weather forecasting using a scalable ML approach, outperforming conventional models in accuracy and speed while maintaining realistic dynamics and probabilistic calibration.", "motivation": "To advance weather modeling by addressing the need for scalable, geometrically accurate, and probabilistically calibrated ensemble forecasting.", "method": "Uses a convolutional neural network tailored for spherical geometry, with a novel training paradigm for large-scale efficiency.", "result": "Achieves faster forecasts (8-60x speedup), superior accuracy, and realistic spectra even at 60-day lead times.", "conclusion": "FourCastNet 3 is a promising tool for meteorological forecasting and early warning systems due to its efficiency and performance."}}
{"id": "2507.12165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12165", "abs": "https://arxiv.org/abs/2507.12165", "authors": ["Fouad Oubari", "Mohamed El-Baha", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Multi-Component VAE with Gaussian Markov Random Field", "comment": null, "summary": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "AI": {"tldr": "The paper introduces GMRF MCVAE, a generative model using Gaussian Markov Random Fields to improve structural coherence in multi-component datasets.", "motivation": "Current methods for multi-component generative modeling oversimplify dependencies, compromising structural coherence.", "method": "The GMRF MCVAE embeds Gaussian Markov Random Fields into prior and posterior distributions to model cross-component relationships.", "result": "The model achieves state-of-the-art performance on synthetic and real-world datasets, enhancing structural coherence.", "conclusion": "GMRF MCVAE is effective for applications requiring robust modeling of multi-component coherence."}}
{"id": "2507.12166", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12166", "abs": "https://arxiv.org/abs/2507.12166", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Junting Chen", "Zezhong Zhang", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "comment": null, "summary": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "AI": {"tldr": "UrbanRadio3D introduces a large-scale 3D radio map dataset with pathloss, DoA, and ToA metrics, addressing limitations of 2D methods. A 3D UNet and diffusion-based RadioDiff-3D framework are proposed, achieving superior performance in high-dimensional radio map construction.", "motivation": "Existing radio map methods focus on 2D pathloss prediction, neglecting key parameters like DoA, ToA, and vertical variations due to static learning paradigms. This limits generalization beyond training data.", "method": "UrbanRadio3D is created via ray tracing in urban environments, offering a 3D dataset with pathloss, DoA, and ToA. A 3D UNet and RadioDiff-3D (a diffusion-model-based framework) are proposed for 3D radio map construction.", "result": "RadioDiff-3D outperforms existing methods, constructing rich, high-dimensional radio maps under diverse conditions. UrbanRadio3D is significantly larger and more detailed than prior datasets.", "conclusion": "UrbanRadio3D and RadioDiff-3D provide a foundational dataset and benchmark for 3D environment-aware communication research, advancing beyond 2D limitations."}}
{"id": "2507.12192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12192", "abs": "https://arxiv.org/abs/2507.12192", "authors": ["Victor F. Lopes de Souza", "Karima Bakhti", "Sofiane Ramdani", "Denis Mottet", "Abdelhak Imoussaten"], "title": "Explainable Evidential Clustering", "comment": null, "summary": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "AI": {"tldr": "The paper addresses the challenge of explaining evidential clustering results, introducing representativity as a key condition for decision trees to serve as explainers. It proposes the IEMM algorithm for interpretable explanations, validated with high satisfaction rates.", "motivation": "Traditional methods struggle with imperfect real-world data. Evidential clustering handles uncertainty but lacks explainability, which is critical in high-stakes domains like healthcare.", "method": "The paper generalizes representativity for partial labeling using utility functions, defines evidential mistakeness, and introduces the IEMM algorithm for cautious decision tree explanations.", "result": "The IEMM algorithm achieved up to 93% satisfaction in providing interpretable explanations for evidential clustering.", "conclusion": "The work successfully bridges the gap in explaining evidential clustering, offering practical and interpretable solutions for real-world applications."}}
{"id": "2507.12218", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.12218", "abs": "https://arxiv.org/abs/2507.12218", "authors": ["Tomohisa Okazaki"], "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "comment": null, "summary": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "AI": {"tldr": "The paper introduces a physics-informed linear model (PILM) for solving PDEs and estimating coefficients or boundary conditions using linear combinations of basis functions, validated on forward and inverse problems and applied to crustal strain rate estimation.", "motivation": "To address the challenge of solving PDEs and estimating their parameters from data, leveraging linear models for analytical solutions and comparing physical vs. mathematical regularization.", "method": "Developed PILM, a linear model using basis functions to represent PDE solutions, tested on forward/inverse problems and applied to geodetic data for strain rate estimation.", "result": "PILM provided analytical solutions for linear problems, with mathematical regularization outperforming physical regularization in Bayesian analysis.", "conclusion": "PILM offers a solvable framework for linear PDE problems, underdetermined systems, and regularization, with mathematical regularization being more effective."}}
{"id": "2507.12224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12224", "abs": "https://arxiv.org/abs/2507.12224", "authors": ["Razvan Pascanu", "Clare Lyle", "Ionut-Vlad Modoranu", "Naima Elosegui Borras", "Dan Alistarh", "Petar Velickovic", "Sarath Chandar", "Soham De", "James Martens"], "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "comment": null, "summary": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "AI": {"tldr": "The paper argues that optimizers in DNNs influence not just convergence rates but also the qualitative properties of learned solutions, advocating for a shift in focus to understand and design optimizers for specific inductive biases.", "motivation": "Early skepticism about DNNs due to nonlinearity and lack of global convergence guarantees has been disproven by empirical success. However, the community's focus on convex optimization as a mental model overlooks the optimizer's role in shaping solution properties.", "method": "The paper critiques the current focus on training efficiency (e.g., iteration, FLOPs, time) and proposes a shift toward understanding and designing optimizers to encode specific inductive biases and desired solution properties.", "result": "The paper highlights that optimizers can change the effective expressivity of models and encode desiderata in learning, suggesting a need for deeper understanding and intentional design.", "conclusion": "The community should prioritize understanding optimizer biases and designing optimizers to induce specific solution properties, recognizing their critical role alongside architecture and data in shaping model outcomes."}}
{"id": "2507.12257", "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2507.12257", "abs": "https://arxiv.org/abs/2507.12257", "authors": ["Matteo Tusoni", "Giuseppe Masi", "Andrea Coletta", "Aldo Glielmo", "Viviana Arrigoni", "Novella Bartolini"], "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "comment": null, "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "AI": {"tldr": "A robust causal discovery method leveraging power-law spectral features to improve accuracy in noisy time series data.", "motivation": "Existing causal discovery methods are sensitive to noise, leading to unreliable inferences in real-world applications.", "method": "Extracts power-law spectral features from time series to amplify genuine causal signals.", "result": "Outperforms state-of-the-art methods on synthetic and real-world datasets.", "conclusion": "The proposed method is robust and practically relevant for causal discovery in noisy time series."}}
{"id": "2507.12297", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "AI": {"tldr": "RegCL is a continual learning framework for SAM, merging domain-specific modules to avoid catastrophic forgetting and maintain efficiency.", "motivation": "Address performance limitations and scalability issues of SAM in multi-domain adaptation.", "method": "Uses model merging (e.g., LoRA modules) with weight optimization to integrate multi-domain knowledge without replay.", "result": "Achieves efficient continual learning across multiple datasets without increasing model size or storing historical data.", "conclusion": "RegCL effectively integrates multi-domain knowledge while maintaining scalability and efficiency."}}
{"id": "2507.12341", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12380", "abs": "https://arxiv.org/abs/2507.12380", "authors": ["Maximilian Krahn", "Vikas Garg"], "title": "Heat Kernel Goes Topological", "comment": null, "summary": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "AI": {"tldr": "A novel topological framework using Laplacian operators on combinatorial complexes (CCs) for efficient computation of heat kernels, outperforming existing methods in efficiency and expressiveness.", "motivation": "To address the computational expense of higher-order message passing in topological neural networks by introducing an efficient and expressive method.", "method": "Introduces a Laplacian operator on combinatorial complexes (CCs) to compute heat kernels as node descriptors, enabling multiscale information capture and permutation-equivariant representations.", "result": "The method is theoretically maximally expressive, outperforms existing topological methods in efficiency, and shows competitive performance on molecular datasets.", "conclusion": "Advances topological deep learning with scalable and expressive representations, opening new possibilities for molecular tasks."}}
{"id": "2507.12383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12383", "abs": "https://arxiv.org/abs/2507.12383", "authors": ["Mohit Prashant", "Arvind Easwaran"], "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "comment": "Preprint", "summary": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "AI": {"tldr": "The paper derives sharper PAC bounds for RL in infinite-horizon MDPs, reducing sample complexity by a logarithmic factor to O(SA log A) timesteps.", "motivation": "Existing PAC bounds for RL in MDPs are suboptimal. The study aims to exploit state transition dependencies to improve sample complexity.", "method": "Approximates the original MDP using smaller MDPs with subsets of the state-space, extending results to a model-free setting with a PAC-MDP algorithm.", "result": "Achieves a logarithmic reduction in sample complexity (O(SA log A)) and demonstrates significant improvement over prior work experimentally.", "conclusion": "The proposed method provides sharper PAC bounds and practical efficiency, validated by experimental comparisons."}}
{"id": "2507.12384", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12384", "abs": "https://arxiv.org/abs/2507.12384", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "comment": null, "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "AI": {"tldr": "A novel hardware-software co-design using $MoS_2$ Flash-based analog CAM with soft boundaries improves robustness and accuracy for tree-based models, addressing interpretability and efficiency challenges.", "motivation": "Addressing concerns about AI trustworthiness, particularly interpretability and robustness, while overcoming computational inefficiencies and device variation vulnerabilities in tree-based models.", "method": "A hardware-software co-design approach leveraging $MoS_2$ Flash-based analog CAM with soft boundaries for efficient inference with soft tree-based models.", "result": "Achieves 96% accuracy on WDBC and minimal accuracy drop (0.6%) on MNIST under device variation, outperforming traditional methods (45.3% drop).", "conclusion": "The approach enhances AI trustworthiness and efficiency, paving the way for specialized hardware solutions."}}
{"id": "2507.12399", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12399", "abs": "https://arxiv.org/abs/2507.12399", "authors": ["Florian E. Dorner", "Yatong Chen", "Andr\u00e9 F. Cruz", "Fanny Yang"], "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "comment": "35 pages, 9 Figures", "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "AI": {"tldr": "The paper analyzes how verifier imperfection impacts test-time scaling methods like Best-of-N (BoN) and rejection sampling, linking performance to the verifier's ROC curve geometry.", "motivation": "To bridge the gap in theoretical understanding of how verifier quality affects test-time scaling techniques.", "method": "Theoretical analysis of BoN and rejection sampling, relating their accuracy to the verifier's ROC curve geometry, supported by experiments on GSM8K using Llama and Qwen models.", "result": "Rejection sampling outperforms BoN for fixed compute, but both converge to the same accuracy in the infinite-compute limit, determined by the ROC curve's slope near the origin.", "conclusion": "Verifier imperfection's impact on test-time scaling is precisely characterized by ROC curve geometry, with practical implications for method selection."}}
{"id": "2507.12435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12435", "abs": "https://arxiv.org/abs/2507.12435", "authors": ["Yi Li", "David Mccoy", "Nolan Gunter", "Kaitlyn Lee", "Alejandro Schuler", "Mark van der Laan"], "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "comment": null, "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "AI": {"tldr": "Targeted Deep Architectures (TDA) embeds TMLE into neural networks for unbiased causal inference, improving bias reduction and coverage for multi-parameter targets.", "motivation": "Existing neural implementations lack guarantees for solving efficient influence function equations or are computationally expensive for multi-parameter settings.", "method": "TDA partitions model parameters, freezing most and iteratively updating a small subset along a targeting gradient derived from influence functions.", "result": "TDA reduces bias and improves coverage on benchmark datasets compared to standard neural-network estimators and prior post-hoc methods.", "conclusion": "TDA provides a scalable, direct pathway for rigorous causal inference in deep architectures for complex multi-parameter targets."}}
{"id": "2507.12439", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12439", "abs": "https://arxiv.org/abs/2507.12439", "authors": ["Daniel Commey", "Rebecca A. Sarpong", "Griffith S. Klogo", "Winful Bagyl-Bac", "Garth V. Crosby"], "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "AI": {"tldr": "A proactive, economic defense for federated learning using a Bayesian incentive mechanism to deter data-poisoning attacks, ensuring robustness and high accuracy.", "motivation": "Federated learning's open-participation nature makes it vulnerable to data-poisoning attacks, and existing reactive defenses are computationally expensive and assume an honest majority.", "method": "Introduces a lightweight Bayesian incentive mechanism where the server verifies updates using a private validation dataset before issuing payments, ensuring IR and IC.", "result": "Maintains 96.7% accuracy with 50% label-flipping adversaries on MNIST, outperforming FedAvg by 51.7 percentage points.", "conclusion": "The mechanism is computationally efficient, budget-bounded, and integrates easily into existing FL frameworks, providing a practical solution for robust FL ecosystems."}}
{"id": "2507.12453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12453", "abs": "https://arxiv.org/abs/2507.12453", "authors": ["Qian Xie", "Linda Cai", "Alexander Terenin", "Peter I. Frazier", "Ziv Scully"], "title": "Cost-aware Stopping for Bayesian Optimization", "comment": null, "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "AI": {"tldr": "A cost-aware stopping rule for Bayesian optimization is proposed, ensuring efficient evaluation without excessive costs, backed by theoretical guarantees and outperforming other methods in experiments.", "motivation": "Addressing the lack of guarantees in existing adaptive stopping rules for Bayesian optimization to prevent excessive evaluation costs.", "method": "Proposes a cost-aware stopping rule adapting to varying costs, grounded in theoretical connections to state-of-the-art cost-aware acquisition functions (PBGI and log expected improvement per cost).", "result": "Theoretical guarantees bound cumulative evaluation costs, and experiments show superior performance in cost-adjusted simple regret.", "conclusion": "The proposed stopping rule, especially with PBGI, effectively balances solution quality and evaluation costs, outperforming alternatives."}}
