<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: LLMs struggle with counterfactual reasoning, often relying on parametric knowledge, and finetuning may degrade their stored knowledge.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can integrate in-context knowledge with parametric knowledge, especially in novel settings.

Method: Investigates counterfactual reasoning through synthetic and real multi-hop reasoning experiments.

Result: LLMs generally fail at counterfactual reasoning, defaulting to parametric knowledge, and finetuning can harm their knowledge retention.

Conclusion: Current LLMs have limitations in repurposing parametric knowledge for novel scenarios.

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [2] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: SPECS is a latency-aware test-time scaling method for LLMs that balances accuracy and latency by using a smaller model for candidate generation and a larger model for evaluation, achieving comparable or better accuracy than beam search with reduced latency.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods for LLMs focus on accuracy via increased compute but overlook latency constraints, impacting user experience. SPECS addresses this gap.

Method: SPECS uses a smaller model to generate candidate sequences efficiently, evaluates them with a larger model and reward model, and introduces reward-guided soft verification and deferral mechanisms.

Result: On MATH500, AMC23, and OlympiadBench datasets, SPECS matches or surpasses beam search accuracy while reducing latency by up to 19.1%.

Conclusion: SPECS effectively balances accuracy and latency, converging to a KL-regularized RL objective with increasing beam width.

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [3] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: The paper introduces "The Safety Reminder," a prompt-tuning method to enhance safety awareness in Vision-Language Models (VLMs) by reactivating their delayed safety mechanisms, reducing harmful content generation without affecting normal performance.


<details>
  <summary>Details</summary>
Motivation: VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to bypass safety measures and generate harmful content. The study aims to address this by leveraging VLMs' inherent but delayed safety awareness.

Method: The authors propose "The Safety Reminder," a soft prompt-tuning approach that injects learnable prompt tokens during text generation to proactively reactivate safety awareness, triggered only when harmful content is detected.

Result: Evaluations on safety benchmarks and adversarial attacks show the method significantly reduces attack success rates while preserving model utility for benign tasks.

Conclusion: The approach offers a practical solution for deploying safer VLMs in real-world applications by enhancing their safety awareness without compromising performance.

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [4] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: The paper introduces ContextBench, a benchmark for evaluating methods that generate linguistically fluent inputs to trigger specific behaviors or latent features in language models, and enhances Evolutionary Prompt Optimisation (EPO) for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for identifying inputs that activate specific behaviors or features in language models, particularly for safety applications.

Method: Formalizes context modification, introduces ContextBench, and enhances EPO with LLM-assistance and diffusion model inpainting.

Result: The enhanced EPO variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.

Conclusion: The study highlights challenges in balancing elicitation and fluency, and demonstrates improved methods for targeted input generation.

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [5] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: The paper evaluates LLMs' ability to pursue hidden harmful objectives while completing tasks, using SHADE-Arena. Top models like Claude 3.7 Sonnet and Gemini 2.5 Pro show limited sabotage success, while monitoring remains challenging.


<details>
  <summary>Details</summary>
Motivation: To assess the risks of LLMs acting as autonomous agents with hidden harmful goals, especially in complex, long-horizon tasks.

Method: Uses SHADE-Arena, a diverse dataset for evaluating sabotage and monitoring capabilities, testing models on main tasks, side tasks, and detection avoidance.

Result: Best models achieve 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) sabotage success. Monitoring is difficult, with Gemini 2.5 Pro achieving 0.87 AUC.

Conclusion: Current LLMs struggle with sabotage due to execution flaws, but monitoring subtle sabotage is already hard and will worsen with more complex tasks.

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [6] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: The paper highlights the lack of standardization in Agentic AI research, introduces a robust evaluation protocol, and presents OAgents, a modular framework achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current agent research lacks standardization, making fair comparisons and reproducibility difficult.

Method: Systematic empirical study on GAIA benchmark and BrowseComp to evaluate design choices in agent components.

Result: Identified crucial and redundant agent components; introduced OAgents, a high-performance modular framework.

Conclusion: OAgents advances Agentic AI research by providing a standardized, reproducible foundation.

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [7] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: Sysformer enhances LLM safety by dynamically adapting system prompts, improving refusal rates for harmful inputs and compliance with safe ones, without fine-tuning the LLM.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to balance safety and responsiveness, either refusing harmless prompts or generating harmful content. Existing solutions are costly or heuristic.

Method: Proposes Sysformer, a transformer model that adapts system prompts in the LLM input embedding space, keeping LLM parameters frozen.

Result: Significant improvements: 80% gain in refusal rate for harmful prompts, 90% compliance for safe ones, and 100% robustness against jailbreaking attacks.

Conclusion: Sysformer offers a cost-effective way to safeguard LLMs, encouraging further research into variable system prompts.

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [8] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: CIfly is a framework for efficient graphical causal inference, using reachability as a core operation, outperforming traditional methods like moralization and latent projection.


<details>
  <summary>Details</summary>
Motivation: To simplify and speed up causal reasoning tasks by reducing them to reachability in state-space graphs.

Method: Formalizes rule tables for specifying algorithms, proving linear-time execution, and provides a Rust implementation for high-performance execution.

Result: CIfly is more efficient than traditional primitives and supports new algorithms, like for instrumental variables.

Conclusion: CIfly offers a flexible, scalable backbone for causal inference, enabling easier and more efficient algorithm development and deployment.

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [9] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: DOCSAT, a new stochastic local search heuristic for 3-SAT, outperforms existing solvers like WalkSAT and Kissat by addressing oversatisfied constraints, especially in hard instances.


<details>
  <summary>Details</summary>
Motivation: Existing solvers like WalkSAT get stuck in local minima due to oversatisfied constraints, limiting performance in hard 3-SAT instances.

Method: DOCSAT dissipates oversatisfied constraints (DOC) to reduce their abundance and avoid local minima, leveraging statistical structure beyond the primary cost function.

Result: DOCSAT outperforms WalkSAT and Kissat, even in the hardest quintile of randomly generated 3-SAT instances up to N=15000.

Conclusion: DOCSAT's approach of addressing oversatisfied constraints opens avenues for generalizing to other optimization problems by escaping local minima traps.

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [10] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR is an automated framework for evaluating and training LLMs via scalable logical reasoning, creating tasks with controlled difficulty and validating outputs symbolically. It benchmarks LLMs, showing their limitations in logical inference and improves performance via logic-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of systematically evaluating and improving LLMs' logical reasoning capabilities without human annotation or dataset bias.

Method: SLR synthesizes tasks with latent rules, validation programs, and prompts. It creates SLR-Bench, a benchmark with 19k+ prompts across 20 difficulty levels.

Result: LLMs often fail at logical inference despite valid syntax. Logic-tuning via SLR doubles Llama-3-8B's accuracy, matching Gemini-Flash-Thinking with lower compute.

Conclusion: SLR provides a scalable, automated solution for advancing LLMs' reasoning, ensuring novelty and efficiency.

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [11] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: A DRL-MCTS system for Xiangqi enhances AI strategy in culturally significant games by addressing unique complexities like high branching factor and asymmetrical dynamics.


<details>
  <summary>Details</summary>
Motivation: To tackle the underexplored complexity of Xiangqi and advance AI in culturally significant strategy games.

Method: Combines policy-value networks with MCTS for strategic self-play and decision refinement.

Result: Overcomes challenges like high branching factor and asymmetrical piece dynamics, improving AI capabilities.

Conclusion: Advances AI in Xiangqi and offers insights for adapting DRL-MCTS to domain-specific rule systems.

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [12] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: The paper introduces a framework to evaluate AI agents in mission-critical negotiations, focusing on how personality traits and AI characteristics affect outcomes. Experiments using Sotopia reveal key insights for reliable AI in high-stakes scenarios.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for adaptable AI agents in diverse human-AI interactions, especially in mission-critical contexts like cross-team coordination and civil-military operations.

Method: Uses Sotopia as a simulation testbed for two experiments: 1) Causal discovery to measure personality impacts on price bargaining. 2) Evaluates human-AI job negotiations by manipulating human personality and AI traits (transparency, competence, adaptability).

Result: Agreeableness and Extraversion significantly influence negotiation outcomes. Sociocognitive measures detect empathic communication and moral foundations. AI trustworthiness impacts mission effectiveness.

Conclusion: The study provides a repeatable methodology for evaluating AI reliability in diverse human-agent dynamics, advancing beyond standard metrics to include social dynamics for mission success.

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [13] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: BEWA is a Bayesian-based architecture for structured scientific belief updates, integrating replication scores, citation weighting, and temporal decay to enhance machine reasoning and truth utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of processing vast scientific literature by formalizing belief updates and ensuring rational, auditable reasoning.

Method: Uses Bayesian inference, replication scores, citation weighting, temporal decay, and graph-based claim propagation to model belief.

Result: Enables probabilistically coherent, dynamic belief updates and supports audit-resilient integrity in scientific reasoning.

Conclusion: BEWA provides a robust framework for machine reasoning, promoting truth utility and rational convergence in scientific domains.

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [14] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Main category: cs.AI

TL;DR: The paper introduces novel value functions for dual-objective RL problems, proposing DO-HJ-PPO to solve Reach-Always-Avoid and Reach-Reach tasks, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Hard constraints in RL degrade performance; Lagrangian methods are complex. The work aims to simplify dual-objective satisfaction using HJ equations.

Method: Extends HJ-RL connections to derive explicit Bellman forms for Reach-Always-Avoid and Reach-Reach problems, proposing DO-HJ-PPO.

Result: DO-HJ-PPO outperforms baselines in safe-arrival and multi-target tasks, showing distinct behaviors.

Conclusion: The approach provides a new perspective on constrained decision-making, solving dual-objective problems effectively.

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [15] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: Generative AI for desktop tasks has high latency due to excessive model calls, and agents take unnecessary steps compared to human efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the impractical latency of current AI systems in desktop tasks and guide future agent development.

Method: Study temporal performance on OSWorld benchmark, analyze latency causes, and create OSWorld-Human for comparison.

Result: Major latency comes from model calls; agents take 1.4-2.7x more steps than humans.

Conclusion: Future AI agents must optimize steps and reduce latency to match human efficiency.

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [16] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: The paper introduces verification mechanisms for an ontology-based process model to ensure correct data retrieval and interpretation in manufacturing.


<details>
  <summary>Details</summary>
Motivation: To address challenges like context-relevant data selection, unit compatibility, and input data completeness for evaluating mathematical expressions in manufacturing processes.

Method: Uses SPARQL-based filtering, unit consistency checks, and data completeness checks within an ontology-based process model.

Result: Demonstrated applicability with a Resin Transfer Molding (RTM) use case, enabling machine-interpretable and verifiable models.

Conclusion: The proposed mechanisms enhance the correctness and reliability of process knowledge formalization in manufacturing.

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [17] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: The paper proposes using graph neural networks and heterogeneous graph structures to predict optimization algorithm performance, outperforming traditional tabular methods by up to 36.6% in MSE.


<details>
  <summary>Details</summary>
Motivation: Traditional performance prediction methods overlook algorithm configurations, which are crucial for accurate predictions. The complex relationships between problem characteristics, algorithm configurations, and performance are better represented as graphs.

Method: The study uses heterogeneous graph data structures and graph neural networks to model these relationships. It evaluates 324 modCMA-ES and 576 modDE variants on 24 BBOB problems across different runtime budgets and dimensions.

Result: The approach achieves up to 36.6% improvement in mean squared error (MSE) compared to traditional tabular-based methods.

Conclusion: Geometric learning, particularly graph-based methods, shows significant potential for improving performance prediction in black-box optimization.

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [18] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: LLMs outperform humans in decision-making tasks involving uncertainty, risk, and adaptability, but their decision processes differ fundamentally from humans, raising concerns about their use as substitutes for human judgment.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs learn and make decisions compared to humans, especially in real-world scenarios involving uncertainty, risk, and adaptability.

Method: Benchmarked five leading LLMs against 360 human participants using three experimental psychology tasks designed to probe uncertainty, risk, and set-shifting.

Result: LLMs often outperformed humans, achieving near-optimal performance, but their decision-making processes were fundamentally different.

Conclusion: While LLMs excel in decision-making tasks, their divergence from human processes highlights risks in relying on them as substitutes, necessitating further research.

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [19] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: The paper extends Approximation Fixpoint Theory (AFT) by introducing more refined approximation spaces to overcome its limitations in certain non-monotonic reasoning formalisms.


<details>
  <summary>Details</summary>
Motivation: AFT has limitations in handling some simple examples despite its broad applicability in non-monotonic reasoning. The paper aims to address these limitations.

Method: The authors extend consistent AFT by introducing a more general notion of approximation spaces, refining beyond intervals.

Result: The proposed extension improves expressiveness and allows for better handling of fixpoints in non-monotonic reasoning.

Conclusion: The refined approximation spaces enhance AFT's applicability and provide a more flexible framework for non-monotonic reasoning.

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [20] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: A structured prompting framework improves LLMs' logical consistency in legal analysis by decomposing reasoning into verifiable steps, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' struggles with rule application and explainability in domains like legal analysis by combining neural and symbolic methods.

Method: Introduces a three-step framework: entity identification, property extraction, and symbolic rule application, with formal verification.

Result: Significant performance gains on LegalBench hearsay task (e.g., F1 scores of 0.929 and 0.867 vs. baselines of 0.714 and 0.74).

Conclusion: Hybrid neural-symbolic approach enhances transparent, consistent reasoning, promising for explainable AI in legal tasks.

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [21] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench is a new benchmark for evaluating interactive safety in embodied AI agents, addressing flaws in current static evaluation methods. It tests agents' ability to perceive and mitigate risks dynamically, revealing significant gaps in current models like GPT-4o and Gemini-2.5.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for embodied agents fail to assess dynamic risks in interactive environments, posing safety hazards for real-world deployment.

Method: IS-Bench introduces a multi-modal benchmark with 161 scenarios and 388 safety risks, enabling process-oriented evaluation of risk mitigation actions.

Result: Experiments show current agents lack interactive safety awareness, and safety-aware Chain-of-Thought improves performance but often hinders task completion.

Conclusion: IS-Bench lays the groundwork for safer embodied AI systems by highlighting critical limitations and enabling better risk assessment.

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [22] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: The paper presents an automated approach to optimize consumer communication, replacing manual marketer work with a sequential decision-making framework, leading to significant engagement improvements.


<details>
  <summary>Details</summary>
Motivation: Manual orchestration of consumer communication is labor-intensive and limits personalization. The paper aims to automate and optimize this process.

Method: Uses a Difference-in-Differences design for Individual Treatment Effect estimation and Thompson sampling for explore-exploit balance.

Result: Significant increases in goal events across product features, deployed for 150 million users.

Conclusion: The automated framework effectively replaces manual efforts, enhancing engagement and scalability.

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [23] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master, an AI4AI agent, integrates exploration and reasoning with a selective memory mechanism, outperforming existing methods by 29.3% in efficiency within half the time.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in AI4AI where agents fail to leverage accumulated experience during reasoning, leading to suboptimal performance.

Method: Proposes ML-Master, which uses a selectively scoped memory mechanism to combine insights from parallel solution trajectories with analytical reasoning.

Result: Achieves a 29.3% average medal rate on MLE-Bench, surpassing baselines, especially in medium-complexity tasks, within a 12-hour limit.

Conclusion: ML-Master demonstrates significant potential as an efficient tool for advancing AI4AI.

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [24] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: The paper introduces an Elo rating-based method to improve LLM performance for analyzing harmful content like microaggressions and hate speech, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: LLMs' moderation systems hinder analysis of harmful content, affecting validity in organizational research.

Method: Elo rating-based method for LLM prompting.

Result: Outperforms traditional LLM techniques and ML models in accuracy, precision, and F1 scores on microaggression and hate speech datasets.

Conclusion: The method enhances reliability, reduces false positives, and supports applications like workplace harassment detection and safer work environments.

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [25] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Main category: cs.AI

TL;DR: The paper discusses the need for a comprehensive, general-purpose knowledge resource in AI, inspired by the Cyc project, and proposes a community-driven framework to address knowledge gaps in AI applications.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge resources like WordNet and ConceptNet are insufficient for AI's needs, leading to gaps in large language models, robotic planning, and fact-checking. A new, widely available knowledge infrastructure is required.

Method: The paper synthesizes findings from a AAAI workshop with over 50 researchers, proposing an open engineering framework for knowledge modules, leveraging modern knowledge representation and reasoning techniques.

Result: The paper outlines a vision for a community-driven knowledge infrastructure, emphasizing conventions and social structures for contributors.

Conclusion: A collaborative, open framework is proposed to address AI's knowledge deficiencies, combining modern technology with community-driven development.

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [26] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: The study explores how explanation styles and perceived AI accuracy affect decision-making in Predictive Process Monitoring (PPM), highlighting gaps in current XAI evaluations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in PPM lack interpretability, reducing user trust. Current XAI evaluations overlook user-centered impacts like task performance and decision-making.

Method: A decision-making experiment tested users with AI predictions, perceived accuracy levels, and different explanation styles (feature importance, rule-based, counterfactual).

Result: Perceived accuracy and explanation style significantly impact decision-making, measured by task performance, agreement, and decision confidence.

Conclusion: The study underscores the importance of user-centered XAI evaluations in PPM, showing that explanation styles and perceived accuracy influence decision outcomes.

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [27] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: The paper proposes a low-dimensional, rule-based model using spatiotemporal data to analyze football tactics, focusing on interpretable state variables for pass success prediction.


<details>
  <summary>Details</summary>
Motivation: Existing models are computationally expensive, lack interpretability, or ignore all players' states. This study aims to address these gaps.

Method: Defines interpretable state variables for the ball-holder and receivers, uses StatsBomb and SkillCorner data, and trains an XGBoost model to predict pass success.

Result: Distance to the ball and space score were key factors in pass success. The model is interpretable and practical for decision-making.

Conclusion: The low-dimensional, rule-based approach effectively captures football tactics and offers actionable insights for managers.

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [28] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: The paper proposes an incentive-aware framework for federated learning (FL) to address challenges of voluntary participation and data heterogeneity, using Wasserstein distance and a two-stage Stackelberg game model.


<details>
  <summary>Details</summary>
Motivation: Existing FL research assumes voluntary participation and ignores data heterogeneity, leading to potential opt-outs or low-quality contributions from self-interested agents.

Method: The framework introduces Wasserstein distance to measure data heterogeneity, reformulates convergence bounds, and uses peer prediction and a Stackelberg game to ensure truthful reporting and equilibrium.

Result: Experiments on real-world datasets show the framework's effectiveness in improving participation and convergence.

Conclusion: The proposed mechanism successfully addresses participation and heterogeneity issues in FL, enhancing collaboration and model quality.

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [29] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Main category: cs.AI

TL;DR: The paper proposes a hybrid charging infrastructure system combining fixed and mobile chargers, optimized using deep reinforcement learning and demand prediction, to improve charging availability and reduce user inconvenience.


<details>
  <summary>Details</summary>
Motivation: Vehicle electrification's success depends on adaptable charging infrastructure. Fixed stations face underutilization or congestion, while mobile chargers offer flexibility to meet dynamic demand.

Method: Introduces the HCSPO problem, optimizing fixed charger locations and mobile charger schedules using a demand prediction model (MPC) and deep reinforcement learning with heuristic scheduling.

Result: Case studies show the method enhances charging availability and reduces user inconvenience compared to existing solutions.

Conclusion: The hybrid approach with optimized planning and dynamic operation effectively addresses urban charging infrastructure challenges.

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [30] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: The paper investigates geographic biases in image generation models like FLUX 1 and Stable Diffusion 3.5, revealing a preference for metropolis-like areas and issues with European-sounding names.


<details>
  <summary>Details</summary>
Motivation: To explore the geographic knowledge and biases embedded in state-of-the-art image generation models when applied to urban analysis and design.

Method: Generated 150 synthetic images per U.S. state and capital using FLUX 1 and Stable Diffusion 3.5, analyzed similarity with DINO-v2 ViT-S/14 and Fréchet Inception Distances.

Result: Models show implicit geographic knowledge but exhibit bias toward metropolis-like areas and disambiguation issues with European-sounding names.

Conclusion: Image generation models need improvement to reduce geographic biases and better represent diverse regions.

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [31] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: A heuristic MAB method for dynamic, discrete environments is proposed by extending a BBO approach using an Ising machine, addressing limitations of conventional MAB algorithms in such settings.


<details>
  <summary>Details</summary>
Motivation: Real-time systems in dynamic environments require optimization of discrete variables, but conventional MAB algorithms fail due to the combinatorial explosion of actions.

Method: Extends a BBO method using an Ising machine to explore actions while accounting for variable interactions and environmental changes.

Result: Demonstrated dynamic adaptability in a wireless communication system with moving users.

Conclusion: The proposed method effectively optimizes dynamic, discrete environments where traditional MAB algorithms fall short.

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [32] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: The paper proposes a Multimodal Fused Learning (MMFL) framework for solving the Generalized Traveling Salesman Problem (GTSP) in robotic task planning, combining graph and image-based representations for real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Task planning for mobile robots in applications like warehouse retrieval and environmental monitoring involves solving GTSP, which is challenging to address accurately and efficiently.

Method: The MMFL framework uses a coordinate-based image builder, adaptive resolution scaling, and a multimodal fusion module to integrate geometric and spatial features.

Result: MMFL outperforms state-of-the-art methods in GTSP instances and maintains computational efficiency for real-time applications, validated by physical robot tests.

Conclusion: The MMFL framework effectively addresses GTSP in robotic task planning, demonstrating superior performance and practical applicability.

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [33] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: A novel LfD algorithm improves Mahjong bot proficiency and preserves play styles, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing bot proficiency and diverse play styles in games, particularly Mahjong, where randomness and out-of-distribution states challenge current algorithms.

Method: Leverages gameplay histories of Mahjong agents with minimal modifications to Proximal Policy Optimization for a new LfD algorithm.

Result: Significantly improves agent proficiency and effectively maintains unique play styles.

Conclusion: The proposed method advances bot development in games with high randomness and diverse play styles.

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [34] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: A novel RUL estimation method using State Space Models (SSM) with Simultaneous Quantile Regression (SQR) outperforms traditional techniques like LSTM and Transformer in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Predictive Maintenance (PdM) is crucial for Industry 4.0/5.0 to optimize maintenance and reduce failures by accurately predicting equipment Remaining Useful Life (RUL).

Method: Proposes SSM for long-term sequence modeling, integrating SQR to handle uncertainty by estimating multiple quantiles.

Result: SSM models show superior accuracy and computational efficiency compared to LSTM, Transformer, and Informer on the C-MAPSS dataset.

Conclusion: SSM with SQR is promising for high-stakes industrial RUL prediction, offering better performance than traditional methods.

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [35] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: BFO 2020 lacks support for functions, dispositions, and roles of generically dependent continuants (e.g., software, datasets). This paper highlights the limitation and proposes two solutions: using defined classes or modifying BFO to include these features.


<details>
  <summary>Details</summary>
Motivation: The inability of BFO 2020 to represent functions, dispositions, and roles of generically dependent continuants (like software or datasets) hinders accurate modeling of computer models and dataset roles.

Method: The paper discusses BFO 2020's limitations and presents two approaches: (a) using defined classes and (b) proposing changes to BFO to support these features.

Result: The analysis identifies gaps in BFO 2020 and suggests practical solutions to enhance its representational capabilities.

Conclusion: Addressing these limitations is crucial for better representation of generically dependent continuants in BFO, with proposed solutions offering viable paths forward.

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [36] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: The paper introduces DREAM, a self-adaptive solution to improve LLMs' multi-step FOL reasoning by enhancing diversity and reasonability in proof strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex mathematical reasoning, especially multi-step FOL tasks, due to limited proof strategy diversity and early errors.

Method: DREAM uses Axiom-Driven Strategy Diversification and Sub-Proposition Error Feedback to improve LLMs' proof generation.

Result: DREAM improves performance by 0.6% to 6.4% on a theorem proving dataset.

Conclusion: The paper advances LLMs' mathematical reasoning and provides a dataset of 447 theorems for evaluation.

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [37] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: Different bias evaluation methods for Large Language Models yield inconsistent rankings, highlighting the need for standardized benchmarks.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of existing benchmarks for evaluating model safety, particularly bias, in Large Language Models.

Method: Investigated how different bias evaluation approaches rank representative models and compared the consistency of these rankings.

Result: Found that widely used bias evaluation methods produce disparate model rankings.

Conclusion: Recommends community guidelines for the usage and standardization of safety benchmarks.

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [38] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Main category: cs.AI

TL;DR: The paper introduces the RFMDataset to evaluate large reasoning models' performance on mathematical proofs, revealing significant shortcomings like low correctness rates, diverse reasoning failures, and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of large reasoning models in mathematical problem-solving, masked by high accuracy on numerical evaluations and potential benchmark leaks.

Method: The authors create the RFMDataset, a collection of 200 diverse mathematical proof problems, and analyze model performance to identify error types.

Result: Models perform poorly on proofs (some <20% correct), exhibit diverse reasoning failures, and lack guarantees for correctness or rigor in single-step reasoning.

Conclusion: Current models' self-reflection is inadequate; formalized, fine-grained logical training is needed to address their fundamental limitations.

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [39] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: The paper explores how model-free RL can lead to 'thinking' behaviors in agents, introduces a theoretical 'thought MDP' model, and validates its predictions with LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand when model-free RL results in 'thinking' strategies for reward maximization, independent of domain.

Method: Introduces a 'thought MDP' model, analyzes policy initialization's role, and tests predictions with LLMs.

Result: Shows thought actions equate to policy improvement steps and validates conditions for thinking-like behavior in LLMs.

Conclusion: Proposes sufficient conditions for learning thinking outside language tasks and demonstrates improved RL efficiency in a toy domain.

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [40] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Main category: cs.AI

TL;DR: A progressive trust evaluation framework, chain-of-trust, is proposed to handle misaligned device attribute data in collaborative systems, improving accuracy and reducing overhead.


<details>
  <summary>Details</summary>
Motivation: Trust evaluation in dynamic networks is challenging due to varying data latencies and incomplete attribute observations.

Method: The framework divides trust evaluation into chained stages, using task decomposition and generative AI for data analysis.

Result: Experimental results show high accuracy in trust evaluation.

Conclusion: The chain-of-trust framework effectively addresses trust evaluation challenges in collaborative systems.

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [41] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.AI

TL;DR: MedPerturb is a dataset designed to evaluate medical LLMs under controlled perturbations, revealing differences in sensitivity to gender, style, and format changes between LLMs and humans.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs and humans differ in responding to real-world clinical variability, ensuring clinical robustness for safe deployment.

Method: Introduces MedPerturb, a dataset of 800 clinical vignettes perturbed along gender, style, and format axes, with outputs from four LLMs and human experts.

Result: LLMs are more sensitive to gender and style changes, while humans are more sensitive to format changes like clinical summaries.

Conclusion: Evaluation frameworks must account for clinical variability to ensure LLM decisions align with human clinician responses.

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Main category: cs.LG

TL;DR: AET introduces an ERM phase before AT to improve robustness and efficiency, achieving better results with lower costs.


<details>
  <summary>Details</summary>
Motivation: Many AT variants focus on stronger attacks but overlook foundational feature representations. AET aims to address this by pre-conditioning features via ERM.

Method: AET prepends an ERM phase to conventional AT, hypothesizing it creates a better feature manifold for robustness.

Result: AET achieves comparable or superior robustness faster, improves clean accuracy, and reduces training costs by 8-25%.

Conclusion: Feature pre-conditioning via ERM enhances AT efficiency and effectiveness, as demonstrated across datasets and architectures.

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [43] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: The paper introduces MDPU, a generalized learning framework for Label Proportion Learning (LLP), addressing challenges in obtaining precise supervisory information by leveraging proportional constraints within tuples.


<details>
  <summary>Details</summary>
Motivation: Practical applications often lack precise supervisory information on instance proportions, necessitating a method to better align with real-world scenarios and utilize proportional constraints effectively.

Method: The paper mathematically models instance distribution within tuples, derives an unbiased risk estimator via ERM, and introduces a risk correction method to mitigate overfitting.

Result: The proposed MDPU framework demonstrates consistency through theoretical generalization error bounds and outperforms baseline methods in experiments.

Conclusion: MDPU effectively addresses LLP challenges, offering a robust solution with validated performance across diverse datasets.

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [44] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Main category: cs.LG

TL;DR: S$^2$GPT-PINN is a compact model for solving parametric PDEs, using minimal parameters and computational power by leveraging high-quality data and customizations like knowledge distillation and down-sampling.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of traditional PINNs by creating a domain-specific, sparse, and small model for parametric PDEs.

Method: Uses a greedy algorithm for data selection, knowledge distillation from pre-trained PINNs, and down-sampling for physics-informed loss calculation.

Result: Achieves high efficiency with significantly fewer parameters than traditional PINNs.

Conclusion: S$^2$GPT-PINN offers a scalable and efficient solution for parametric PDEs with minimal computational resources.

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [45] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Main category: cs.LG

TL;DR: Proposes an end-to-end framework with two variants for cellular traffic prediction, combining CNN with attention and Kalman filter, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Cellular traffic is dynamic and affected by exogenous factors, degrading prediction accuracy.

Method: Uses CNN with attention for spatial dynamics and Kalman filter for temporal modeling, leveraging auxiliary data like social activities.

Result: Outperforms state-of-the-art machine learning techniques on three real-world datasets.

Conclusion: The framework effectively captures spatiotemporal patterns and improves prediction accuracy.

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [46] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: The paper introduces BASE-Q, a method combining bias correction and asymmetric scaling to reduce errors in rotational quantization for LLMs, enabling blockwise optimization and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current rotational quantization methods for LLMs have limitations like misaligned channel means and Gaussian-like activation distributions, leading to increased errors and memory overhead.

Method: BASE-Q combines bias correction and asymmetric scaling to address rounding and clipping errors, and enables blockwise optimization to reduce memory usage.

Result: BASE-Q reduces the accuracy gap to full-precision models by 50.5%, 42.9%, and 29.2% compared to QuaRot, SpinQuant, and OSTQuant, respectively.

Conclusion: BASE-Q is an effective solution for improving quantization in LLMs, offering better accuracy and reduced memory overhead.

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [47] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Main category: cs.LG

TL;DR: The paper introduces LLM Web Dynamics (LWD) to study model collapse in LLMs at the network level, using a RAG database for simulation and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored threat of model collapse in LLM training due to synthetic data usage.

Method: Proposes LWD framework, simulating the Internet with a RAG database and analyzing output convergence patterns.

Result: Provides theoretical guarantees for convergence by analogy to interacting Gaussian Mixture Models.

Conclusion: LWD offers a novel approach to understanding and mitigating model collapse in LLMs.

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [48] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Main category: cs.LG

TL;DR: The paper analyzes whether latent action models (LAMs) capture controllable changes or noise, using a tractable linear model to connect LAMs with PCA and suggesting strategies like data augmentation and cleaning.


<details>
  <summary>Details</summary>
Motivation: To address whether LAMs learn action-relevant changes or noise from unlabeled videos, given the ambiguity in frame differences.

Method: A tractable linear model is used to analytically study LAM learning, connecting it to PCA and evaluating strategies like data augmentation and auxiliary action-prediction.

Result: Insights include connections to PCA, policy desiderata, and justification for strategies to learn controllable changes. Numerical simulations illustrate the impact of observation, action, and noise structures.

Conclusion: The study provides analytical insights into LAM learning, highlighting the importance of data quality and augmentation to focus on controllable changes.

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [49] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Main category: cs.LG

TL;DR: MLE-STAR is a novel approach for building LLM-based MLE agents that leverages external knowledge and iterative refinement to outperform existing methods in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based MLE agents rely too much on inherent knowledge and coarse exploration, limiting their ability to select task-specific models and explore components deeply.

Method: MLE-STAR retrieves effective models from the web, forms an initial solution, and iteratively refines it by exploring specific ML components, guided by ablation studies. It also introduces a novel ensembling method.

Result: MLE-STAR achieves medals in 44% of Kaggle competitions on MLE-bench, significantly outperforming alternatives.

Conclusion: MLE-STAR demonstrates the effectiveness of combining external knowledge and targeted exploration for building superior MLE agents.

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [50] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Main category: cs.LG

TL;DR: A verifiable model-free safety filter using Hamilton-Jacobi reachability is introduced, addressing the lack of formal guarantees in learning-based safety filters.


<details>
  <summary>Details</summary>
Motivation: Learning-based safety filters lack formal safety guarantees despite outperforming conventional methods like CBFs.

Method: Extends verifiable self-consistency for Q value functions, uses a multiplicative Q-network to prevent shrinkage, and develops a verification pipeline.

Result: Successfully synthesizes formally verified, model-free safety certificates in four safe-control benchmarks.

Conclusion: The approach provides a verifiable solution to enhance safety in learning-based filters.

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [51] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Main category: cs.LG

TL;DR: A framework combining kernel PCA, MLP, and MIGA for disease prediction achieves high accuracy (up to 100%) and reduces tuning time by 60%.


<details>
  <summary>Details</summary>
Motivation: To improve disease prediction accuracy and efficiency by integrating nonlinear feature extraction, classification, and parallel optimization.

Method: Uses kernel PCA for dimensionality reduction, MLP for classification, and MIGA for parallel hyperparameter optimization.

Result: Achieved 99.12% (breast cancer), 94.87% (Parkinson's), and 100% (CKD) accuracy, outperforming other methods.

Conclusion: The framework is effective for disease prediction, with MIGA significantly reducing computational time.

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [52] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Main category: cs.LG

TL;DR: SimuGen is a multimodal agent-based framework designed to generate accurate Simulink simulation code by combining visual diagrams and domain knowledge, addressing LLMs' limitations in this domain.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with generating reliable Simulink models due to lack of domain-specific pretraining data.

Method: SimuGen uses a collaborative, modular approach with specialized agents (investigator, reviewer, generator, executor, debug locator, report writer) and a domain-specific knowledge base.

Result: The framework enables interpretable, robust, and reproducible Simulink simulation generation.

Conclusion: SimuGen effectively bridges the gap in LLMs' capabilities for Simulink model generation, with publicly available source code.

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [53] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Main category: cs.LG

TL;DR: The paper proposes a multimodal framework (Chain-of-Cancer) for cancer survival prediction, integrating clinical data and language, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook epigenetic data and textual descriptions, prompting the exploration of four modalities for improved survival prediction.

Method: The Chain-of-Cancer (CoC) framework uses intra-learning (domain-specific clinical data) and inter-learning (language prompting and synergistic representation) for joint multimodal learning.

Result: Extensive experiments on five cancer datasets validate the framework's effectiveness, yielding state-of-the-art results.

Conclusion: The proposed CoC framework successfully integrates multiple modalities for enhanced survival prediction, with plans to release the code.

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [54] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape is a novel framework for Spatially Resolved Transcriptomics (SRT) that improves spot representations by capturing global relationships and regulating distances between spots, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based methods in SRT fail to provide meaningful spot representations, especially near boundaries, due to overemphasis on adjacent spots with minimal feature differences.

Method: Spotscape introduces a Similarity Telescope module for global spot relationships and a similarity scaling strategy for multi-slice integration.

Result: Experiments show Spotscape's superiority in single-slice and multi-slice downstream tasks.

Conclusion: Spotscape effectively addresses limitations of current SRT methods, offering improved performance and multi-slice integration.

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [55] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: The paper introduces $	exttt{BLUR}$, a benchmark for LLM unlearning, addressing the limitations of current benchmarks by providing realistic forget-retain overlap scenarios.


<details>
  <summary>Details</summary>
Motivation: Current LLM unlearning benchmarks are flawed due to disparate forget and retain sets, leading to misleading evaluations of unlearning methods.

Method: The authors develop $	exttt{BLUR}$, a benchmark with extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying difficulty.

Result: Existing unlearning methods perform poorly on $	exttt{BLUR}$, with simpler approaches outperforming recent ones, highlighting the need for robust evaluation.

Conclusion: $	exttt{BLUR}$ underscores the importance of realistic benchmarks for LLM unlearning and suggests future research directions.

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [56] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Main category: cs.LG

TL;DR: The paper integrates Control Contraction Metrics (CCMs) with reinforcement learning (RL) to enhance optimality and scalability in control policies under unknown dynamics.


<details>
  <summary>Details</summary>
Motivation: CCMs alone lack trajectory optimality and require known dynamics, limiting scalability. Combining CCMs with RL addresses these issues.

Method: Proposes the Contraction Actor-Critic (CAC) algorithm, which learns a contraction metric generator (CMG) and an optimal tracking policy using actor-critic RL.

Result: CAC improves CCMs by providing contracting policies with long-term optimality, validated in simulated and real-world robot experiments.

Conclusion: Integrating CCMs with RL enhances control policy performance and scalability, supported by empirical and theoretical evidence.

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [57] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: Compiler-R1 is an RL-driven framework enhancing LLMs for compiler auto-tuning, featuring a high-quality dataset and a two-stage RL pipeline, achieving 8.46% IR instruction count reduction.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of high-quality reasoning datasets and limited interactions in compiler tuning using LLMs.

Method: Introduces Compiler-R1 with a curated dataset and a two-stage RL training pipeline for efficient exploration and learning.

Result: Achieves an average 8.46% reduction in IR instruction count compared to opt -Oz across seven datasets.

Conclusion: Compiler-R1 demonstrates the potential of RL-trained LLMs for effective compiler optimization.

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [58] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: Minifinetuning (MFT) reduces overfitting-induced performance loss in low-data domain adaptation without pre-training data, outperforming standard finetuning and parameter-efficient methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the deterioration of general performance in language models when finetuned for new domains, especially with limited data.

Method: Uses corrective self-distillation individualized per sample to mitigate degeneralization, requiring as little as 500 samples.

Result: MFT shows 2-10x better specialization-to-degeneralization ratios than standard finetuning and is robust to overfitting.

Conclusion: MFT effectively balances domain adaptation and general performance, even in low-data settings, and can be combined with other methods.

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [59] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Main category: cs.LG

TL;DR: FIMCFG is a federated incomplete multi-view clustering method using globally fused graph guidance to address missing data and leverage global information.


<details>
  <summary>Details</summary>
Motivation: Existing federated multi-view clustering methods lack global information exploitation and ignore missing data issues.

Method: Uses dual-head graph convolutional encoders for global and view-specific feature extraction, fused under a global graph, with pseudo-label supervision.

Result: Demonstrates effectiveness and superiority in experiments.

Conclusion: FIMCFG successfully addresses missing data and leverages global information for improved clustering.

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [60] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: LFPS accelerates sparse indexing in LLMs by leveraging historical attention patterns, achieving significant speedups while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The memory demand for KV caches in LLMs grows with longer contexts, creating bottlenecks in GPU memory and bandwidth. Existing sparse attention methods inefficiently retrieve indices.

Method: LFPS dynamically constructs sparse indexing candidates using historical attention patterns (vertical and slash) and a positional expansion strategy.

Result: LFPS achieves up to 22.8x speedup over full attention and 9.6x over exact Top-k retrieval, preserving accuracy.

Conclusion: LFPS is a practical and efficient solution for optimizing long-context LLM decoding.

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [61] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.LG

TL;DR: TSFMs (Time Series Foundation Models) show promise for zero-shot forecasting of macroeconomic indicators, matching or outperforming classical models in stable conditions but struggling during rapid shocks.


<details>
  <summary>Details</summary>
Motivation: To explore the zero-shot forecasting capabilities of TSFMs for macroeconomic indicators, avoiding the need for extensive training datasets or custom models.

Method: Applied three TSFMs (Chronos, TimeGPT, Moirai) to univariate forecasting of economic indicators, tested under data-scarce conditions and structural breaks.

Result: TSFMs internalize economic dynamics, handle regime shifts, and provide uncertainty estimates, matching multivariate models in stable conditions but degrading during rapid shocks.

Conclusion: TSFMs are viable for zero-shot macroeconomic forecasting in stable conditions but require caution during periods of rapid shocks.

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [62] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: The paper introduces Multi-Granularity Direct Preference Optimization (MDPO) to improve LLMs' mathematical reasoning by optimizing at three granularities, outperforming DPO and its variants.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO struggle with long-chain mathematical reasoning due to ineffective preference capture and misalignment with generation metrics.

Method: Proposes MDPO, optimizing at Solution2Solution, Inference2Inference, and Step2Step granularities, unifying training objectives with generation metrics.

Result: Achieved improvements of 1.7% and 0.9% on GSM8K, and 2.3% and 1.2% on MATH datasets for Qwen2 and Llama3 models.

Conclusion: MDPO effectively enhances LLMs' mathematical reasoning and provides a cost-efficient data construction pipeline.

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [63] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: Test-Time Scaling (TTS) improves LLM performance by optimizing resource allocation during search. The proposed Direction-Oriented Resource Allocation (DORA) method outperforms baselines, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing search methods inefficiently allocate compute resources, favoring reasoning directions with more candidates, leading to suboptimal performance.

Method: Formulates test-time search as a resource allocation problem and introduces DORA, a provably optimal method that decouples direction quality from candidate count.

Result: DORA outperforms baselines on MATH500, AIME2024, and AIME2025 benchmarks, achieving state-of-the-art accuracy with comparable compute cost.

Conclusion: DORA addresses inefficiencies in TTS, contributing to better understanding of optimal resource allocation for LLMs.

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [64] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Main category: cs.LG

TL;DR: CGB, a novel GNN framework, improves brain disease detection by modeling causal relationships between brain ROIs using transfer entropy and geometric curvature, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs for brain disease detection often overlook causal relationships between ROIs, which are crucial for understanding signal interactions.

Method: CGB uses causal discovery (transfer entropy) and geometric curvature to model and refine causal brain networks, enhancing GNN performance.

Result: CGB achieves higher F1 scores in brain disease classification compared to state-of-the-art methods.

Conclusion: CGB's focus on causality and graph refinement significantly improves brain disease detection, offering a more expressive and accurate framework.

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [65] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Main category: cs.LG

TL;DR: The paper proposes a method for direct motif significance-profile (SP) estimation using GNNs, shifting from subgraph frequency counting to multitarget regression, validated on synthetic and real-world graphs.


<details>
  <summary>Details</summary>
Motivation: The application of GNNs to network motif SP prediction is under-explored, lacking benchmarks. The study aims to address this gap by framing SP estimation independently of subgraph frequency.

Method: The approach reformulates SP estimation as multitarget regression, focusing on interpretability, stability, and scalability. It uses 1-WL limited models and validates on synthetic and real-world graphs.

Result: Experiments show 1-WL limited models struggle with precise SP estimation but can generalize to approximate graph generation processes by comparing predicted SPs with synthetic generators.

Conclusion: Direct SP estimation with GNNs can overcome theoretical limitations of motif estimation via subgraph counting, offering a promising direction for future research.

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [66] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Main category: cs.LG

TL;DR: RAST transfers RL-induced probability adjustments from small to large models, enhancing reasoning capabilities efficiently.


<details>
  <summary>Details</summary>
Motivation: RL improves LLMs but is resource-intensive; RAST aims to scale benefits without full computational cost.

Method: Token-level analysis of RL-induced distributions; propose RAST to transfer adjustments from small to large models.

Result: RAST enhances reasoning in base models with lower GPU memory, sometimes outperforming direct RL training.

Conclusion: RAST offers insights into RL-driven reasoning and practical scaling strategies, reducing computational costs.

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [67] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: A framework for targeted noise injection in federated learning to defend against gradient inversion attacks, balancing privacy and model accuracy.


<details>
  <summary>Details</summary>
Motivation: Privacy leakage in federated learning, especially in healthcare, poses risks like gradient inversion attacks, which current defenses inadequately address.

Method: Uses a shadow model with interpretability to identify sensitive areas for sample-specific noise injection.

Result: Achieves significant privacy protection (e.g., 3.73 PSNR discrepancy on ChestXRay) with minimal model performance impact (<1% F1 reduction).

Conclusion: The framework effectively defends against gradient inversion attacks while preserving model utility, validated across diverse medical datasets.

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [68] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Main category: cs.LG

TL;DR: A novel BERT-style pretraining framework for battery fault detection improves accuracy by leveraging self-supervised learning on time-series data.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture complex temporal dependencies and underutilize unlabeled data, while LLMs are not suited for numerical time-series data.

Method: Adapts BERT with a time-series-to-token module and point-MSM pretraining for battery data, combining embeddings with metadata for classification.

Result: Achieves AUROC of 0.945, outperforming existing methods in representation quality and classification accuracy.

Conclusion: BERT-style pretraining is effective for time-series fault detection in batteries.

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [69] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Main category: cs.LG

TL;DR: A machine learning model predicts floating offshore asset behavior with high accuracy, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail in extreme sea states and nonlinear responses, necessitating a more robust solution.

Method: Supervised machine learning with multivariate regression, combining gradient-boosted ensemble and a custom solver, trained on 1M samples.

Result: Mean prediction errors <5% for mooring parameters and <2.5° for vessel heading, outperforming frequency-domain methods.

Conclusion: The model is effective for real-time monitoring and decision-making in offshore operations.

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [70] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: A learnable two-sided short-time Laplace transform (STLT) replaces self-attention in transformers, offering dynamic adaptation of token relevance and frequency responses with scalable complexity.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottleneck of self-attention in transformers while maintaining or improving performance, especially for ultra-long sequences.

Method: Introduces trainable parameters for Laplace nodes, enabling end-to-end learning of decay rates, frequencies, and window bandwidth. Uses fast recursive convolution, FFT-based relevance matrix computation, and adaptive node allocation.

Result: Achieves comparable or better perplexities and scores on tasks like language modeling, translation, and QA, extending to 100k+ token contexts.

Conclusion: The STLT combines interpretability, scalability, and robustness, providing a viable alternative to self-attention for long-sequence modeling.

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [71] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: The paper introduces NeuronSeek-TD, a framework that replaces symbolic regression with tensor decomposition to optimize neuron formulations, improving stability and convergence. It provides theoretical guarantees for function approximation and shows competitive performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Inspired by the human brain's task-specific neurons, the paper aims to develop a more stable and efficient method for designing task-driven neurons in deep learning networks.

Method: The framework uses tensor decomposition (TD) instead of symbolic regression (SR) to discover optimal neuron formulations and modifies aggregation functions with common activation functions.

Result: NeuronSeek-TD achieves enhanced stability, faster convergence, and competitive performance compared to state-of-the-art models in diverse benchmarks.

Conclusion: The NeuronSeek-TD framework offers a robust and efficient approach to designing task-driven neurons, supported by theoretical guarantees and empirical validation.

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [72] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Main category: cs.LG

TL;DR: An optimization framework for selecting alternates in citizens' assemblies improves representation by estimating dropout probabilities and minimizing misrepresentation.


<details>
  <summary>Details</summary>
Motivation: Address the issue of unbalanced composition in citizens' assemblies due to panelist dropout, which existing methods ignore.

Method: Introduce a learning-theoretic algorithmic approach to estimate dropout probabilities and optimize alternate selection.

Result: Theoretical guarantees on sample complexity and misrepresentation loss; empirical results show improved representation with fewer alternates.

Conclusion: The proposed method effectively enhances representation in deliberative democracy panels by addressing dropout issues.

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [73] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: The paper introduces daDPO, a method combining preference optimization and distribution-based distillation to enhance smaller LLMs, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Smaller LLMs struggle with conversational abilities, and current knowledge distillation methods overlook the teacher model's output distribution.

Method: Proposes daDPO (Distribution-Aware DPO), integrating preference optimization and distribution-based distillation.

Result: daDPO improves pruned models and smaller LLMs, achieving near-teacher performance and even outperforming the teacher in some cases.

Conclusion: daDPO effectively bridges the performance gap for smaller LLMs by leveraging the teacher model's output distribution.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [74] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Main category: cs.LG

TL;DR: BuildingBRep-11K is a dataset of 11,978 multi-storey buildings with detailed annotations, designed for AI training in 3D object generation. Two lightweight models demonstrate its learnability for geometric regression and defect detection.


<details>
  <summary>Details</summary>
Motivation: The need for large, clean, and richly annotated datasets for AI-driven 3D building generation motivates the creation of BuildingBRep-11K, which incorporates architectural standards and design principles.

Method: A shape-grammar-driven pipeline generates the dataset, ensuring compliance with architectural standards through multi-stage filters. Two PointNet baselines are trained for multi-attribute regression and defect detection.

Result: The regression model achieves 0.37-storey MAE, 5.7-room MAE, and 3.2 m² MAE on mean area. The defect detection model reaches 54% accuracy, with 82% recall for true defects.

Conclusion: BuildingBRep-11K is learnable but non-trivial, suitable for tasks like geometric regression and topological quality assessment in AI-driven 3D building generation.

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [75] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Main category: cs.LG

TL;DR: The paper introduces a novel ML-based approach combining predictive modeling and anomaly detection to optimize heat pump efficiency for household hot water production, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Conventional threshold-based control methods limit heat pump efficiency for hot water production, and ML-based optimization for household demand forecasting is understudied.

Method: A composite approach using ML (LightGBM, LSTM variants) and isolation forest (iForest) for demand forecasting and anomaly detection, with multi-step feature selection and time-series analysis.

Result: LightGBM outperformed LSTM variants with RMSE improvements up to 9.37% and R² values of 0.748-0.983. iForest achieved an F1-score of 0.87 with a 5.2% false alarm rate.

Conclusion: The proposed method demonstrates strong generalization across household types, making it suitable for real-world heat pump deployments.

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [76] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: A novel FSCIL method, Tri-WE, addresses catastrophic forgetting and overfitting by interpolating base, previous, and current models in weight-space and using amplified data distillation.


<details>
  <summary>Details</summary>
Motivation: Fixed feature extractors in FSCIL limit adaptability to new classes, leading to forgetting and overfitting.

Method: Proposes Tri-WE for weight-space ensemble and amplified data knowledge distillation.

Result: Achieves state-of-the-art performance on miniImageNet, CUB200, and CIFAR100.

Conclusion: Tri-WE effectively balances adaptability and stability in FSCIL, outperforming existing methods.

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [77] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: Bohdi is a synthetic-data-only framework for heterogeneous LLM fusion, addressing limitations of existing methods by dynamically adjusting domain exploration and data allocation via hierarchical multi-armed bandit and Introspection-Rebirth mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLM fusion rely on limited real data and fixed domain allocations, leading to incomplete knowledge acquisition and capability imbalance.

Method: Bohdi uses a hierarchical tree for domain organization, multi-model collaboration for data generation, and DynaBranches for adaptive sampling. The Introspection-Rebirth mechanism tracks capability shifts.

Result: Bohdi outperforms baselines, shows higher data efficiency, and eliminates capability imbalance in target LLMs.

Conclusion: Bohdi effectively integrates diverse knowledge from source LLMs with adaptive and efficient fusion, validated by benchmark results.

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [78] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Main category: cs.LG

TL;DR: UNIMATE is a unified model for mechanical metamaterial design, addressing all three key modalities (3D topology, density condition, mechanical property) simultaneously, outperforming baselines in tasks like topology generation and property prediction.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for mechanical metamaterials often overlook one of the three key modalities, creating a gap in comprehensive design.

Method: UNIMATE combines a modality alignment module and a synergetic diffusion generation module to handle all three modalities together.

Result: UNIMATE outperforms baselines by up to 80.2% in topology generation, 5.1% in property prediction, and 50.2% in condition confirmation.

Conclusion: UNIMATE bridges the gap in comprehensive mechanical metamaterial design and is open-sourced for further research.

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [79] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: MadaKV is a modality-adaptive KV cache eviction strategy for MLLMs, improving efficiency in long-context inference by addressing modality-specific disparities.


<details>
  <summary>Details</summary>
Motivation: Traditional KV cache eviction methods are unimodal and fail to capture modality-specific information, leading to suboptimal performance in multimodal scenarios.

Method: MadaKV uses modality preference adaptation and hierarchical compression compensation to dynamically retain critical tokens.

Result: Achieves 1.3 to 1.5 times improvement in KV cache memory and decoding latency while maintaining accuracy.

Conclusion: MadaKV outperforms existing methods, as demonstrated by experiments on MLLMs and the MileBench benchmark.

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [80] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Main category: cs.LG

TL;DR: GrIDDD introduces a size-adaptive graph diffusion model for molecular generation, overcoming limitations of fixed graph sizes in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models cannot adapt graph size during generation, limiting their use in property-driven molecular design.

Method: Reformulates noising/denoising processes to allow dynamic node insertion/deletion, creating the GrIDDD model.

Result: GrIDDD matches/exceeds existing models in performance and shows competitive results in molecular optimization.

Conclusion: GrIDDD enables size-adaptive molecular generation, advancing graph diffusion applications.

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [81] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Main category: cs.LG

TL;DR: CheMeleon, a molecular foundation model, outperforms baselines in predicting molecular properties using deterministic descriptors and a Directed Message-Passing Neural Network.


<details>
  <summary>Details</summary>
Motivation: To enable accurate molecular property prediction using low-noise descriptors, avoiding noisy experimental data or biased simulations.

Method: Pre-trained on Mordred package descriptors using a Directed Message-Passing Neural Network.

Result: Achieves 79% win rate on Polaris tasks and 97% on MoleculeACE, outperforming baselines like Random Forest and Chemprop.

Conclusion: Descriptor-based pre-training shows promise for scalable molecular property prediction, though challenges like activity cliffs remain.

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [82] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: DeepJ, a graph convolutional transformer model, improves EHR analysis by capturing intra- and inter-encounter medical event interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based EHR models fail to capture interactions across longitudinal encounters and temporal dependencies, limiting their effectiveness.

Method: DeepJ combines graph convolutional networks with transformers and differentiable graph pooling to model medical event interactions dynamically.

Result: DeepJ outperforms five state-of-the-art baselines, identifying key event clusters for better patient outcome prediction.

Conclusion: DeepJ enhances interpretability and patient risk stratification, demonstrating its potential for advanced EHR analysis.

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [83] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Main category: cs.LG

TL;DR: A new algorithm in the BROAD-OMD framework improves regret bounds for Vickrey first-price auctions using binary feedback and machine learning predictions.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the increasing importance of first-price auctions and the potential of machine learning to predict highest competing bids.

Method: The paper introduces an algorithm within the BROAD-OMD framework that uses past data and predictions of the highest competing bid.

Result: The algorithm achieves zero regret with accurate predictions and a bounded regret of O(T^(3/4) * Vt^(1/4)) under normality conditions.

Conclusion: The proposed algorithm enhances performance in first-price auctions by leveraging machine learning predictions, achieving optimal regret bounds.

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [84] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Main category: cs.LG

TL;DR: DHEAL-COM project develops digital health solutions using machine learning to analyze multi-modal data for predictive insights in community medicine.


<details>
  <summary>Details</summary>
Motivation: To advance proximity healthcare through digital solutions and data-driven insights.

Method: An automated pipeline combining unsupervised and supervised machine learning methods for data ingestion, prediction, and feature interpretation.

Result: Predictive results and interpretable models from multi-modal health data.

Conclusion: The pipeline enhances digital health solutions by enabling data-driven predictions and interpretations in community medicine.

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [85] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: Proposes FedWB, a model fusion algorithm using Wasserstein barycenters for global DNN training in distributed settings, and extends it to HFRL with DQNs in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training global models in distributed architectures and heterogeneous environments efficiently.

Method: Uses Wasserstein barycenters for aggregating local DNN weights (FedWB) and applies it to HFRL with DQNs in varied CartPole environments.

Result: Develops a global DQN effective across all heterogeneous environments through periodic aggregation.

Conclusion: FedWB and its HFRL extension demonstrate effective model fusion and generalization in distributed and heterogeneous settings.

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [86] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Main category: cs.LG

TL;DR: A model using XGBoost for in-field calibration of low-cost air quality sensors to improve accuracy and coverage.


<details>
  <summary>Details</summary>
Motivation: High-cost sensors limit spatial coverage, while low-cost sensors suffer from drift due to environmental and manufacturing issues.

Method: Uses XGBoost ensemble learning to calibrate sensors by consolidating data from neighboring sensors.

Result: Reduces reliance on individual sensor accuracy and enhances generalization across locations.

Conclusion: The proposed model effectively addresses the limitations of low-cost sensors, improving air quality monitoring.

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [87] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: The paper examines the misalignment between neural network uncertainty estimates and human-perceived uncertainty, finding weak correlations. It suggests integrating human-derived labels to improve calibration.


<details>
  <summary>Details</summary>
Motivation: Neural networks often produce overconfident predictions, lacking reliable uncertainty estimates, which is problematic for critical applications.

Method: The study uses three vision benchmarks with human disagreement and crowdsourced confidence to compare NN uncertainty with human uncertainty.

Result: Current methods weakly align with human intuition, but incorporating human-derived soft labels improves calibration without losing accuracy.

Conclusion: There's a gap between model and human uncertainty, and leveraging human insights can enhance AI trustworthiness.

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [88] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Main category: cs.LG

TL;DR: Boundary-enforced Rectified Flow Model (Boundary RF Model) improves generative modeling by enforcing boundary conditions, outperforming vanilla RF with significant FID score improvements.


<details>
  <summary>Details</summary>
Motivation: The vanilla RF model's unconstrained neural network for velocity field learning fails to satisfy boundary conditions, causing inaccuracies, especially during stochastic sampling.

Method: Proposes Boundary RF Model, enforcing boundary conditions with minimal code changes.

Result: Achieves 8.01% and 8.98% FID score improvements on ImageNet for ODE and SDE sampling, respectively.

Conclusion: Boundary RF Model effectively addresses boundary condition issues, enhancing generative modeling performance.

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [89] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Main category: cs.LG

TL;DR: POLCA identifies hidden training breakthroughs by decomposing loss changes into interpretable clusters, revealing unsupervised interpretability.


<details>
  <summary>Details</summary>
Motivation: Visible discontinuities in loss curves indicate breakthroughs, but many are hidden due to scalar loss metrics. This paper aims to uncover these hidden transitions for deeper learning insights.

Method: Introduces POLCA, a method to decompose loss changes along low-rank training subspaces, identifying clusters of samples with similar loss dynamics.

Result: Validated on synthetic and natural language tasks, POLCA successfully recovers interpretable clusters representing model breakthroughs.

Conclusion: POLCA reveals hidden phase transitions, offering a tool for unsupervised interpretability in model training.

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [90] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Main category: cs.LG

TL;DR: A machine learning prototype analyzes synthetic job listing data to predict salaries, group job roles, and identify market trends using regression, classification, clustering, and NLP.


<details>
  <summary>Details</summary>
Motivation: To uncover key job market dynamics and provide insights for job seekers, employers, and researchers.

Method: Uses regression for salary prediction, classification for job titles, clustering for grouping roles, and NLP for text feature extraction.

Result: Identified significant salary and job role factors, and distinct job clusters.

Conclusion: The methodology offers a transferable framework for job market analysis, though based on synthetic data.

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [91] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SHRED models, using RNNs and MLPs, predict chaotic systems from sparse sensor data. Improved as T-SHRED with transformers and SINDy attention, enhancing performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and interpretability of SHRED models for system identification and forecasting from sparse sensor data.

Method: Enhances SHRED with transformers (T-SHRED) and introduces a SINDy attention mechanism for symbolic regression in the latent space.

Result: T-SHRED with SINDy attention accurately predicts future states and learns interpretable symbolic models across various datasets.

Conclusion: T-SHRED with SINDy attention improves prediction accuracy and model interpretability, making it effective for diverse dynamical systems.

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [92] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: Fractional Reasoning is a framework for dynamically adjusting reasoning intensity in LLMs at inference time, improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods apply uniform reasoning across inputs, ignoring varying problem complexities.

Method: Extracts and scales latent steering vectors for deeper reasoning, enabling tailored reasoning per input.

Result: Improves performance on GSM8K, MATH500, and GPQA benchmarks.

Conclusion: Fractional Reasoning offers flexible, effective control over reasoning depth, enhancing LLM performance.

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [93] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework to study how contrastive examples impact active learning, focusing on sample complexity and revealing a connection to self-directed learning.


<details>
  <summary>Details</summary>
Motivation: To understand how contrastive examples (pairs of similar instances with different labels) can improve machine learning by explaining label differences and reducing sample complexity.

Method: A theoretical framework is developed to analyze the effect of contrastive examples on active learners, with experiments on geometric concept classes and Boolean functions.

Result: The study shows how contrastive examples influence sample complexity and uncovers a link between learning from contrastive examples and self-directed learning.

Conclusion: Contrastive examples can enhance learning efficiency by reducing sample complexity, and their impact is theoretically connected to self-directed learning.

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [94] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Main category: cs.LG

TL;DR: A knowledge-guided graph neural network framework is proposed to predict soil GHG fluxes by integrating process-based models and GNNs, addressing data scarcity in agriculture.


<details>
  <summary>Details</summary>
Motivation: Precision soil GHG flux prediction is crucial for sustainable agriculture, but data scarcity hinders machine learning applications.

Method: The framework combines an agricultural process-based model to generate data and a GNN with autoencoder for feature extraction and correlation integration.

Result: The approach outperforms baseline methods in accuracy and stability for predicting fertilization-oriented soil GHG fluxes.

Conclusion: The proposed framework effectively addresses data scarcity and improves GHG flux prediction, aiding sustainable agriculture.

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [95] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Main category: cs.LG

TL;DR: TrajDiff is a novel framework for trajectory similarity computation, addressing semantic gaps, noise, and global ranking challenges, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for accurate trajectory similarity computation due to the proliferation of location-tracking technologies and the limitations of existing methods.

Method: Uses semantic alignment, noise-robust pre-training, and ranking-aware regularization to improve trajectory embeddings and similarity computation.

Result: TrajDiff achieves a 33.38% average HR@1 gain across three datasets, outperforming state-of-the-art baselines.

Conclusion: TrajDiff effectively addresses key challenges in trajectory similarity computation and demonstrates superior performance.

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [96] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Main category: cs.LG

TL;DR: An interpretable ML model was developed to predict 28-day mortality in ICU patients with DM and AF, outperforming complex models with key predictors like RAS, age, bilirubin, and extubation.


<details>
  <summary>Details</summary>
Motivation: Limited models exist for predicting mortality in high-risk ICU patients with both DM and AF, necessitating an interpretable ML solution.

Method: Retrospective analysis of 1,535 ICU patients from MIMIC-IV, using feature selection and ML models (e.g., logistic regression) with cross-validation and SMOTE oversampling.

Result: Logistic regression performed best (AUROC: 0.825), with key predictors identified. ALE plots revealed non-linear risk patterns.

Conclusion: The model provides accurate, interpretable risk prediction for early ICU triage in DM and AF patients.

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [97] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Main category: cs.LG

TL;DR: A large-scale dataset for instruction-guided vector image editing is introduced, with 270K SVG-text pairs, highlighting challenges in current methods.


<details>
  <summary>Details</summary>
Motivation: To enable training and evaluation of models for editing vector graphics via natural language commands.

Method: Dataset creation involves CLIP-based image pairing and instruction generation using vision-language models.

Result: Initial tests show state-of-the-art models struggle with accurate edits, indicating task difficulty.

Conclusion: The dataset is released to advance research in natural language-driven vector graphic editing.

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [98] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Main category: cs.LG

TL;DR: Pieceformer is a scalable, self-supervised framework for graph similarity in VLSI design, using a hybrid encoder and partitioned training. It reduces MAE by 24.9% and improves clustering accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient knowledge transfer and reuse in VLSI design, reducing engineering effort and turnaround time.

Method: Hybrid message-passing and graph transformer encoder with a linear transformer backbone and partitioned training pipeline for scalability.

Result: 24.9% reduction in MAE, correct clustering of all real-world design groups, and 89% runtime reduction in a partitioning task.

Conclusion: Pieceformer is effective for scalable, unbiased design reuse in modern VLSI systems.

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [99] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: Accelerating neural speech transcription by sparsifying early encoder layers in transformer models, achieving up to 1.6x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Speech signals are highly compressible, and leveraging transformer interpretability can optimize neural speech processing efficiency.

Method: Systematic architecture search over sparsification stages and compression ratios using Whisper models.

Result: Best solutions sparsify hidden states to 40-60% early, achieving 1.6x runtime acceleration with <1% accuracy drop.

Conclusion: Early-stage sparsification in transformers can significantly speed up speech transcription without fine-tuning.

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [100] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: A novel FL framework using Power-Norm Cosine Similarity (PNCS) improves client selection for model aggregation, addressing non-IID data challenges and enhancing convergence speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing FL approaches often overlook gradient correlations between clients, which is problematic in data heterogeneity scenarios.

Method: Proposes PNCS to capture higher-order gradient moments and introduces a client selection algorithm using a selection history queue.

Result: Experiments with a VGG16 model show consistent improvements over state-of-the-art methods.

Conclusion: The PNCS-based framework effectively addresses non-IID data challenges, improving FL performance.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [101] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Main category: cs.LG

TL;DR: The paper extends bandit learning to two-sided reward uncertainty in matching markets, showing the Extended Gale-Shapley algorithm outperforms the standard one in achieving stable matchings under incomplete information. It achieves logarithmic pessimal stable regret and adapts to decentralized settings with minimal regret increase. A novel lower bound for binary stable regret is also established.


<details>
  <summary>Details</summary>
Motivation: To address the gap in prior research focused on single-sided uncertainty in matching markets, this study explores two-sided reward uncertainty and its impact on stable matchings.

Method: The paper leverages the Extended Gale-Shapley algorithm, adapting it for centralized and decentralized settings, and analyzes its performance in terms of stable regret.

Result: The centralized algorithm achieves logarithmic pessimal stable regret, while the decentralized version incurs only a constant regret increase. A novel lower bound for binary stable regret is also derived.

Conclusion: The study highlights the effectiveness of the Extended GS algorithm in handling two-sided uncertainty and provides insights into the complexity of stable matching with bandit feedback.

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [102] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Main category: cs.LG

TL;DR: Diffusion models struggle with long-tailed data distributions, producing poor-quality tail-class samples due to overlapping latent representations. CORAL, a contrastive regularization method, improves tail-class generation quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-class data often follows long-tailed distributions, where standard diffusion models fail to generate high-quality samples for tail classes. The underlying cause of this degradation is not well understood.

Method: The paper investigates diffusion models on long-tailed datasets, identifying overlapping latent representations as the issue. It proposes CORAL, a contrastive latent alignment framework using supervised contrastive losses to separate latent class representations.

Result: CORAL significantly improves the diversity and visual quality of tail-class samples compared to state-of-the-art methods.

Conclusion: The study highlights the impact of class imbalance on diffusion models and demonstrates CORAL's effectiveness in addressing this issue for better tail-class generation.

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [103] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [104] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: The paper introduces a method to identify critical learning periods in deep neural network training, reducing computational costs and emissions without performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing works confirm critical learning periods but lack precise identification. This work aims to fill that gap and improve training efficiency.

Method: A systematic approach using generalization prediction to pinpoint critical phases, halting resource-intensive techniques beyond these periods.

Result: Achieves up to 59.67% faster training, 59.47% lower CO$_2$ emissions, and 60% cost reduction without performance compromise.

Conclusion: Enhances training dynamics understanding and promotes sustainable, efficient deep learning, especially in resource-constrained settings.

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [105] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: The paper identifies conditions for sparse autoencoders (SAEs) to recover ground truth monosemantic features from polysemantic ones and proposes a reweighting strategy to enhance identifiability when conditions aren't fully met.


<details>
  <summary>Details</summary>
Motivation: To clarify when SAEs can uniquely and accurately recover monosemantic features from polysemantic ones, a gap in current understanding.

Method: Theoretical analysis to derive necessary and sufficient conditions for identifiable SAEs, followed by a reweighting strategy to improve identifiability.

Result: Identified conditions include extreme sparsity, sparse activation, and sufficient hidden dimensions. The reweighted SAE outperforms uniformly weighted ones in feature reconstruction.

Conclusion: The proposed weighted SAE enhances monosemanticity and interpretability, validated by experiments.

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [106] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: LazyEviction reduces KV cache size by 50% while maintaining accuracy in long reasoning tasks by preserving recurring tokens.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods struggle with long reasoning tasks, failing to capture Token Importance Recurrence, leading to unpredictable eviction of critical tokens.

Method: Proposes LazyEviction, a lagged KV eviction framework with Recurrence Interval Tracking and a Maximum Recurrence Interval-Centric Eviction Policy.

Result: LazyEviction outperforms state-of-the-art methods, reducing KV cache size by 50% without compromising accuracy.

Conclusion: Preserving recurring tokens is crucial for knowledge continuity in multi-step reasoning tasks.

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [107] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Main category: cs.LG

TL;DR: AutoHFormer is a hierarchical autoregressive transformer for time series forecasting, addressing temporal causality, scalability, and multi-scale pattern recognition with innovations like hierarchical temporal modeling, dynamic windowed attention, and adaptive temporal encoding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenges of strict temporal causality, sub-quadratic complexity, and multi-scale pattern recognition in time series forecasting.

Method: AutoHFormer uses hierarchical temporal modeling, dynamic windowed attention, and adaptive temporal encoding to achieve efficient and accurate forecasting.

Result: AutoHFormer shows 10.76X faster training, 6.06X memory reduction, and consistent accuracy across long horizons compared to PatchTST.

Conclusion: AutoHFormer sets new benchmarks for efficient and precise time series modeling, with open-source implementation available.

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [108] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Main category: cs.LG

TL;DR: The paper reviews the use of foundation models (FMs) and self-supervised learning (SSL) for brain signal analysis, addressing challenges like noise and variability, and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning is limited by scarce labeled neural data, while SSL can learn from unlabeled data, making it promising for brain signal analysis.

Method: The survey reviews SSL techniques, brain-specific FMs, their adaptation to tasks, and multimodal SSL frameworks, along with evaluation metrics and datasets.

Result: The paper provides a structured understanding of SSL's role in brain signal analysis and highlights its potential for generalizable models.

Conclusion: The work aims to guide researchers in developing brain foundation models using SSL, addressing current challenges and future opportunities.

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [109] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Main category: cs.LG

TL;DR: VRAIL is a bi-level RL framework combining deep learning and RL to learn interpretable weight representations from state features, improving stability and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and training stability in value-based RL by attributing importance to state features and their interactions.

Method: VRAIL uses a two-stage approach: a DL stage to estimate value functions and an RL stage for reward shaping. Estimators are linear or quadratic for feature attribution.

Result: VRAIL improves training stability and convergence in Taxi-v3, identifies meaningful subgoals (e.g., passenger possession), and is model-agnostic.

Conclusion: VRAIL is a general framework for reward shaping that boosts learning performance and interpretability without modifying environments.

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [110] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Main category: cs.LG

TL;DR: A unified framework for tensor decomposition using Riemannian gradient descent (RGD) is proposed, ensuring linear convergence to the ground-truth tensor with polynomial scaling in tensor order.


<details>
  <summary>Details</summary>
Motivation: Tensor decompositions reduce parameters for high-order tensors but face nonconvex optimization challenges. This work aims to provide a scalable and efficient solution.

Method: Leverages canonical tensor decompositions with orthonormal factors, applying RGD on the Stiefel manifold under mild loss function conditions.

Result: Proves linear convergence to the ground-truth tensor with polynomial scaling in tensor order, improving upon existing methods.

Conclusion: The framework offers a scalable and efficient approach for tensor decomposition, with theoretical guarantees for convergence.

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [111] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Main category: cs.LG

TL;DR: A novel multimodal document chunking method using Large Multimodal Models (LMMs) improves RAG systems by handling complex document structures like multi-page tables and embedded figures, outperforming traditional text-based chunking.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based chunking in RAG systems fails to handle complex document structures, such as multi-page tables and embedded figures, necessitating a more robust approach.

Method: The proposed method uses LMMs to process PDF documents in batches, preserving cross-batch context and semantic coherence, enabling accurate handling of complex structures.

Result: Evaluation on a curated dataset shows improved chunk quality and RAG performance, with better accuracy and preservation of document structure compared to traditional methods.

Conclusion: The multimodal approach enhances RAG systems by maintaining semantic coherence and structural integrity, offering superior performance for complex documents.

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [112] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: A framework for lifecycle-aware reproducibility in ML, using structured artifacts to improve transparency and traceability in collaborative projects.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of reproducibility in ML, especially in collaborative eScience projects, where fragmented workflows hinder transparency and adaptability.

Method: Introduces a data-centric framework with six structured artifacts (Dataset, Feature, Workflow, Execution, Asset, Controlled Vocabulary) to formalize relationships between data, code, and decisions.

Result: Demonstrated in a clinical ML use case (glaucoma detection), the framework supports iterative exploration, improves reproducibility, and preserves decision provenance.

Conclusion: The proposed framework enhances reproducibility and traceability in ML workflows, making collaborative experiments more transparent and adaptable.

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [113] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: CRIA is a novel EEG pre-training framework that integrates temporal, spectral, and spatial views using cross-attention and masking strategies, outperforming existing methods in classification and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing EEG pre-training methods lack the ability to capture complex interactions across multiple views (temporal, spectral, spatial), limiting representation expressiveness and generalization.

Method: CRIA employs variable-length and variable-channel coding, cross-attention for feature fusion, and a viewpoint masking pre-training scheme based on the information bottleneck principle.

Result: CRIA achieves 57.02% balanced accuracy for multi-class event classification and 80.03% for anomaly detection, surpassing existing methods.

Conclusion: CRIA demonstrates strong generalization and effectiveness in EEG representation learning by leveraging cross-view interactions.

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [114] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: The paper introduces the first interval universal approximation (IUA) theorem for floating-point neural networks, proving their ability to perfectly approximate the direct image map of any rounded target function, highlighting differences from the real-valued setting.


<details>
  <summary>Details</summary>
Motivation: The classical UA and IUA theorems assume infinite precision, but real-world neural networks use finite-precision floating-point numbers. This gap raises the question of whether the IUA theorem holds in the floating-point setting.

Method: The paper develops an IUA theorem specifically for floating-point neural networks, analyzing their ability to approximate the direct image map of a function under finite-precision constraints.

Result: The theorem confirms that floating-point neural networks can perfectly capture the direct image map of any rounded target function, with no expressiveness limits. It also reveals material differences from the real-valued setting.

Conclusion: The findings imply the existence of provably robust floating-point neural networks and computational completeness for certain floating-point programs, bridging theory and practical implementation.

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [115] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: The paper introduces a lightweight RL-driven deep unfolding network (RLDDU-Net) to improve the WMMSE precoding algorithm for massive MU-MIMO OFDM systems, addressing imperfect CSI and high complexity.


<details>
  <summary>Details</summary>
Motivation: Practical deployment of WMMSE precoding in massive MU-MIMO OFDM systems is limited by perfect CSI assumptions and high computational demands.

Method: Developed a wideband stochastic WMMSE (SWMMSE) algorithm for imperfect CSI, then proposed RLDDU-Net, mapping SWMMSE iterations to network layers with RL for adaptive adjustments.

Result: RLDDU-Net outperforms baselines in ergodic weighted sum-rate (EWSR) performance under imperfect CSI, with better computational and convergence efficiency.

Conclusion: RLDDU-Net effectively addresses WMMSE limitations, offering improved performance and efficiency for massive MU-MIMO OFDM systems.

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [116] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: Proposes CAAC, a CRL algorithm for dynamic user priority and power allocation in 6G networks, outperforming baselines in energy efficiency and QoS.


<details>
  <summary>Details</summary>
Motivation: Traditional WMMSE precoding lacks flexibility for user-specific QoS and time-varying channels, necessitating adaptive solutions.

Method: CAAC integrates CSSCA for policy optimization and uses attention-enhanced Q-networks for efficient learning without prior model knowledge.

Result: CAAC achieves better energy efficiency and QoS satisfaction than existing methods.

Conclusion: CAAC offers a flexible and efficient solution for dynamic resource allocation in 6G networks.

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [117] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: The paper highlights vulnerabilities in current safety alignment methods for AI, showing minor latent shifts can trigger unsafe responses. It introduces a probing method (ASA) and a training strategy (LAPT) to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods are shallow, focusing on surface-level behaviors, leaving models vulnerable to latent shifts that re-trigger harmful behaviors.

Method: The authors propose a probing method (ASA) to measure latent sensitivity and introduce LAPT, a fine-tuning strategy injecting controlled perturbations into hidden representations.

Result: LAPT improves alignment robustness without compromising general capabilities, revealing flaws in current paradigms.

Conclusion: The study calls for representation-level training strategies to enhance safety alignment beyond surface-level supervision.

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [118] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: cs.LG

TL;DR: The paper introduces B2P-GL, a two-stage framework for brain disorder diagnosis, combining brain atlas knowledge and population graph modeling to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based methods overlook atlas information and site/phenotype variability, limiting diagnostic performance.

Method: B2P-GL uses GPT-4 for brain representation learning and phenotypic data for population graph modeling in two stages.

Result: Outperforms state-of-the-art methods on ABIDE I, ADHD-200, and Rest-meta-MDD datasets.

Conclusion: B2P-GL provides a reliable, personalized approach for brain disorder diagnosis, enhancing clinical applicability.

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [119] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Main category: cs.LG

TL;DR: A novel graph rewiring method using spectrum-preserving sparsification to mitigate over-squashing in GNNs, balancing connectivity and property preservation.


<details>
  <summary>Details</summary>
Motivation: Existing graph rewiring techniques often fail to preserve critical spectral properties and introduce computational overhead or over-smoothing.

Method: Proposes a spectrum-preserving graph sparsification method to enhance connectivity while maintaining sparsity and spectral properties.

Result: Outperforms baselines in classification accuracy and Laplacian spectrum retention.

Conclusion: The method effectively reduces over-squashing while preserving key graph properties.

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [120] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Main category: cs.LG

TL;DR: KD from large teacher to smaller student models reduces memorization risks while lowering computational costs.


<details>
  <summary>Details</summary>
Motivation: Address privacy and security concerns by studying how KD affects memorization in fine-tuned models.

Method: Explore different KD methods' influence on memorization when distilling fine-tuned teacher models into smaller students.

Result: KD significantly reduces memorization risks compared to standard fine-tuning.

Conclusion: KD offers a practical solution to mitigate memorization risks while maintaining efficiency.

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [121] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.LG

TL;DR: The paper proposes using a Conditional Wasserstein Generative Adversarial Network (CWGAN) to generate synthetic EEG signals for ALS patients, addressing data scarcity and class imbalance in training machine learning classifiers.


<details>
  <summary>Details</summary>
Motivation: High-quality EEG data for ALS patients is scarce, and severe class imbalance between ALS and healthy controls hinders reliable classifier training.

Method: A CWGAN is trained on a private EEG dataset to learn ALS EEG signal distributions and generate synthetic samples. Preprocessing, normalization, and stable training hyperparameters are detailed.

Result: Generated signals mimic real ALS EEG patterns, and CWGAN training converges with stable loss curves. Synthetic data shows potential for improving classifier training.

Conclusion: Synthetic EEG signals can mitigate class imbalance, enhance ALS detection accuracy, and facilitate data sharing for diagnostic models.

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [122] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: The paper explores the challenge of transcribing Finnish rap lyrics using AI, comparing Faster Whisperer and YouTube's speech-to-text, with Mc Timo's lyrics as the reference.


<details>
  <summary>Details</summary>
Motivation: Finnish is complex, and artistic use adds difficulty, making it a fun yet challenging test for AI transcription.

Method: Compare Faster Whisperer and YouTube's speech-to-text against Mc Timo's rap lyrics, measuring errors due to music and artistic pronunciation.

Result: Hallucination and mishearing levels of AI are measured against the original lyrics.

Conclusion: The informal error function suffices to evaluate AI performance in this unique challenge.

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [123] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Main category: cs.LG

TL;DR: The paper explores the challenges of ambiguity in auto-regressive models, proposing a method inspired by cognitive science to improve high-ambiguity predictions.


<details>
  <summary>Details</summary>
Motivation: Auto-regressive models struggle with high-ambiguity predictions due to computational intractability, a limitation recognized in cognitive science.

Method: Introduces MetaHMM, a synthetic benchmark, and converts pre-trained models into Monte Carlo predictors to decouple task inference from token prediction.

Result: Transformers struggle with high-ambiguity predictions, but the proposed method shows gains in ambiguous contexts.

Conclusion: The method improves capacity allocation and scalable inference in ambiguous settings, though challenges persist.

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [124] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Main category: cs.LG

TL;DR: POST enables private tuning of soft prompts on small models and transfers them to larger LLMs, reducing costs and preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Soft prompts are efficient but tied to specific LLMs, raising computational and privacy concerns.

Method: POST uses knowledge distillation, local tuning (optionally with differential privacy), and public dataset transfer.

Result: POST reduces costs, preserves privacy, and effectively transfers high-utility soft prompts.

Conclusion: POST addresses efficiency and privacy issues in soft prompt tuning for LLMs.

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [125] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Main category: cs.LG

TL;DR: The paper introduces ENN-GFN-Enhanced, integrating epistemic neural networks (ENN) with GFlowNets for better uncertainty-driven exploration and trajectory identification.


<details>
  <summary>Details</summary>
Motivation: Efficiently identifying optimal trajectories in GFlowNets requires prioritizing exploration in under-learned reward regions, necessitating uncertainty awareness.

Method: Combines ENN with GFlowNets for improved joint predictions and uncertainty quantification, tested in grid environments and sequence generation.

Result: ENN-GFN-Enhanced outperforms baseline GFlowNets, showing efficacy and efficiency in exploration and trajectory identification.

Conclusion: The integration of ENN with GFlowNets enhances exploration and trajectory optimization, validated in diverse settings.

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [126] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Main category: cs.LG

TL;DR: Proposes a novel ML technique using coupled JEPAs to optimize radio resource management in a control system, reducing transmit power by 50% without compromising control performance.


<details>
  <summary>Details</summary>
Motivation: To optimize radio resource management between a remote controller and its device without degrading control task performance.

Method: Uses two coupled JEPAs (control and wireless) to model dynamics, trains a deep RL algorithm for control policy, and a power predictor for scheduling.

Result: Reduces transmit power by over 50% while maintaining control performance comparable to baselines.

Conclusion: The approach effectively minimizes radio resource usage by leveraging latent space predictions, demonstrating significant efficiency gains.

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [127] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: First token-level watermarking for autoregressive image models, addressing reverse cycle-consistency and robustness to attacks.


<details>
  <summary>Details</summary>
Motivation: Track provenance of generative model outputs, especially autoregressive image models, which lack token-level watermarking solutions.

Method: Adapt language model watermarking, improve reverse cycle-consistency via tokenizer-detokenizer finetuning, and add a watermark synchronization layer.

Result: Reliable and robust watermark detection with theoretically grounded p-values.

Conclusion: The approach successfully enables watermarking for autoregressive image models, addressing key challenges.

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [128] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Main category: cs.LG

TL;DR: BLANCE is a hybrid Bayesian framework integrating batch data and LM-derived expert knowledge for causal discovery, outperforming prior methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in causal discovery due to batch data arrival and scarce expert knowledge, while mitigating LM issues like hallucinations and biases.

Method: Uses a Bayesian framework to combine sequential batch data with noisy LM knowledge, shifting from DAG to PAG representation and employing adaptive edge queries.

Result: Outperforms prior work in structural accuracy and extends to Bayesian parameter estimation, showing robustness to LM noise.

Conclusion: BLANCE effectively bridges gaps in causal discovery by integrating data and LM knowledge adaptively, offering improved accuracy and robustness.

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [129] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: DnD introduces a prompt-conditioned parameter generator for LLMs, eliminating per-task training by mapping prompts to LoRA weight updates, achieving significant efficiency and performance gains.


<details>
  <summary>Details</summary>
Motivation: To reduce the overhead and time required for fine-tuning LLMs on every downstream task by leveraging prompt-conditioned parameter generation.

Method: Uses a lightweight text encoder and hyper-convolutional decoder to generate LoRA matrices from task prompts, trained on diverse prompt-checkpoint pairs.

Result: Achieves up to 12,000× lower overhead than full fine-tuning, 30% performance gains over LoRA, and robust cross-domain generalization.

Conclusion: Prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly customizing LLMs.

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [130] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Main category: cs.LG

TL;DR: The paper proposes a sequential Bayesian experimental design (BED) method to optimize MRI acquisition by adaptively selecting the most informative measurements, balancing speed and image quality.


<details>
  <summary>Details</summary>
Motivation: To accelerate MRI acquisition times without degrading image quality, requiring a balance between under-sampling and high-fidelity reconstruction.

Method: Uses sequential BED with gradient-based optimization and diffusion-based generative models for adaptive measurement selection.

Result: Demonstrates versatility and performance in optimizing MRI acquisitions for reconstruction and analysis tasks.

Conclusion: The approach effectively balances speed and quality, enhancing MRI utility in clinical settings.

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [131] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Main category: cs.LG

TL;DR: The paper explores router architectures in Mixture of Experts (MoE) models, comparing six variants to address load imbalance and accuracy issues, with MLP-Hadamard showing unique sparse routing capabilities.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and performance of large language models by optimizing the router module in MoE architectures, addressing issues like load imbalance and reduced accuracy.

Method: Designed and implemented six router variants (Linear, Attention, MLP, Hybrid, Hash, MLP-Hadamard) in Transformer models, evaluated using BERT and Qwen1.5-MoE on metrics like parameter efficiency, inference latency, and expert utilization.

Result: Linear routers are faster, MLP and Attention routers are more expressive, and MLP-Hadamard excels in structured, sparse routing. Custom routers were successfully fine-tuned in Qwen1.5-MoE.

Conclusion: The study provides a comparative analysis of MoE router designs, offering insights for optimizing large-scale model deployment.

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [132] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: PINNs solve PDEs using neural networks. This paper explores using influence functions (from XAI) to improve training data sampling, enhancing PINN accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the training efficiency and prediction accuracy of PINNs by leveraging explainable AI methods like influence functions for targeted data sampling.

Method: Apply influence function-based sampling approaches to select training data points for PINNs, focusing on their impact on model performance.

Result: Targeted resampling using influence functions enhances prediction accuracy in PINNs.

Conclusion: Influence function-based sampling is a practical XAI method to improve PINN training, demonstrating its potential in scientific machine learning.

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [133] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: The paper analyzes the Online Bookmaking problem, showing the bookmaker's optimal loss is the largest root of a polynomial. It provides an efficient algorithm for optimal betting strategy, linking regret to Hermite polynomials and using Bellman-Pareto frontier for multi-criteria optimization.


<details>
  <summary>Details</summary>
Motivation: To maximize profit and mitigate loss in dynamic betting scenarios, the paper aims to derive optimal strategies for bookmakers facing gamblers of varying skill levels.

Method: The study characterizes the bookmaker's optimal loss via polynomial roots, develops an efficient algorithm for strategy computation, and uses the Bellman-Pareto frontier for dynamic programming and multi-criteria optimization.

Result: The bookmaker's optimal loss is the largest root of a polynomial. The algorithm achieves optimal loss against optimal gamblers and reduces loss for suboptimal gamblers.

Conclusion: Bookmakers can balance fairness and financial risk, with the solution revealing connections to Hermite polynomials and subgame perfect Nash equilibrium.

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [134] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Main category: cs.LG

TL;DR: A novel deep learning model using multi-scale CNNs for EEG-based emotion recognition outperforms the state-of-the-art TSception model.


<details>
  <summary>Details</summary>
Motivation: To develop a deep learning model for real-life EEG-based emotion recognition, leveraging advancements in EEG technology and machine learning.

Method: Proposes multi-scale CNNs with feature extraction kernels of varying ratios and a new kernel type that learns from four brain areas.

Result: The model consistently surpasses the TSception model in predicting valence, arousal, and dominance scores across multiple metrics.

Conclusion: The approach demonstrates superior performance in EEG-based emotion recognition, suitable for real-life applications.

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [135] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Main category: cs.LG

TL;DR: TensorGuide improves LoRA by using a tensor-train-guided framework to create correlated low-rank matrices, enhancing expressivity and efficiency without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA's independent optimization of low-rank matrices limits expressivity and generalization, while classical TT decomposition fails to improve efficiency or performance.

Method: TensorGuide generates two correlated low-rank LoRA matrices via a unified TT structure with controlled Gaussian noise, enabling structured adaptations.

Result: TensorGuide outperforms standard LoRA and TT-LoRA in accuracy and scalability on benchmarks like quantum dot classification and GPT-2 fine-tuning.

Conclusion: TensorGuide enhances LoRA's expressivity, generalization, and parameter efficiency, validated theoretically and experimentally.

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [136] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: A new TTS architecture improves multilingual accent accuracy and emotion modeling for Hindi and Indian English, outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: Current TTS systems struggle with multilingual accents and context-relevant emotions due to cultural nuances.

Method: Extends Parler-TTS with a hybrid encoder-decoder, culture-sensitive emotion layers, and dynamic accent code switching.

Result: 23.7% accent accuracy improvement, 85.3% emotion recognition, and 4.2/5 MOS for cultural correctness.

Conclusion: The system advances cross-lingual synthesis, with applications in EdTech and accessibility.

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [137] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Main category: cs.LG

TL;DR: PITA introduces a novel framework combining annealing and diffusion smoothing for efficient sampling from unnormalized densities, enabling equilibrium sampling of complex systems with fewer evaluations.


<details>
  <summary>Details</summary>
Motivation: Efficient sampling from unnormalized densities is a core challenge in scientific applications, but existing diffusion-based samplers fail at scale.

Method: PITA combines annealing of the Boltzmann distribution and diffusion smoothing, training diffusion models sequentially from high to low temperatures and using inference-time annealing with a novel Feynman-Kac PDE and Sequential Monte Carlo.

Result: PITA achieves equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with significantly fewer energy function evaluations.

Conclusion: PITA advances diffusion-based sampling by enabling scalable and efficient sampling of complex systems, demonstrated through empirical success.

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [138] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: Model merging combines expert models but faces diminishing returns due to rank collapse in task vector space. Subspace Boosting mitigates this, improving merging efficacy by over 10% for up to 20 models.


<details>
  <summary>Details</summary>
Motivation: To address diminishing performance gains in merging multiple expert models by analyzing and mitigating rank collapse in task vector space.

Method: Introduces Subspace Boosting, which maintains task vector ranks via singular value decomposition, and uses Higher-Order Generalized SVD to quantify task similarity.

Result: Subspace Boosting improves merging efficacy by over 10% for up to 20 models on vision benchmarks.

Conclusion: Subspace Boosting effectively mitigates rank collapse, enhancing model merging performance and providing interpretable task similarity insights.

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [139] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: The paper introduces 'anomaly signatures' to improve interpretability in machine learning-based anomaly detection by highlighting contributing features.


<details>
  <summary>Details</summary>
Motivation: Machine learning outputs, especially in anomaly detection, are often opaque, leaving users to independently analyze why events are flagged as anomalies.

Method: The authors propose the concept of 'anomaly signatures' to highlight the features influencing anomaly detection decisions.

Result: The approach aims to make anomaly detection more interpretable by revealing the contributing factors behind flagged events.

Conclusion: Anomaly signatures enhance interpretability in machine learning, aiding users in understanding anomaly detection outputs.

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [140] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: RCP1 introduces a single-sample robust conformal prediction method, reducing computational cost while maintaining robustness and smaller prediction sets compared to SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Current smoothing-based RCP methods are computationally expensive due to requiring many model forward passes per input.

Method: Proposes RCP1, which uses a single forward pass on a randomly perturbed input and certifies the conformal prediction procedure itself.

Result: RCP1 achieves robust prediction sets with smaller average size compared to methods requiring many passes.

Conclusion: RCP1 is a computationally efficient, robust, and versatile approach applicable to classification and regression tasks.

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [141] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Main category: cs.LG

TL;DR: The paper introduces the Beta kernel for Bayesian optimization, addressing limitations of Matérn and RBF kernels in bounded domains, and demonstrates its superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing kernels like Matérn and RBF lack assumptions about function domains, limiting their effectiveness in bounded domains.

Method: The Beta kernel, based on Beta distribution density functions, is proposed to model functions on bounded domains. Its spectral properties are analyzed.

Result: Empirical evidence shows the Beta kernel's exponential eigendecay rate and superior performance in optimization tasks, especially near hypercube boundaries.

Conclusion: The Beta kernel outperforms traditional kernels in bounded domains, proving robust for optimization and model compression tasks.

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [142] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Main category: cs.LG

TL;DR: Proposes an energy-based transfer learning method to improve sample efficiency in reinforcement learning by selectively using teacher guidance in familiar states.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with sample efficiency, especially in multi-task or continual learning. Transferring knowledge from a teacher policy can help but may be sub-optimal if tasks differ too much.

Method: Uses energy-based transfer learning with out-of-distribution detection to selectively apply teacher guidance in states within its training distribution.

Result: Theoretical proof that energy scores reflect state-visitation density, with empirical improvements in sample efficiency and performance in single- and multi-task settings.

Conclusion: The method effectively improves sample efficiency by ensuring teacher guidance is only used where reliable, enhancing performance in diverse tasks.

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [143] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: FLAME introduces a federated learning framework using Sparse Mixture-of-Experts (SMoE) to avoid performance loss from LoRA compression, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA compression methods lead to suboptimal performance due to information loss, prompting the need for a better solution.

Method: FLAME uses SMoE architecture, retaining full LoRA matrices and varying activated experts per client, with rescaling and activation-aware aggregation to address challenges.

Result: FLAME consistently outperforms existing methods in diverse computational settings.

Conclusion: FLAME provides a robust, effective solution for resource-adaptive federated learning.

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [144] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: A three-step RL-based BEMS using clustering, forecasting, and constrained policy learning reduces costs by 15%, ensures safety, and adapts to new buildings without retraining.


<details>
  <summary>Details</summary>
Motivation: Address scalability, adaptability, and safety challenges in building energy management due to rising energy demands and renewable integration.

Method: 1. Cluster non-shiftable loads for policy generalization. 2. Use LSTM forecasting for dynamic responsiveness. 3. Apply action masking for safe operation.

Result: 15% cost reduction, stable environmental performance, quick adaptation to new buildings, and resilience to tariff changes.

Conclusion: The framework is scalable, robust, and cost-effective for sustainable energy management in buildings.

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [145] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Main category: cs.LG

TL;DR: A novel RL framework reparameterizes actions as distribution parameters, introducing DPPG for lower variance gradients and DPAC, a practical algorithm outperforming TD3 in continuous control tasks.


<details>
  <summary>Details</summary>
Motivation: To redefine the agent-environment boundary by treating distribution parameters as actions, enabling continuous action spaces regardless of original action types.

Method: Developed DPPG for lower variance gradients and DPAC, an actor-critic algorithm, with ICL to enhance critic learning.

Result: DPAC outperforms TD3 in MuJoCo tasks and shows competitive performance in discretized action spaces.

Conclusion: The framework and DPAC algorithm effectively generalize RL across action types, improving performance in continuous control tasks.

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [146] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Main category: cs.LG

TL;DR: A novel system using Bluetooth collars and machine learning monitors cattle behavior and detects estrus with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable, low-cost solution for precision livestock monitoring, particularly in resource-limited settings.

Method: Deployed Bluetooth collars with accelerometer/gyroscope sensors, created a labeled dataset using CCTV, and tested SVM, RF, CNN, and LSTM models.

Result: Achieved 93% behavior classification accuracy and 96% estrus detection accuracy.

Conclusion: The system is effective and scalable for livestock monitoring, especially in constrained environments.

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [147] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: The paper reviews Relational Deep Learning (RDL), which transforms relational databases into relational entity graphs for end-to-end learning, addressing challenges like multi-table integration and temporal dynamics, and explores unifying these for foundation models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between graph machine learning and relational databases by introducing RDL, enabling representation learning without traditional feature engineering.

Method: Represent relational databases as relational entity graphs, review benchmark datasets, and survey GNN-based methods and architectural advances for RDL.

Result: Identifies challenges (e.g., multi-table integration, temporal dynamics) and opportunities to unify them for foundation models in relational data processing.

Conclusion: RDL converges graph machine learning sub-fields, offering a blueprint for foundation models to revolutionize relational data processing.

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [148] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Main category: cs.LG

TL;DR: SS-KAN integrates Kolmogorov-Arnold Networks into a state-space framework for interpretable nonlinear system identification, balancing accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Black-box system identification models lack interpretability of underlying dynamics, prompting the need for a more transparent approach.

Method: Proposes State-Space Kolmogorov-Arnold Networks (SS-KAN) with sparsity-promoting regularization and visualization of learned univariate functions.

Result: Validated on Silverbox and Wiener-Hammerstein benchmarks, SS-KAN enhances interpretability but slightly sacrifices accuracy compared to black-box models.

Conclusion: SS-KAN is a promising approach for interpretable nonlinear system identification, balancing accuracy and interpretability.

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [149] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Main category: cs.LG

TL;DR: SCALE, a new optimizer combining column-normalized SGD with last-layer momentum, matches or exceeds Adam's performance while using only 35-45% memory, outperforming other memory-efficient optimizers.


<details>
  <summary>Details</summary>
Motivation: To determine the minimal optimizer state needed for state-of-the-art performance in LLM pretraining, addressing memory constraints.

Method: Systematic investigation using column-wise gradient normalization and first-order momentum only in the output layer, proposing SCALE.

Result: SCALE matches/exceeds Adam's performance with 35-45% memory, outperforming GaLore, Fira, and APOLLO, especially for LLaMA 7B.

Conclusion: SCALE is a memory-efficient, high-performance optimizer and a minimalist baseline for future designs.

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [150] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Main category: cs.LG

TL;DR: GoalLadder uses vision-language models (VLMs) to train RL agents from a single language instruction in visual environments, outperforming competitors with a 95% success rate.


<details>
  <summary>Details</summary>
Motivation: To enable robotic systems to learn from human guidance by extracting rewards from language instructions, addressing challenges in noisy feedback and visual environments.

Method: GoalLadder incrementally discovers task-progress states using VLMs, ranks them with an ELO-based system, and minimizes distance to top-ranked goals in a learned embedding space.

Result: Achieves a 95% average success rate, significantly outperforming competitors (~45%).

Conclusion: GoalLadder effectively trains RL agents from language instructions with minimal feedback, leveraging VLMs and a robust ranking system.

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [151] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Main category: cs.LG

TL;DR: A novel uncertainty-aware weighting function for diffusion models in offline RL reduces training steps by 10x while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies and instability in training diffusion models for offline RL, particularly with transformer-based backbones.

Method: Derives a variationally optimal uncertainty-aware weighting function and introduces a closed-form polynomial approximation for online estimation.

Result: Achieves competitive performance on Maze2D and Kitchen tasks with significantly fewer training steps.

Conclusion: The proposed method offers a practical solution for efficient and stable training of diffusion models in offline RL.

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [152] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Main category: cs.LG

TL;DR: The paper introduces Directo, a generative model for directed graphs, addressing challenges like edge directionality and lack of benchmarks. It combines positional encodings, dual-attention, and a discrete framework, outperforming specialized models.


<details>
  <summary>Details</summary>
Motivation: Directed graphs are crucial in many fields, but generating them is underexplored due to challenges like complex dependencies and missing benchmarks.

Method: Directo uses discrete flow matching with tailored positional encodings, dual-attention for dependencies, and a robust generative framework.

Result: Directo performs strongly across diverse settings and competes with specialized models, as shown by a new benchmark suite.

Conclusion: Directo provides an effective, general foundation for future directed graph generation research.

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [153] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Main category: cs.LG

TL;DR: TriCon-SF is a serial federated learning framework with triple shuffling and contribution awareness, enhancing privacy, robustness, and accountability in cross-silo settings.


<details>
  <summary>Details</summary>
Motivation: Address privacy violations, gradient leakage, and malicious client behavior in serial pipeline training for federated learning, especially in healthcare.

Method: Integrates triple shuffling (model layers, data segments, training sequences) and Shapley value-based contribution evaluation.

Result: Outperforms standard serial and parallel federated learning in accuracy and communication efficiency on non-IID healthcare datasets.

Conclusion: TriCon-SF enhances privacy, robustness, and accountability, proving resilient against client-side attacks.

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [154] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: The paper highlights a misalignment between training and testing in unsupervised combinatorial optimization (UCO) and proposes a preliminary idea to align them by integrating differentiable derandomization into training.


<details>
  <summary>Details</summary>
Motivation: Existing UCO methods show a disconnect between training losses and post-derandomization performance, even without data distribution shifts.

Method: The study explores integrating a differentiable version of derandomization into the training phase to align training and testing.

Result: Empirical results show improved alignment but introduce new training challenges.

Conclusion: The proposed idea enhances training-test alignment in UCO but presents nontrivial training difficulties.

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [155] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Main category: cs.LG

TL;DR: EFormer, an Edge-based Transformer model, improves VRP solutions by using edge inputs and parallel encoding, outperforming baselines on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional neural heuristics for VRPs rely on node coordinates, which may not reflect real-world edge-based costs. EFormer addresses this by focusing on edge inputs.

Method: EFormer employs a precoder module and parallel encoding (graph and node encoders) to process edge information. It uses reinforcement learning for training.

Result: EFormer outperforms baselines on TSP and CVRP datasets, including large-scale and diverse distributions, and generalizes well to real-world instances.

Conclusion: EFormer's edge-based design and parallel encoding are effective for solving VRPs, demonstrating strong performance and generalization.

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [156] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Main category: cs.LG

TL;DR: A novel off-policy robust RL method is proposed, reformulating adversarial learning as a soft-constrained optimization problem to avoid inefficient environmental interactions.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies and dependencies in existing robust RL methods for adversarial input observations.

Method: Reformulates adversarial learning as a soft-constrained optimization problem, leveraging symmetric policy evaluation properties.

Result: Eliminates the need for additional environmental interactions, improving efficiency.

Conclusion: The proposed method offers a theoretically supported and practical solution for robust RL in adversarial settings.

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [157] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: An innovative collision avoidance system using event-based cameras and a Stack-CNN algorithm improves space debris detection for SSA/STM.


<details>
  <summary>Details</summary>
Motivation: Space debris is a growing threat, necessitating advanced mitigation strategies.

Method: The system uses event-based cameras and a Stack-CNN algorithm to detect faint moving objects in real-time.

Result: Terrestrial testing shows enhanced signal-to-noise ratio, proving effectiveness for space imaging.

Conclusion: The approach is promising for improving space traffic and situational awareness.

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [158] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: Proposes MHCL, a framework using multiple hyperbolic spaces and contrastive learning to capture diverse structures in heterogeneous graphs, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic heterogeneous graph embedding models use a single hyperbolic space, failing to capture diverse power-law structures.

Method: MHCL employs multiple hyperbolic spaces for metapath-based structures and contrastive learning to enhance discriminability.

Result: MHCL outperforms state-of-the-art baselines in graph tasks, effectively capturing complex structures.

Conclusion: MHCL successfully addresses the limitation of single-space models by leveraging multiple hyperbolic spaces and contrastive learning.

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [159] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Main category: cs.LG

TL;DR: The paper critiques the focus on distributive equality in fair-ML research, advocating for a broader egalitarian framework that includes relational equality to address structural inequality and representational harms.


<details>
  <summary>Details</summary>
Motivation: Current fair-ML research primarily focuses on distributive equality, which inadequately addresses representational harms and structural inequality. The paper seeks to provide a more comprehensive ethical foundation.

Method: The paper proposes a multifaceted egalitarian framework integrating distributive and relational equality, drawing on critical social and political philosophy.

Result: The framework offers a more holistic approach to addressing allocative and representational harms in ML systems.

Conclusion: A broader egalitarian framework is necessary for fair ML, with practical steps outlined for implementation across the ML pipeline.

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [160] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Main category: cs.LG

TL;DR: TabArena introduces a continuously maintained benchmarking system for tabular data, addressing flaws in static benchmarks by curating datasets, models, and a public leaderboard.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for tabular data are static and outdated, failing to adapt to new models or discovered flaws.

Method: TabArena manually curates datasets and models, conducts large-scale benchmarking, and establishes a maintenance team.

Result: Gradient-boosted trees remain strong, but deep learning catches up with ensembling, and foundation models excel on smaller datasets. Ensembles advance state-of-the-art.

Conclusion: TabArena provides a living benchmark with a public leaderboard and reproducible code, aiming to improve tabular ML evaluation.

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [161] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: The paper explores advanced bandwidth selectors for Semiparametric Bayesian Networks (SPBNs) to improve density estimation and predictive performance, showing that cross-validation outperforms the traditional normal rule.


<details>
  <summary>Details</summary>
Motivation: Real-world data often deviates from normality, making the normal rule for bandwidth selection in SPBNs suboptimal. The paper aims to enhance SPBN performance by evaluating state-of-the-art bandwidth selectors.

Method: Theoretical framework for bandwidth selectors (cross-validation and plug-in) is established. The open-source PyBNesian package is extended to include these techniques, followed by experimental analysis.

Result: Unbiased cross-validation generally outperforms the normal rule, especially in high sample size scenarios, leveraging data more effectively.

Conclusion: Advanced bandwidth selectors, particularly cross-validation, enhance SPBN performance, offering better applicability and learning capability compared to the normal rule.

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [162] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Main category: cs.LG

TL;DR: The paper investigates privacy risks in multitask learning (MTL) by analyzing how shared representations can leak task-specific information, proposing black-box inference attacks to detect task inclusion in training.


<details>
  <summary>Details</summary>
Motivation: MTL's shared representations, while efficient, may unintentionally reveal sensitive task-specific information, raising privacy concerns.

Method: The authors propose a black-box task-inference attack model, exploiting dependencies in embeddings without needing shadow models or labeled data.

Result: Experiments show adversaries can infer task inclusion in training using only fresh samples, with theoretical analysis supporting the findings.

Conclusion: The study highlights privacy vulnerabilities in MTL and underscores the need for safeguards against such inference attacks.

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [163] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Main category: cs.LG

TL;DR: The paper reconciles contradictory findings on model scale in continual learning by differentiating lazy and rich training regimes, showing that optimal performance depends on task similarity and feature learning.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of model scale and feature learning on catastrophic forgetting in non-stationary environments.

Method: Systematic study using variable parameterization of architecture and dynamical mean field theory to analyze infinite width dynamics.

Result: Increasing model width benefits only when reducing feature learning (laziness). High feature learning helps only with highly similar tasks, with a transition between lazy and rich regimes.

Conclusion: Neural networks perform optimally at a critical feature learning level, dependent on task non-stationarity, providing a unified view on scale and feature learning in continual learning.

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [164] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Main category: cs.LG

TL;DR: The paper uses machine learning and deep learning to predict neonatal mortality, with LSTM achieving the highest accuracy (99%).


<details>
  <summary>Details</summary>
Motivation: To reduce neonatal deaths by enabling early prediction of at-risk newborns.

Method: Trained models on 1.4M newborns' data using logistic regression, KNN, random forest, XGBoost, CNN, and LSTM.

Result: XGBoost and random forest achieved 94% accuracy; LSTM outperformed with 99%.

Conclusion: LSTM is the most effective method for predicting neonatal mortality and guiding precautionary measures.

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [165] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Main category: cs.LG

TL;DR: NLDR methods like t-SNE and UMAP effectively classify ECG signals and arrhythmias without labeled data, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual ECG analysis is error-prone, and existing ML methods struggle with signal variability and label inconsistencies.

Method: Nonlinear dimensionality reduction (NLDR) techniques (t-SNE and UMAP) are applied to ECG signals from the MIT-BIH dataset.

Result: NLDR achieved >=90% accuracy in discriminating recordings and 98.96% median accuracy for arrhythmia classification.

Conclusion: NLDR is promising for automated ECG analysis, even in challenging cases like single-lead or 12-lead ECGs, and has broader healthcare applications.

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [166] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: The paper investigates whether transformers in LLMs represent latent structures during in-context learning (ICL) or take shortcuts. It shows transformers can identify and compose latent concepts in tasks, refining understanding of ICL.


<details>
  <summary>Details</summary>
Motivation: To determine if transformers represent latent structures or use shortcuts in ICL, addressing gaps in prior mechanistic work.

Method: Examines transformers in 2-hop reasoning tasks with discrete latent concepts and continuous latent parameterizations, analyzing representation subspaces.

Result: Transformers successfully identify and compose latent concepts in discrete tasks and mimic continuous parameterizations in low-dimensional subspaces.

Conclusion: The findings refine ICL understanding, showing localized structures in transformers that disentangle latent concepts.

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [167] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Main category: cs.LG

TL;DR: SparseLoRA accelerates LLM fine-tuning by using contextual sparsity, reducing computational cost by up to 2.2x and achieving 1.6x speedup without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is resource-intensive; existing methods like QLoRA and DoRA reduce memory but not computation. SparseLoRA aims to address this gap.

Method: Proposes a lightweight, training-free SVD sparsity estimator to dynamically select sparse weights for loss/gradient computation, addressing sensitivity across layers, tokens, and steps.

Result: SparseLoRA reduces computational cost by up to 2.2x and achieves a 1.6x speedup while maintaining accuracy across tasks.

Conclusion: SparseLoRA effectively balances efficiency and performance in LLM fine-tuning.

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [168] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.LG

TL;DR: A differentiable Lomb-Scargle layer is introduced for computing power spectra of irregularly sampled data, integrated into a score-based diffusion model (LSCD) for accurate time series imputation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing or irregularly sampled data in time series, which distorts spectra when using FFT-based methods requiring uniform sampling.

Method: Proposes a differentiable Lomb-Scargle layer for spectral computation and integrates it into a score-based diffusion model (LSCD) for imputation.

Result: Outperforms time-domain baselines in accuracy for missing data recovery and provides consistent frequency estimates.

Conclusion: The method enables spectral guidance in machine learning for incomplete or irregular data, with potential for broader adoption.

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [169] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Main category: cs.LG

TL;DR: MAWIFlow is a flow-based benchmark from MAWILAB v1.1 for realistic anomaly detection evaluation, outperforming synthetic datasets. Tree-based models degrade over time, while CNN-BiLSTM maintains performance.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic datasets lack realism and temporal variability, limiting the evaluation of anomaly detection methods in operational environments.

Method: MAWIFlow transforms raw packet captures into flow representations (CICFlowMeter format) with preserved anomaly labels. Evaluated using traditional ML (Decision Trees, Random Forests, XGBoost, Logistic Regression) and CNN-BiLSTM.

Result: Tree-based models perform well on static data but degrade over time; CNN-BiLSTM shows better generalization and sustained performance.

Conclusion: Realistic datasets with temporal structure are crucial. MAWIFlow and its pipeline are publicly available for transparency and reproducibility.

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [170] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Main category: cs.LG

TL;DR: Crome (Causally Robust Reward Modeling) is a framework to mitigate reward hacking in reward models by using causal and neutral augmentations, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Reward models often suffer from reward hacking, focusing on superficial attributes instead of true quality drivers like factuality and relevance.

Method: Crome employs causal augmentations (to enforce sensitivity to causal attributes) and neutral augmentations (to enforce invariance to spurious attributes), generated via answer interventions.

Result: Crome outperforms baselines, improving accuracy by up to 5.4% on RewardBench, with gains up to 13.2% in specific categories.

Conclusion: Crome is a robust solution for reward hacking, demonstrating consistent improvements across multiple benchmarks.

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [171] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: The paper introduces SAMD and SAMI, methods to interpret and manipulate transformer attention heads for complex concepts, improving performance and control.


<details>
  <summary>Details</summary>
Motivation: To address gaps in current attribution research, which overlooks attention mechanisms and lacks unified methods for complex concepts.

Method: SAMD maps concepts to attention heads via cosine similarity; SAMI adjusts attention modules with a scalar parameter.

Result: Demonstrated stable module locations, improved benchmarks (e.g., GSM8K +1.6%), and domain-agnostic applicability.

Conclusion: SAMD and SAMI enhance interpretability and control of transformers, with broad applicability across domains.

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [172] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Main category: cs.LG

TL;DR: Proposes a new ASR metric combining NLI, semantic, and phonetic similarity to better assess intelligibility for dysarthric speech, outperforming WER/CER.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR metrics (WER, CER) fail to measure intelligibility for dysarthric/dysphonic speech, where semantic alignment is more critical than exact word matches.

Method: Introduces a novel metric integrating Natural Language Inference (NLI) scores, semantic similarity, and phonetic similarity.

Result: Achieves a 0.890 correlation with human judgments on Speech Accessibility Project data, outperforming traditional metrics.

Conclusion: Highlights the need to prioritize intelligibility over error-based measures in ASR evaluation, especially for atypical speech.

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [173] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Main category: cs.LG

TL;DR: FANTOM is a framework for causal discovery in non-stationary, non-Gaussian, heteroscedastic multivariate time series, identifying regimes and their causal structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to address non-stationarity and complex noise in multivariate time series, necessitating a unified approach.

Method: FANTOM uses a Bayesian Expectation Maximization algorithm to infer regimes, their boundaries, and Directed Acyclic Graphs for each regime.

Result: Theoretical proofs show identifiability, and experiments demonstrate FANTOM's superiority over existing methods.

Conclusion: FANTOM effectively addresses challenges in causal discovery for non-stationary, complex-noise time series.

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [174] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Main category: cs.LG

TL;DR: The paper introduces the Adaptive Representation Misdirection Unlearning (RMU) technique to remove sensitive information from LLMs, addressing privacy and security concerns.


<details>
  <summary>Details</summary>
Motivation: LLMs' tendency to memorize training data poses privacy and security risks, especially with PII. Existing unlearning methods are inadequate for LLMs.

Method: The study applies RMU to unlearn sensitive data, analyzing its effects across decoder layers to identify optimal removal regions.

Result: RMU ranked 4th on leaderboards for both 1B and 7B parameter models.

Conclusion: RMU is effective for unlearning sensitive information in LLMs, offering a practical solution to privacy and security challenges.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [175] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Main category: cs.LG

TL;DR: The paper analyzes the identifiability of deep Polynomial Neural Networks (PNNs), revealing how activation degrees and layer widths affect it. It provides conditions for identifiability, connects PNNs to tensor decompositions, and resolves open conjectures.


<details>
  <summary>Details</summary>
Motivation: Understanding the identifiability of PNNs is crucial for interpretability, but it remains poorly understood. This work aims to fill this gap.

Method: The study uses a constructive approach, linking deep PNNs to low-rank tensor decompositions and applying Kruskal-type uniqueness theorems.

Result: Architectures with non-increasing layer widths are generically identifiable under mild conditions, and encoder-decoder networks are identifiable if decoder widths don't grow too rapidly. The paper also resolves conjectures on neurovarieties' dimensions.

Conclusion: The work provides a comprehensive understanding of PNN identifiability, offering both generic and parameter-dependent conditions, and advances theoretical foundations for interpretable PNNs.

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [176] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Main category: cs.LG

TL;DR: A framework using free probability theory to analyze transformers, reinterpreting attention as non-commutative convolution and revealing spectral dynamics in deep transformer stacks.


<details>
  <summary>Details</summary>
Motivation: To provide a principled analysis of transformer-based models by leveraging free probability theory, bridging neural architecture with non-commutative harmonic analysis.

Method: Represent token embeddings and attention as self-adjoint operators in a free probability space, modeling attention as non-commutative convolution and layer-wise propagation as free additive convolution.

Result: Reveals spectral dynamics in transformers, derives a generalization bound based on free entropy, and shows predictable spectral trace evolution with depth.

Conclusion: The framework offers insights into inductive biases, generalization, and entropy dynamics, enabling principled analysis of information flow in large language models.

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [177] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Main category: cs.LG

TL;DR: TransDreamerV3 enhances DreamerV3 with a transformer encoder, improving memory and decision-making in complex tasks like Atari and Crafter, though challenges remain in Minecraft.


<details>
  <summary>Details</summary>
Motivation: To improve memory and decision-making in reinforcement learning by integrating a transformer encoder into the DreamerV3 architecture.

Method: Integration of a transformer encoder into DreamerV3, tested on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks.

Result: Improved performance over DreamerV3, especially in Atari-Freeway and Crafter, but issues in Minecraft and limited training scope.

Conclusion: TransDreamerV3 advances world model-based reinforcement learning with transformers, despite some limitations.

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [178] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Main category: cs.LG

TL;DR: The paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) for collaborative systems, using ACFGs and a Siamese model to dynamically assess trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Trust evaluation in collaborative systems is challenging due to distributed devices, dynamic environments, and changing resources.

Method: SRCTE uses ACFGs to represent trust-related data and a Siamese model (Structure2vec networks) to learn and compare embeddings for real-time trust evaluation.

Result: SRCTE converges quickly with minimal data and outperforms baseline algorithms in anomaly trust detection.

Conclusion: The SRCTE framework effectively enables rapid and continuous trust evaluation in collaborative systems.

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [179] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: Diffusion models trained on equilibrium molecular distributions show inconsistencies in generated samples and forces. A proposed energy-based model with Fokker-Planck regularization improves consistency and sampling.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies in diffusion models when used for molecular dynamics simulations, particularly at small timesteps, by ensuring adherence to the Fokker-Planck equation.

Method: Introduce an energy-based diffusion model with a Fokker-Planck-derived regularization term to enforce consistency between generated samples and simulations.

Result: Demonstrated effectiveness on toy systems and alanine dipeptide, achieving enhanced consistency and efficient sampling.

Conclusion: The proposed model provides a state-of-the-art transferable Boltzmann emulator for dipeptides, improving simulation reliability and sampling efficiency.

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [180] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Main category: cs.LG

TL;DR: The paper addresses overfitting in offline RL with small datasets by introducing Sparse-Reg, a sparsity-based regularization technique, which outperforms existing methods in limited data settings.


<details>
  <summary>Details</summary>
Motivation: Many offline RL applications rely on small datasets, but current benchmarks use large datasets, leading to overfitting and poor performance in real-world scenarios.

Method: The authors propose Sparse-Reg, a regularization technique leveraging sparsity to prevent overfitting in offline RL with small datasets.

Result: Sparse-Reg outperforms state-of-the-art baselines in continuous control tasks, demonstrating effectiveness in limited data settings.

Conclusion: Sparse-Reg successfully mitigates overfitting in offline RL for small datasets, offering a practical solution for real-world applications.

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [181] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: SlepNet, a novel GCN architecture using Slepian bases, outperforms traditional GNNs and graph signal processing methods by focusing signal energy on relevant subgraphs, offering better resolution for neural and traffic data.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs and graph signal processing methods struggle to efficiently represent localized signal patterning on graphs, which is crucial in domains like neuroscience.

Method: SlepNet employs Slepian bases to optimally concentrate signal energy on learned subgraphs, providing canonical and high-resolution representations.

Result: SlepNet outperforms baselines in fMRI and traffic datasets, with improved pattern distinction and informative trajectory representations.

Conclusion: SlepNet is effective for prediction and representation learning in spatiotemporal data, with potential for downstream tasks.

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [182] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: Using static network sparsity (one-shot random pruning) improves scaling and performance in deep reinforcement learning compared to dense networks.


<details>
  <summary>Details</summary>
Motivation: Scaling deep reinforcement learning is challenging due to training pathologies, and existing solutions like periodic reset or architectural changes are complex.

Method: Introduce static sparsity via one-shot random pruning before training, removing a fixed percentage of weights.

Result: Sparse networks outperform dense ones in parameter efficiency, plasticity loss resistance, and gradient interference mitigation, with consistent benefits in visual and streaming RL.

Conclusion: Static sparsity is a simple yet effective method to enhance scaling and performance in deep reinforcement learning.

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [183] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Main category: cs.LG

TL;DR: RLIF (Reinforcement Learning from Internal Feedback) uses intrinsic signals like token-level entropy and self-certainty to improve LLM reasoning, showing early gains but degrading later, especially for instruction-tuned models.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on external supervision in post-training LLMs by exploring intrinsic feedback methods.

Method: Leverages unsupervised reward proxies (token-level entropy, trajectory-level entropy, self-certainty) for RLIF, comparing it to RLHF and RLVR on math reasoning tasks.

Result: RLIF boosts early reasoning performance but degrades later, and shows minimal improvement for instruction-tuned models.

Conclusion: RLIF has potential but limitations; guidelines are provided for integrating internal feedback into LLM training.

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


### [184] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: DEBIAS optimizes outcome definitions to maximize causal identifiability in psychiatric longitudinal data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of causal inference in psychiatry due to symptom heterogeneity and latent confounding, which undermines classical estimators.

Method: DEBIAS algorithm learns non-negative, interpretable weights for outcome aggregation, maximizing durable treatment effects and minimizing confounding.

Result: Outperforms state-of-the-art methods in recovering causal effects for clinically interpretable outcomes in depression and schizophrenia.

Conclusion: DEBIAS provides a robust solution for causal inference in psychiatric data by optimizing outcome definitions and addressing confounding.

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [185] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Main category: cs.LG

TL;DR: SORE is a cost-effective method for removing unwanted text segments using multilingual embeddings, outperforming traditional methods and nearing LLM precision.


<details>
  <summary>Details</summary>
Motivation: Traditional text processing methods struggle with multilingual and context-sensitive content, while LLMs are costly.

Method: SORE uses multilingual sentence embeddings and nearest-neighbor search to identify and remove outliers.

Result: SORE achieves near-LLM precision at lower cost, outperforming structural methods in experiments.

Conclusion: SORE is efficient, accurate, and deployed in production, with released implementation for reproducibility.

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [186] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: cs.LG

TL;DR: The paper introduces a distributional-lifting theorem to upgrade learners from limited distribution families to any distribution, improving efficiency and applicability. It critiques prior work for requiring strong access (conditional sample oracle) and proposes a simpler, more general PAC model solution.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of distribution-specific learning by enabling efficient learning for any distribution without restrictive assumptions.

Method: Prove a distributional-lifting theorem and develop a lifter that works in the standard PAC model, avoiding the need for strong access or learning the target distribution.

Result: The proposed lifter is efficient, works for all base distribution families, preserves noise tolerance, and has better sample complexity.

Conclusion: The new approach overcomes the intractability of prior methods and provides a simpler, more general solution for distribution-free learning.

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [187] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Main category: cs.LG

TL;DR: The paper introduces the Mesh-Informed Neural Operator (MINO) to overcome limitations of current functional generative models, expanding their applicability to diverse domains and tasks.


<details>
  <summary>Details</summary>
Motivation: Current functional generative models, reliant on Fourier Neural Operators, are limited to regular grids and rectangular domains, restricting their broader scientific and engineering applications.

Method: The authors propose MINO, which uses graph neural operators and cross-attention mechanisms to create a domain- and discretization-agnostic backbone for generative modeling in function spaces.

Result: MINO significantly broadens the applicability of functional generative models and provides a unified framework for integrating neural operators with advanced deep learning architectures.

Conclusion: The paper advances the field by introducing MINO and standardized evaluation metrics, enabling objective comparisons and expanding the potential of functional generative models.

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [188] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Main category: cs.LG

TL;DR: A novel DP synthetic image embedding method using GMM in an embedding space achieves SOTA classification accuracy and generates realistic synthetic images.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in deep neural network training by using DP to protect sensitive data while maintaining high performance.

Method: Privately generate synthetic datasets by fitting a GMM in an embedding space using DP clustering, then train a simple neural network on these embeddings.

Result: Achieves SOTA classification accuracy on benchmark datasets and generates realistic synthetic images comparable to SOTA methods.

Conclusion: The method is general, scalable, and effective for DP synthetic data generation, balancing privacy and performance.

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [189] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: Proposes a novel approach using vector quantization (VQ) to create compact Semantic IDs (SIDs) for ad-recommendation systems, improving efficiency and reducing storage costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of incorporating large-scale embeddings into real-time prediction models due to high storage and inference costs.

Method: Introduces a multi-task VQ-VAE framework (VQ fusion), a parameter-free SID-to-embedding technique (SIDE), and a novel quantization method (DPCA).

Result: Achieves 2.4X improvement in normalized entropy gain and 3X reduction in data footprint compared to traditional methods.

Conclusion: The proposed innovations significantly enhance efficiency and performance in large-scale ad-recommendation systems.

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [190] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Main category: cs.LG

TL;DR: The paper introduces the domain shattering dimension to quantify the number of domains needed for domain generalization, linking it to VC dimension.


<details>
  <summary>Details</summary>
Motivation: To understand how many domains are required to learn a model that generalizes well across seen and unseen domains.

Method: Model the problem in the PAC framework and introduce the domain shattering dimension as a combinatorial measure.

Result: The domain shattering dimension characterizes domain sample complexity and relates tightly to VC dimension.

Conclusion: Any hypothesis class learnable in standard PAC is also learnable in this domain generalization setting.

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [191] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Main category: cs.LG

TL;DR: Optimistic Fictitious Play achieves constant regret in two-strategy zero-sum games without regularization, outperforming Alternating Fictitious Play, which has a regret lower bound of Ω(√T).


<details>
  <summary>Details</summary>
Motivation: To explore whether non-no-regret algorithms like Optimistic Fictitious Play can achieve fast learning rates (constant regret) in two-player zero-sum games without regularization.

Method: Analyzes Optimistic Fictitious Play using a geometric view in the dual space of payoff vectors, showing bounded energy function of iterates. Also examines Alternating Fictitious Play for comparison.

Result: Optimistic Fictitious Play achieves constant regret in two-strategy games, while Alternating Fictitious Play has a regret lower bound of Ω(√T).

Conclusion: Optimism enables fast learning (constant regret) in unregularized settings, whereas alternation does not, highlighting a key difference in their performance.

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [192] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Main category: cs.LG

TL;DR: The paper compares multimodal fusion strategies for decoding hand gestures, finding that a Hierarchical Transformer with attention-based fusion outperforms other methods, with cross-modal interactions contributing significantly to decision-making.


<details>
  <summary>Details</summary>
Motivation: To improve the decoding of neuromuscular signatures for applications like prosthetics by leveraging multimodal fusion of biosignals.

Method: Systematic comparison of linear and attention-based fusion strategies across three architectures (Multimodal MLP, Multimodal Transformer, Hierarchical Transformer) using two datasets (NinaPro DB2 and HD-sEMG 65-Gesture).

Result: The Hierarchical Transformer with attention-based fusion achieved the highest accuracy, surpassing baselines by over 10% on NinaPro DB2 and 3.7% on HD-sEMG. Cross-modal interactions contributed ~30% of the decision signal.

Conclusion: Attention-driven multimodal fusion enhances biosignal classification, providing insights for neurorobotic system design and human muscle activity understanding.

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [193] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Main category: cs.LG

TL;DR: SeLoRA improves LoRA by reducing parameter redundancy using spectral bases, enhancing efficiency and performance without losing expressiveness.


<details>
  <summary>Details</summary>
Motivation: Parameter redundancy in LoRA limits its capacity and efficiency, prompting the need for a more streamlined approach.

Method: SeLoRA re-parameterizes LoRA using spectral bases from a sparse spectral subspace, integrating easily with existing LoRA variants.

Result: SeLoRA achieves better efficiency and performance with fewer parameters across tasks like commonsense reasoning and code generation.

Conclusion: SeLoRA is a scalable, plug-and-play framework that boosts LoRA's performance by addressing redundancy.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [194] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: The paper introduces SPoGInit, a method to improve signal propagation in deep GNNs by optimizing initialization variances for forward/backward propagation and graph embedding variation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in deep GNNs by enhancing signal propagation through better initialization.

Method: Propose SPoGInit, which optimizes initialization variances for three key metrics: forward/backward propagation and graph embedding variation.

Result: SPoGInit outperforms common initialization methods, enabling performance improvements as GNNs deepen.

Conclusion: SPoGInit effectively addresses depth-related challenges in GNNs, validating the SP analysis framework.

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [195] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Main category: cs.LG

TL;DR: The paper proposes seq2GMM, a framework for identifying unusual quasi-periodic time series in networks, outperforming existing anomaly detection methods.


<details>
  <summary>Details</summary>
Motivation: To detect and understand anomalous quasi-periodic time series in networks with timing variations.

Method: Develops a sequence to Gaussian Mixture Model (seq2GMM) and a surrogate-based optimization algorithm for training.

Result: Seq2GMM outperforms state-of-the-art anomaly detection techniques on benchmark datasets.

Conclusion: The framework effectively identifies anomalies and provides interpretable insights, with theoretical and empirical validation.

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [196] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Main category: cs.LG

TL;DR: LLMs are used to extract main concepts from materials science abstracts, building a concept graph to predict new research ideas, outperforming keyword methods and inspiring scientists.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of research articles makes it impossible for scientists to read all publications, necessitating automated methods to extract and link concepts for new research directions.

Method: Large language models (LLMs) extract concepts from abstracts to build a concept graph. A machine learning model predicts emerging concept combinations based on historical data.

Result: LLMs outperform keyword extraction methods, and integrating semantic information improves prediction performance. The model inspires scientists with innovative topic combinations.

Conclusion: LLMs effectively extract and link concepts, inspiring new research directions in materials science, demonstrating practical utility in expert interviews.

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [197] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: The paper introduces FedFitTech, a federated learning baseline for wearable fitness tracking, addressing privacy and efficiency challenges while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To overcome privacy concerns, regulatory constraints, and inefficiencies in centralized learning for fitness activity detection using wearable sensors.

Method: Proposes FedFitTech under the Flower framework, incorporating client-side early stopping and balancing personalization with generalization.

Result: Reduces redundant communications by 13% with only a 1% recognition cost, maintaining performance.

Conclusion: FedFitTech provides a scalable, efficient, and privacy-aware foundation for FitTech research and is open-source.

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [198] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: A new soft survival tree (SST) model is proposed, offering interpretability and flexibility by using soft splitting rules and optimizing via nonlinear decomposition. It outperforms benchmarks in discrimination and calibration.


<details>
  <summary>Details</summary>
Motivation: To improve survival analysis by combining interpretability of decision trees with global optimization, addressing limitations of heuristic approaches.

Method: SST uses soft splitting rules and nonlinear optimization, allowing any smooth survival function (parametric, semiparametric, or nonparametric) via maximum likelihood.

Result: SST outperforms three benchmark survival trees on 15 datasets in discrimination and calibration measures.

Conclusion: SSTs provide a flexible, interpretable, and high-performing alternative for survival analysis, with potential for fairness extensions.

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [199] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Main category: cs.LG

TL;DR: RATTPO is a reward-agnostic method for optimizing prompts in text-to-image models, outperforming specialized and learning-based approaches in efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing prompt engineering methods are tailored to specific rewards, limiting their adaptability to new scenarios.

Method: RATTPO iteratively optimizes prompts using LLMs and reward-aware hints, without requiring reward-specific task descriptions.

Result: RATTPO enhances prompts across diverse rewards, uses fewer resources, and matches fine-tuned baselines with sufficient budget.

Conclusion: RATTPO offers a versatile and efficient solution for prompt optimization in text-to-image models.

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [200] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised learning framework for measuring similarities in event-triggered time series, combining hierarchical autoencoders and GMM for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing similarity metrics for event-triggered time series in cybersecurity tasks like anomaly detection are unclear, necessitating a robust framework.

Method: The framework uses hierarchical multi-resolution sequential autoencoders and Gaussian Mixture Model (GMM) to learn low-dimensional representations.

Result: The method outperforms state-of-the-art techniques in qualitative and quantitative experiments.

Conclusion: The framework provides a systematic approach for learning and visualizing similarities in event-triggered time series, advancing cybersecurity applications.

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [201] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework to determine the optimal depth of neural networks by modeling the forward pass as an optimal stopping problem, proving finite stopping depth under certain conditions, and proposing a practical regularization term for efficient early exiting.


<details>
  <summary>Details</summary>
Motivation: The challenge of determining neural network depth through costly experimentation motivates a formal theoretical approach to balance accuracy and computational efficiency.

Method: The forward pass of a ResNet is recast as an optimal stopping problem, with a proof of finite stopping depth under diminishing returns. A regularization term, $\mathcal{L}_{\rm depth}$, is proposed to encourage early exiting.

Result: Empirical validation on ImageNet shows the regularizer improves computational efficiency without sacrificing accuracy, sometimes enhancing it.

Conclusion: The framework provides a principled way to optimize neural network depth, with practical benefits demonstrated in ResNets and Transformers.

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [202] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: The paper discusses unsupervised machine learning for detecting quality issues in industrial products, focusing on robustness and practical pitfalls in real-world settings.


<details>
  <summary>Details</summary>
Motivation: Manual inspection is costly and error-prone; machine learning can automate this, but unsupervised methods are needed due to unpredictable defects.

Method: Evaluates two state-of-the-art models for anomaly detection on low-quality RGB images of metal parts, addressing robustness and data issues.

Result: Identifies common pitfalls in likelihood-based methods and proposes a framework for better empirical risk estimation in real-world scenarios.

Conclusion: Provides practical guardrails for practitioners to improve model and data robustness in industrial anomaly detection.

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [203] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Main category: cs.LG

TL;DR: RocketStack introduces a level-aware recursive ensemble framework for deep stacking, addressing complexity and performance saturation through pruning, mild noise, and feature compression, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in deep stacking like model complexity, feature redundancy, and computational burden by enabling deeper recursive ensembling without excessive overhead.

Method: RocketStack uses incremental pruning of weaker learners, mild Gaussian noise on OOF scores, and feature compression (attention-based, SFE filter, autoencoders) to enable deeper stacking.

Result: Achieved 97.08% accuracy (binary) and 98.60% (multi-class) at level 10, outperforming baselines by 5.14% and 6.11%, respectively, while reducing runtime and feature dimensionality.

Conclusion: Mild randomization and periodic compression stabilize deep stacking, making RocketStack an effective framework for recursive ensembling with tractable complexity.

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [204] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Main category: cs.LG

TL;DR: The paper addresses the challenge of narrowing large combinatorial sets in scientific discovery by improving reinforcement learning methods to handle uncertain proxy reward functions, introducing a robust RL approach for better candidate selection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the inadequacy of existing methods in handling uncertain reward functions, which leads to suboptimal candidate selection in large search spaces.

Method: The authors propose a robust RL approach with a unified operator that ensures robustness to reward uncertainty, targeting peakier sampling distributions and encompassing soft RL operators.

Result: The method identifies higher-quality, diverse candidates in synthetic and real-world tasks.

Conclusion: The work provides a flexible perspective on discrete compositional generation tasks, improving candidate selection in scientific discovery.

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [205] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: The paper investigates the environmental impact of AI image generation by analyzing energy consumption across 17 models, considering factors like resolution, quantization, and prompt length. Results reveal significant energy variations, inconsistent resolution effects, and no clear trade-off between quality and energy.


<details>
  <summary>Details</summary>
Motivation: The study addresses the environmental concerns of AI image generation, aiming to quantify energy consumption and identify factors influencing it.

Method: A comprehensive empirical experiment compares 17 state-of-the-art models, evaluating energy consumption against factors like quantization, resolution, and prompt length, alongside image quality metrics.

Result: Energy consumption varies drastically (up to 46x difference). Resolution impacts inconsistently (1.3x-4.7x increase). U-Net models are more efficient than Transformer-based ones. Quantization reduces efficiency, while prompt length has no significant effect. High-quality images don't always require more energy.

Conclusion: The findings highlight the need for energy-efficient AI image generation, as some high-quality models are also energy-efficient, and factors like resolution and model type significantly impact consumption.

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [206] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: MARL-OD-DA, a scalable and reliable MARL framework for traffic assignment, outperforms traditional methods by redefining agents as OD pair routers and improving convergence efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability and reliability challenges of MARL in large-scale traffic assignment problems due to increasing travel demands in metropolitan cities.

Method: Introduces MARL-OD-DA, which uses OD pair routers, a Dirichlet-based action space with pruning, and a local relative gap reward function.

Result: Achieves a 94.99% lower relative gap than conventional methods in the SiouxFalls network, handling medium-sized networks effectively.

Conclusion: MARL-OD-DA is a promising solution for large-scale traffic assignment, offering scalability and reliability improvements over existing methods.

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [207] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Main category: cs.LG

TL;DR: A scoping review identified 62 fairness metrics for clinical predictive AI, revealing fragmentation, limited validation, and gaps in clinical relevance.


<details>
  <summary>Details</summary>
Motivation: To address the unclear definition of fairness in AI and its risks of perpetuating biases in clinical practice.

Method: Conducted a scoping review of five databases (2014-2024), screening 820 records to include 41 studies and extract 62 fairness metrics.

Result: Found a fragmented landscape with limited clinical validation, overreliance on threshold-dependent measures, and gaps in uncertainty quantification and real-world applicability.

Conclusion: Future work should prioritize clinically meaningful fairness metrics to improve AI's fairness in healthcare.

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [208] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Main category: cs.LG

TL;DR: The paper refines neural network signature extraction, addressing limitations like rank deficiency and noise propagation, enabling deeper network extraction with higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Neural network model extraction is a security concern, but prior methods like Carlini et al.'s had limitations (e.g., shallow extraction). This work aims to overcome these to enable deeper network extraction.

Method: Proposes algorithmic solutions for rank deficiency and noise propagation, improving signature extraction efficiency. Validated on ReLU-based networks, including CIFAR-10.

Result: Achieves 95% accuracy for eight layers (vs. prior three-layer limit), demonstrating significant depth and accuracy improvements.

Conclusion: The refined method advances practical attacks on deeper, more complex neural networks, marking a crucial step in model extraction security.

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [209] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Main category: cs.LG

TL;DR: A unified view of deep generative models as probability transformation functions, highlighting their commonality despite architectural differences.


<details>
  <summary>Details</summary>
Motivation: To provide a unifying theoretical framework for diverse generative models, enabling methodological transfer and improved techniques.

Method: Analyzes various generative models (autoencoders, autoregressive models, GANs, normalizing flows, diffusion models, flow matching) as transformations of simple distributions into complex data distributions.

Result: Demonstrates that all generative models fundamentally perform probability transformations, enabling cross-architecture improvements.

Conclusion: The unified perspective fosters theoretical advancements and more efficient generative modeling.

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [210] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Main category: cs.LG

TL;DR: DISCoVeR is a new variational framework for disentangling condition-invariant and condition-specific factors in data, improving generalization in multi-condition settings.


<details>
  <summary>Details</summary>
Motivation: To address leakage in latent representations of existing VAE extensions, which limits generalization to unseen conditions in domains like biomedical data analysis.

Method: Introduces DISCoVeR with a dual-latent architecture, parallel reconstructions, and a max-min objective for clean separation of factors.

Result: Achieves improved disentanglement on synthetic datasets, natural images, and single-cell RNA-seq data.

Conclusion: DISCoVeR is a principled approach for learning disentangled representations in multi-condition settings.

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [211] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Main category: cs.LG

TL;DR: The paper analyzes the implicit bias of optimization algorithms in over-parameterized linear regression, identifying the optimal bias for best generalization performance.


<details>
  <summary>Details</summary>
Motivation: Understanding the implicit bias leading to the best generalization in over-parameterized learning problems.

Method: Asymptotic analysis of generalization performance for interpolators from convex potential minimization in non-isotropic Gaussian data settings.

Result: A tight lower bound on generalization error is derived, and the optimal convex implicit bias achieving this bound is identified under certain conditions.

Conclusion: The study provides insights into optimal implicit bias for generalization in over-parameterized regimes, with conditions involving log-concavity of convolved distributions.

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [212] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: BREAD, a GRPO variant, unifies SFT and RL stages with expert guidance and branched rollouts, improving SLM reasoning by addressing limitations of the SFT + RL paradigm.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) struggle with complex reasoning due to scarce or difficult-to-learn high-quality traces, and the SFT + RL paradigm fails under certain conditions.

Method: Introduces BREAD, which combines partial expert guidance and branched rollouts to ensure successful traces in updates, densifying rewards and creating a learning curriculum.

Result: BREAD outperforms standard GRPO, requires fewer ground-truth traces, speeds up training by 3x, and solves problems unsolvable by SFT + RL.

Conclusion: BREAD's branched rollouts and expert guidance significantly enhance SLM reasoning, overcoming fundamental limitations of the SFT + RL approach.

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [213] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Main category: cs.MA

TL;DR: RecBayes is a novel method for ad hoc teamwork in partially observable environments, identifying teams and tasks without needing environment states or teammate actions, outperforming existing approaches in scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ad hoc teamwork under partial observability, where agents must integrate into existing teams without access to full environmental or teammate data.

Method: Uses a recurrent Bayesian classifier trained on past experiences to identify teams and tasks from observations alone, without requiring states or actions.

Result: Effective in large-scale environments (up to 1M states and 2^125 observations), outperforming methods like PO-GPL, FEAT, and ATPO.

Conclusion: RecBayes successfully enables ad hoc agents to assist teams in solving tasks using only partial observations, without scalability limitations.

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


### [214] [Learning to Coordinate Under Threshold Rewards: A Cooperative Multi-Agent Bandit Framework](https://arxiv.org/abs/2506.15856)
*Michael Ledford,William Regli*

Main category: cs.MA

TL;DR: T-Coop-UCB is a decentralized algorithm for multi-agent systems to learn threshold-activated rewards and avoid decoys, outperforming baselines in reward and coordination.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of threshold-activated rewards and decoys in cooperative multi-agent systems, where prior work assumes individually attainable rewards.

Method: Introduces T-Coop-UCB, a decentralized algorithm for joint learning of activation thresholds and reward distributions, enabling effective coalition formation.

Result: Empirical results show T-Coop-UCB outperforms baselines in cumulative reward, regret, and coordination, achieving near-Oracle performance.

Conclusion: Joint threshold learning and decoy avoidance are crucial for scalable, decentralized cooperation in complex multi-agent systems.

Abstract: Cooperative multi-agent systems often face tasks that require coordinated
actions under uncertainty. While multi-armed bandit (MAB) problems provide a
powerful framework for decentralized learning, most prior work assumes
individually attainable rewards. We address the challenging setting where
rewards are threshold-activated: an arm yields a payoff only when a minimum
number of agents pull it simultaneously, with this threshold unknown in
advance. Complicating matters further, some arms are decoys - requiring
coordination to activate but yielding no reward - introducing a new challenge
of wasted joint exploration. We introduce Threshold-Coop-UCB (T-Coop-UCB), a
decentralized algorithm that enables agents to jointly learn activation
thresholds and reward distributions, forming effective coalitions without
centralized control. Empirical results show that T-Coop-UCB consistently
outperforms baseline methods in cumulative reward, regret, and coordination
metrics, achieving near-Oracle performance. Our findings underscore the
importance of joint threshold learning and decoy avoidance for scalable,
decentralized cooperation in complex multi-agent

</details>


### [215] [Coordination of Electrical and Heating Resources by Self-Interested Agents](https://arxiv.org/abs/2506.16277)
*Rico Schrage,Jari Radler,Astrid Nieße*

Main category: cs.MA

TL;DR: A novel distributed hybrid algorithm optimizes multi-energy scheduling for decentralized resources, balancing private and collective objectives.


<details>
  <summary>Details</summary>
Motivation: To coordinate decentralized energy resources and optimize heating/electricity output in systems like district heating and local energy communities.

Method: A distributed hybrid algorithm combining gossiping and local search heuristics to optimize energy scheduling.

Result: Globally near-optimal solutions achieved, protecting stakeholders' economic goals and technical properties.

Conclusion: The algorithm effectively solves multi-energy scheduling, validated in electrical and gas-based test cases.

Abstract: With the rise of distributed energy resources and sector coupling,
distributed optimization can be a sensible approach to coordinate decentralized
energy resources. Further, district heating, heat pumps, cogeneration, and
sharing concepts like local energy communities introduce the potential to
optimize heating and electricity output simultaneously. To solve this issue, we
tackle the distributed multi-energy scheduling optimization problem, which
describes the optimization of distributed energy generators over multiple time
steps to reach a specific target schedule. This work describes a novel
distributed hybrid algorithm as a solution approach. This approach is based on
the heuristics of gossiping and local search and can simultaneously optimize
the private objective of the participants and the collective objective,
considering multiple energy sectors. We show that the algorithm finds globally
near-optimal solutions while protecting the stakeholders' economic goals and
the plants' technical properties. Two test cases representing pure electrical
and gas-based technologies are evaluated.

</details>


### [216] [Towards Emergency Scenarios: An Integrated Decision-making Framework of Multi-lane Platoon Reorganization](https://arxiv.org/abs/2506.16311)
*Aijing Kong,Chengkai Xu,Xian Wu,Xinbo Chen,Peng Hang*

Main category: cs.MA

TL;DR: A framework for vehicle platoon reorganization in emergencies uses reinforcement learning, coalition games, and a novel Platoon Disposition Index (PDI) to reduce collisions and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing vehicle platoon responses to emergencies by improving reorganization efficiency and reducing collision risks.

Method: Combines reinforcement learning for risk assessment, coalition games for cooperative decision-making, and PDI for measuring reorganization states.

Result: Reduces collision rates and reorganization time, improving driving efficiency compared to baseline models.

Conclusion: The proposed framework effectively enhances emergency response in vehicle platoons, with PDI further boosting reorganization efficiency.

Abstract: To enhance the ability for vehicle platoons to respond to emergency
scenarios, a platoon distribution reorganization decision-making framework is
proposed. This framework contains platoon distribution layer, vehicle
cooperative decision-making layer and vehicle planning and control layer.
Firstly, a reinforcement-learning-based platoon distribution model is
presented, where a risk potential field is established to quantitatively assess
driving risks, and a reward function tailored to the platoon reorganization
process is constructed. Then, a coalition-game-based vehicle cooperative
decision-making model is put forward, modeling the cooperative relationships
among vehicles through dividing coalitions and generating the optimal decision
results for each vehicle. Additionally, a novel graph-theory-based Platoon
Disposition Index (PDI) is incorporated into the game reward function to
measure the platoon's distribution state during the reorganization process, in
order to accelerating the reorganization process. Finally, the validation of
the proposed framework is conducted in two high-risk scenarios under random
traffic flows. The results show that, compared to the baseline models, the
proposed method can significantly reduce the collision rate and improve driving
efficiency. Moreover, the model with PDI can significantly decrease the platoon
formation reorganization time and improve the reorganization efficiency.

</details>


### [217] [Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation](https://arxiv.org/abs/2506.16718)
*Chenxu Wang,Yonggang Jin,Cheng Hu,Youpeng Zhao,Zipeng Dai,Jian Zhao,Shiyu Huang,Liuyu Xiang,Junge Zhang,Zhaofeng He*

Main category: cs.MA

TL;DR: The paper introduces Agent Collaborative-Competitive Adaptation (ACCA) and a method called Multi-Retrieval and Dynamic Generation (MRDG) to improve agent generalization in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting agents to new multi-agent systems with unknown teammates and opponents.

Method: Proposes MRDG, which models teammates and opponents using behavioral trajectories, includes a positional encoder, hypernetwork module, and viewpoint alignment.

Result: MRDG outperforms baselines in benchmark scenarios (SMAC, Overcooked-AI, Melting Pot) for robust collaboration and competition.

Conclusion: ACCA and MRDG provide a comprehensive framework for agent adaptation, enhancing performance in diverse multi-agent scenarios.

Abstract: Adapting a single agent to a new multi-agent system brings challenges,
necessitating adjustments across various tasks, environments, and interactions
with unknown teammates and opponents. Addressing this challenge is highly
complex, and researchers have proposed two simplified scenarios, Multi-agent
reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on
these foundations, we propose a more comprehensive setting, Agent
Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to
generalize across diverse scenarios, tasks, and interactions with both
unfamiliar opponents and teammates. In ACCA, agents adjust to task and
environmental changes, collaborate with unseen teammates, and compete against
unknown opponents. We introduce a new modeling approach, Multi-Retrieval and
Dynamic Generation (MRDG), that effectively models both teammates and opponents
using their behavioral trajectories. This method incorporates a positional
encoder for varying team sizes and a hypernetwork module to boost agents'
learning and adaptive capabilities. Additionally, a viewpoint alignment module
harmonizes the observational perspectives of retrieved teammates and opponents
with the learning agent. Extensive tests in benchmark scenarios like SMAC,
Overcooked-AI, and Melting Pot show that MRDG significantly improves robust
collaboration and competition with unseen teammates and opponents, surpassing
established baselines. Our code is available at:
https://github.com/vcis-wangchenxu/MRDG.git

</details>


### [218] [Engineering Resilience: An Energy-Based Approach to Sustainable Behavioural Interventions](https://arxiv.org/abs/2506.16836)
*Arpitha Srivathsa Malavalli,Karthik Sama,Janvi Chhabra,Pooja Bassin,Srinath Srinivasa*

Main category: cs.MA

TL;DR: The paper proposes incorporating a nature-inspired postulate (lower energy states are more resilient) into intervention design to ensure sustainability, demonstrated via an agent-based simulation of eco-friendly commuting choices.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on sustainability (resilience) in intervention science, which traditionally aims to shift systems toward desired states without ensuring long-term stability.

Method: Uses a nature-inspired postulate (lower energy states enhance resilience) as a regularization mechanism in intervention optimization, tested via an agent-based simulation of commuter behavior.

Result: The energy-based approach ensures interventions align with agents' motivators, leading to resilient behavioral states, as shown in the simulation.

Conclusion: Incorporating energy-based resilience into intervention design can create sustainable, effective nudges for complex societal challenges.

Abstract: Addressing complex societal challenges, such as improving public health,
fostering honesty in workplaces, or encouraging eco-friendly behaviour requires
effective nudges to influence human behaviour at scale. Intervention science
seeks to design such nudges within complex societal systems. While
interventions primarily aim to shift the system toward a desired state, less
attention is given to the sustainability of that state, which we define in
terms of resilience: the system's ability to retain the desired state even
under perturbations. In this work, we offer a more holistic perspective to
intervention design by incorporating a nature-inspired postulate i.e., lower
energy states tend to exhibit greater resilience, as a regularization mechanism
within intervention optimization to ensure that the resulting state is also
sustainable. Using a simple agent-based simulation where commuters are nudged
to choose eco-friendly options (e.g., cycles) over individually attractive but
less eco-friendly ones (e.g., cars), we demonstrate how embedding lower energy
postulate into intervention design induces resilience. The system energy is
defined in terms of motivators that drive its agent's behaviour. By inherently
ensuring that agents are not pushed into actions that contradict their
motivators, the energy-based approach helps design effective interventions that
contribute to resilient behavioural states.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [219] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2 is a versatile LLM addressing both generation- and embedding-based tasks in EDA, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous work focused on generation tasks in EDA, neglecting embedding-based tasks critical for hardware design.

Method: DeepRTL2 unifies generation and embedding tasks, offering a comprehensive solution for RTL-related challenges.

Result: DeepRTL2 achieves state-of-the-art performance across all evaluated tasks.

Conclusion: DeepRTL2 provides a unified and effective approach for diverse EDA challenges.

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [220] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Main category: eess.SP

TL;DR: The paper explores using large language models (LLM) to address challenges in near-field communications for low-altitude economy (LAE) applications, such as signal processing complexity and user distinction.


<details>
  <summary>Details</summary>
Motivation: The low-altitude economy (LAE) aligns with near-field communications in XL-MIMO systems, but challenges like signal processing complexity and user distinction persist. LLMs offer a potential solution due to their problem-solving capabilities.

Method: The paper introduces LLM and near-field communication fundamentals, analyzes challenges in LAE, and proposes an LLM-based scheme for user distinction and precoding matrix design.

Result: A case study demonstrates the LLM-based scheme's effectiveness in addressing near-field communication challenges in LAE.

Conclusion: The paper highlights future research directions and open issues for LLM-empowered near-field communications in LAE.

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>
