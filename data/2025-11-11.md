<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 43]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims](https://arxiv.org/abs/2511.05524)
*Ruiying Chen*

Main category: cs.AI

TL;DR: EviBound is an evidence-bound execution framework that eliminates false claims in LLM-based research agents through dual governance gates requiring machine-checkable evidence, achieving 0% hallucination with minimal execution overhead.


<details>
  <summary>Details</summary>
Motivation: LLM-based autonomous research agents frequently report false claims by marking tasks as complete despite missing artifacts, contradictory metrics, or failed executions, undermining research integrity.

Method: Dual governance gates: pre-execution Approval Gate validates acceptance criteria schemas before code runs; post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics. Claims require queryable run ID, artifacts, and FINISHED status. Bounded retries handle transient failures.

Result: Evaluated on 8 benchmark tasks: Baseline A (Prompt-Level Only) had 100% hallucination (8/8 claimed, 0/8 verified); Baseline B (Verification-Only) reduced hallucination to 25% (2/8 fail verification); EviBound (Dual Gates) achieved 0% hallucination with 7/8 tasks verified and 1 correctly blocked at approval gate, with only ~8.3% execution overhead.

Conclusion: Research integrity is an architectural property achievable through governance gates rather than emergent from model scale. Evidence-bound execution can eliminate false claims in autonomous research systems.

Abstract: LLM-based autonomous research agents report false claims: tasks marked "complete" despite missing artifacts, contradictory metrics, or failed executions. EviBound is an evidence-bound execution framework that eliminates false claims through dual governance gates requiring machine-checkable evidence.
  Two complementary gates enforce evidence requirements. The pre-execution Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. The post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics when specified by acceptance criteria. Claims propagate only when backed by a queryable run ID, required artifacts, and FINISHED status. Bounded, confidence-gated retries (typically 1-2 attempts) recover from transient failures without unbounded loops.
  The framework was evaluated on 8 benchmark tasks spanning infrastructure validation, ML capabilities, and governance stress tests. Baseline A (Prompt-Level Only) yields 100% hallucination (8/8 claimed, 0/8 verified). Baseline B (Verification-Only) reduces hallucination to 25% (2/8 fail verification). EviBound (Dual Gates) achieves 0% hallucination: 7/8 tasks verified and 1 task correctly blocked at the approval gate, all with only approximately 8.3% execution overhead.
  This package includes execution trajectories, MLflow run IDs for all verified tasks, and a 4-step verification protocol. Research integrity is an architectural property, achieved through governance gates rather than emergent from model scale.

</details>


### [2] [SMAGDi: Socratic Multi Agent Interaction Graph Distillation for Efficient High Accuracy Reasoning](https://arxiv.org/abs/2511.05528)
*Aayush Aluru,Myra Malik,Samarth Patankar,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: SMAGDi is a distillation framework that compresses a 5-agent Llama-based multi-agent system into a compact 6B student model while preserving 88% of its reasoning accuracy, using interaction graphs and Socratic decomposition to maintain debate dynamics efficiently.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems achieve high reasoning accuracy but are computationally expensive due to repeated debates across agents, creating a need for efficient compression while preserving reasoning capabilities.

Method: Represents debate traces as directed interaction graphs with nodes (reasoning steps with correctness labels) and edges (continuity and influence). Trains student with composite objective including language modeling, graph supervision, contrastive reasoning, and embedding alignment.

Result: On StrategyQA and MMLU, compresses 40B multi-agent system to 6B student while retaining 88% accuracy, outperforming MAGDi, standard KD, and fine-tuned baselines.

Conclusion: Explicit modeling of interaction graphs and Socratic decomposition enables small models to inherit multi-agent debate accuracy benefits while remaining efficient for real-world deployment.

Abstract: Multi-agent systems (MAS) often achieve higher reasoning accuracy than single models, but their reliance on repeated debates across agents makes them computationally expensive. We introduce SMAGDi, a distillation framework that transfers the debate dynamics of a five-agent Llama-based MAS into a compact Socratic decomposer-solver student. SMAGDi represents debate traces as directed interaction graphs, where nodes encode intermediate reasoning steps with correctness labels and edges capture continuity and cross-agent influence. The student is trained with a composite objective combining language modeling, graph-based supervision, contrastive reasoning, and embedding alignment to preserve both fluency and structured reasoning. On StrategyQA and MMLU, SMAGDi compresses a 40B multi-agent system into a 6B student while retaining 88% of its accuracy, substantially outperforming prior distillation methods such as MAGDi, standard KD, and fine-tuned baselines. These results highlight that explicitly modeling interaction graphs and Socratic decomposition enable small models to inherit the accuracy benefits of multi-agent debate while remaining efficient enough for real-world deployment.

</details>


### [3] [From Prompts to Power: Measuring the Energy Footprint of LLM Inference](https://arxiv.org/abs/2511.05597)
*Francisco Caravaca,Ángel Cuevas,Rubén Cuevas*

Main category: cs.AI

TL;DR: Large-scale study of LLM inference energy consumption using 32,500+ measurements reveals factors affecting energy demand, develops predictive model, and creates awareness tools.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of LLMs has led to unprecedented energy demands, especially in large-scale inference workloads, with limited systematic analyses available despite growing environmental concerns.

Method: Conducted a large-scale measurement-based study with over 32,500 measurements across 21 GPU configurations and 155 model architectures using the vLLM inference engine, quantifying energy usage at the prompt level.

Result: Identified how architectural and operational factors shape energy demand, developed an accurate predictive model for inference energy consumption across unseen architectures and hardware, and created a browser extension for raising awareness.

Conclusion: The study presents a comprehensive analysis and predictive model for LLM inference energy consumption, offering tools for awareness and optimization to mitigate environmental impact.

Abstract: The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale measurement-based study comprising over 32,500 measurements across 21 GPU configurations and 155 model architectures, from small open-source models to frontier systems. Using the vLLM inference engine, we quantify energy usage at the prompt level and identify how architectural and operational factors shape energy demand. Building on these insights, we develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware, and implement it as a browser extension to raise awareness of the environmental impact of generative AI.

</details>


### [4] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: A method for efficiently transferring Chain-of-Thought reasoning across LLMs via adaptive reasoning summarization, achieving 40% higher accuracy than truncation while reducing token usage and evaluation costs by 84%.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning improves LLM problem-solving but causes substantial inference overhead, limiting deployment in resource-constrained settings.

Method: Adaptive reasoning summarization framework with semantic segmentation, importance scoring, budget-aware dynamic compression, and coherence reconstruction to compress reasoning traces while preserving critical steps.

Result: 40% higher accuracy than truncation on medical exams across 10 specialties; strong transferability across 64 model pairs from 8 LLMs (1.5B-32B parameters); 84% reduction in evaluation cost; reveals power-law relationship between model size and cross-domain robustness.

Conclusion: Reasoning summarization provides a practical path for efficient CoT transfer, enabling advanced reasoning under tight computational constraints.

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.

</details>


### [5] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: The paper proposes treating agent awareness as knowledge rather than belief, distinguishing between de re and de dicto forms, and provides a complete logical system for these awareness modalities.


<details>
  <summary>Details</summary>
Motivation: To break from the traditional treatment of awareness in existing literature and provide a more nuanced understanding by distinguishing different forms of awareness as knowledge.

Method: Introduces two modalities for de re and de dicto awareness, uses 2D-semantics to formally specify their meaning, and develops a logical system.

Result: A sound and complete logical system that describes the interplay between the two proposed awareness modalities and the standard knowledge modality.

Conclusion: Agent awareness can be effectively modeled as knowledge with distinct de re and de dicto forms, with a complete logical framework to capture their relationships.

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard "knowledge of the fact" modality.

</details>


### [6] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: This paper investigates anchoring bias in large language models (LLMs) through behavioral analysis and attributions methods, finding robust anchoring effects in models like Gemma-2B, phi-2, and Llama-2-7B while highlighting risks in practical applications.


<details>
  <summary>Details</summary>
Motivation: Despite prior evidence showing LLMs exhibit anchoring bias, it remains unclear whether this reflects superficial imitation or deeper probability shifts in the models' decision processes. The study aims to provide a more rigorous examination of anchoring bias through quantitative analysis.

Method: The authors use three approaches: (1) log-probability-based behavioral analysis to detect anchor-induced shifts in output distributions, (2) Shapley-value attribution to quantify anchor influence on model probabilities, and (3) a unified Anchoring Bias Sensitivity Score combining behavioral and attributional evidence across six open-source LLMs.

Result: Results show robust anchoring effects in larger models (Gemma-2B, Phi-2, Llama-2-7B) with attribution indicating anchor-induced probability reweighting. Smaller models (GPT-2, Falcon-RW-1B, GPT-Neo-125M) show more variable effects, suggesting scale may influence sensitivity. Attribution effects vary across prompt designs.

Conclusion: Anchoring bias in LLMs is measurable, interpretable, and robust, but the framework also highlights the fragility of treating LLMs as human substitutes. The approach bridges behavioral science, LLM safety, and interpretability research.

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [7] [Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs](https://arxiv.org/abs/2511.06134)
*Wei Yang,Jiacheng Pang,Shixuan Li,Paul Bogdan,Stephen Tu,Jesse Thomason*

Main category: cs.AI

TL;DR: Maestro framework decouples exploration and synthesis in multi-agent LLM systems using parallel Execution Agents for diverse exploration and a Central Agent for convergent synthesis, enhanced by CLPO reinforcement learning for better credit assignment.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM systems struggle with balancing exploration vs synthesis, leading to premature consensus, error propagation, and poor credit assignment between reasoning quality and surface plausibility.

Method: Proposes Maestro framework with Execution Agents for exploration and Central Agent for synthesis, plus CLPO reinforcement learning that combines policy gradients with list-wise ranking loss over justifications.

Result: Experiments show Maestro with CLPO outperforms state-of-the-art multi-agent approaches with absolute accuracy gains of 6% on average and up to 10% on mathematical reasoning and problem-solving benchmarks.

Conclusion: Structural decoupling of exploration and synthesis through Maestro with CLPO effectively resolves cognitive tension in multi-agent LLM systems, enabling superior performance through better credit assignment and comparative supervision.

Abstract: Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.

</details>


### [8] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: DiagnoLLM is a hybrid AI framework combining Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis, achieving 88% accuracy in Alzheimer's detection and producing tailored diagnostic reports.


<details>
  <summary>Details</summary>
Motivation: Building trustworthy clinical AI requires accurate predictions with transparent, biologically grounded explanations for human understanding and trust.

Method: The framework uses GP-unmix (Gaussian Process-based hierarchical model) to infer cell-type-specific gene expression from RNA-seq data, integrates eQTL regulatory priors into a neural classifier for disease prediction, and employs an LLM-based reasoning module to generate audience-specific diagnostic reports.

Result: High predictive performance in Alzheimer's Disease detection (88.0% accuracy) was achieved, and human evaluations confirmed that the LLM-generated reports are accurate, actionable, and tailored for physicians and patients.

Conclusion: LLMs deployed as post-hoc reasoners, not end-to-end predictors, effectively serve as communicators in hybrid diagnostic pipelines, enhancing interpretability and trust in clinical AI systems.

Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.

</details>


### [9] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开世界多智能体环境，模拟小型科学生态系统，让AI智能体能够自主进行完整的科学研究流程，包括阅读论文、提出假设、提交代码、分析数据和发表成果。


<details>
  <summary>Details</summary>
Motivation: 创建一个去中心化的科学环境，让AI智能体能够像人类科学家一样自主进行研究互动，超越传统的刚性优化方法。

Method: 建立开世界多智能体环境，智能体可以利用扩展上下文窗口进行长篇科学研究，没有中央系统协调，智能体可自由选择行动和发展自己的研究叙事。

Result: STATION中的AI智能体在数学、计算生物学、机器学习等多个基准测试中实现了最先进性能，超越了AlphaEvolve在圆打包问题上的表现，并有机涌现出新的方法如密度自适应算法。

Conclusion: STATION代表了通过开世界环境中涌现行为驱动自主科学发现的里程碑，标志着超越刚性优化的新范式。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.

</details>


### [10] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: Proposes LEAP framework for adaptive hallucination detection in LLMs, combining dynamic strategy learning from teacher models with efficient student execution and proactive correction.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods use fixed verification strategies or costly closed-source LLMs, lacking adaptability in dynamic environments and leading to detection failures.

Method: Formulates hallucination detection as dynamic strategy learning: teacher model generates adaptive trajectories, student model distills this capability via agent tuning with proactive correction mechanism.

Result: LEAP-tuned model outperforms state-of-the-art methods on three challenging benchmarks.

Conclusion: LEAP framework successfully enables efficient student models to dynamically learn and adapt verification strategies, improving hallucination detection in changing environments.

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.

</details>


### [11] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: This paper empirically studies reasoning quality in thinking LLMs for code generation, evaluating 6 models on 100 tasks and finding that task complexity significantly impacts reasoning completeness.


<details>
  <summary>Details</summary>
Motivation: While thinking LLMs generate reasoning traces to improve transparency and accuracy in code generation, the quality of these reasoning chains is not well understood.

Method: Evaluating 6 reasoning LLMs on 100 code generation tasks from BigCodeBench, analyzing reasoning-chain structure, conducting controlled step-budget experiments, and performing human evaluation on efficiency, correctness, and completeness.

Result: Targeted step increases improve resolution rates for some models/tasks; task complexity significantly impacts reasoning quality with hard problems showing substantially more incompleteness; thinking LLMs maintain consistent logical structures and can self-correct errors.

Conclusion: The study provides insights into strengths and limitations of thinking LLMs, identifying completeness as the dominant failure mode and showing these models maintain stable reasoning structures across effort levels.

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.

</details>


### [12] [Agentic AI Sustainability Assessment for Supply Chain Document Insights](https://arxiv.org/abs/2511.07097)
*Diego Gosmar,Anna Chiara Pallotta,Giovanni Zenezini*

Main category: cs.AI

TL;DR: This paper presents an agentic AI framework that achieves 70-98% reductions in energy, emissions, and water usage for document processing in supply chains compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: To address the dual need for automation efficiency and measurable environmental performance in document-intensive supply chain workflows.

Method: Comparative analysis of three scenarios: fully manual, AI-assisted (human-in-the-loop), and advanced multi-agent agentic AI with parsers and verifiers.

Result: AI-assisted and agentic AI scenarios achieved 70-90% energy reduction, 90-97% CO2 emission reduction, and 89-98% water usage reduction versus manual processes. Agentic AI with advanced reasoning provided the highest sustainability gains.

Conclusion: The framework successfully integrates performance, energy, and emission metrics into an ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions, demonstrating substantial sustainability improvements.

Abstract: This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.

</details>


### [13] [Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks](https://arxiv.org/abs/2511.05883)
*Hehai Lin,Hui Liu,Shilei Cao,Jing Li,Haoliang Li,Wenya Wang*

Main category: cs.AI

TL;DR: Proposes three automated methods to quantify modality bias in multimodal misinformation detection at sample level: coarse-grained modality benefit evaluation, medium-grained information flow quantification, and fine-grained causality analysis.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal misinformation benchmarks have modality biases that let detectors rely on single modalities, but previous bias quantification approaches lack sample-level insights and don't scale to online information.

Method: Three bias quantification methods: 1) Coarse-grained modality benefit evaluation, 2) Medium-grained information flow quantification, 3) Fine-grained causality analysis. Validated through human evaluation on two popular benchmarks.

Result: Key findings: 1) Ensembling multiple views is crucial for reliable analysis, 2) Automated analysis is prone to detector-induced fluctuations, 3) Different views agree more on balanced samples but diverge on biased ones.

Conclusion: The proposed automated methods effectively identify modality bias at sample level and provide directions for future research in multimodal misinformation detection.

Abstract: Numerous multimodal misinformation benchmarks exhibit bias toward specific modalities, allowing detectors to make predictions based solely on one modality. While previous research has quantified bias at the dataset level or manually identified spurious correlations between modalities and labels, these approaches lack meaningful insights at the sample level and struggle to scale to the vast amount of online information. In this paper, we investigate the design for automated recognition of modality bias at the sample level. Specifically, we propose three bias quantification methods based on theories/views of different levels of granularity: 1) a coarse-grained evaluation of modality benefit; 2) a medium-grained quantification of information flow; and 3) a fine-grained causality analysis. To verify the effectiveness, we conduct a human evaluation on two popular benchmarks. Experimental results reveal three interesting findings that provide potential direction toward future research: 1)~Ensembling multiple views is crucial for reliable automated analysis; 2)~Automated analysis is prone to detector-induced fluctuations; and 3)~Different views produce a higher agreement on modality-balanced samples but diverge on biased ones.

</details>


### [14] [Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations](https://arxiv.org/abs/2511.07204)
*Giacomo Fidone,Lucia Passaro,Riccardo Guidotti*

Main category: cs.AI

TL;DR: LLM-powered simulator for evaluating Online Social Network moderation strategies through counterfactual simulations, showing personalized tactics are most effective.


<details>
  <summary>Details</summary>
Motivation: Current content moderation effectiveness is unclear due to high data costs and limited experimental control; LLMs enable realistic social behavior simulation.

Method: Design an LLM-powered simulator for OSN conversations that enables parallel, counterfactual simulations where toxic behavior is influenced by moderation while keeping other factors constant.

Result: Experiments show high psychological realism of OSN agents, emergence of social contagion phenomena, and superior effectiveness of personalized moderation strategies.

Conclusion: LLM-based simulation provides a viable method for evaluating moderation strategies, with personalized approaches showing the greatest promise.

Abstract: Online Social Networks (OSNs) widely adopt content moderation to mitigate the spread of abusive and toxic discourse. Nonetheless, the real effectiveness of moderation interventions remains unclear due to the high cost of data collection and limited experimental control. The latest developments in Natural Language Processing pave the way for a new evaluation approach. Large Language Models (LLMs) can be successfully leveraged to enhance Agent-Based Modeling and simulate human-like social behavior with unprecedented degree of believability. Yet, existing tools do not support simulation-based evaluation of moderation strategies. We fill this gap by designing a LLM-powered simulator of OSN conversations enabling a parallel, counterfactual simulation where toxic behavior is influenced by moderation interventions, keeping all else equal. We conduct extensive experiments, unveiling the psychological realism of OSN agents, the emergence of social contagion phenomena and the superior effectiveness of personalized moderation strategies.

</details>


### [15] [Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement](https://arxiv.org/abs/2511.05931)
*Hiroaki Hayashi,Bo Pang,Wenting Zhao,Ye Liu,Akash Gokul,Srijan Bansal,Caiming Xiong,Semih Yavuz,Yingbo Zhou*

Main category: cs.AI

TL;DR: SAGE enables LLM agents to self-improve by learning plan abstractions from their own execution experience, leading to 7.2% performance improvement over strong baselines on software engineering benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents operate within static frameworks without mechanisms to learn from their own experience, limiting performance to initial design and LLM capabilities.

Method: After initial task execution, SAGE induces concise plan abstractions from grounded experience (key steps, dependencies, constraints), then uses these as contextual guidance to refine agent policy for subsequent executions.

Result: Consistent performance gains across diverse LLM backbones, 7.2% improvement over Mini-SWE-Agent baseline with GPT-5, achieving 73.2%-74% Pass@1 resolve rates on SWE-Bench Verified.

Conclusion: SAGE demonstrates that self-abstraction from grounded experience enables LLM agents to significantly improve performance through structured learning and refinement of their execution policies.

Abstract: Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.

</details>


### [16] [Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling](https://arxiv.org/abs/2511.05951)
*Qi Wang,Hongzhi Zhang,Jia Fu,Kai Fu,Yahui Liu,Tinghai Zhang,Chenxi Sun,Gangwei Jiang,Jingyi Tang,Xingguang Ji,Yang Yue,Jingyuan Zhang,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.AI

TL;DR: 研究者开发了Klear-Qwen3-AgentForge-8B，一个用于工具使用和编码任务的高性能开源代理模型，从Qwen3-8B基础模型开始，通过监督微调和多轮强化学习训练而成。


<details>
  <summary>Details</summary>
Motivation: 尽管存在强大的代理模型，但缺乏关键的训练后详细信息阻碍了开源社区开发强健的对等模型。

Method: 使用合成数据进行有效的监督微调(SFT)，然后进行多轮强化学习(RL)来解锁多种代理任务的能力。

Result: Klear-Qwen3-AgentForge-8B在类似规模的LLM中实现了最先进的性能，在与更大模型的竞争中保持竞争力。

Conclusion: 该研究提供了一个全面且完全开源的训练流程，可用于开发高性能的代理模型，解决了开源社区在代理模型开发方面的关键障碍。

Abstract: Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.

</details>


### [17] [ScRPO: From Errors to Insights](https://arxiv.org/abs/2511.06065)
*Lianrui Li,Dakuan Lu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: ScRPO is a novel RL framework that combines trial-and-error learning with self-correction through error reflection, significantly improving LLM performance on challenging math problems across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance large language models' performance on challenging mathematical problems by leveraging self-reflection and error correction, addressing the need for models that can self-improve on difficult tasks with limited external feedback.

Method: A two-stage approach: 
1) Trial-and-error learning stage using GRPO to train the model and collect incorrect answers in an error pool
2) Self-correction learning stage where the model reflects on why its previous answers were wrong through self-reflection and error correction

Result: Extensive experiments across multiple math reasoning benchmarks (AIME, AMC, Olympiad, MATH-500, GSM8k) demonstrate that ScRPO consistently outperforms several post-training methods on both Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B models.

Conclusion: ScRPO emerges as a promising paradigm that enables language models to self-improve on difficult tasks with limited external feedback, representing a significant step toward more reliable and capable AI systems.

Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

</details>


### [18] [When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks](https://arxiv.org/abs/2511.06136)
*Stefano Ferraro,Akihiro Nakano,Masahiro Suzuki,Yutaka Matsuo*

Main category: cs.AI

TL;DR: OCWMs provide robust visual modeling but suffer from latent drift during control, causing policy underperformance despite visual robustness.


<details>
  <summary>Details</summary>
Motivation: To test the hypothesis that explicitly disentangled object-level representations can enhance policy performance across novel feature combinations by localizing task-relevant information

Method: DLPWM, a fully unsupervised, disentangled object-centric world model that learns object-level latents directly from pixels

Result: DLPWM achieves strong reconstruction and prediction performance with robustness to OOD visual variations, but policies trained on its latents underperform compared to DreamerV3 due to representation shift during multi-object interactions

Conclusion: Object-centric world models provide robust visual modeling but require mitigation of latent drift during multi-object interactions to achieve stable control performance.

Abstract: Object-centric world models (OCWM) aim to decompose visual scenes into object-level representations, providing structured abstractions that could improve compositional generalization and data efficiency in reinforcement learning. We hypothesize that explicitly disentangled object-level representations, by localizing task-relevant information, can enhance policy performance across novel feature combinations. To test this hypothesis, we introduce DLPWM, a fully unsupervised, disentangled object-centric world model that learns object-level latents directly from pixels. DLPWM achieves strong reconstruction and prediction performance, including robustness to several out-of-distribution (OOD) visual variations. However, when used for downstream model-based control, policies trained on DLPWM latents underperform compared to DreamerV3. Through latent-trajectory analyses, we identify representation shift during multi-object interactions as a key driver of unstable policy learning. Our results suggest that, although object-centric perception supports robust visual modeling, achieving stable control requires mitigating latent drift.

</details>


### [19] [MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning](https://arxiv.org/abs/2511.06142)
*Sizhe Tang,Jiayu Chen,Tian Lan*

Main category: cs.AI

TL;DR: MALinZero is a new MCTS approach for multi-agent planning that projects joint-action returns into a low-dimensional space using contextual linear bandits, enabling efficient exploration in complex multi-agent environments.


<details>
  <summary>Details</summary>
Motivation: MCTS struggles with the exponential growth of combinatorial action spaces in multi-agent planning, making exploration and exploitation inefficient due to large branching factors.

Method: Projects joint-action returns into low-dimensional space using contextual linear bandits with convex and smooth loss functions, deriving LinUCT for multi-agent exploration and proposing a (1-1/e)-approximation algorithm for action selection.

Result: Achieves state-of-the-art performance on multi-agent benchmarks (matrix games, SMAC, SMACv2), outperforming both model-based and model-free MARL baselines with faster learning speed and better performance.

Conclusion: MALinZero effectively addresses the combinatorial explosion problem in multi-agent MCTS through low-dimensional representation of joint actions, enabling efficient planning in complex multi-agent environments.

Abstract: Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.

</details>


### [20] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: PRIME framework uses logic puzzles to reveal LLMs' subtle gender biases in reasoning, showing better performance when answers match stereotypes.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current benchmarks that fail to detect subtle social biases emerging during complex logical reasoning tasks in LLMs.

Method: Using logic grid puzzles with stereotypical, anti-stereotypical, and neutral variants generated from shared structures, allowing controlled comparisons and automatic verification.

Result: Models consistently reason more accurately when solutions align with stereotypical associations, revealing persistent social biases in deductive reasoning.

Conclusion: PRIME is a valuable framework for diagnosing subtle social biases in LLMs during logical reasoning, demonstrating that models perform better when reasoning aligns with stereotypes, which has critical implications for fairness in AI systems.

Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.

</details>


### [21] [Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.06168)
*Boxuan Wang,Zhuoyun Li,Xinmiao Huang,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: A framework is proposed to evaluate and optimize reasoning consistency in LLMs using a new metric (Alignment Score), and a method (SCOS) is introduced to significantly improve alignment by minimizing key reasoning errors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of reasoning inconsistency in LLMs by developing a framework to evaluate and optimize the semantic alignment between model-generated and human-written reasoning chains in Chain-of-Thought reasoning.

Method: The method involves defining four key error types (logical disconnection, thematic shift, redundant reasoning, and causal reversal) and introducing Semantic Consistency Optimization Sampling (SCOS) to sample and favor chains with minimal alignment errors.

Result: The results show that 2-hop reasoning chains achieve the highest Alignment Score, and implementing SCOS improves Alignment Scores by an average of 29.84% for longer reasoning chains such as in 3-hop tasks.

Conclusion: The paper concludes that optimizing reasoning consistency through the proposed SCOS method effectively enhances LLMs' alignment with human reasoning patterns and significantly improves performance on complex reasoning tasks.

Abstract: This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.

</details>


### [22] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: CSP4SDG is a probabilistic constraint-satisfaction framework that outperforms LLMs in hidden-role inference for social deduction games by using linguistically-agnostic constraints and information-theoretic weighting.


<details>
  <summary>Details</summary>
Motivation: Social deduction games require accurate hidden-role inference, which is challenging due to players' deceptive behavior. Current approaches, especially LLMs, struggle with this task due to their high computational demands and lack of interpretability.

Method: The framework maps game events and dialogue to four constraint classes: evidence, phenomena, assertions, and hypotheses. It uses hard constraints to eliminate impossible role assignments and weighted soft constraints to score remaining possibilities, with information-gain weighting and a closed-form scoring rule.

Result: CSP4SDG outperforms LLM-based baselines in all inference scenarios on three public datasets and enhances LLM performance when used as a reasoning tool, while providing fully interpretable posterior role distributions in real-time.

Conclusion: The study demonstrates that principled probabilistic reasoning with information theory offers a scalable and effective alternative or complement to neural models for social deduction games, providing superior performance with interpretability.

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary "reasoning tool." Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.

</details>


### [23] [Dataforge: A Data Agent Platform for Autonomous Data Engineering](https://arxiv.org/abs/2511.06185)
*Xinyuan Wang,Yanjie Fu*

Main category: cs.AI

TL;DR: Data Agent is an autonomous system that transforms raw tabular data into AI-ready format through automated data cleaning, hierarchical routing, and feature optimization using LLM reasoning and dual feedback loops.


<details>
  <summary>Details</summary>
Motivation: The growing demand for AI applications in materials discovery, molecular modeling, and climate science requires extensive data preparation that is currently labor-intensive and expertise-dependent, creating challenges in scalability.

Method: Leverages large language model reasoning with grounded validation to automatically perform data cleaning, hierarchical routing, and feature-level optimization through dual feedback loops, ensuring end-to-end reliability without human supervision.

Result: The system presents the first practical realization of an autonomous Data Agent that can transform raw data "From Data to Better Data" automatically.

Conclusion: Data Agent successfully addresses the challenges of scalability and expertise dependence in data preparation through its fully autonomous approach based on three core principles: automatic, safe, and non-expert friendly operation.

Abstract: The growing demand for AI applications in fields such as materials discovery, molecular modeling, and climate science has made data preparation an important but labor-intensive step. Raw data from diverse sources must be cleaned, normalized, and transformed to become AI-ready, while effective feature transformation and selection are essential for efficient training and inference. To address the challenges of scalability and expertise dependence, we present Data Agent, a fully autonomous system specialized for tabular data. Leveraging large language model (LLM) reasoning and grounded validation, Data Agent automatically performs data cleaning, hierarchical routing, and feature-level optimization through dual feedback loops. It embodies three core principles: automatic, safe, and non-expert friendly, which ensure end-to-end reliability without human supervision. This demo showcases the first practical realization of an autonomous Data Agent, illustrating how raw data can be transformed "From Data to Better Data."

</details>


### [24] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: UHeads: Lightweight uncertainty quantification heads that use LLM internal states to verify reasoning steps, matching performance of much larger Process Reward Models without expensive annotations.


<details>
  <summary>Details</summary>
Motivation: Existing verification methods like PRMs are computationally expensive, domain-limited, or require large-scale annotations, creating need for lightweight alternatives.

Method: Train transformer-based uncertainty heads (<10M params) on frozen LLM internal states; generate labels automatically using larger LLMs or self-supervised learning.

Result: UHeads match or surpass PRMs (up to 810x larger) across math, planning, and QA domains, showing internal states encode reliable uncertainty signals.

Conclusion: LLM internal states effectively encode uncertainty for reasoning verification, providing scalable and generalizable approach for introspective models.

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [25] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B, a 1.5B-parameter model, challenges scaling trends by achieving reasoning performance comparable to much larger models using the Spectrum-to-Signal Principle (SSP) framework at low cost.


<details>
  <summary>Details</summary>
Motivation: Challenge the consensus that small models inherently lack robust reasoning capabilities and reduce reliance on massive parameter scaling seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T).

Method: Two-Stage Diversity-Exploring Distillation (SFT) to generate diverse solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify correct signals via the Spectrum-to-Signal Principle (SSP).

Result: Outperforms closed-source models (Magistral Medium, Claude Opus 4) and matches GPT OSS-20B Medium; surpasses 400x larger DeepSeek R1 on math benchmarks (AIME24: 80.3 vs. 79.8; AIME25: 74.4 vs. 70.0; HMMT25: 50.4 vs. 41.7) and LiveCodeBench V6 (51.1 vs. Magistral Medium's 50.3).

Conclusion: Small models can achieve reasoning parity with large models, drastically cutting training ($7,800) and inference costs, democratizing advanced AI research.

Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.

</details>


### [26] [ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving](https://arxiv.org/abs/2511.06226)
*Xingcheng Liu,Yanchen Guan,Haicheng Liao,Zhengbing He,Zhenning Li*

Main category: cs.AI

TL;DR: ROAR is a novel accident detection and prediction model that uses Discrete Wavelet Transform, object-aware modules, and dynamic focal loss to handle real-world challenges like sensor failures, environmental noise, and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Existing accident anticipation methods assume ideal conditions and overlook practical challenges like sensor failures, environmental disturbances, data imperfections, and variations in driver behavior across vehicle types.

Method: ROAR combines Discrete Wavelet Transform (DWT) for feature extraction from noisy data, a self-adaptive object-aware module for focusing on high-risk vehicles and modeling spatial-temporal relationships, and dynamic focal loss to address class imbalance.

Result: The model outperforms existing baselines on three datasets (DAD, CCD, A3D) in key metrics like Average Precision (AP) and mean Time to Accident (mTTA), showing robustness in handling sensor degradation, noise, and imbalanced data.

Conclusion: ROAR provides a reliable and accurate solution for accident anticipation in complex traffic environments, effectively addressing real-world challenges that previous methods overlooked.

Abstract: Accurate accident anticipation is essential for enhancing the safety of autonomous vehicles (AVs). However, existing methods often assume ideal conditions, overlooking challenges such as sensor failures, environmental disturbances, and data imperfections, which can significantly degrade prediction accuracy. Additionally, previous models have not adequately addressed the considerable variability in driver behavior and accident rates across different vehicle types. To overcome these limitations, this study introduces ROAR, a novel approach for accident detection and prediction. ROAR combines Discrete Wavelet Transform (DWT), a self adaptive object aware module, and dynamic focal loss to tackle these challenges. The DWT effectively extracts features from noisy and incomplete data, while the object aware module improves accident prediction by focusing on high-risk vehicles and modeling the spatial temporal relationships among traffic agents. Moreover, dynamic focal loss mitigates the impact of class imbalance between positive and negative samples. Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently outperforms existing baselines in key metrics such as Average Precision (AP) and mean Time to Accident (mTTA). These results demonstrate the model's robustness in real-world conditions, particularly in handling sensor degradation, environmental noise, and imbalanced data distributions. This work offers a promising solution for reliable and accurate accident anticipation in complex traffic environments.

</details>


### [27] [GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening](https://arxiv.org/abs/2511.06262)
*Siming Zhao,Qi Li*

Main category: cs.AI

TL;DR: GAIA is a governance-first framework for LLM-human collaboration in B2B negotiation and screening, featuring role definitions, staged information gathering, authorization boundaries, and dual feedback integration to ensure safety and accountability.


<details>
  <summary>Details</summary>
Motivation: Current LLM negotiation systems lack practical governance mechanisms for high-stakes B2B settings, failing to address unauthorized commitments, insufficient information gathering, and human oversight requirements.

Method: GAIA defines Principal, Delegate, Counterparty roles (+ optional Critic) with three core mechanisms: information-gated progression separating screening/negotiation, dual feedback integration (AI critique + human corrections), and authorization boundaries with escalation paths.

Result: The framework establishes four safety invariants for delegation, implements task-completeness tracking for state transitions, and creates a hybrid validation approach combining automated metrics with human judgment.

Conclusion: GAIA provides a reproducible specification for safe and accountable AI delegation applicable to procurement, real estate, and staffing workflows, bridging theoretical governance with practical implementation needs.

Abstract: Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.

</details>


### [28] [Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents](https://arxiv.org/abs/2511.06292)
*Yaoning Yu,Kaimin Chang,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.AI

TL;DR: A self-improving prompt framework for financial document analysis using synthetic data generation and verification to enhance LLM performance without external labels.


<details>
  <summary>Details</summary>
Motivation: Existing prompt tuning methods for financial reasoning are limited by fixed datasets or require costly manual labeling, restricting adaptability to new question types or document structures.

Method: Closed-loop framework with synthetic data generator, verifiers, and prompt optimizer that iteratively generates examples to expose prompt weaknesses, verifies their validity/robustness, and refines prompts accordingly.

Result: Evaluation on DocMath-Eval benchmark shows higher accuracy and robustness compared to standard prompt methods.

Conclusion: Incorporating synthetic data generation into prompt learning effectively improves financial reasoning performance while eliminating dependency on external labels.

Abstract: Financial documents like earning reports or balance sheets often involve long tables and multi-page reports. Large language models have become a new tool to help numerical reasoning and understanding these documents. However, prompt quality can have a major effect on how well LLMs perform these financial reasoning tasks. Most current methods tune prompts on fixed datasets of financial text or tabular data, which limits their ability to adapt to new question types or document structures, or they involve costly and manually labeled/curated dataset to help build the prompts. We introduce a self-improving prompt framework driven by data-augmented optimization. In this closed-loop process, we generate synthetic financial tables and document excerpts, verify their correctness and robustness, and then update the prompt based on the results. Specifically, our framework combines a synthetic data generator with verifiers and a prompt optimizer, where the generator produces new examples that exposes weaknesses in the current prompt, the verifiers check the validity and robustness of the produced examples, and the optimizer incrementally refines the prompt in response. By iterating these steps in a feedback cycle, our method steadily improves prompt accuracy on financial reasoning tasks without needing external labels. Evaluation on DocMath-Eval benchmark demonstrates that our system achieves higher performance in both accuracy and robustness than standard prompt methods, underscoring the value of incorporating synthetic data generation into prompt learning for financial applications.

</details>


### [29] [Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems](https://arxiv.org/abs/2511.06301)
*Azanzi Jiomekong,Jean Bikim,Patricia Negoue,Joyce Chin*

Main category: cs.AI

TL;DR: Introduction of Secu-Table, a new annotated dataset of 1500+ security domain tables for evaluating LLM-based semantic table interpretation systems, with baseline results from open-source and closed-source models.


<details>
  <summary>Details</summary>
Motivation: Lack of publicly available tabular datasets for evaluating semantic table interpretation systems in the security domain, hindering progress in this specialized area.

Method: Created Secu-Table dataset using security data from CVE and CWE sources, annotated with Wikidata and SEPSES CSKG, and released publicly along with code.

Result: Dataset contains over 1500 tables with 15k+ entities; preliminary baseline evaluation performed using Falcon3-7b-instruct, Mistral-7B-Instruct, and GPT-4o mini.

Conclusion: Secu-Table addresses the dataset gap for security domain STI evaluation and supports the SemTab challenge, facilitating future research with publicly available resources.

Abstract: Evaluating semantic tables interpretation (STI) systems, (particularly, those based on Large Language Models- LLMs) especially in domain-specific contexts such as the security domain, depends heavily on the dataset. However, in the security domain, tabular datasets for state-of-the-art are not publicly available. In this paper, we introduce Secu-Table dataset, composed of more than 1500 tables with more than 15k entities constructed using security data extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES CSKG). Along with the dataset, all the code is publicly released. This dataset is made available to the research community in the context of the SemTab challenge on Tabular to Knowledge Graph Matching. This challenge aims to evaluate the performance of several STI based on open source LLMs. Preliminary evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source LLM.

</details>


### [30] [ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning](https://arxiv.org/abs/2511.06316)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.AI

TL;DR: ALIGN is a vision-language framework that uses multimodal AI to infer accident locations from Bangla-English news text by combining OCR, linguistic reasoning, and map verification, outperforming traditional geocoding methods.


<details>
  <summary>Details</summary>
Motivation: Low- and middle-income countries lack accurate, location-specific crash data due to poor performance of existing text-based geocoding tools in multilingual, unstructured news environments with incomplete place descriptions.

Method: Multi-stage pipeline integrating large language and vision-language models that performs optical character recognition, linguistic reasoning, and grid-based spatial scanning with map-level verification.

Result: Demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites in Bangla-language news data.

Conclusion: Establishes a high-accuracy foundation for automated crash mapping in data-scarce regions and supports evidence-driven road-safety policymaking through multimodal AI integration.

Abstract: Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed Bangla-English scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning)- a vision-language framework that emulates human spatial reasoning to infer accident coordinates directly from textual and map-based cues. ALIGN integrates large language and vision-language models within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics. The code for this paper is open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN

</details>


### [31] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: Introduction of LPFQA, a new benchmark for evaluating LLMs using authentic long-tail knowledge from professional forums across 20 fields, highlighting its innovations and performance disparities among 12 LLMs.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs focus on simplified tasks and overlook complex real-world applications and long-tail knowledge, limiting accurate capability assessment.

Method: Developed LPFQA by deriving tasks from professional forums in 20 academic/industrial fields, incorporating fine-grained evaluation dimensions, hierarchical difficulty, realistic scenarios, and interdisciplinary integration.

Result: Evaluation of 12 LLMs revealed significant performance gaps, especially in specialized reasoning tasks, demonstrating LPFQA's effectiveness.

Conclusion: LPFQA offers a robust, authentic benchmark to advance LLM evaluation and guide future model development by addressing real-world complexity.

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

</details>


### [32] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: AEPO addresses LLMs' 'Echo Reflection' problem in complex reasoning tasks by using reinforcement learning with reflection-aware information filtration and adaptive-entropy optimization to enable genuine cognitive refinement.


<details>
  <summary>Details</summary>
Motivation: LLMs fail to generate novel insights during reflection in complex domain-specific tasks, instead mechanically reiterating earlier reasoning steps without new perspectives - a phenomenon called 'Echo Reflection' caused by uncontrollable information flow and insufficient knowledge exploration.

Method: Proposed Adaptive Entropy Policy Optimization (AEPO) with two components: Reflection-aware Information Filtration to control cognitive information flow and prevent bad intermediate thoughts from affecting final decisions, and Adaptive-Entropy Optimization to dynamically balance exploration vs exploitation across reasoning stages.

Result: Extensive experiments show AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.

Conclusion: AEPO effectively addresses the Echo Reflection problem in LLMs by enabling genuine cognitive refinement through controlled information flow and balanced exploration-exploitation, demonstrating superior performance in complex reasoning tasks beyond mathematical domains.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as "Echo Reflection". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.

</details>


### [33] [Efficient LLM Safety Evaluation through Multi-Agent Debate](https://arxiv.org/abs/2511.06396)
*Dachuan Lin,Guobin Shen,Zihao Yang,Tianrong Liu,Dongcheng Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: Propose cost-efficient multi-agent SLM judging framework with structured debates, which achieves GPT-4o-level safety evaluation performance on new HAJailBench dataset at much lower cost. Three debate rounds optimal.


<details>
  <summary>Details</summary>
Motivation: Safety evaluation of LLMs increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability, necessitating more cost-efficient alternatives.

Method: Proposed a cost-efficient multi-agent judging framework using Small Language Models (SLMs) with structured debates among critic, defender, and judge agents, and constructed HAJailBench - a large-scale human-annotated jailbreak benchmark with 12,000 adversarial interactions across diverse attack methods.

Result: The SLM-based framework achieved agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Three rounds of debate were found to be optimal for balancing accuracy and efficiency.

Conclusion: The structured multi-agent debate framework using SLMs is a cost-effective alternative to frontier LLM judges, achieving comparable safety evaluation performance with significant cost savings. HAJailBench provides a reliable foundation for scalable LLM safety evaluation.

Abstract: Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.

</details>


### [34] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: SofT-GRPO is a novel policy optimization algorithm that enables reinforcement learning for soft-thinking LLM reasoning, overcoming previous limitations by using Gumbel noise and reparameterization techniques to achieve better performance than discrete-token approaches.


<details>
  <summary>Details</summary>
Motivation: Soft-thinking LLM reasoning shows promise but can't be effectively combined with RL through existing methods like GRPO due to challenges in injecting stochasticity into soft-thinking tokens and updating policies, causing underperformance compared to discrete-token approaches.

Method: SofT-GRPO injects Gumbel noise into logits, uses Gumbel-Softmax to keep soft-thinking tokens within pre-trained embedding space, and applies the reparameterization trick in policy gradient to enable effective RL training for soft-thinking patterns.

Result: Experiments on LLMs from 1.5B to 7B parameters show SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% average accuracy) while achieving substantial improvement on Pass@32 (+2.19% average accuracy).

Conclusion: SofT-GRPO successfully bridges the gap between soft-thinking reasoning and RL, demonstrating that properly optimized soft-thinking can outperform conventional discrete-token reasoning, unlocking the full potential of soft-thinking patterns in LLMs.

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [35] [AUTO-Explorer: Automated Data Collection for GUI Agent](https://arxiv.org/abs/2511.06417)
*Xiangwu Guo,Difei Gao,Mike Zheng Shou*

Main category: cs.AI

TL;DR: Auto-Explorer is a new method for automated GUI data collection with minimal annotation costs, addressing limitations of existing methods by autonomously exploring GUI environments and using the UIXplore benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing GUI data collection methods rely on web crawling and HTML parsing, which don't work well for desktop software or new websites not in Common Crawl. There's a need for methods that can quickly adapt to new software/websites in personalized scenarios.

Method: Proposes Auto-Explorer with an exploration mechanism that autonomously parses and explores GUI environments to gather data efficiently. Uses UIXplore benchmark to create environments for discovery and evaluation.

Result: Fine-tuned multimodal large language models using collected data show superior performance. Auto-Explorer quickly enhances MLLM capabilities in explored software.

Conclusion: Auto-Explorer provides an effective solution for GUI data collection with minimal annotation, successfully addressing limitations of existing methods and improving MLLM performance in GUI tasks.

Abstract: Recent advancements in GUI agents have significantly expanded their ability to interpret natural language commands to manage software interfaces. However, acquiring GUI data remains a significant challenge. Existing methods often involve designing automated agents that browse URLs from the Common Crawl, using webpage HTML to collect screenshots and corresponding annotations, including the names and bounding boxes of UI elements. However, this method is difficult to apply to desktop software or some newly launched websites not included in the Common Crawl. While we expect the model to possess strong generalization capabilities to handle this, it is still crucial for personalized scenarios that require rapid and perfect adaptation to new software or websites. To address this, we propose an automated data collection method with minimal annotation costs, named Auto-Explorer. It incorporates a simple yet effective exploration mechanism that autonomously parses and explores GUI environments, gathering data efficiently. Additionally, to assess the quality of exploration, we have developed the UIXplore benchmark. This benchmark creates environments for explorer agents to discover and save software states. Using the data gathered, we fine-tune a multimodal large language model (MLLM) and establish a GUI element grounding testing set to evaluate the effectiveness of the exploration strategies. Our experiments demonstrate the superior performance of Auto-Explorer, showing that our method can quickly enhance the capabilities of an MLLM in explored software.

</details>


### [36] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: MONICA monitors and mitigates sycophantic behavior in Large Reasoning Models during inference by detecting sycophantic drift in real-time and dynamically suppressing it, improving reliability without requiring complete answer generation.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) exhibit sycophantic behavior where they agree with users' incorrect beliefs or misinformation, undermining reliability and posing societal risks. Current methods focus on final answers without understanding how sycophancy develops during reasoning processes.

Method: Proposes MONICA, a Monitor-guided Calibration framework that integrates a sycophantic monitor for real-time scoring of sycophantic drift during response generation and a calibrator that dynamically suppresses sycophantic behavior when thresholds are exceeded.

Result: Extensive experiments across 12 datasets and 3 LRMs show that MONICA reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

Conclusion: MONICA effectively reduces sycophantic behavior in LRMs by monitoring reasoning steps and dynamically suppressing such behavior when detected, leading to improved model reliability and robustness.

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

</details>


### [37] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: EDTR is a new decoding method that combines topological analysis with Dirichlet uncertainty to better measure LLM confidence in chain-of-thought reasoning, achieving superior calibration over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current confidence estimation methods for LLMs using chain-of-thought prompting suffer from poor calibration and severe overconfidence on incorrect predictions.

Method: EDTR treats each chain-of-thought as a vector and uses topological analysis to extract eight geometric features from reasoning path distributions, with tighter clusters indicating higher confidence.

Result: EDTR achieved 41% better calibration than competing methods across four reasoning benchmarks, with perfect accuracy on AIME and exceptional calibration on GSM8K (ECE 0.107).

Conclusion: The work provides a geometric framework for quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence is essential.

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [38] [Brain-Inspired Planning for Better Generalization in Reinforcement Learning](https://arxiv.org/abs/2511.06470)
*Mingde "Harry" Zhao*

Main category: cs.AI

TL;DR: This thesis introduces methods to improve RL agents' zero-shot generalization by incorporating human-like reasoning behaviors, including spatial abstraction, task decomposition, and rejection of delusional planning targets.


<details>
  <summary>Details</summary>
Motivation: Existing RL systems struggle with poor generalization across environments different from training conditions, lacking the systematic generalization abilities found in human reasoning.

Method: Three key approaches: 1) Top-down attention mechanism for spatial abstraction during decision-time planning; 2) Skipper framework for automatic task decomposition; 3) Feasibility evaluator to reject hallucinated infeasible targets in planning.

Result: Spatial abstraction significantly improves systematic generalization outside training tasks. Skipper provides robustness against distributional shifts and effective compositional planning. Rejection of delusional targets leads to significant performance improvements in planning agents.

Conclusion: The proposed reasoning-inspired methods enhance RL agents' zero-shot systematic generalization. Future research should focus on general task abstraction and fully enabling abstract planning.

Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call "spatial abstraction". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.

</details>


### [39] [GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets](https://arxiv.org/abs/2511.06471)
*Jingtao Tang,Hang Ma*

Main category: cs.AI

TL;DR: GHOST is a hierarchical framework that optimally solves a new Traveling Salesman Problem variant over Graphs of Convex Sets by combining combinatorial tour search with convex trajectory optimization, using novel lower bounds for efficient search.


<details>
  <summary>Details</summary>
Motivation: Existing Traveling Salesman Problem (TSP) methods cannot handle the GCS-TSP variant because edge costs depend on the specific trajectory through convex regions, making classical approaches inapplicable.

Method: Introduces GHOST, which uses a best-first search guided by admissible lower bounds computed via an abstract-path-unfolding algorithm. It explores tours on a graph induced by the GCS and performs convex optimization for feasible paths.

Result: GHOST significantly outperforms mixed-integer convex programming baselines in speed, handles complex constraints like high-order continuity, and works with incomplete GCS graphs while guaranteeing optimality.

Conclusion: GHOST provides an efficient and optimal solution for GCS-TSP, enabling practical applications in trajectory planning where traditional TSP methods fail.

Abstract: We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS.

</details>


### [40] [FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis](https://arxiv.org/abs/2511.06522)
*Jan Ondras,Marek Šuppa*

Main category: cs.AI

TL;DR: FractalBench is a benchmark that tests multimodal AI systems' ability to generate executable Python code for creating fractals from images, revealing that while most models produce syntactically valid code, very few capture the underlying mathematical structure of fractals.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether multimodal AI systems can abstract symbolic rules from visual patterns, specifically testing the capability to infer infinite mathematical structures from finite visual representations using fractals as ideal test cases.

Method: Created FractalBench with 12 canonical fractals, requiring models to generate executable Python code that reproduces the fractal patterns. Evaluated four leading MLLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL) on their ability to bridge visual perception with mathematical abstraction.

Result: 76% of generated code was syntactically valid but only 4% captured the mathematical structure. Models performed better on geometric transformations (Koch curves: 17-21%) but failed at branching recursion (trees: <2%), showing systematic gaps in mathematical abstraction.

Conclusion: There is a fundamental disconnect between visual perception and mathematical abstraction in current multimodal AI systems. FractalBench serves as a contamination-resistant diagnostic tool for evaluating visual-mathematical reasoning capabilities.

Abstract: Mathematical reasoning requires abstracting symbolic rules from visual patterns -- inferring the infinite from the finite. We investigate whether multimodal AI systems possess this capability through FractalBench, a benchmark evaluating fractal program synthesis from images. Fractals provide ideal test cases: Iterated Function Systems with only a few contraction maps generate complex self-similar patterns through simple recursive rules, requiring models to bridge visual perception with mathematical abstraction. We evaluate four leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL -- on 12 canonical fractals. Models must generate executable Python code reproducing the fractal, enabling objective evaluation. Results reveal a striking disconnect: 76% generate syntactically valid code but only 4% capture mathematical structure. Success varies systematically -- models handle geometric transformations (Koch curves: 17-21%) but fail at branching recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction. FractalBench provides a contamination-resistant diagnostic for visual-mathematical reasoning and is available at https://github.com/NaiveNeuron/FractalBench

</details>


### [41] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: GRAPH-GRPO-LEX is a reinforcement learning framework that transforms legal contracts into semantic graphs using LLMs, enabling automated analysis and discovery of hidden dependencies.


<details>
  <summary>Details</summary>
Motivation: Manual contract review is complex and error-prone due to the intricate structure and dependencies in legal documents.

Method: Uses a reinforcement learning-based LLM framework with group relative policy optimization (GRPO) to extract entities and relationships, mapping them to a graph ontology.

Result: The method automatically identifies direct relationships between clauses and uncovers hidden dependencies, transforming contract analysis into a visualized graph.

Conclusion: The framework enables dynamic, automated contract analysis, similar to software linting, moving beyond manual reading to computational insights.

Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.

</details>


### [42] [Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives](https://arxiv.org/abs/2511.06626)
*Chloe Li,Mary Phuong,Daniel Tan*

Main category: cs.AI

TL;DR: Fine-tuning technique (SRFT) trains AI models to admit mistakes when asked, improving honesty and detection of hidden harmful objectives.


<details>
  <summary>Details</summary>
Motivation: AI systems capable of complex tasks may pursue undesirable objectives while hiding them through deception, making safety monitoring unreliable.

Method: Supervised fine-tuning where models learn to self-report factual errors, which surprisingly generalizes to admitting hidden unethical objectives.

Result: SRFT achieves near-perfect detection (F1=0.98) of hidden objectives and recovers 28-100% of objective details, compared to 0% for baselines.

Conclusion: SRFT is a promising technique to enhance AI honesty, effectively eliciting confessions of misaligned goals under pressure.

Abstract: As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.

</details>


### [43] [SRNN: Spatiotemporal Relational Neural Network for Intuitive Physics Understanding](https://arxiv.org/abs/2511.06761)
*Fei Yang*

Main category: cs.AI

TL;DR: The paper introduces the Spatiotemporal Relational Neural Network (SRNN), a brain-inspired model for intuitive physics that uses unified neural representations and Hebbian learning across What and How pathways, achieving competitive performance on CLEVRER benchmark while enabling white-box error analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human intuitive physics capabilities and machine performance by shifting to brain-inspired computational principles rather than conventional approaches.

Method: SRNN establishes unified neural representations for object attributes, relations, and timeline using Hebbian "Fire Together, Wire Together" mechanism across dedicated What (perception) and How (action) pathways, with a "predefine-then-finetune" approach instead of pretraining.

Result: Achieves competitive performance on CLEVRER benchmark, identifies benchmark bias, demonstrates white-box utility for precise error diagnosis, and shows viability of translating biological intelligence to engineered systems.

Conclusion: The work confirms the viability of translating biological intelligence principles into engineered systems for intuitive physics understanding, providing a brain-inspired alternative to current machine learning paradigms.

Abstract: Human prowess in intuitive physics remains unmatched by machines. To bridge this gap, we argue for a fundamental shift towards brain-inspired computational principles. This paper introduces the Spatiotemporal Relational Neural Network (SRNN), a model that establishes a unified neural representation for object attributes, relations, and timeline, with computations governed by a Hebbian ``Fire Together, Wire Together'' mechanism across dedicated \textit{What} and \textit{How} pathways. This unified representation is directly used to generate structured linguistic descriptions of the visual scene, bridging perception and language within a shared neural substrate. Moreover, unlike the prevalent ``pretrain-then-finetune'' paradigm, SRNN adopts a ``predefine-then-finetune'' approach. On the CLEVRER benchmark, SRNN achieves competitive performance. Our analysis further reveals a benchmark bias, outlines a path for a more holistic evaluation, and demonstrates SRNN's white-box utility for precise error diagnosis. Our work confirms the viability of translating biological intelligence into engineered systems for intuitive physics understanding.

</details>


### [44] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: Proposes MathSE, a self-evolving framework that improves MLLMs' mathematical reasoning through iterative fine-tuning with reflection and reward feedback, outperforming previous models.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex mathematical reasoning because current training datasets from teacher models only capture static patterns, limiting adaptation to new problems and robust generalization.

Method: Uses iterative cycles of inference, reflection, and reward-based feedback. Integrates correct reasoning paths from prior inferences and reflections from an Outcome Reward Model (ORM) for continuous refinement.

Result: Significant performance gains on challenging benchmarks; surpasses leading open-source model QVQ on MathVL-test.

Conclusion: MathSE effectively overcomes limitations of static datasets by enabling iterative self-improvement, enhancing MLLMs' mathematical reasoning capabilities and generalization.

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs](https://arxiv.org/abs/2511.05549)
*Yubo Wang,Haoyang Li,Fei Teng,Lei Chen*

Main category: cs.LG

TL;DR: AGRAG is a novel graph-based RAG framework that addresses LLM hallucination in graph construction using statistical methods, improves reasoning via MCMI subgraph generation, and enhances answer comprehensiveness through complex graph structures.


<details>
  <summary>Details</summary>
Motivation: Current graph-based RAG methods suffer from inaccurate graph construction due to LLM hallucination, poor reasoning ability from lack of explicit reasoning paths, and inadequate answering that underperforms even basic NaiveRAG on some tasks.

Method: Uses statistics-based entity extraction instead of LLM-based to avoid hallucination; formulates reasoning as Minimum Cost Maximum Influence (MCMI) subgraph generation problem; employs greedy algorithm to solve NP-hard MCMI problem; generates explicit reasoning paths with complex graph structures including cycles.

Result: The framework generates more comprehensive reasoning paths, reduces noise impact, improves LLM focus on query-relevant content, and enables complex graph structures beyond simple trees.

Conclusion: AGRAG addresses key limitations in existing graph-based RAG systems by providing a robust framework that improves graph construction accuracy, reasoning ability, and answer comprehensiveness through innovative statistical methods and MCMI subgraph generation.

Abstract: Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.

</details>


### [46] [Deep one-gate per layer networks with skip connections are universal classifiers](https://arxiv.org/abs/2511.05552)
*Raul Rojas*

Main category: cs.LG

TL;DR: Transforming a 2-hidden-layer MLP into a deep network with gated layers and skip connections.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how simple architectures can be extended to deeper networks with modern components.

Method: Modifying a multilayer perceptron by adding gating mechanisms and skip connections between layers.

Result: Successful transformation showing architectural flexibility.

Conclusion: Standard classifiers can be adapted into deeper, more complex networks efficiently.

Abstract: This paper shows how a multilayer perceptron with two hidden layers, which has been designed to classify two classes of data points, can easily be transformed into a deep neural network with one-gate layers and skip connections.

</details>


### [47] [Daily Forecasting for Annual Time Series Datasets Using Similarity-Based Machine Learning Methods: A Case Study in the Energy Market](https://arxiv.org/abs/2511.05556)
*Mahdi Goldani*

Main category: cs.LG

TL;DR: 本研究开发了一种将年度能源安全指数转换为每日代理指标的方法，使用时间序列相似性度量和XGBoost算法进行15天预测，为实时监测能源安全提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 由于能源安全指数仅每年报告一次，无法捕捉短期波动和政策环境的快速变化，需要开发高频监测工具帮助政策制定者实时响应。

Method: 采用两阶段方法：首先运用六种时间序列相似性度量识别合适的每日代理指标，然后使用XGBoost算法对选定代理进行建模，生成15天预测。

Result: 布伦特原油成交量被确定为最佳代理指标，模型表现优异（训练集R平方0.981，测试集0.945），预测显示短期内波动模式：第4天达峰值，第8天下降，第10天回升，之后下降至第15天。

Conclusion: 该研究提供了将低频宏观经济指标转换为高频可操作信号的新框架，为政策制定者和分析师在数据稀缺环境中实时监测能源安全提供了实用工具。

Abstract: The policy environment of countries changes rapidly, influencing macro-level indicators such as the Energy Security Index. However, this index is only reported annually, limiting its responsiveness to short-term fluctuations. To address this gap, the present study introduces a daily proxy for the Energy Security Index and applies it to forecast energy security at a daily frequency.The study employs a two stage approach first, a suitable daily proxy for the annual Energy Security Index is identified by applying six time series similarity measures to key energy related variables. Second, the selected proxy is modeled using the XGBoost algorithm to generate 15 day ahead forecasts, enabling high frequency monitoring of energy security dynamics.As the result of proxy choosing, Volume Brent consistently emerged as the most suitable proxy across the majority of methods. The model demonstrated strong performance, with an R squared of 0.981 on the training set and 0.945 on the test set, and acceptable error metrics . The 15 day forecast of Brent volume indicates short term fluctuations, with a peak around day 4, a decline until day 8, a rise near day 10, and a downward trend toward day 15, accompanied by prediction intervals.By integrating time series similarity measures with machine learning based forecasting, this study provides a novel framework for converting low frequency macroeconomic indicators into high frequency, actionable signals. The approach enables real time monitoring of the Energy Security Index, offering policymakers and analysts a scalable and practical tool to respond more rapidly to fast changing policy and market conditions, especially in data scarce environments.

</details>


### [48] [Diversified Flow Matching with Translation Identifiability](https://arxiv.org/abs/2511.05558)
*Sagar Shrestha,Xiao Fu*

Main category: cs.LG

TL;DR: Introduction of diversified flow matching (DFM), an ODE-based method achieving translation identifiability like DDM without GAN limitations, with novel training components validated on various datasets.


<details>
  <summary>Details</summary>
Motivation: DDM resolves content misalignment in unpaired domain translation but relies on unstable GANs and lacks transport trajectory information, which is critical for applications like single-cell analysis.

Method: Proposes DFM by adapting flow matching (FM) with a bilevel optimization loss, nonlinear interpolant, and structural reformulation to enforce a unified translation function despite FM learning velocities.

Result: Experiments on synthetic and real-world datasets confirm DFM's effectiveness as the first ODE-based approach ensuring translation identifiability.

Conclusion: DFM successfully overcomes GAN limitations in DDM, providing stable training and valuable trajectory data, with practical utility demonstrated across domains.

Abstract: Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information -- yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.

</details>


### [49] [Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement](https://arxiv.org/abs/2511.05562)
*Sanghyun Lee,Sunwoo Kim,Seungryong Kim,Jongho Park,Dongmin Park*

Main category: cs.LG

TL;DR: IterRef is a test-time scaling method for discrete diffusion models that uses reward-guided noising-denoising transitions to refine misaligned intermediate states, achieving significant quality improvements especially under low compute budgets.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling through reward-guided generation is largely unexplored for discrete diffusion models despite being a promising alternative to existing methods.

Method: Iterative Reward-Guided Refinement (IterRef) formalized within a Multiple-Try Metropolis (MTM) framework, using reward-guided noising-denoising transitions to progressively refine misaligned intermediate states.

Result: Consistent improvements in reward-guided generation quality across text and image domains, with striking gains under low compute budgets that surpass prior state-of-the-art baselines.

Conclusion: IterRef effectively refines misaligned states in discrete diffusion models and converges to the reward-aligned distribution, demonstrating superior performance particularly in resource-constrained scenarios.

Abstract: Test-time scaling through reward-guided generation remains largely unexplored for discrete diffusion models despite its potential as a promising alternative. In this work, we introduce Iterative Reward-Guided Refinement (IterRef), a novel test-time scaling method tailored to discrete diffusion that leverages reward- guided noising-denoising transitions to progressively refine misaligned intermediate states. We formalize this process within a Multiple-Try Metropolis (MTM) framework, proving convergence to the reward-aligned distribution. Unlike prior methods that assume the current state is already aligned with the reward distribution and only guide the subsequent transition, our approach explicitly refines each state in situ, progressively steering it toward the optimal intermediate distribution. Across both text and image domains, we evaluate IterRef on diverse discrete diffusion models and observe consistent improvements in reward-guided generation quality. In particular, IterRef achieves striking gains under low compute budgets, far surpassing prior state-of-the-art baselines.

</details>


### [50] [Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models](https://arxiv.org/abs/2511.05563)
*Sanghyun Lee,Seungryong Kim,Jongho Park,Dongmin Park*

Main category: cs.LG

TL;DR: Lookahead Unmasking (LookUM) improves masked diffusion models by optimizing unmasking order through path selection with generator and verifier components, achieving consistent performance gains across multiple benchmarks with minimal compute.


<details>
  <summary>Details</summary>
Motivation: Existing heuristic methods for unmasking tokens in masked diffusion models are myopic, causing error cascades and failing to utilize extra test-time computation effectively.

Method: Proposes LookUM framework with: (1) path generator sampling from unmasking sets, (2) verifier computing path uncertainty and performing importance sampling for final selection.

Result: Achieves consistent improvements across 6 benchmarks (mathematics, planning, coding); LookUM-enhanced LLaDA matches RL-tuned LLaDA 1.5 performance, while further boosting LLaDA 1.5 itself.

Conclusion: Uncertainty-based verification provides orthogonal benefits to reinforcement learning, demonstrating LookUM's versatility and efficient path selection capability (2-3 paths suffice for peak performance).

Abstract: Masked Diffusion Models (MDMs) as language models generate by iteratively unmasking tokens, yet their performance crucially depends on the inference time order of unmasking. Prevailing heuristics, such as confidence based sampling, are myopic: they optimize locally, fail to leverage extra test-time compute, and let early decoding mistakes cascade. We propose Lookahead Unmasking (LookUM), which addresses these concerns by reformulating sampling as path selection over all possible unmasking orders without the need for an external reward model. Our framework couples (i) a path generator that proposes paths by sampling from pools of unmasking sets with (ii) a verifier that computes the uncertainty of the proposed paths and performs importance sampling to subsequently select the final paths. Empirically, erroneous unmasking measurably inflates sequence level uncertainty, and our method exploits this to avoid error-prone trajectories. We validate our framework across six benchmarks, such as mathematics, planning, and coding, and demonstrate consistent performance improvements. LookUM requires only two to three paths to achieve peak performance, demonstrating remarkably efficient path selection. The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with LookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that uncertainty based verification provides orthogonal benefits to reinforcement learning and underscoring the versatility of our framework. Code will be publicly released.

</details>


### [51] [Adaptive Sample-Level Framework Motivated by Distributionally Robust Optimization with Variance-Based Radius Assignment for Enhanced Neural Network Generalization Under Distribution Shift](https://arxiv.org/abs/2511.05568)
*Aheer Sravon,Devdyuti Mazumder,Md. Ibrahim*

Main category: cs.LG

TL;DR: Proposes Var-DRO, a variance-driven adaptive DRO method that assigns personalized robustness budgets to high-risk samples based on loss variance, improving performance on distribution shifts without group labels.


<details>
  <summary>Details</summary>
Motivation: Standard DRO uses a single global robustness budget, which can cause overly conservative models or misallocation of robustness, failing to effectively handle distribution shifts and minority subpopulations.

Method: Uses online loss variance to identify high-risk samples and assign personalized budgets via two-sided KL-divergence bounds, forming a convex polytope maximization solved efficiently with water-filling. Includes warmup, budget ramp schedule, and label smoothing for stability.

Result: Achieves highest mean accuracy on CIFAR-10-C vs. ERM and KL-DRO; improves overall performance on Waterbirds; remains competitive on CIFAR-10 with expected robustness trade-off.

Conclusion: Var-DRO is an unsupervised, theoretically sound, and efficient framework that adaptively allocates robustness, outperforming existing methods on distribution shifts without needing group labels.

Abstract: Distribution shifts and minority subpopulations frequently undermine the reliability of deep neural networks trained using Empirical Risk Minimization (ERM). Distributionally Robust Optimization (DRO) addresses this by optimizing for the worst-case risk within a neighborhood of the training distribution. However, conventional methods depend on a single, global robustness budget, which can lead to overly conservative models or a misallocation of robustness. We propose a variance-driven, adaptive, sample-level DRO (Var-DRO) framework that automatically identifies high-risk training samples and assigns a personalized robustness budget to each based on its online loss variance. Our formulation employs two-sided, KL-divergence-style bounds to constrain the ratio between adversarial and empirical weights for every sample. This results in a linear inner maximization problem over a convex polytope, which admits an efficient water-filling solution. To stabilize training, we introduce a warmup phase and a linear ramp schedule for the global cap on per-sample budgets, complemented by label smoothing for numerical robustness. Evaluated on CIFAR-10-C (corruptions), our method achieves the highest overall mean accuracy compared to ERM and KL-DRO. On Waterbirds, Var-DRO improves overall performance while matching or surpassing KL-DRO. On the original CIFAR-10 dataset, Var-DRO remains competitive, exhibiting the modest trade-off anticipated when prioritizing robustness. The proposed framework is unsupervised (requiring no group labels), straightforward to implement, theoretically sound, and computationally efficient.

</details>


### [52] [Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions](https://arxiv.org/abs/2511.06791)
*Beichen Zhang,Mohammed T. Zaki,Hanna Breunig,Newsha K. Ajami*

Main category: cs.LG

TL;DR: An integrated modeling framework combining agent-based modeling and Life Cycle Assessment to analyze trade-offs in energy transition pathways, applied to Southern California.


<details>
  <summary>Details</summary>
Motivation: Existing energy assessments often evaluate pathways in silos, overlooking critical interactions like regional resource competition and cumulative impacts.

Method: Integrated modeling framework coupling agent-based modeling with Life Cycle Assessment to simulate energy transition pathway interactions with regional resource competition, ecological constraints, and community-level burdens.

Result: Demonstrates how integrated multiscale decision making shapes energy pathway deployment and reveals spatially explicit trade-offs under scenario-driven constraints.

Conclusion: The framework supports more adaptive and resilient energy transition planning on spatial and institutional scales.

Abstract: Transitioning to sustainable and resilient energy systems requires navigating complex and interdependent trade-offs across environmental, social, and resource dimensions. Neglecting these trade-offs can lead to unintended consequences across sectors. However, existing assessments often evaluate emerging energy pathways and their impacts in silos, overlooking critical interactions such as regional resource competition and cumulative impacts. We present an integrated modeling framework that couples agent-based modeling and Life Cycle Assessment (LCA) to simulate how energy transition pathways interact with regional resource competition, ecological constraints, and community-level burdens. We apply the model to a case study in Southern California. The results demonstrate how integrated and multiscale decision making can shape energy pathway deployment and reveal spatially explicit trade-offs under scenario-driven constraints. This modeling framework can further support more adaptive and resilient energy transition planning on spatial and institutional scales.

</details>


### [53] [Data-driven jet fuel demand forecasting: A case study of Copenhagen Airport](https://arxiv.org/abs/2511.05569)
*Alessandro Contini,Davide Cacciarelli,Murat Kulahci*

Main category: cs.LG

TL;DR: This paper evaluates machine learning models for jet fuel demand forecasting to improve supply chain optimization, comparing traditional time series models, Prophet, LSTM, and hybrid approaches using data from a Danish fuel distributor.


<details>
  <summary>Details</summary>
Motivation: There is a lack of studies using machine learning for jet fuel demand forecasting, while industry relies on deterministic or expertise-based models, creating a need for data-driven approaches to enhance prediction accuracy.

Method: The research uses substantial data from a major Danish aviation fuel distributor to compare traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models, analyzing three different datasets with a 30-day forecasting horizon.

Result: The study demonstrates the predictive capabilities of various data-driven models and shows the impact of incorporating additional variables, though specific performance metrics are not detailed in the abstract.

Conclusion: Data-driven models offer advantages for jet fuel demand forecasting, providing valuable insights for practitioners to optimize sourcing strategies and supply chain operations.

Abstract: Accurate forecasting of jet fuel demand is crucial for optimizing supply chain operations in the aviation market. Fuel distributors specifically require precise estimates to avoid inventory shortages or excesses. However, there is a lack of studies that analyze the jet fuel demand forecasting problem using machine learning models. Instead, many industry practitioners rely on deterministic or expertise-based models. In this research, we evaluate the performance of data-driven approaches using a substantial amount of data obtained from a major aviation fuel distributor in the Danish market. Our analysis compares the predictive capabilities of traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. A key challenge in developing these models is the required forecasting horizon, as fuel demand needs to be predicted for the next 30 days to optimize sourcing strategies. To ensure the reliability of the data-driven approaches and provide valuable insights to practitioners, we analyze three different datasets. The primary objective of this study is to present a comprehensive case study on jet fuel demand forecasting, demonstrating the advantages of employing data-driven models and highlighting the impact of incorporating additional variables in the predictive models.

</details>


### [54] [Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction](https://arxiv.org/abs/2511.05577)
*An Vuong,Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: Fine-tuning VLMs with multimodal polymer data via LoRA improves property prediction accuracy and reduces the need for multiple specialized models.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models perform well in general tasks but lack effectiveness in scientific domains like materials science, with no existing foundation models for broad polymer property prediction using multimodal data.

Method: The researchers created a multimodal polymer dataset and fine-tuned Vision-Language Models using instruction-tuning pairs and LoRA (Low-Rank Adaptation) techniques.

Result: The fine-tuned models using LoRA outperformed unimodal and baseline approaches, showing enhanced prediction performance and the benefits of multimodal learning.

Conclusion: The study successfully demonstrates that fine-tuning Vision-Language Models with a multimodal polymer dataset significantly improves prediction accuracy and offers practical advantages for industrial deployment.

Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.

</details>


### [55] [Distillation-Accelerated Uncertainty Modeling for Multi-Objective RTA Interception](https://arxiv.org/abs/2511.05582)
*Gaoxiang Zhao,Ruina Qiu,Pengpeng Zhao,Rongjin Wang,Zhangang Lin,Xiaoqiang Wang*

Main category: cs.LG

TL;DR: DAUM is a framework for real-time auction interception that combines multi-objective learning with uncertainty modeling to filter invalid traffic, with knowledge distillation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Current RTA interception faces challenges in accurately estimating traffic quality with high confidence, and the computational inefficiency of uncertainty modeling in real-time applications.

Method: Proposes DAUM framework integrating multi-objective learning with uncertainty modeling, then applies knowledge distillation to reduce computational overhead.

Result: Experiments show DAUM improves predictive performance and the distilled model achieves 10x faster inference speed while maintaining accuracy.

Conclusion: DAUM effectively addresses RTA interception challenges by providing accurate traffic filtering with reliable confidence estimates and practical efficiency.

Abstract: Real-Time Auction (RTA) Interception aims to filter out invalid or irrelevant traffic to enhance the integrity and reliability of downstream data. However, two key challenges remain: (i) the need for accurate estimation of traffic quality together with sufficiently high confidence in the model's predictions, typically addressed through uncertainty modeling, and (ii) the efficiency bottlenecks that such uncertainty modeling introduces in real-time applications due to repeated inference. To address these challenges, we propose DAUM, a joint modeling framework that integrates multi-objective learning with uncertainty modeling, yielding both traffic quality predictions and reliable confidence estimates. Building on DAUM, we further apply knowledge distillation to reduce the computational overhead of uncertainty modeling, while largely preserving predictive accuracy and retaining the benefits of uncertainty estimation. Experiments on the JD advertisement dataset demonstrate that DAUM consistently improves predictive performance, with the distilled model delivering a tenfold increase in inference speed.

</details>


### [56] [Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural Kernels](https://arxiv.org/abs/2511.05585)
*Yong-Ming Tian,Shuang Liang,Shao-Qun Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: Proposes depth-induced NTK kernel based on shortcut architecture, extending neural kernel theory to finite-width regimes and analyzing depth's representational role.


<details>
  <summary>Details</summary>
Motivation: Existing NTK paradigm limited to infinite-width regime, overlooking representational role of network depth in deep learning.

Method: Develop depth-induced NTK kernel using shortcut-related architecture, analyze training invariance and spectrum properties.

Result: Kernel converges to Gaussian process as depth → ∞, stabilizes kernel dynamics and mitigates degeneration.

Conclusion: Extends neural kernel theory landscape, provides deeper understanding of deep learning scaling laws.

Abstract: While deep learning has achieved remarkable success across a wide range of applications, its theoretical understanding of representation learning remains limited. Deep neural kernels provide a principled framework to interpret over-parameterized neural networks by mapping hierarchical feature transformations into kernel spaces, thereby combining the expressive power of deep architectures with the analytical tractability of kernel methods. Recent advances, particularly neural tangent kernels (NTKs) derived by gradient inner products, have established connections between infinitely wide neural networks and nonparametric Bayesian inference. However, the existing NTK paradigm has been predominantly confined to the infinite-width regime, while overlooking the representational role of network depth. To address this gap, we propose a depth-induced NTK kernel based on a shortcut-related architecture, which converges to a Gaussian process as the network depth approaches infinity. We theoretically analyze the training invariance and spectrum properties of the proposed kernel, which stabilizes the kernel dynamics and mitigates degeneration. Experimental results further underscore the effectiveness of our proposed method. Our findings significantly extend the existing landscape of the neural kernel theory and provide an in-depth understanding of deep learning and the scaling law.

</details>


### [57] [Prompting Neural-Guided Equation Discovery Based on Residuals](https://arxiv.org/abs/2511.05586)
*Jannis Brugger,Viktor Pfanschilling,David Richter,Mira Mezini,Stefan Kramer*

Main category: cs.LG

TL;DR: RED is a post-processing method that improves neural-guided equation discovery by using residuals to generate better equation suggestions through iterative refinement of subequations.


<details>
  <summary>Details</summary>
Motivation: Existing equation discovery systems lack efficient ways to provide alternative equation suggestions when initial results don't meet user expectations without extensive manual work.

Method: Parses initial equations into syntax trees, calculates residuals for each subequation, uses residuals as new target variables to generate improved prompts, and replaces subequations when better ones are found.

Result: Improves both neural-guided and classical genetic programming systems on the Feynman benchmark with 53 equations.

Conclusion: RED is versatile, fast, and effective for enhancing equation discovery systems across different approaches.

Abstract: Neural-guided equation discovery systems use a data set as prompt and predict an equation that describes the data set without extensive search. However, if the equation does not meet the user's expectations, there are few options for getting other equation suggestions without intensive work with the system. To fill this gap, we propose Residuals for Equation Discovery (RED), a post-processing method that improves a given equation in a targeted manner, based on its residuals. By parsing the initial equation to a syntax tree, we can use node-based calculation rules to compute the residual for each subequation of the initial equation. It is then possible to use this residual as new target variable in the original data set and generate a new prompt. If, with the new prompt, the equation discovery system suggests a subequation better than the old subequation on a validation set, we replace the latter by the former. RED is usable with any equation discovery system, is fast to calculate, and is easy to extend for new mathematical operations. In experiments on 53 equations from the Feynman benchmark, we show that it not only helps to improve all tested neural-guided systems, but also all tested classical genetic programming systems.

</details>


### [58] [CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling](https://arxiv.org/abs/2511.05589)
*Zekai Qu,Yinxu Pan,Ao Sun,Chaojun Xiao,Xu Han*

Main category: cs.LG

TL;DR: CoPRIS introduces asynchronous reinforcement learning for LLMs by using partial rollouts and importance sampling to eliminate GPU idle time caused by long trajectories, achieving 1.94x faster training while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL systems for LLMs operate synchronously, causing severe inefficiencies as long trajectories stall the entire rollout process and leave GPUs idle.

Method: CoPRIS uses concurrency-controlled partial rollouts with importance sampling, maintaining fixed concurrent rollouts, early-terminating when sufficient samples are collected, and reusing unfinished trajectories. It employs Cross-stage Importance Sampling Correction to handle off-policy trajectories.

Result: Experiments on mathematical reasoning benchmarks show CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems.

Conclusion: CoPRIS effectively addresses the inefficiency problem in RL post-training for LLMs through asynchronous partial rollouts and importance sampling correction, enabling significantly faster training without performance degradation.

Abstract: Reinforcement learning (RL) post-training has become a trending paradigm for enhancing the capabilities of large language models (LLMs). Most existing RL systems for LLMs operate in a fully synchronous manner, where training must wait for the rollout of an entire batch to complete. This design leads to severe inefficiencies, as extremely long trajectories can stall the entire rollout process and leave many GPUs idle. To address this issue, we propose Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS), which mitigates long-tail inefficiencies by maintaining a fixed number of concurrent rollouts, early-terminating once sufficient samples are collected, and reusing unfinished trajectories in subsequent rollouts. To mitigate the impact of off-policy trajectories, we introduce Cross-stage Importance Sampling Correction, which concatenates buffered log probabilities from the previous policy with those recomputed under the current policy for importance sampling correction. Experiments on challenging mathematical reasoning benchmarks show that CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems. The code of CoPRIS is available at https://github.com/777pomingzi/CoPRIS.

</details>


### [59] [FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust & Efficient Federated Learning](https://arxiv.org/abs/2511.05591)
*Chaimaa Medjadji,Sadi Alawadi,Feras M. Awaysheh,Guilain Leduc,Sylvain Kubler,Yves Le Traon*

Main category: cs.LG

TL;DR: FedSparQ is a lightweight compression framework for federated learning that reduces communication overhead by 90% through adaptive gradient sparsification, half-precision quantization, and error feedback while maintaining or improving model accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated Learning suffers from significant communication overhead due to frequent exchange of high-dimensional model updates over constrained networks, which limits its practical deployment.

Method: Dynamic gradient sparsification with adaptive threshold, half-precision quantization of retained entries, and integration of residuals from error feedback to prevent information loss. The method is automatic, requires no manual tuning, and works with any model architecture.

Result: Achieves 90% reduction in communication overhead compared to FedAvg, improves model accuracy by 6% compared to uncompressed FedAvg and state-of-the-art compression methods, and enhances convergence robustness by 50% compared to other baselines.

Conclusion: FedSparQ provides a practical, easy-to-deploy solution for bandwidth-constrained federated deployments and enables future extensions in adaptive precision and privacy-preserving protocols.

Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy by keeping raw data local. However, FL suffers from significant communication overhead due to the frequent exchange of high-dimensional model updates over constrained networks. In this paper, we present FedSparQ, a lightweight compression framework that dynamically sparsifies the gradient of each client through an adaptive threshold, applies half-precision quantization to retained entries and integrates residuals from error feedback to prevent loss of information. FedSparQ requires no manual tuning of sparsity rates or quantization schedules, adapts seamlessly to both homogeneous and heterogeneous data distributions, and is agnostic to model architecture. Through extensive empirical evaluation on vision benchmarks under independent and identically distributed (IID) and non-IID data, we show that FedSparQ substantially reduces communication overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or improving model accuracy (improving by 6% compared to FedAvg non-compressed solution or to state-of-the-art compression models) and enhancing convergence robustness (by 50%, compared to the other baselines). Our approach provides a practical, easy-to-deploy solution for bandwidth-constrained federated deployments and lays the groundwork for future extensions in adaptive precision and privacy-preserving protocols.

</details>


### [60] [GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning](https://arxiv.org/abs/2511.05592)
*Haonan Yuan,Qingyun Sun,Junhua Shi,Xingcheng Fu,Bryan Hooi,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: GRAVER is a framework that enables robust and efficient fine-tuning of Graph Foundation Models by using generative graph vocabularies to address instability in few-shot learning scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Foundation Models suffer from unstable few-shot fine-tuning due to randomness in support sample selection and structural discrepancies between pre-trained and target graphs, making robust knowledge transfer challenging.

Method: The method involves: 1) Extracting transferable class-specific subgraph patterns via ego-graph disentanglement; 2) Using universal task templates and graphon-based generative experts to build graph vocabularies; 3) Prompt fine-tuning with in-context vocabularies using lightweight MoE-CoE networks for knowledge routing.

Result: Extensive experiments show GRAVER outperforms 15 state-of-the-art baselines in effectiveness, robustness, and efficiency on downstream few-shot node and graph classification tasks.

Conclusion: GRAVER provides a robust solution for fine-tuning Graph Foundation Models by leveraging generative augmentations and transferable subgraph patterns, enabling trustworthy knowledge transfer across domains and tasks.

Abstract: Inspired by the remarkable success of foundation models in language and vision, Graph Foundation Models (GFMs) hold significant promise for broad applicability across diverse graph tasks and domains. However, existing GFMs struggle with unstable few-shot fine-tuning, where both performance and adaptation efficiency exhibit significant fluctuations caused by the randomness in the support sample selection and structural discrepancies between the pre-trained and target graphs. How to fine-tune GFMs robustly and efficiently to enable trustworthy knowledge transfer across domains and tasks is the major challenge. In this paper, we propose GRAVER, a novel Generative gRAph VocabulariEs for Robust GFM fine-tuning framework that tackles the aforementioned instability via generative augmentations. Specifically, to identify transferable units, we analyze and extract key class-specific subgraph patterns by ego-graph disentanglement and validate their transferability both theoretically and empirically. To enable effective pre-training across diverse domains, we leverage a universal task template based on ego-graph similarity and construct graph vocabularies via graphon-based generative experts. To facilitate robust and efficient prompt fine-tuning, we grave the support samples with in-context vocabularies, where the lightweight MoE-CoE network attentively routes knowledge from source domains. Extensive experiments demonstrate the superiority of GRAVER over effectiveness, robustness, and efficiency on downstream few-shot node and graph classification tasks compared with 15 state-of-the-art baselines.

</details>


### [61] [Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning](https://arxiv.org/abs/2511.05593)
*Arnaud Descours,Léonard Deroose,Jan Ramon*

Main category: cs.LG

TL;DR: Novel communication-efficient FL algorithms (ProjFL and ProjFL+EF) using gradient projection and error feedback to reduce communication overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Communication efficiency is a critical bottleneck in Federated Learning, especially for large-scale models.

Method: ProjFL uses gradient projection onto shared subspace for unbiased compressors; ProjFL+EF adds Error Feedback mechanism for biased compressors.

Result: Both algorithms achieve accuracy comparable to baselines while substantially reducing communication costs.

Conclusion: ProjFL and ProjFL+EF effectively address the communication bottleneck in federated learning while maintaining model accuracy across various settings.

Abstract: Federated Learning (FL) enables decentralized model training across multiple clients while optionally preserving data privacy. However, communication efficiency remains a critical bottleneck, particularly for large-scale models. In this work, we introduce two complementary algorithms: ProjFL, designed for unbiased compressors, and ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism. Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead. We establish convergence guarantees for both algorithms under strongly convex, convex, and non-convex settings. Empirical evaluations on standard FL classification benchmarks with deep neural networks show that ProjFL and ProjFL+EF achieve accuracy comparable to existing baselines while substantially reducing communication costs.

</details>


### [62] [Optimizing Predictive Maintenance in Intelligent Manufacturing: An Integrated FNO-DAE-GNN-PPO MDP Framework](https://arxiv.org/abs/2511.05594)
*Shiqing Qiu*

Main category: cs.LG

TL;DR: Proposes a novel MDP framework combining FNO, DAE, GNN, and PPO techniques for predictive maintenance in manufacturing, achieving 13% cost reduction and improved system reliability.


<details>
  <summary>Details</summary>
Motivation: Address multidimensional challenges of predictive maintenance in complex manufacturing systems to improve equipment reliability and reduce operating costs in smart manufacturing era.

Method: Integrates Fourier Neural Operator (FNO) for temporal patterns, Denoising Autoencoder (DAE) for robust state embedding, Graph Neural Network (GNN) for device dependencies, and Proximal Policy Optimization (PPO) for stable optimization of maintenance strategies.

Result: Significantly outperforms deep learning baselines with up to 13% cost reduction, demonstrates strong convergence and inter-module synergy.

Conclusion: The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven predictive maintenance strategies.

Abstract: In the era of smart manufacturing, predictive maintenance (PdM) plays a pivotal role in improving equipment reliability and reducing operating costs. In this paper, we propose a novel Markov Decision Process (MDP) framework that integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address the multidimensional challenges of predictive maintenance in complex manufacturing systems. Specifically, the proposed framework innovatively combines the powerful frequency-domain representation capability of FNOs to capture high-dimensional temporal patterns; DAEs to achieve robust, noise-resistant latent state embedding from complex non-Gaussian sensor data; and GNNs to accurately represent inter-device dependencies for coordinated system-wide maintenance decisions. Furthermore, by exploiting PPO, the framework ensures stable and efficient optimisation of long-term maintenance strategies to effectively handle uncertainty and non-stationary dynamics. Experimental validation demonstrates that the approach significantly outperforms multiple deep learning baseline models with up to 13% cost reduction, as well as strong convergence and inter-module synergy. The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven strategies.

</details>


### [63] [FlowNet: Modeling Dynamic Spatio-Temporal Systems via Flow Propagation](https://arxiv.org/abs/2511.05595)
*Yutong Feng,Xu Liu,Yutong Xia,Yuxuan Liang*

Main category: cs.LG

TL;DR: FlowNet is a physics-inspired architecture that models spatio-temporal systems using quantifiable flow transfers governed by conservation principles, outperforming existing methods across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based and attention-driven methods fail to capture asymmetric flow exchanges that govern system evolution in complex spatio-temporal systems.

Method: Proposes Spatio-Temporal Flow paradigm using flow tokens as information carriers with Flow Allocation Modules for source-to-destination transfers, Adaptive Spatial Masking for dynamic interaction radius, and cascaded architecture for scalability.

Result: Outperforms state-of-the-art approaches on seven metrics across three real-world systems, demonstrating superior efficiency and physical interpretability.

Conclusion: Establishes a principled methodology for modeling complex systems through spatio-temporal flow interactions with conservation principles.

Abstract: Accurately modeling complex dynamic spatio-temporal systems requires capturing flow-mediated interdependencies and context-sensitive interaction dynamics. Existing methods, predominantly graph-based or attention-driven, rely on similarity-driven connectivity assumptions, neglecting asymmetric flow exchanges that govern system evolution. We propose Spatio-Temporal Flow, a physics-inspired paradigm that explicitly models dynamic node couplings through quantifiable flow transfers governed by conservation principles. Building on this, we design FlowNet, a novel architecture leveraging flow tokens as information carriers to simulate source-to-destination transfers via Flow Allocation Modules, ensuring state redistribution aligns with conservation laws. FlowNet dynamically adjusts the interaction radius through an Adaptive Spatial Masking module, suppressing irrelevant noise while enabling context-aware propagation. A cascaded architecture enhances scalability and nonlinear representation capacity. Experiments demonstrate that FlowNet significantly outperforms existing state-of-the-art approaches on seven metrics in the modeling of three real-world systems, validating its efficiency and physical interpretability. We establish a principled methodology for modeling complex systems through spatio-temporal flow interactions.

</details>


### [64] [AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction](https://arxiv.org/abs/2511.05596)
*Vansh Sharma,Harish Jai Ganesh,Maryam Akram,Wanjiao Liu,Venkat Raman*

Main category: cs.LG

TL;DR: AutoHood3D is a high-fidelity multi-modal dataset of 16,000+ automotive hood variants with coupled LES-FEA simulations for fluid-structure interaction during painting, enabling physics-aware ML for design optimization.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack 3D geometric diversity and multi-modal annotations needed for realistic multiphysics ML applications in automotive design.

Method: Created 16,000+ hood variants with coupled Large-Eddy Simulation (1.2M cells) and Finite Element Analysis to model deformation during rotary-dip painting.

Result: Validated numerical methodology and established baselines across 5 neural architectures, revealing systematic surrogate errors in displacement and force predictions.

Conclusion: The dataset enables physics-aware ML development, accelerates generative design iteration, and facilitates new fluid-structure interaction benchmarks for automotive applications.

Abstract: This study presents a new high-fidelity multi-modal dataset containing 16000+ geometric variants of automotive hoods useful for machine learning (ML) applications such as engineering component design and process optimization, and multiphysics system surrogates. The dataset is centered on a practical multiphysics problem-hood deformation from fluid entrapment and inertial loading during rotary-dip painting. Each hood is numerically modeled with a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA), using 1.2M cells in total to ensure spatial and temporal accuracy. The dataset provides time-resolved physical fields, along with STL meshes and structured natural language prompts for text-to-geometry synthesis. Existing datasets are either confined to 2D cases, exhibit limited geometric variations, or lack the multi-modal annotations and data structures - shortcomings we address with AutoHood3D. We validate our numerical methodology, establish quantitative baselines across five neural architectures, and demonstrate systematic surrogate errors in displacement and force predictions. These findings motivate the design of novel approaches and multiphysics loss functions that enforce fluid-solid coupling during model training. By providing fully reproducible workflows, AutoHood3D enables physics-aware ML development, accelerates generative-design iteration, and facilitates the creation of new FSI benchmarks. Dataset and code URLs in Appendix.

</details>


### [65] [FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI](https://arxiv.org/abs/2511.05605)
*Eun-Su Cho,Jongin Choi,Jeongmin Jin,Jae-Jin Lee,Woojoo Lee*

Main category: cs.LG

TL;DR: FiCABU is a software-hardware co-design for efficient machine unlearning on edge AI processors, combining context-adaptive unlearning starting from back-end layers and balanced dampening scaled by depth, achieving significant computation and energy savings while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning is needed at the edge due to privacy regulations like the 'right to be forgotten', but server-centric or retraining-heavy methods are impractical under tight computation and energy constraints.

Method: FiCABU combines (i) Context-Adaptive Unlearning that begins edits from back-end layers and stops once target forgetting is reached, with (ii) Balanced Dampening that scales dampening strength by depth. Implemented in a RISC-V edge AI processor with lightweight IPs for Fisher estimation and dampening integrated into a GEMM-centric streaming pipeline.

Result: Achieves random-guess forget accuracy while matching Selective Synaptic Dampening baseline on retain accuracy, reducing computation by up to 87.52% (ResNet-18) and 71.03% (ViT). On INT8 hardware prototype, reduces energy to 6.48% (CIFAR-20) and 0.13% (PinsFaceRecognition) of SSD baseline.

Conclusion: FiCABU demonstrates that back-end-first, depth-aware unlearning can be made practical and efficient for resource-constrained edge AI devices.

Abstract: Machine unlearning, driven by privacy regulations and the "right to be forgotten", is increasingly needed at the edge, yet server-centric or retraining-heavy methods are impractical under tight computation and energy budgets. We present FiCABU (Fisher-based Context-Adaptive Balanced Unlearning), a software-hardware co-design that brings unlearning to edge AI processors. FiCABU combines (i) Context-Adaptive Unlearning, which begins edits from back-end layers and halts once the target forgetting is reached, with (ii) Balanced Dampening, which scales dampening strength by depth to preserve retain accuracy. These methods are realized in a full RTL design of a RISC-V edge AI processor that integrates two lightweight IPs for Fisher estimation and dampening into a GEMM-centric streaming pipeline, validated on an FPGA prototype and synthesized in 45 nm for power analysis. Across CIFAR-20 and PinsFaceRecognition with ResNet-18 and ViT, FiCABU achieves random-guess forget accuracy while matching the retraining-free Selective Synaptic Dampening (SSD) baseline on retain accuracy, reducing computation by up to 87.52 percent (ResNet-18) and 71.03 percent (ViT). On the INT8 hardware prototype, FiCABU further improves retain preservation and reduces energy to 6.48 percent (CIFAR-20) and 0.13 percent (PinsFaceRecognition) of the SSD baseline. In sum, FiCABU demonstrates that back-end-first, depth-aware unlearning can be made both practical and efficient for resource-constrained edge AI devices.

</details>


### [66] [Conformal Prediction-Driven Adaptive Sampling for Digital Twins of Water Distribution Networks](https://arxiv.org/abs/2511.05610)
*Mohammadhossein Homaei,Oscar Mogollon Gutierrez,Ruben Molano,Andres Caro,Mar Avila*

Main category: cs.LG

TL;DR: Adaptive framework using LSTM and conformal prediction to optimize sensor placement in water network digital twins, reducing demand error by 33-34% compared to uniform sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform sensor sampling wastes resources across nodes with varying uncertainty levels in water distribution networks.

Method: Combines LSTM forecasting with conformal prediction to estimate node-wise uncertainty and adaptively focus sensing on most uncertain points.

Result: Achieved 33-34% lower demand error than uniform sampling at 40% coverage while maintaining 89.4-90.2% empirical coverage.

Conclusion: The adaptive framework provides efficient uncertainty quantification with minimal computational overhead, making it suitable for real-time digital twin applications.

Abstract: Digital Twins (DTs) for Water Distribution Networks (WDNs) require accurate state estimation with limited sensors. Uniform sampling often wastes resources across nodes with different uncertainty. We propose an adaptive framework combining LSTM forecasting and Conformal Prediction (CP) to estimate node-wise uncertainty and focus sensing on the most uncertain points. Marginal CP is used for its low computational cost, suitable for real-time DTs. Experiments on Hanoi, Net3, and CTOWN show 33-34% lower demand error than uniform sampling at 40% coverage and maintain 89.4-90.2% empirical coverage with only 5-10% extra computation.

</details>


### [67] [An MLCommons Scientific Benchmarks Ontology](https://arxiv.org/abs/2511.05614)
*Ben Hawks,Gregor von Laszewski,Matthew D. Sinclair,Marco Colombo,Shivaram Venkataraman,Rutwik Jain,Yiwei Jiang,Nhan Tran,Geoffrey Fox*

Main category: cs.LG

TL;DR: Introduces MLCommons Science Benchmarks Ontology - a unified, community-driven framework for standardizing scientific machine learning benchmarks across physics, chemistry, materials science, biology, climate science, and other domains.


<details>
  <summary>Details</summary>
Motivation: Existing scientific ML benchmarks are siloed and lack standardization, making applications to critical scientific use-cases fragmented and unclear in impact pathways.

Method: Develops an extensible ontology through community effort, consolidating disparate benchmarks into a single taxonomy with a six-category rating rubric. Uses open submission workflow coordinated by MLCommons Science Working Group.

Result: Creates a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific ML that supports future scientific and AI/ML motifs.

Conclusion: The MLCommons Science Benchmarks Ontology provides a standardized framework to advance scientific machine learning research through unified, high-quality benchmarking practices.

Abstract: Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: https://mlcommons-science.github.io/benchmark/

</details>


### [68] [wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency Estimation](https://arxiv.org/abs/2511.05615)
*Benjamin Hawks,Jason Weitz,Dmitri Demler,Karla Tame-Narvaez,Dennis Plotnikov,Mohammad Mehdi Rahimifar,Hamza Ezzaoui Rahali,Audrey C. Therrien,Donovan Sproule,Elham E Khoda,Keegan A. Smith,Russell Marroquin,Giuseppe Di Guglielmo,Nhan Tran,Javier Duarte,Vladimir Loncar*

Main category: cs.LG

TL;DR: Introduces wa-hls4ml benchmark and ML-based surrogate models for predicting FPGA resource usage and latency of ML accelerators, achieving within several percent accuracy.


<details>
  <summary>Details</summary>
Motivation: Hardware synthesis has become a bottleneck in ML accelerator design iteration; need for faster resource estimation methods.

Method: Developed benchmark with 680k synthesized networks and created GNN/transformer-based surrogate models for prediction.

Result: Models predict latency/resources within several percent of actual synthesized values for 75th percentile on test dataset.

Conclusion: ML-based surrogates can effectively accelerate hardware design cycles by providing accurate resource estimates.

Abstract: As machine learning (ML) is increasingly implemented in hardware to address real-time challenges in scientific applications, the development of advanced toolchains has significantly reduced the time required to iterate on various designs. These advancements have solved major obstacles, but also exposed new challenges. For example, processes that were not previously considered bottlenecks, such as hardware synthesis, are becoming limiting factors in the rapid iteration of designs. To mitigate these emerging constraints, multiple efforts have been undertaken to develop an ML-based surrogate model that estimates resource usage of ML accelerator architectures. We introduce wa-hls4ml, a benchmark for ML accelerator resource and latency estimation, and its corresponding initial dataset of over 680,000 fully connected and convolutional neural networks, all synthesized using hls4ml and targeting Xilinx FPGAs. The benchmark evaluates the performance of resource and latency predictors against several common ML model architectures, primarily originating from scientific domains, as exemplar models, and the average performance across a subset of the dataset. Additionally, we introduce GNN- and transformer-based surrogate models that predict latency and resources for ML accelerators. We present the architecture and performance of the models and find that the models generally predict latency and resources for the 75% percentile within several percent of the synthesized resources on the synthetic test dataset.

</details>


### [69] [Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift](https://arxiv.org/abs/2511.05619)
*Tianze Wang,Sofiane Ennadir,John Pertoft,Gabriela Zarzar Gandler,Lele Cao,Zineb Senane,Styliani Katsarou,Sahar Asadi,Axel Karlsson,Oleg Smirnov*

Main category: cs.LG

TL;DR: Time series foundation models perform well on benchmarks but struggle in industrial applications due to spectral shift - a mismatch between downstream task frequencies and pretraining data frequencies.


<details>
  <summary>Details</summary>
Motivation: To investigate why TSFMs fail to generalize effectively from public benchmarks to real-world industrial settings, focusing on frequency domain analysis.

Method: Combined analysis of an industrial player engagement prediction task in mobile gaming with controlled synthetic experiments comparing signals with seen versus unseen frequency bands.

Result: TSFMs underperform domain-adapted baselines in industrial settings, with systematic performance degradation observed when there's spectral mismatch between pretraining and downstream frequencies.

Conclusion: Frequency awareness is critical for robust TSFM deployment, necessitating new pretraining and evaluation protocols that explicitly account for spectral diversity.

Abstract: Time series foundation models (TSFMs) have shown strong results on public benchmarks, prompting comparisons to a "BERT moment" for time series. Their effectiveness in industrial settings, however, remains uncertain. We examine why TSFMs often struggle to generalize and highlight spectral shift (a mismatch between the dominant frequency components in downstream tasks and those represented during pretraining) as a key factor. We present evidence from an industrial-scale player engagement prediction task in mobile gaming, where TSFMs underperform domain-adapted baselines. To isolate the mechanism, we design controlled synthetic experiments contrasting signals with seen versus unseen frequency bands, observing systematic degradation under spectral mismatch. These findings position frequency awareness as critical for robust TSFM deployment and motivate new pretraining and evaluation protocols that explicitly account for spectral diversity.

</details>


### [70] [Fooling Algorithms in Non-Stationary Bandits using Belief Inertia](https://arxiv.org/abs/2511.05620)
*Gal Mendelson,Eyal Tadmor*

Main category: cs.LG

TL;DR: This paper introduces a new belief inertia approach to prove worst-case linear regret for multi-armed bandit algorithms in non-stationary environments, even for algorithms that periodically restart.


<details>
  <summary>Details</summary>
Motivation: Existing lower bounds for non-stationary bandits rely on infrequent sampling arguments, but a more fundamental understanding of algorithm limitations is needed.

Method: The paper introduces a belief inertia argument that captures how algorithms' empirical beliefs create momentum resistant to change, and constructs adversarial instances to exploit this inertia.

Result: The analysis shows classical algorithms (Explore Then Commit, epsilon-greedy, UCB) suffer linear regret with substantial constant factors regardless of parameter tuning, even with periodic restarts.

Conclusion: Belief inertia provides a powerful method for deriving sharp lower bounds in non-stationary bandits, revealing fundamental limitations of current approaches.

Abstract: We study the problem of worst case regret in piecewise stationary multi armed bandits. While the minimax theory for stationary bandits is well established, understanding analogous limits in time-varying settings is challenging. Existing lower bounds rely on what we refer to as infrequent sampling arguments, where long intervals without exploration allow adversarial reward changes that induce large regret.
  In this paper, we introduce a fundamentally different approach based on a belief inertia argument. Our analysis captures how an algorithm's empirical beliefs, encoded through historical reward averages, create momentum that resists new evidence after a change. We show how this inertia can be exploited to construct adversarial instances that mislead classical algorithms such as Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret that grows linearly with T and with a substantial constant factor, regardless of how their parameters are tuned, even with a single change point.
  We extend the analysis to algorithms that periodically restart to handle non stationarity and prove that, even then, the worst case regret remains linear in T. Our results indicate that utilizing belief inertia can be a powerful method for deriving sharp lower bounds in non stationary bandits.

</details>


### [71] [Unveiling the Training Dynamics of ReLU Networks through a Linear Lens](https://arxiv.org/abs/2511.05628)
*Longqing Ye*

Main category: cs.LG

TL;DR: A framework that transforms multi-layer ReLU networks into equivalent single-layer linear models with input-dependent effective weights, revealing how class-specific representations emerge during training.


<details>
  <summary>Details</summary>
Motivation: Understanding the complex internal learning mechanisms of deep neural networks with ReLU activations is challenging due to their high-dimensional, non-linear nature.

Method: Recast multi-layer ReLU networks into equivalent single-layer linear models by computing input-dependent effective weights that capture the active computational path for each sample through ReLU activation patterns.

Result: During training, effective weights for samples from the same class converge while those from different classes diverge, revealing the formation of class-specific decision boundaries and semantic representations.

Conclusion: The evolution of sample-wise effective weights provides a powerful analytical tool to interpret representation learning in deep networks, showing how networks learn to separate classes through weight trajectory convergence and divergence.

Abstract: Deep neural networks, particularly those employing Rectified Linear Units (ReLU), are often perceived as complex, high-dimensional, non-linear systems. This complexity poses a significant challenge to understanding their internal learning mechanisms. In this work, we propose a novel analytical framework that recasts a multi-layer ReLU network into an equivalent single-layer linear model with input-dependent "effective weights". For any given input sample, the activation pattern of ReLU units creates a unique computational path, effectively zeroing out a subset of weights in the network. By composing the active weights across all layers, we can derive an effective weight matrix, $W_{\text{eff}}(x)$, that maps the input directly to the output for that specific sample. We posit that the evolution of these effective weights reveals fundamental principles of representation learning. Our work demonstrates that as training progresses, the effective weights corresponding to samples from the same class converge, while those from different classes diverge. By tracking the trajectories of these sample-wise effective weights, we provide a new lens through which to interpret the formation of class-specific decision boundaries and the emergence of semantic representations within the network.

</details>


### [72] [SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction](https://arxiv.org/abs/2511.05629)
*Zheng Jiang,Wei Wang,Gaowei Zhang,Yi Wang*

Main category: cs.LG

TL;DR: SSTODE is a physics-informed Neural ODE framework that incorporates fluid transport principles and ocean heat budget equations to achieve interpretable and accurate sea surface temperature prediction while explicitly modeling advection, diffusion, and external forcing factors.


<details>
  <summary>Details</summary>
Motivation: Current data-driven SST models lack interpretability and fail to adequately capture key physical processes like seawater movement and external SST drivers, limiting their practical utility for understanding ocean-atmosphere interactions.

Method: The framework derives ODEs from fluid transport principles (advection and diffusion), uses variational optimization to recover latent velocity fields, and integrates an Energy Exchanges Integrator inspired by ocean heat budget equations to account for external forcing factors.

Result: SSTODE achieves state-of-the-art performance in global and regional SST forecasting benchmarks and visually reveals advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles, demonstrating both accuracy and interpretability.

Conclusion: The proposed SSTODE framework successfully bridges the gap between data-driven models and physical oceanography by providing interpretable, physically consistent SST predictions while maintaining competitive forecasting performance.

Abstract: Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.

</details>


### [73] [Physics-Guided Machine Learning for Uncertainty Quantification in Turbulence Models](https://arxiv.org/abs/2511.05633)
*Minghan Chu,Weicheng Qian*

Main category: cs.LG

TL;DR: A hybrid ML-physics approach combining CNN with Eigenspace Perturbation Method (EPM) to improve turbulence model uncertainty quantification by modulating perturbation magnitudes for better calibration while maintaining physical consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-based EPM for turbulence model uncertainty quantification tends to overpredict uncertainty bounds, requiring a more calibrated approach that preserves physical consistency.

Method: Proposes a convolutional neural network (CNN) that modulates EPM perturbation magnitudes to adjust uncertainty estimates while maintaining the physical structure of EPM perturbations.

Result: The hybrid ML-EPM framework produces substantially tighter and better-calibrated uncertainty estimates compared to baseline EPM alone across canonical test cases.

Conclusion: Integrating machine learning with physics-based uncertainty quantification methods like EPM can effectively improve calibration of uncertainty bounds in turbulent flow predictions while preserving physical consistency.

Abstract: Predicting the evolution of turbulent flows is central across science and engineering. Most studies rely on simulations with turbulence models, whose empirical simplifications introduce epistemic uncertainty. The Eigenspace Perturbation Method (EPM) is a widely used physics-based approach to quantify model-form uncertainty, but being purely physics-based it can overpredict uncertainty bounds. We propose a convolutional neural network (CNN)-based modulation of EPM perturbation magnitudes to improve calibration while preserving physical consistency. Across canonical cases, the hybrid ML-EPM framework yields substantially tighter, better-calibrated uncertainty estimates than baseline EPM alone.

</details>


### [74] [Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games](https://arxiv.org/abs/2511.05640)
*Hamza Virk,Sandro Amaglobeli,Zuhayr Syed*

Main category: cs.LG

TL;DR: Blind-IGT is the first framework to jointly recover both reward parameters (θ) and rationality parameter (τ) in inverse game theory, solving the statistical unidentifiability problem when τ is unknown. It provides optimal convergence rates and extends to Markov games.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse game theory methods assume the agents' rationality parameter τ is known, but when τ is unknown, a scale ambiguity emerges that makes reward parameters statistically unidentifiable. This creates a fundamental limitation in competitive settings.

Method: The authors introduce a normalization constraint to resolve the scale ambiguity, propose a Normalized Least Squares (NLS) estimator, and extend the framework to Markov games. They establish identifiability conditions and prove convergence rates.

Result: The proposed Blind-IGT framework achieves optimal O(N^{-1/2}) convergence rate for joint parameter recovery. When strong identifiability conditions fail, it provides partial identification guarantees through confidence set construction.

Conclusion: Blind-IGT successfully addresses the fundamental identifiability problem in inverse game theory when rationality parameters are unknown, with strong theoretical guarantees and empirical performance even in complex scenarios like Markov games with unknown transitions.

Abstract: Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $τ$) is known a priori. When $τ$ is unknown, a fundamental scale ambiguity emerges that couples $τ$ with the reward parameters ($θ$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $θ$ and $τ$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.

</details>


### [75] [KLASS: KL-Guided Fast Inference in Masked Diffusion Models](https://arxiv.org/abs/2511.05664)
*Seo Hyun Kim,Sunwoo Hong,Hojung Jung,Youngrok Park,Se-Young Yun*

Main category: cs.LG

TL;DR: KLASS is a fast diffusion model sampling method that uses token-level KL divergence to identify stable predictions, enabling parallel unmasking without extra training while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Masked diffusion models suffer from slow and static sampling speed due to iterative refinement processes, creating inference bottlenecks.

Method: KL-Adaptive Stability Sampling (KLASS) exploits token-level KL divergence to identify stable, high-confidence predictions and unmask multiple tokens in each iteration.

Result: KLASS achieves up to 2.78× wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers across text, image, and molecular generation domains.

Conclusion: KLASS demonstrates significant potential as a broadly applicable sampler for diffusion models, achieving both efficiency and quality improvements across diverse domains without requiring additional training.

Abstract: Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.

</details>


### [76] [Distributionally Robust Self Paced Curriculum Reinforcement Learning](https://arxiv.org/abs/2511.05694)
*Anirudh Satheesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: DR-SPCRL treats the robustness budget ε as a continuous curriculum, adaptively scheduling it to balance nominal and robust performance, achieving better trade-offs than fixed-ε approaches.


<details>
  <summary>Details</summary>
Motivation: Fixed robustness budgets in DRRL create a tradeoff between performance and robustness - small ε gives high nominal performance but weak robustness, while large ε causes instability and overly conservative policies.

Method: Proposes Distributionally Robust Self-Paced Curriculum RL (DR-SPCRL) that treats ε as a continuous curriculum and adaptively schedules it according to the agent's learning progress.

Result: Achieves 11.8% average increase in episodic return under perturbations compared to fixed/heuristic scheduling, and approximately 1.9× performance of nominal RL algorithms.

Conclusion: DR-SPCRL stabilizes training and achieves superior robustness-performance trade-off by dynamically adjusting the robustness budget during learning.

Abstract: A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $ε$. However, fixing $ε$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $ε$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\times$ the performance of the corresponding nominal RL algorithms.

</details>


### [77] [AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening](https://arxiv.org/abs/2511.05696)
*Jacob T. Rosenthal,Emma Hahesy,Sulov Chalise,Menglei Zhu,Mert R. Sabuncu,Lior Z. Braunstein,Anyi Li*

Main category: cs.LG

TL;DR: MSK-MATCH is an AI system that automates clinical trial eligibility screening with high accuracy (98.6%) and dramatically reduces manual screening time from 20 minutes to 43 seconds per case.


<details>
  <summary>Details</summary>
Motivation: Clinical trial participation rates remain low despite their importance in cancer care and research, creating a need for more efficient screening processes.

Method: The system integrates a large language model with an oncology trial knowledge base using retrieval-augmented architecture, providing explainable AI predictions grounded in source clinical text.

Result: In evaluation with 88,518 documents from 731 patients across 6 breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases, achieved 98.6% accuracy, and reduced screening time from 20 minutes to 43 seconds at $0.96 per patient-trial pair.

Conclusion: MSK-MATCH demonstrates that AI-assisted workflows can match or exceed human performance in clinical trial screening while dramatically improving efficiency and reducing costs.

Abstract: Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.

</details>


### [78] [TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification](https://arxiv.org/abs/2511.05704)
*Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: TabDistill is a knowledge distillation framework that transfers pre-trained transformer knowledge to simpler neural networks for tabular data classification, achieving parameter efficiency while maintaining strong few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Transformer models perform well on tabular data in few-shot scenarios but are computationally expensive and parameter-heavy. There's a need to maintain performance while reducing complexity.

Method: Knowledge distillation approach that transfers knowledge from pre-trained transformer models into simpler neural network architectures for tabular data classification.

Result: Distilled neural networks outperform classical baselines (neural networks, XGBoost, logistic regression) with equal training data, and sometimes even surpass the original transformer models they were distilled from.

Conclusion: TabDistill successfully provides parameter-efficient tabular data classification while maintaining strong few-shot performance, achieving the best of both worlds between transformers and simpler models.

Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.

</details>


### [79] [Distributionally Robust Multimodal Machine Learning](https://arxiv.org/abs/2511.05716)
*Peilin Yang,Yu Ma*

Main category: cs.LG

TL;DR: A distributionally robust optimization framework for multimodal learning that provides theoretical guarantees and improves robustness in practical applications, overcoming limitations of existing early fusion and heuristic uncertainty methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches rely on early fusion or heuristic uncertainty modeling, which downplay modality-aware effects and provide limited insights. There's a need for a principled framework that addresses distributional robustness in multimodal settings.

Method: Proposes a distributionally robust optimization (DRO) framework for multimodal learning, including theoretical analysis through complexity analysis, generalization bounds, and minimax lower bounds, extended to encoder-specific error propagation settings.

Result: The proposed approach improves robustness in both simulation settings and real-world datasets, demonstrating practical effectiveness while providing theoretical performance guarantees.

Conclusion: The paper provides a principled foundation for using multimodal machine learning in high-stakes applications with uncertainty, supported by both theoretical guarantees and empirical demonstrations of improved robustness.

Abstract: We consider the problem of distributionally robust multimodal machine learning. Existing approaches often rely on merging modalities on the feature level (early fusion) or heuristic uncertainty modeling, which downplays modality-aware effects and provide limited insights. We propose a novel distributionally robust optimization (DRO) framework that aims to study both the theoretical and practical insights of multimodal machine learning. We first justify this setup and show the significance of this problem through complexity analysis. We then establish both generalization upper bounds and minimax lower bounds which provide performance guarantees. These results are further extended in settings where we consider encoder-specific error propogations. Empirically, we demonstrate that our approach improves robustness in both simulation settings and real-world datasets. Together, these findings provide a principled foundation for employing multimodal machine learning models in high-stakes applications where uncertainty is unavoidable.

</details>


### [80] [GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery](https://arxiv.org/abs/2511.05726)
*Ziyang Gao,Annie Cheung,Yihao Ou*

Main category: cs.LG

TL;DR: GastroDL-Fusion: A dual-modal deep learning framework combining protein-ligand structural data and disease-associated gene sequences for improved binding affinity prediction in gastrointestinal disease drug/vaccine development.


<details>
  <summary>Details</summary>
Motivation: Traditional models focus only on structural information and miss genetic determinants influencing disease mechanisms and therapeutic responses, creating a gap in accurate binding affinity prediction for gastrointestinal diseases.

Method: Uses Graph Isomorphism Network (GIN) for protein-ligand molecular graphs and pre-trained Transformers (ProtBERT/ESM) for gene sequence embeddings, fused through multi-layer perceptron for cross-modal interaction learning.

Result: Achieves MAE of 1.12 and RMSE of 1.75 on GI disease-related targets, significantly outperforming CNN, BiLSTM, GIN, and Transformer-only baselines.

Conclusion: Integrating both structural and genetic features provides more accurate binding affinity predictions, offering a reliable computational tool for accelerating targeted therapies and vaccines for gastrointestinal diseases.

Abstract: Accurate prediction of protein-ligand binding affinity plays a pivotal role in accelerating the discovery of novel drugs and vaccines, particularly for gastrointestinal (GI) diseases such as gastric ulcers, Crohn's disease, and ulcerative colitis. Traditional computational models often rely on structural information alone and thus fail to capture the genetic determinants that influence disease mechanisms and therapeutic responses. To address this gap, we propose GastroDL-Fusion, a dual-modal deep learning framework that integrates protein-ligand complex data with disease-associated gene sequence information for drug and vaccine development. In our approach, protein-ligand complexes are represented as molecular graphs and modeled using a Graph Isomorphism Network (GIN), while gene sequences are encoded into biologically meaningful embeddings via a pre-trained Transformer (ProtBERT/ESM). These complementary modalities are fused through a multi-layer perceptron to enable robust cross-modal interaction learning. We evaluate the model on benchmark datasets of GI disease-related targets, demonstrating that GastroDL-Fusion significantly improves predictive performance over conventional methods. Specifically, the model achieves a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75, outperforming CNN, BiLSTM, GIN, and Transformer-only baselines. These results confirm that incorporating both structural and genetic features yields more accurate predictions of binding affinities, providing a reliable computational tool for accelerating the design of targeted therapies and vaccines in the context of gastrointestinal diseases.

</details>


### [81] [Compressing Chemistry Reveals Functional Groups](https://arxiv.org/abs/2511.05728)
*Ruben Sharma,Ross D. King*

Main category: cs.LG

TL;DR: Large-scale analysis shows traditional chemical functional groups are useful explanations that compress molecular data well, and dataset-specific groups discovered via MML outperform standard fingerprints in bioactivity prediction.


<details>
  <summary>Details</summary>
Motivation: To formally assess the utility of traditional chemical functional groups as explanations using computational learning theory principles.

Method: Unsupervised learning algorithm based on Minimum Message Length (MML) principle that searches for compressing substructures in ~3 million biologically relevant molecules.

Result: Discovered substructures include most human-curated functional groups plus novel larger patterns; dataset-specific groups significantly outperform MACCS and Morgan fingerprints in bioactivity regression tasks.

Conclusion: Traditional functional groups are validated as meaningful explanations, and dataset-specific functional groups discovered through compression-based methods provide superior predictive performance for bioactivity tasks.

Abstract: We introduce the first formal large-scale assessment of the utility of traditional chemical functional groups as used in chemical explanations. Our assessment employs a fundamental principle from computational learning theory: a good explanation of data should also compress the data. We introduce an unsupervised learning algorithm based on the Minimum Message Length (MML) principle that searches for substructures that compress around three million biologically relevant molecules. We demonstrate that the discovered substructures contain most human-curated functional groups as well as novel larger patterns with more specific functions. We also run our algorithm on 24 specific bioactivity prediction datasets to discover dataset-specific functional groups. Fingerprints constructed from dataset-specific functional groups are shown to significantly outperform other fingerprint representations, including the MACCS and Morgan fingerprint, when training ridge regression models on bioactivity regression tasks.

</details>


### [82] [QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification](https://arxiv.org/abs/2511.05730)
*Amin Golnari,Jamileh Yousefi,Reza Moheimani,Saeid Sanei*

Main category: cs.LG

TL;DR: QiVC框架将概率推断、变分优化和量子启发变换整合到卷积架构中，通过量子启发的旋转集成机制增强表达的稳定性和不确定性建模能力，在PCG信号分类任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号分类面临高噪声、个体间差异和类别不平衡等挑战，需要更鲁棒和不确定性感知的模型来提高分类准确性和可靠性。

Method: 提出量子启发变分卷积框架(QiVC)，包含QiRE机制对卷积权重进行可微分的低维子空间旋转，模拟量子态演化，在不增加参数的情况下增强表达能力。

Result: 在两个PCG数据集上的实验表明，QiVC-Net分别达到97.84%和97.89%的准确率，超越了现有方法。

Conclusion: QiVC框架为生物医学信号分析提供了有效的、不确定性感知的建模方法，具有实际应用潜力。

Abstract: This work introduces the quantum-inspired variational convolution (QiVC) framework, a novel learning paradigm that integrates principles of probabilistic inference, variational optimization, and quantum-inspired transformations within convolutional architectures. The central innovation of QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE performs differentiable low-dimensional subspace rotations of convolutional weights, analogously to quantum state evolution. This approach enables structured uncertainty modeling while preserving the intrinsic geometry of the parameter space, resulting in more expressive, stable, and uncertainty-aware representations. To demonstrate its practical potential, the concept is instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in the context of biosignal classification, focusing on phonocardiogram (PCG) recordings, a challenging domain characterized by high noise, inter-subject variability, and often imbalanced data. The proposed QiVC-Net integrates an architecture in which the QiVC layer does not introduce additional parameters, instead performing an ensemble rotation of the convolutional weights through a structured mechanism ensuring robustness without added highly computational burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art performance, reaching accuracies of 97.84% and 97.89%, respectively. These findings highlight the versatility of the QiVC framework and its promise for advancing uncertainty-aware modeling in real-world biomedical signal analysis. The implementation of the QiVConv layer is openly available in GitHub.

</details>


### [83] [Near-Exponential Savings for Mean Estimation with Active Learning](https://arxiv.org/abs/2511.05736)
*Julian M. Morimoto,Jacob Goldin,Daniel E. Ho*

Main category: cs.LG

TL;DR: Proposes PartiBandits, an active learning algorithm for efficient mean estimation of k-class variables using limited labels and auxiliary covariates, achieving minimax optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: Need to efficiently estimate E[Y] with limited labeled data when auxiliary covariates X are available, bridging UCB and disagreement-based active learning approaches.

Method: Two-stage algorithm: first learns partition to reduce conditional variance, then uses UCB-style subroutine (WarmStart-UCB) for label acquisition from strata.

Result: Achieves squared error ~O((ν + exp(-cN/logN))/N) with minimax optimal convergence rates.

Conclusion: PartiBandits effectively combines active learning strategies, validated through EHR simulations, with available R package implementation.

Abstract: We study the problem of efficiently estimating the mean of a $k$-class random variable, $Y$, using a limited number of labels, $N$, in settings where the analyst has access to auxiliary information (i.e.: covariates) $X$ that may be informative about $Y$. We propose an active learning algorithm ("PartiBandits") to estimate $\mathbb{E}[Y]$. The algorithm yields an estimate, $\widehatμ_{\text{PB}}$, such that $\left( \widehatμ_{\text{PB}} - \mathbb{E}[Y]\right)^2$ is $\tilde{\mathcal{O}}\left( \frac{ν+ \exp(c \cdot (-N/\log(N))) }{N} \right)$, where $c > 0$ is a constant and $ν$ is the risk of the Bayes-optimal classifier. PartiBandits is essentially a two-stage algorithm. In the first stage, it learns a partition of the unlabeled data that shrinks the average conditional variance of $Y$. In the second stage it uses a UCB-style subroutine ("WarmStart-UCB") to request labels from each stratum round-by-round. Both the main algorithm's and the subroutine's convergence rates are minimax optimal in classical settings. PartiBandits bridges the UCB and disagreement-based approaches to active learning despite these two approaches being designed to tackle very different tasks. We illustrate our methods through simulation using nationwide electronic health records. Our methods can be implemented using the PartiBandits package in R.

</details>


### [84] [Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder](https://arxiv.org/abs/2511.05745)
*Zhen Xu,Zhen Tan,Song Wang,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: This paper identifies a specialization problem in MoE-SAE architectures where experts fail to learn distinct features, and proposes two innovations—Multiple Expert Activation and Feature Scaling—that significantly reduce reconstruction error and feature redundancy while maintaining computational efficiency for LLM interpretability.


<details>
  <summary>Details</summary>
Motivation: Sparse autoencoders (SAEs) face a trade-off between interpretability (requiring high-dimensional hidden layers for sparsity) and computational efficiency. MoE approaches attempt to address this but suffer from experts learning overlapping features rather than specializing in distinct feature sets.

Method: The paper introduces two key innovations: 1) Multiple Expert Activation that engages semantically weighted expert subsets to encourage specialization, and 2) Feature Scaling that enhances diversity through adaptive high-frequency scaling.

Result: Experiments show a 24% reduction in reconstruction error and a 99% decrease in feature redundancy compared to existing MoE-SAE methods.

Conclusion: This work bridges the interpretability-efficiency gap in LLM analysis, enabling transparent model inspection without compromising computational feasibility.

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\% lower reconstruction error and a 99\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.

</details>


### [85] [Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs](https://arxiv.org/abs/2511.05758)
*Anirudh Satheesh,Sooraj Sathish,Swetha Ganesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Novel actor-critic algorithm for Robust Constrained Average-Cost MDPs that achieves ε-optimality and ε-feasibility with competitive sample complexity.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in Robust Constrained Average-Cost MDPs where standard primal-dual methods fail due to lack of strong duality and non-contractive Robust Bellman operator in average-cost setting.

Method: Proposes an actor-critic algorithm specifically designed for Average-Cost RCMDPs.

Result: Method achieves both ε-feasibility and ε-optimality with sample complexities of Õ(ε⁻⁴) and Õ(ε⁻⁶) under slackness/no-slackness assumptions, comparable to discounted setting performance.

Conclusion: The algorithm successfully overcomes fundamental challenges in average-cost constrained robust RL, providing the first practical solution with theoretical guarantees for this problem class.

Abstract: In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \(ε\)-feasibility and \(ε\)-optimality, and we establish a sample complexities of \(\tilde{O}\left(ε^{-4}\right)\) and \(\tilde{O}\left(ε^{-6}\right)\) with and without slackness assumption, which is comparable to the discounted setting.

</details>


### [86] [An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning](https://arxiv.org/abs/2511.05770)
*Zhijing Ye,Sheng Di,Jiamin Wang,Zhiqing Zhong,Zhaorui Zhang,Xiaodong Yu*

Main category: cs.LG

TL;DR: A new error-bounded lossy compression framework for federated learning gradients that uses temporal correlations and kernel structural regularities to achieve higher compression ratios than existing methods while preserving model accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces communication bottlenecks due to gradient transmission costs, especially with low-bandwidth clients. Existing error-bounded compression methods designed for smooth scientific data perform poorly on gradient tensors which have low smoothness and weak spatial correlation.

Method: Proposes a prediction mechanism exploiting temporal correlations across training rounds and structural regularities in convolutional kernels using: 1) cross-round magnitude predictor based on normalized exponential moving average, and 2) sign predictor leveraging gradient oscillation and kernel-level sign consistency.

Result: Achieves up to 1.53x higher compression ratios than SZ3 with lower accuracy loss, and reduces end-to-end communication time by 76.1%-96.2% in constrained-bandwidth FL scenarios.

Conclusion: The framework demonstrates strong scalability for real-world FL deployments by effectively addressing the communication bottleneck through gradient compression specifically designed for FL characteristics.

Abstract: Federated learning (FL) enables collaborative model training without exposing clients' private data, but its deployment is often constrained by the communication cost of transmitting gradients between clients and the central server, especially under system heterogeneity where low-bandwidth clients bottleneck overall performance. Lossy compression of gradient data can mitigate this overhead, and error-bounded lossy compression (EBLC) is particularly appealing for its fine-grained utility-compression tradeoff. However, existing EBLC methods (e.g., SZ), originally designed for smooth scientific data with strong spatial locality, rely on generic predictors such as Lorenzo and interpolation for entropy reduction to improve compression ratio. Gradient tensors, in contrast, exhibit low smoothness and weak spatial correlation, rendering these predictors ineffective and leading to poor compression ratios. To address this limitation, we propose an EBLC framework tailored for FL gradient data to achieve high compression ratios while preserving model accuracy. The core of it is an innovative prediction mechanism that exploits temporal correlations across FL training rounds and structural regularities within convolutional kernels to reduce residual entropy. The predictor is compatible with standard quantizers and entropy coders and comprises (1) a cross-round magnitude predictor based on a normalized exponential moving average, and (2) a sign predictor that leverages gradient oscillation and kernel-level sign consistency. Experiments show that this new EBLC yields up to 1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated into a real-world FL framework, APPFL, it reduces end-to-end communication time by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating strong scalability for real-world FL deployments.

</details>


### [87] [MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories](https://arxiv.org/abs/2511.05773)
*Zishuai Liu,Weihang You,Jin Lu,Fei Dou*

Main category: cs.LG

TL;DR: MARAuder's Map is a real-time human activity recognition framework that projects sensor data onto floorplans and uses hybrid deep learning with time embeddings and attention to handle unsegmented sensor streams in smart homes.


<details>
  <summary>Details</summary>
Motivation: Existing ambient sensor-based HAR methods fail to address real-time inference needs, spatial reasoning about physical layouts, and temporal context awareness, limiting their effectiveness in continuous real-world deployments with unsegmented data.

Method: Projects sensor activations onto physical floorplans to create trajectory-aware image sequences, processes them with hybrid deep learning, incorporates learnable time embeddings for contextual cues, and uses attention-based encoders to focus on informative segments during activity transitions.

Result: Extensive experiments on multiple real-world smart home datasets show the method outperforms strong baselines in real-time activity recognition.

Conclusion: MARAuder's Map provides a practical and effective solution for real-time human activity recognition from raw, unsegmented sensor streams in ambient sensor environments.

Abstract: Ambient sensor-based human activity recognition (HAR) in smart homes remains challenging due to the need for real-time inference, spatially grounded reasoning, and context-aware temporal modeling. Existing approaches often rely on pre-segmented, within-activity data and overlook the physical layout of the environment, limiting their robustness in continuous, real-world deployments. In this paper, we propose MARAuder's Map, a novel framework for real-time activity recognition from raw, unsegmented sensor streams. Our method projects sensor activations onto the physical floorplan to generate trajectory-aware, image-like sequences that capture the spatial flow of human movement. These representations are processed by a hybrid deep learning model that jointly captures spatial structure and temporal dependencies. To enhance temporal awareness, we introduce a learnable time embedding module that encodes contextual cues such as hour-of-day and day-of-week. Additionally, an attention-based encoder selectively focuses on informative segments within each observation window, enabling accurate recognition even under cross-activity transitions and temporal ambiguity. Extensive experiments on multiple real-world smart home datasets demonstrate that our method outperforms strong baselines, offering a practical solution for real-time HAR in ambient sensor environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [88] [Novel Concepts for Agent-Based Population Modelling and Simulation: Updates from GEPOC ABM](https://arxiv.org/abs/2511.05637)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Niki Popper*

Main category: cs.MA

TL;DR: This paper presents three transferable innovations from the GEPOC ABM population model: an innovative agent time-update concept, co-simulation-inspired simulation strategy, and accurate model parametrisation strategy.


<details>
  <summary>Details</summary>
Motivation: Dynamic agent-based population models are popular for decision support due to flexibility. The paper aims to share transferable innovations from GEPOC ABM to benefit other population models.

Method: Description of three selected methods: 1) innovative time-update concept for individual agents, 2) co-simulation-inspired simulation strategy, 3) strategy for accurate model parametrisation. Methods are described reproducibly.

Result: Successful development and application of these innovations in GEPOC ABM, demonstrating their effectiveness across various population-level research domains.

Conclusion: These transferable innovations can be successfully applied to other population models, enhancing their capabilities and performance.

Abstract: In recent years, dynamic agent-based population models, which model every inhabitant of a country as a statistically representative agent, have been gaining in popularity for decision support. This is mainly due to their high degree of flexibility with respect to their area of application. GEPOC ABM is one of these models. Developed in 2015, it is now a well-established decision support tool and has been successfully applied for a wide range of population-level research questions ranging from health-care to logistics. At least in part, this success is attributable to continuous improvement and development of new methods. While some of these are very application- or implementation-specific, others can be well transferred to other population models. The focus of the present work lies on the presentation of three selected transferable innovations. We illustrate an innovative time-update concept for the individual agents, a co-simulation-inspired simulation strategy, and a strategy for accurate model parametrisation. We describe these methods in a reproducible manner, explain their advantages and provide ideas on how they can be transferred to other population models.

</details>


### [89] [STAIR: Stability criterion for Time-windowed Assignment and Internal adversarial influence in Routing and decision-making](https://arxiv.org/abs/2511.05715)
*Roee M. Francos,Daniel Garces,Orhan Eren Akgün,Stephanie Gil*

Main category: cs.MA

TL;DR: New STAIR stability criterion proposed for multi-agent routing with adversaries, which is easier to analyze and monitor than existing methods, validated on real-world data.


<details>
  <summary>Details</summary>
Motivation: Current routing algorithms don't account for adversarial agents, which can cause severe performance degradation through coordinated denial-of-service attacks by spoofing locations.

Method: Introduction of a new stability criterion called STAIR that links stability directly to operational metrics like finite rejected requests, and implementation of time-window constraints in decision-making algorithms.

Result: STAIR is demonstrated through simulations on real-world San Francisco mobility-on-demand data as being practically relevant, and the phenomenon of degenerate stability is identified and mitigated.

Conclusion: STAIR is a practical alternative to existing stability criteria that is easier to analyze in adversarial settings and doesn't rely on discount factors, and time-window constraints are introduced to address degenerate stability issues.

Abstract: A major limitation of existing routing algorithms for multi-agent systems is that they are designed without considering the potential presence of adversarial agents in the decision-making loop, which could lead to severe performance degradation in real-life applications where adversarial agents may be present. We study autonomous pickup-and-delivery routing problems in which adversarial agents launch coordinated denial-of-service attacks by spoofing their locations. This deception causes the central scheduler to assign pickup requests to adversarial agents instead of cooperative agents. Adversarial agents then choose not to service the requests with the goal of disrupting the operation of the system, leading to delays, cancellations, and potential instability in the routing policy. Policy stability in routing problems is typically defined as the cost of the policy being uniformly bounded over time, and it has been studied through two different lenses: queuing theory and reinforcement learning (RL), which are not well suited for routing with adversaries. In this paper, we propose a new stability criterion, STAIR, which is easier to analyze than queuing-theory-based stability in adversarial settings. Furthermore, STAIR does not depend on a chosen discount factor as is the case in discounted RL stability. STAIR directly links stability to desired operational metrics, like a finite number of rejected requests. This characterization is particularly useful in adversarial settings as it provides a metric for monitoring the effect of adversaries in the operation of the system. Furthermore, we demonstrate STAIR's practical relevance through simulations on real-world San Francisco mobility-on-demand data. We also identify a phenomenon of degenerate stability that arises in the adversarial routing problem, and we introduce time-window constraints in the decision-making algorithm to mitigate it.

</details>


### [90] [S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning](https://arxiv.org/abs/2511.06727)
*Jiangwen Dong,Zehui Lin,Wanyu Lin,Mingjin Zhang*

Main category: cs.MA

TL;DR: Proposes a fine-grained subject-level framework using graph neural networks and multi-agent collaboration for complex reasoning problems, achieving better accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing task-level approaches are too coarse for heterogeneous problems requiring multiple domain knowledge areas; need finer subject-level reasoning capability.

Method: Uses Graph Neural Network to create Subject-based DAG identifying relevant subjects, assigns LLMs subject expertise scores, and enables graph-structured multi-agent collaboration based on S-DAG.

Result: Outperforms existing task-level model selection and multi-agent collaboration baselines on multi-subject benchmarks (MMLU-Pro, GPQA, MedMCQA) in accuracy and efficiency.

Conclusion: Subject-aware reasoning with structured multi-agent collaboration effectively addresses complex multi-subject problems, demonstrating significant improvements over coarse-grained approaches.

Abstract: Large Language Models (LLMs) have achieved impressive performance in complex reasoning problems. Their effectiveness highly depends on the specific nature of the task, especially the required domain knowledge. Existing approaches, such as mixture-of-experts, typically operate at the task level; they are too coarse to effectively solve the heterogeneous problems involving multiple subjects. This work proposes a novel framework that performs fine-grained analysis at subject level equipped with a designated multi-agent collaboration strategy for addressing heterogeneous problem reasoning. Specifically, given an input query, we first employ a Graph Neural Network to identify the relevant subjects and infer their interdependencies to generate an \textit{Subject-based Directed Acyclic Graph} (S-DAG), where nodes represent subjects and edges encode information flow. Then we profile the LLM models by assigning each model a subject-specific expertise score, and select the top-performing one for matching corresponding subject of the S-DAG. Such subject-model matching enables graph-structured multi-agent collaboration where information flows from the starting model to the ending model over S-DAG. We curate and release multi-subject subsets of standard benchmarks (MMLU-Pro, GPQA, MedMCQA) to better reflect complex, real-world reasoning tasks. Extensive experiments show that our approach significantly outperforms existing task-level model selection and multi-agent collaboration baselines in accuracy and efficiency. These results highlight the effectiveness of subject-aware reasoning and structured collaboration in addressing complex and multi-subject problems.

</details>


### [91] [Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots](https://arxiv.org/abs/2511.07071)
*Marcel Müller*

Main category: cs.MA

TL;DR: MARL-based strategies using CTDE outperform rule-based methods for deadlock handling in complex intralogistics systems with AMRs, but rule-based methods remain competitive in simpler environments.


<details>
  <summary>Details</summary>
Motivation: AMRs increase operational flexibility but also deadlock risk, degrading system throughput and reliability. Existing approaches neglect deadlock handling in planning phase and use rigid control rules that can't adapt to dynamic conditions.

Method: Developed structured methodology for integrating MARL into logistics planning and control. Created reference models for deadlock-capable MAPF problems. Compared traditional deadlock handling with MARL solutions (PPO and IMPALA algorithms) using grid-based environments and external simulation software.

Result: MARL-based strategies, especially with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. Rule-based methods remain competitive in simpler environments or those with ample spatial freedom due to lower computational demands.

Conclusion: MARL provides flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to operational context.

Abstract: This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.
  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.
  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.

</details>
