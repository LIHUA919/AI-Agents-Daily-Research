<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: This paper systematically evaluates AI control protocols for safety against both default and adaptive attacks, finding that deferring on critical actions maintains high robustness while resampling is vulnerable to sophisticated adversaries.


<details>
  <summary>Details</summary>
Motivation: As AI agents become more capable and widely deployed, ensuring their safe operation is critical. AI control protocols need evaluation against both current attacks and adaptive adversaries to understand their effectiveness and robustness.

Method: The authors systematically evaluate control protocols using SHADE-Arena, a dataset of diverse agentic environments. They analyze blue team protocols (deferral to trusted models, resampling, deferring on critical actions) against default attacks, then iterate red team strategies against these protocols.

Result: Resampling for incrimination and deferring on critical actions performed best against default attacks, increasing safety from 50% to 96%. However, adaptive red team strategies with additional affordances reduced safety for resampling to 17%, while deferring on critical actions remained highly robust.

Conclusion: Deferring on critical actions demonstrates strong robustness against adaptive adversaries, highlighting the importance of denying attack policies access to protocol internals for maintaining AI safety.

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [2] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent is a multi-agent LLM framework that decomposes data analysis workflows into specialized agents (intent, discovery, analysis, reporting) to overcome limitations of single LLMs, enabling non-experts to access open data repositories through natural language interfaces.


<details>
  <summary>Details</summary>
Motivation: Open data repositories are inaccessible to non-experts due to required expertise in dataset discovery, schema mapping, and statistical analysis. Single LLMs fail in end-to-end workflows due to attention dilution, interference of specialized reasoning patterns, and error propagation.

Method: Multi-agent framework with specialized agents: intent clarification, dataset discovery, analysis, and reporting. This maintains focused attention within agent contexts and enables validation at each stage of the workflow.

Result: Evaluation across 5 models and 50 queries showed: 97.5% agent win rates even for strongest models, universal agents (discovery, analysis) show consistent effectiveness (std dev 12.4%), conditional agents (report, intent) vary by model (std dev 20.5%), removing discovery/analysis causes catastrophic failures (243-280 instances), architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery).

Conclusion: Five design principles guide when and why specialization is necessary for complex analytical workflows, enabling broader access to public data through natural language interfaces. Specialization provides value independent of model strength, and wide variance in agent effectiveness across models requires model-aware architecture design.

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [3] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval is a large-scale benchmarking study comparing 36 LLMs as evaluators using a consensus-driven protocol, finding Claude 3.5 Sonnet has highest confidence, Gemini 1.5 Pro best overall performance, GPT-4o best tradeoff, and GPT-OSS 20B leads open-source models.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and trustworthy evaluation pipelines for large language models (LLMs) as judges is increasingly critical for reliable AI assessment.

Method: Multi-agent framework using consensus-driven evaluation protocol with scalable majority voting to aggregate pattern audits and issue codes into ground-truth labels, applied to complementary-item recommendation.

Result: Claude 3.5 Sonnet has highest decision confidence; Gemini 1.5 Pro best overall performance; GPT-4o best latency-accuracy-cost tradeoff; GPT-OSS 20B leads open-source models. Strong consensus in structured domains but disagreement in lifestyle categories.

Conclusion: ScalingEval establishes a reproducible benchmark and evaluation protocol for LLMs as judges, providing actionable guidance on scaling, reliability, and model family tradeoffs.

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [4] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: This paper introduces a benchmark to test whether large language models (LLMs) internalize real-world probabilistic knowledge, finding they perform poorly and lack observational distribution knowledge.


<details>
  <summary>Details</summary>
Motivation: Despite claims that LLMs are universal approximators of real-world distributions, statistical challenges like the curse of dimensionality question this ability. The paper aims to directly evaluate if LLMs capture empirical distributions from domains like economics and health.

Method: The authors develop the first benchmark to test LLMs' knowledge of probability distributions describing real-world populations, assessing performance across various domains.

Result: LLMs perform poorly overall and do not naturally internalize real-world statistics. They lack knowledge of observational distributions (Pearl's Causal Hierarchy Layer 1), implying limitations in interventional and counterfactual knowledge.

Conclusion: The findings challenge the notion of LLMs as universal distributional learners, highlighting fundamental gaps in their probabilistic knowledge and implications for causal reasoning capabilities.

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [5] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream is a KV cache compression method that enables 4x improved on-chip memory usage with minimal accuracy degradation, deployed in production inference systems with static graphs and continuous batching.


<details>
  <summary>Details</summary>
Motivation: Large LLMs with 100k+ context lengths require substantial on-chip memory for KV caches, but existing compression techniques like StreamingLLM and SnapKV are not widely adopted in industrial deployments due to framework constraints and unclear accuracy implications.

Method: Developed SnapStream, a KV cache compression method that works with static graphs and continuous batching frameworks like vLLM and SGLang. Tested accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1.

Result: 4x improved on-chip memory usage with minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. Successfully deployed in 16-way tensor-parallel DeepSeek-671B on SambaNova SN40L accelerators at 128k context length achieving up to 1832 tokens per second.

Conclusion: SnapStream is the first implementation of sparse KV attention techniques successfully deployed in production inference systems with static graphs and continuous batching, demonstrating practical KV cache compression at scale.

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [6] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: Proposes capability-based monitoring for LLMs in healthcare, shifting from task-based to capability-focused evaluation to detect systemic weaknesses and emergent behaviors across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional ML monitoring approaches are task-based and assume performance degradation from dataset drift, but LLMs are generalist systems not trained for specific tasks, requiring a new monitoring paradigm.

Method: Organizes monitoring around shared model capabilities (summarization, reasoning, translation, safety guardrails) rather than individual downstream tasks, enabling cross-task detection of systemic issues.

Result: A scalable framework for detecting systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss.

Conclusion: Capability-based monitoring provides a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist AI models in healthcare.

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [7] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: A multi-agent AI framework with specialized agents (Graph Ontologist, Design Engineer, Systems Engineer) collaborates to optimize NACA airfoil designs through iterative design-review loops using domain knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Traditional engineering design processes are resource-intensive and inefficient due to complex multi-domain collaborations and iterative refinements.

Method: Three-agent framework: Graph Ontologist builds domain knowledge graphs using LLMs, Systems Engineer formulates requirements, Design Engineer generates candidates, and iterative review loops with quantitative/qualitative feedback until validation.

Result: Successfully applied to aerodynamic optimization of 4-digit NACA airfoils, demonstrating enhanced efficiency, consistency, and quality in engineering design.

Conclusion: Collaborative AI agents with structured knowledge representations can significantly improve engineering design processes by enhancing efficiency, consistency, and quality.

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [8] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: Analysis of formal/informal statement discrepancies in miniF2F benchmark reveals alignment issues between autoformalization and theorem proving models, leading to creation of improved miniF2F-v2 with 70% accuracy.


<details>
  <summary>Details</summary>
Motivation: To analyze challenges in AI formal reasoning pipeline where models must understand natural language math problems, formalize them in Lean, and prove them, particularly focusing on discrepancies between formal and informal statements.

Method: Thorough comparison of formal and informal statements in miniF2F benchmark, identification of discrepancies, corrections of errors, and evaluation using state-of-the-art autoformalization and theorem proving models.

Result: Found significant statement discrepancies affecting more than half of miniF2F problems; after corrections in miniF2F-v2, pipeline accuracy improved from 40% to 70%, though still showing model misalignment.

Conclusion: Higher quality benchmarks like miniF2F-v2 are crucial for proper evaluation of formal reasoning systems and better diagnosis of autoformalization/theorem proving model failures.

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [9] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly is the first AI to achieve elite human-level play in multi-player Liar's Poker, outperforming both humans and LLMs through self-play reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Previous AI breakthroughs in poker-like games focused on two-player scenarios with subdued multi-player dynamics, while Liar's Poker features extensive multi-player engagement that presents a more challenging testbed.

Method: Used self-play with a model-free, actor-critic, deep reinforcement learning algorithm to train the Solly agent.

Result: Solly achieved elite human level performance (won over 50% of hands and positive equity) in both heads-up and multi-player Liar's Poker, outperformed LLMs, developed novel strategies, effectively randomized play, and was not easily exploitable by world-class players.

Conclusion: The success of Solly demonstrates that deep reinforcement learning can achieve elite performance in complex multi-player games with extensive engagement, advancing beyond previous poker AI achievements.

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


### [10] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: A novel framework that uses multi-modal large language models (MLLM) to enhance the fireworks algorithm (FWA) for solving complex optimization problems like TSP and EDA, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods struggle with non-convex, high-dimensional, black-box problems due to low efficiency and inaccurate gradient information. The advancement in LLMs' language understanding and code generation capabilities offers new opportunities for optimization algorithm design.

Method: Proposed Critical Part (CP) concept to extend FWA for complex high-dimensional tasks, leveraging MLLMs' multi-modal characteristics to utilize optimization process information more effectively.

Result: Experimental results show that FWAs generated under the new framework achieved or surpassed state-of-the-art results on many problem instances in traveling salesman problem (TSP) and electronic design automation (EDA) tasks.

Conclusion: The integration of multi-modal large language models with optimization algorithms like FWA provides an effective approach to address complex optimization challenges, demonstrating significant performance improvements over traditional methods.

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [11] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: A novel safety response framework for LLMs that uses input-level safety classification and output-level RAG with interpretation models to ensure secure responses and eliminate information fabrication.


<details>
  <summary>Details</summary>
Motivation: Security issues in LLMs are limiting their trustworthy deployment in critical domains, requiring systematic safeguards at both input and output levels.

Method: Input: supervised fine-tuning-based safety classification with 4-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention). Output: RAG integrated with fine-tuned interpretation model for real-time trustworthy knowledge grounding.

Result: Achieved 99.3% risk recall rate, significantly higher safety scores on public benchmarks than baseline, and perfect 100% safety score on proprietary high-risk test set.

Conclusion: The framework provides an effective engineering pathway for building high-security, high-trust LLM applications with exceptional protective capabilities in complex risk scenarios.

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [12] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: Proposes a novel validation methodology for formal XAI explainers and identifies incorrect explanations in PyXAI across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Formal XAI methods provide theoretical rigor but lack practical validation of their implementations, with little attention given to verifying real-world explainer tools.

Method: Developed a novel validation methodology for formal explainers and applied it to assess PyXAI, a publicly available formal explainer.

Result: Found incorrect explanations computed by PyXAI on most datasets analyzed, confirming the need for rigorous validation of formal explainer implementations.

Conclusion: The proposed validation methodology is crucial for ensuring the reliability of formal XAI explainers, as demonstrated by the identification of flaws in PyXAI.

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [13] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: Domain-specific AI assistant for Adobe Summit that handles event queries using human-in-the-loop development to overcome data sparsity and deployment challenges.


<details>
  <summary>Details</summary>
Motivation: Enhance productivity, streamline information access, and improve user experience in enterprise contexts through generative AI assistants.

Method: Human-in-the-loop workflow combining prompt engineering, retrieval grounding, and lightweight human validation to address data sparsity, quality assurance, and rapid deployment constraints.

Result: Successful real-world deployment of Summit Concierge assistant capable of handling wide range of event-related queries under challenging constraints.

Conclusion: Agile, feedback-driven development enables scalable and reliable AI assistants even in cold-start scenarios, demonstrating practical viability of the approach.

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [14] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: LLMs can accurately model human psychological trait correlations by role-playing based on Big Five Personality Scale responses, achieving near-human-level accuracy through a two-stage process of personality summarization and reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can model the correlational structure of human psychological traits from minimal quantitative inputs and understand their reasoning process.

Method: Prompted various LLMs with Big Five Personality Scale responses from 816 individuals to role-play responses on nine other psychological scales, analyzed reasoning traces and information selection patterns.

Result: LLMs achieved remarkable accuracy (RÂ² > 0.89) in capturing human psychological structure, exceeding semantic similarity predictions and approaching trained ML algorithm performance. They use a systematic two-stage process: personality summarization followed by reasoning.

Conclusion: LLMs can precisely predict individual psychological traits through abstraction and reasoning, offering powerful tools for psychological simulation and insights into emergent reasoning capabilities.

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [15] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: The paper presents AAA, a scalable web accessibility auditing framework that combines GRASP (graph-based sampling) and MaC (multimodal AI copilot) to operationalize WCAG-EM evaluations with reduced human effort.


<details>
  <summary>Details</summary>
Motivation: Current web accessibility auditing methods like WCAG-EM are resource-intensive and not scalable, leaving most websites non-compliant despite the importance of accessibility for digital equality.

Method: The AAA framework uses GRASP for representative page sampling via multimodal embeddings and MaC, a fine-tuned multimodal LLM, to assist auditors with cross-modal reasoning and task automation.

Result: Experiments show the framework enables effective, scalable auditing, with insights that small-scale fine-tuned models can perform well as AI copilots in accessibility tasks.

Conclusion: AAA demonstrates a viable human-AI partnership for scalable web accessibility auditing, supported by novel datasets and efficient AI assistance, advancing practical compliance efforts.

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [16] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: Theoretical analysis of parameterized complexity for explanation problems in transparent ML models, covering abductive and contrastive explanations across various model types.


<details>
  <summary>Details</summary>
Motivation: Address the gap in explainable AI by providing foundational understanding of explanation complexities for transparent ML models, contrary to black-box perceptions.

Method: Comprehensive theoretical investigation using parameterized complexity analysis on diverse ML models including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles.

Result: Provides insights into the computational complexities of generating explanations for various transparent ML models.

Conclusion: This work contributes vital insights for XAI research and advances the discourse on transparency and accountability in AI systems.

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE is a new benchmark for formal algebra that goes beyond contest math to evaluate research-level mathematical reasoning, revealing major gaps in LLMs' formalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks focus on contest-based math problems, which don't reflect the depth and abstraction of modern mathematical research. There's a need for benchmarks that evaluate research-level formal reasoning.

Method: Created FATE benchmark series with two components: FATE-H (100 problems in abstract algebra) and FATE-X (100 problems in commutative algebra), spanning from undergraduate to PhD+ difficulty levels. Evaluated state-of-the-art LLM provers using two-stage evaluation (natural language reasoning vs. formalization).

Result: Best LLM achieved only 3% accuracy on FATE-H and 0% on FATE-X (pass@64). Models showed better natural-language reasoning than formalization ability. Specialized provers had worse reflection than general-purpose models.

Conclusion: FATE establishes essential checkpoints for research-level formal mathematical reasoning and reveals significant challenges in LLMs' formalization capabilities that need to be addressed.

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [18] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: DiCoDe is a scalable co-design framework that uses diffusion models and critic distillation to jointly optimize agent policies and environment configurations, achieving state-of-the-art performance with significantly fewer samples.


<details>
  <summary>Details</summary>
Motivation: Current co-design methods struggle with high-dimensional design spaces and sample inefficiency when dealing with the moving targets of joint optimization in multi-agent systems.

Method: DiCoDe incorporates Projected Universal Guidance (PUG) for constraint-satisfying environment exploration and critic distillation to share knowledge from RL critics, enabling adaptation to evolving agent policies.

Result: Achieves 39% higher rewards in warehouse settings with 66% fewer simulation samples, consistently exceeding state-of-the-art across multi-agent benchmarks including warehouse automation, pathfinding, and wind farm optimization.

Conclusion: DiCoDe sets a new standard in agent-environment co-design and represents a stepping stone toward practical real-world applications of co-design principles.

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [19] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm is a stochastic deep graph clustering framework for dynamic group recommendation that handles real-time group formation with adaptive group numbers using efficient GCN architecture.


<details>
  <summary>Details</summary>
Motivation: Traditional group recommender systems assume static groups and focus only on accuracy, making them unsuitable for dynamic real-world scenarios where group formation itself is a core challenge.

Method: Uses lightweight GCN to capture high-order user information, stochastic cluster learning for adaptive group reconfiguration without retraining, and contrastive learning for dynamic refinement.

Result: Experiments show superior group formation quality, efficiency, and recommendation accuracy across multiple datasets compared to baselines.

Conclusion: DeepForm effectively addresses dynamic group formation challenges in recommender systems while maintaining high performance and adaptability.

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [20] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: A 7M parameter recursive neural network pre-trained on public ARC tasks achieves 6.67% on semi-private evaluation after efficient fine-tuning within competition compute limits.


<details>
  <summary>Details</summary>
Motivation: Previous open-source approaches like TRM scored well on ARC tasks but exceeded competition compute limits, requiring a more efficient method.

Method: Pre-trained a tiny recursive model on 1,280 public tasks for 700k+ steps over 48 hours, then full fine-tuned it in just 12,500 gradient steps during competition.

Result: Achieved 6.67% score on semi-private evaluation tasks, demonstrating efficient adaptation within allowed compute constraints.

Conclusion: Starting from pre-trained tiny recursive models enables effective fine-tuning on competition tasks while staying within compute limits, outperforming previous approaches in efficiency.

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [21] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: AI framework predicts fishing zones in the North Indian Ocean using ocean data to help fishermen save time and fuel.


<details>
  <summary>Details</summary>
Motivation: Fishermen face uncertainty in locating productive fishing grounds, impacting their livelihood and resource efficiency.

Method: Uses AI to analyze oceanographic parameters like sea surface temperature and chlorophyll concentration to identify Potential Fishing Zones.

Result: Preliminary results show reduced search time, lower fuel consumption, and improved resource utilization for fishermen.

Conclusion: The AI-assisted framework enhances sustainable fishing by providing accurate, region-specific PFZ predictions.

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [22] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: A framework using large language models for detecting and cleaning data poisoning attacks in human activity recognition systems, enabling zero-shot to few-shot learning with role-play and step-by-step reasoning.


<details>
  <summary>Details</summary>
Motivation: Wearable IoT systems need robust HAR but machine learning models are vulnerable to data poisoning attacks, while conventional defenses require extensive labeled data and lack adaptability.

Method: Uses LLMs with role-play prompting (acting as expert) and think step-by-step reasoning to detect anomalies and generate clean sensor data alternatives in zero-shot, one-shot, and few-shot learning paradigms.

Result: Evaluation shows effectiveness in detection accuracy, sanitization quality, latency, and communication cost, demonstrating practical security improvement.

Conclusion: LLM-based framework provides adaptable, real-time defense against data poisoning in wearable IoT HAR systems, reducing reliance on large datasets.

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [23] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: Using Llama 3.1-405B LLM to automatically generate structured data use case labels for genomic dataset citations in scientific literature, achieving promising F1 score of .674 in zero-shot classification.


<details>
  <summary>Details</summary>
Motivation: Need to scale analysis of how specific datasets are used in scientific literature beyond just identifying citations, avoiding expensive manual labeling and training data development.

Method: Applied open-source Llama 3.1-405B LLM for zero-shot classification of data use cases in publications citing genomic datasets, with novel evaluation framework.

Result: Achieved F1 score of .674 on zero-shot data citation classification task without predefined categories, showing promise for automated analysis.

Conclusion: While results are promising, barriers exist including data availability, prompt overfitting, computational infrastructure requirements, and evaluation costs that qualify the findings.

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [24] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: Developed ROGUE-TS, a Thompson Sampling algorithm for nonstationary bandit problems with habituation and recovery effects, plus a probability clipping method to balance personalization and population-level learning in micro-randomized trials.


<details>
  <summary>Details</summary>
Motivation: Existing bandit algorithms for nonstationary environments focus too much on exploitation, limiting exploration needed to estimate population-level treatment effects in micro-randomized trials for behavioral health interventions.

Method: Created ROGUE-TS (Thompson Sampling for ROGUE framework) with theoretical guarantees, and introduced probability clipping to balance regret minimization with minimum exploration probability.

Result: Validated on two MRT datasets (physical activity and bipolar disorder), achieving lower regret than existing approaches while maintaining high statistical power through clipping without significant regret increase.

Conclusion: The framework provides practical guidance for MRT designers to balance personalization with statistical validity, enabling reliable treatment effect detection while accounting for individual behavioral dynamics.

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [25] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: A Digital Twin-Graph Neural Network framework for proactive pavement management that models road networks as graphs and uses real-time data to predict deterioration and optimize maintenance.


<details>
  <summary>Details</summary>
Motivation: Traditional Pavement Management Systems are reactive and lack real-time intelligence for failure prevention and optimal maintenance planning in complex road networks.

Method: Model pavement segments as graph nodes and spatial relations as edges, stream real-time UAV, sensor, and LiDAR data into Digital Twin, and use inductive Graph Neural Networks to learn deterioration patterns from graph-structured inputs.

Result: Achieved R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation patterns. Developed interactive dashboard and reinforcement learning module for simulation and adaptive maintenance planning.

Conclusion: The DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, providing a foundation for proactive, intelligent pavement management with potential for real-world deployment and smart-city integration.

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [26] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru PÄdurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: UserAlign uses pairwise response comparisons and logistic bandit theory for efficient personalized alignment of AI-generated content with minimal user queries.


<details>
  <summary>Details</summary>
Motivation: Existing personalized alignment methods require many user preference queries or explicit text specifications, which are impractical or inefficient.

Method: Proposes UserAlign, an inference-time method based on best-arm identification in logistic bandits, using few pairwise comparisons to select optimal responses from a fixed pool.

Result: Experimental results demonstrate UserAlign's effectiveness in personalized text and image generation tasks.

Conclusion: Personalized alignment can be achieved efficiently with minimal user interaction using the UserAlign framework.

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [27] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: Integrating expected value of information (EVOI) into Bootstrapped DQN to enhance deep exploration in sparse-reward environments without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Traditional exploration methods like Îµ-greedy and Boltzmann struggle with efficient exploration-exploitation balance in high-dimensional, sparse-reward environments.

Method: Developed two novel algorithms that incorporate EVOI into Bootstrapped DQN, using value of information estimates to measure network head discrepancies and drive exploration to high-potential areas.

Result: Experiments in complex, sparse-reward Atari games show increased performance, better uncertainty utilization, and no additional hyperparameters needed.

Conclusion: EVOI integration significantly improves Bootstrapped DQN's deep exploration capability in challenging sparse-reward environments while maintaining simplicity.

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [28] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: A neural network-based framework for designing heterogeneous metamaterials that learns continuous two-scale representations, ensuring compatibility between unit cells without predefined datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional metamaterial design methods face challenges with enormous design spaces, compatibility issues between neighboring cells, and limitations of data-driven approaches that rely on fixed microstructure libraries.

Method: Uses a multiscale neural representation where the network takes both global and local coordinates as inputs, outputs an implicit field representing multiscale structures, and employs a compatibility loss term to enforce connectivity between adjacent unit cells.

Result: The framework can produce metamaterial designs at arbitrarily high resolution, enabling infinite upsampling for fabrication or simulation, and demonstrates effectiveness on mechanical metamaterial design, negative Poisson's ratio, and mechanical cloaking problems.

Conclusion: The proposed neural network-based approach successfully addresses key challenges in heterogeneous metamaterial design by providing a continuous representation that ensures compatibility across unit cells without requiring predefined datasets.

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [29] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan GÃ¼nnemann*

Main category: cs.LG

TL;DR: GraphBSI is a one-shot graph generative model using Bayesian Sample Inference that handles discrete structures via continuous belief refinement and achieves SOTA performance on molecular graph generation.


<details>
  <summary>Details</summary>
Motivation: Graph-structured data is important for applications like molecular generation and network analysis, but their discrete and unordered nature makes them challenging for traditional generative models, creating a need for discrete diffusion and flow matching approaches.

Method: GraphBSI uses Bayesian Sample Inference to iteratively refine a belief over graphs in continuous distribution parameter space. It formulates BSI as a stochastic differential equation and derives a noise-controlled SDE family that preserves marginal distributions via score function approximation.

Result: GraphBSI achieves state-of-the-art performance on molecular and synthetic graph generation benchmarks (Moses and GuacaMol), outperforming existing one-shot graph generative models.

Conclusion: GraphBSI is a novel and effective approach for graph generation that outperforms existing one-shot methods on standard benchmarks.

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [30] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: Proposes adaptive-sensorless monitoring with residual correction to improve container temperature/humidity predictions by correcting systematic errors using live telemetry data.


<details>
  <summary>Details</summary>
Motivation: Sensorless monitoring models have systematic errors and don't incorporate telemetry data, causing inaccurate predictions that confuse users during cargo transportation.

Method: Introduces residual correction method - a framework to correct systematic biases in sensorless models after observing live telemetry data, creating adaptive-sensorless monitoring.

Result: Achieved MAEs of 2.24-2.31Â°C (vs 2.43Â°C baseline) for temperature and 5.72-7.09% (vs 7.99% baseline) for humidity on 3.48 million data points - the largest container dataset used in research.

Conclusion: Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and reduced dependence on full connectivity in global shipping.

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [31] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: Proposes DADO, a novel distributional optimization algorithm that leverages decomposability structure in design spaces for more efficient in silico design of discrete objects.


<details>
  <summary>Details</summary>
Motivation: Current distributional optimization algorithms cannot utilize the decomposable structure of property predictors in scientific applications, making optimization over combinatorial design spaces challenging.

Method: DADO employs a soft-factorized generative model with graph message-passing to coordinate optimization across linked factors based on junction tree decomposability.

Result: The algorithm demonstrates improved efficiency in navigating complex design spaces by leveraging decomposable property predictors.

Conclusion: DADO provides a general framework for decomposition-aware distributional optimization that can be applied to various scientific design problems like protein engineering and materials science.

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [32] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: Vision Transformer (ViT) applied to options data can predict 30-day realized volatility from implied volatility surfaces, learning seasonal patterns and nonlinear features.


<details>
  <summary>Details</summary>
Motivation: Transformers show promise in financial forecasting but haven't been explored for options data, despite the known benefits of complex models in financial machine learning.

Method: Used Vision Transformer (ViT) architecture, typically for image recognition, to predict 30-day realized volatility from single-day implied volatility surfaces augmented with date information.

Result: ViT successfully learned seasonal patterns and nonlinear features from the implied volatility surface.

Conclusion: Transformer models show promising potential for options data analysis and volatility forecasting, suggesting a new direction for financial model development.

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [ALAS: Transactional and Dynamic Multi-Agent LLM Planning](https://arxiv.org/abs/2511.03094)
*Longling Geng,Edward Y. Chang*

Main category: cs.MA

TL;DR: ALAS is a framework that improves multi-agent LLM planning by separating planning from validation, using versioned logs for tracking, and enabling localized repair to avoid costly global recomputation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent planning is fragile due to circular verification, lack of state change tracking, and minor faults triggering expensive global recomputation.

Method: ALAS employs a stateful, disruption-aware design with a validator independent of the planning LLM, versioned execution logs for checks and restore points, and a repair protocol with policies like retry and compensation in a workflow IR.

Result: On job-shop scheduling benchmarks, ALAS achieved 83.7% success, reduced token usage by 60%, ran 1.82x faster, and effectively detected faults with localized repair minimizing disruption.

Conclusion: The integration of validator isolation, versioned logs, and localized repair enhances efficiency, feasibility, and scalability for multi-agent LLM planning systems.

Abstract: Large language models enable flexible multi-agent planning but remain fragile
in practice: verification is often circular, state changes are not tracked for
repair, and small faults trigger costly global recomputation. We present ALAS,
a stateful, disruption-aware framework that separates planning from
non-circular validation, records a versioned execution log for grounded checks
and restore points, and performs localized repair that preserves work in
progress. The validator operates independently of the planning LLM with fresh,
bounded context, avoiding self-check loops and mid-context attrition. The
repair protocol edits only the minimal affected region under explicit policies
(retry, catch, timeout, backoff, idempotency keys, compensation, loop guards)
defined in a canonical workflow IR that maps to Amazon States Language and Argo
Workflows. On job-shop scheduling suites (DMU, TA) across five classical
benchmarks, ALAS matches or exceeds strong single-LLM and multi-agent
baselines, achieving 83.7% success, reducing token usage by 60%, and running
1.82times faster under comparable settings. A minimal reliability study shows
that the validator detects injected structural faults with low overhead, and
that localized repair contains runtime perturbations with a bounded edit radius
and less makespan degradation than global recompute. Results indicate that the
combination of validator isolation, versioned execution logs, and localized
repair provides measurable efficiency, feasibility, and scalability for
multi-agent LLM planning. Code and seeds will be released.

</details>


### [34] [Learning Communication Skills in Multi-task Multi-agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.03348)
*Changxi Zhu,Mehdi Dastani,Shihan Wang*

Main category: cs.MA

TL;DR: MCS is a multi-agent deep reinforcement learning method that enables agents to learn communication protocols across multiple tasks using Transformer encoders and prediction networks, outperforming both multi-task and single-task baselines.


<details>
  <summary>Details</summary>
Motivation: In multi-agent deep reinforcement learning, agents need to coordinate through communication, and knowledge transfer across multiple tasks can improve learning efficiency and performance.

Method: Uses Transformer encoders to encode observations into a shared message space and introduces a prediction network to correlate messages with sender agents' actions for better coordination in multi-task environments.

Result: MCS achieves superior performance compared to multi-task MADRL without communication and single-task MADRL with/without communication across adapted multi-agent benchmark environments.

Conclusion: The proposed MCS framework effectively leverages communication skills transfer across tasks, demonstrating significant improvements in multi-task multi-agent reinforcement learning scenarios.

Abstract: In multi-agent deep reinforcement learning (MADRL), agents can communicate
with one another to perform a task in a coordinated manner. When multiple tasks
are involved, agents can also leverage knowledge from one task to improve
learning in other tasks. In this paper, we propose Multi-task Communication
Skills (MCS), a MADRL with communication method that learns and performs
multiple tasks simultaneously, with agents interacting through learnable
communication protocols. MCS employs a Transformer encoder to encode
task-specific observations into a shared message space, capturing shared
communication skills among agents. To enhance coordination among agents, we
introduce a prediction network that correlates messages with the actions of
sender agents in each task. We adapt three multi-agent benchmark environments
to multi-task settings, where the number of agents as well as the observation
and action spaces vary across tasks. Experimental results demonstrate that MCS
achieves better performance than multi-task MADRL baselines without
communication, as well as single-task MADRL baselines with and without
communication.

</details>
