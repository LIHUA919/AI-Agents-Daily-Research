<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: The paper proposes an intelligent grounding procedure for first-order ASPIC+ to manage grounding size while ensuring reasoning correctness, using Datalog translation and simplifications.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for rule-based argumentation in ASPIC+ lack efficient grounding solutions for first-order rules, leading to exponential input growth.

Method: Translate first-order ASPIC+ into Datalog, query for ground substitutions, and apply ASPIC+-specific simplifications to avoid unnecessary grounding.

Result: The proposed method maintains manageable grounding size and preserves reasoning correctness, with empirical evaluation showing scalability.

Conclusion: The intelligent grounding procedure effectively addresses the grounding challenge in first-order ASPIC+, enhancing scalability and practicality.

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [2] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent algorithmic recourse framework for scenarios with multiple stakeholders, optimizing social welfare while balancing individual actionability.


<details>
  <summary>Details</summary>
Motivation: Existing algorithmic recourse methods focus on single-individual and single-model scenarios, ignoring the multi-agent nature of real-world systems where individuals compete for resources.

Method: The framework models many-to-many interactions as a capacitated weighted bipartite matching problem, with three optimization layers: basic matching, capacity redistribution, and cost-aware optimization.

Result: Experiments show the framework achieves near-optimal welfare with minimal system modifications, extending recourse from individual to system-level design.

Conclusion: The work bridges individual recommendations and system-level welfare, offering a practical approach to multi-agent algorithmic recourse.

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [3] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: A data-driven inverse optimizer is integrated into a PPO-based framework for automated proton PBS treatment planning, improving efficiency and plan quality.


<details>
  <summary>Details</summary>
Motivation: Manual treatment planning for H&N cancers is time-consuming and relies heavily on human expertise. Automating this process can reduce effort and improve consistency.

Method: A Transformer-based L2O inverse optimizer predicts update steps using task-specific data. Integrated with a PPO-based virtual planner, it autonomously adjusts objectives and computes MU values.

Result: The L2O optimizer improves effectiveness by 22.97% and efficiency by 36.41% over L-BFGSB. Plans generated in ~2.55 hours match or exceed human-generated plans in quality.

Conclusion: The proposed framework automates and enhances proton PBS treatment planning, achieving high-quality results efficiently.

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [4] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Bl√ºmel,Anna Rapberger*

Main category: cs.AI

TL;DR: The paper explores strong and weak admissibility in assumption-based argumentation (ABA), extending these notions to non-flat ABA frameworks using bipolar set-based argumentation frameworks (BSAFs). It introduces new semantics and analyzes their properties, highlighting modularity and addressing shortcomings.


<details>
  <summary>Details</summary>
Motivation: To broaden the understanding of admissibility in ABA by investigating strong and weak admissibility, which have been underexplored, especially in non-flat ABA frameworks.

Method: Uses abstract bipolar set-based argumentation frameworks (BSAFs) to formalize and analyze strong and weak admissibility in non-flat ABA, introducing preferred, complete, and grounded semantics.

Result: Demonstrates that modularization properties hold for classical, strong, and weak admissibility, while also identifying and addressing some shortcomings of these semantics.

Conclusion: The study advances the theoretical foundations of ABA by formalizing strong and weak admissibility for non-flat frameworks, providing insights into their properties and limitations.

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [5] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: The paper highlights the gap in evaluating Large Reasoning Models (LRMs) by focusing on their inability to proactively ask for missing information in incomplete problems, revealing issues like overthinking and hallucination.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LRMs only assess problem-solving on well-defined tasks, missing the need for proactive intelligence in handling incomplete problems.

Method: A new dataset of incomplete problems is introduced, and LRMs are systematically evaluated on their ability to request missing information.

Result: LRMs struggle with proactively asking for information and exhibit behaviors like overthinking and hallucination. Supervised fine-tuning shows potential but faces challenges.

Conclusion: The study underscores the need for developing LRMs with genuine intelligence, beyond mere problem-solving, and highlights key challenges in achieving this.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [6] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE is a scale-aware gradual evolution framework for continual knowledge graph embedding (CKGE), addressing dynamic KG updates by adapting embedding dimensions and balancing knowledge preservation with new facts.


<details>
  <summary>Details</summary>
Motivation: Real-world KGs evolve dynamically, but existing CKGE methods fail to handle varying update scales and lack systematic evaluation.

Method: SAGE determines embedding dimensions based on update scales, expands the embedding space, and uses Dynamic Distillation to balance old and new knowledge.

Result: SAGE outperforms baselines with improvements of 1.38% in MRR, 1.25% in H@1, and 1.6% in H@10, showing the importance of adaptive dimensions.

Conclusion: SAGE demonstrates superior performance in dynamic KG embedding, highlighting the need for adaptive approaches in CKGE.

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [7] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI is a curriculum learning framework using GRPO to address RL limitations in GUI tasks, improving performance by 5.6-10.3% over prior methods.


<details>
  <summary>Details</summary>
Motivation: Current RL methods in GUI tasks treat training data uniformly and use coarse rewards, limiting adaptability and policy efficiency.

Method: Proposes CRAFT-GUI, combining curriculum learning (GRPO) with a nuanced reward function (rule-based + model-judged signals).

Result: Outperforms state-of-the-art by 5.6% (Android Control) and 10.3% (internal benchmarks).

Conclusion: Integrating RL with curriculum learning effectively enhances GUI task performance.

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [8] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: AIM-Bench assesses LLM agents' decision-making in uncertain supply chain scenarios, revealing biases similar to humans and suggesting mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To explore LLM agents' capabilities and biases in inventory decision-making under uncertainty, addressing gaps in real-world applicability.

Method: Introduces AIM-Bench, a benchmark for evaluating LLM agents through diverse inventory replenishment experiments.

Result: LLMs exhibit human-like decision biases; strategies like cognitive reflection and information sharing mitigate effects like pull-to-centre and bullwhip.

Conclusion: Highlights the need to address LLM biases in inventory decisions and supports developing human-centred decision systems for supply chains.

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [9] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena is a live leaderboard for LLMs and MLLMs, ranking models using human feedback from real-world applications, with innovations like Placement Matches and Proximity Sampling for reliable rankings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs and MLLMs rely on static datasets or general prompts, failing to reflect real-world performance.

Method: Inclusion Arena integrates pairwise model comparisons into user interactions, using the Bradley-Terry model with Placement Matches and Proximity Sampling for robust rankings.

Result: The platform provides reliable rankings, higher data transitivity, and reduced manipulation risks.

Conclusion: Inclusion Arena bridges the gap between benchmarks and real-world applications, accelerating practical LLM and MLLM development.

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [10] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: The paper formalizes probabilistic landmarks and adapts UCT to use them as subgoals in MDPs, improving performance in online probabilistic planning.


<details>
  <summary>Details</summary>
Motivation: Landmarks are underutilized in stochastic domains despite their success in classical planning. This work explores their potential in probabilistic settings.

Method: The authors formalize probabilistic landmarks and modify the UCT algorithm to balance greedy landmark achievement with long-term goal achievement.

Result: Benchmark tests show landmarks significantly enhance UCT performance, though the optimal balance between greedy and long-term goals varies by problem.

Conclusion: Landmarks can effectively guide anytime algorithms in solving MDPs, demonstrating their utility in stochastic domains.

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [11] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: The paper proposes an LLM-assisted planner with problem decomposition to address large-scale planning challenges, using LLM4Inspire and LLM4Predict paradigms for heuristic guidance and domain-specific knowledge integration.


<details>
  <summary>Details</summary>
Motivation: Large-scale planning problems suffer from state-space explosion, and prior LLM-based approaches lack domain-specific knowledge integration for valid plans.

Method: The planner decomposes problems into sub-tasks, using LLM4Inspire for general heuristic guidance and LLM4Predict for domain-specific intermediate condition inference.

Result: Empirical validation shows effective search space pruning, with LLM4Predict outperforming LLM4Inspire due to domain-specific knowledge.

Conclusion: Integrating domain-specific knowledge into LLMs (LLM4Predict) is promising for solving large-scale planning problems.

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: The paper proposes a cooperative game-based method for ensemble learning to address limitations like overfitting, underfitting, and class imbalance by considering multiple evaluation criteria for weight distribution, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble methods often rely on single evaluation criteria, limiting their ability to incorporate diverse pre-information about classifiers, which is crucial for realistic model performance.

Method: The paper introduces a cooperative game approach in multi-criteria situations to simultaneously consider various pre-information in classifiers, enabling better weight distribution.

Result: Experiments on the Open-ML-CC18 dataset showed the proposed method outperformed existing ensemble weighting techniques.

Conclusion: The cooperative game-based method effectively improves ensemble learning by leveraging multiple criteria, leading to superior performance.

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [13] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker is a 15B-parameter model that matches or outperforms larger 32B-parameter models while using half the memory, trained via a four-stage pipeline.


<details>
  <summary>Details</summary>
Motivation: Address the impractical memory and computational costs of large language models (LLMs) in enterprise settings by developing a smaller yet equally performant model.

Method: Four-stage training pipeline: Base Model upscaling, Continual Pre-training, Supervised Fine-tuning (SFT), and Reinforcement Learning using GRPO.

Result: Matches or exceeds performance of 32B-parameter models (e.g., o1-mini, QWQ32B, EXAONE-Deep-32B) with half the memory footprint.

Conclusion: Apriel-Nemotron-15B-Thinker offers a practical, efficient alternative to larger models for enterprise use.

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [14] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: Proposes a prompt-based continual learning (PCL) method for medical AI to address data-sharing constraints, catastrophic forgetting, and distribution shifts, improving accuracy and F1-score by 10% and 9 points, respectively.


<details>
  <summary>Details</summary>
Motivation: Ethical and institutional constraints limit medical data sharing, making centralized learning impractical. Local data updates risk overfitting and forgetting, while data distributions vary.

Method: Introduces PCL with a unified prompt pool and minimal expansion strategy, freezing subsets to reduce overhead. Includes a novel regularization term for balance.

Result: Outperforms state-of-the-art methods by 10% in accuracy and 9 points in F1-score on diabetic retinopathy datasets, with lower inference cost.

Conclusion: PCL enables sustainable medical AI advances, supporting real-time diagnosis and telemedicine in distributed healthcare.

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [15] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert is an interpretable retrosynthesis framework combining LLMs and specialized models via reinforcement learning, outperforming existing models and providing expert-aligned explanations.


<details>
  <summary>Details</summary>
Motivation: Existing retrosynthesis models lack interpretability and logic decision-making, limiting their practical utility.

Method: Retro-Expert integrates specialized models for shallow reasoning, LLMs for critical reasoning, and reinforcement learning for policy optimization.

Result: The framework outperforms both LLM-based and specialized models and offers interpretable, expert-aligned explanations.

Conclusion: Retro-Expert bridges the gap between AI predictions and actionable chemical insights, enhancing interpretability and performance.

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [16] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb introduces a synthetic data generation framework for LLM pretraining, outperforming existing datasets and offering faster training. It highlights the complexity of optimizing synthetic data quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the diminishing returns of scaling data quantity in LLM pretraining by exploring synthetic data as a solution.

Method: Developed BeyondWeb, a framework for generating high-quality synthetic pretraining data, and evaluated its performance against benchmarks.

Result: BeyondWeb outperforms state-of-the-art datasets by up to 5.1pp and 2.6pp, with faster training speeds. A 3B model trained on BeyondWeb surpasses an 8B model trained on Cosmopedia.

Conclusion: Optimizing synthetic data quality requires a multifaceted approach; BeyondWeb demonstrates the potential of well-executed methods for transformative improvements.

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [17] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: Proposes M&C, a framework for selecting the best pretrained T2I model for fine-tuning without exhaustive testing, achieving 61.3% accuracy in predicting the top model.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of selecting the best pretrained T2I model for fine-tuning on target data, a problem unexplored in current literature.

Method: Introduces M&C, a framework with a matching graph of models and datasets, using graph embedding features to predict the best model for fine-tuning.

Result: M&C predicts the best model 61.3% of the time and a close alternative otherwise, outperforming three baselines.

Conclusion: M&C effectively solves the model selection problem for T2I fine-tuning, enhancing efficiency and performance.

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [18] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE introduces a two-stage framework to prevent entropy collapse in RLVR, balancing exploration and exploitation for improved LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Prior RLVR methods suffer from low diversity due to static initial-state sampling, leading to entropy collapse and limited performance gains.

Method: CURE uses critical-token-guided regeneration for exploration in stage one and static sampling for exploitation in stage two.

Result: CURE achieves a 5% performance gain on math benchmarks and maintains high entropy.

Conclusion: CURE outperforms other RLVR methods, balancing exploration and exploitation effectively.

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [19] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: The paper extends the Strong Lottery Ticket Hypothesis (SLTH) to quantized neural networks, proving exact representation and optimal bounds for finite-precision networks.


<details>
  <summary>Details</summary>
Motivation: Previous SLTH results were limited to continuous settings, leaving a gap in understanding quantized networks.

Method: Leverages Borgs et al.'s work on the Number Partitioning Problem to analyze the Random Subset Sum Problem in quantized settings, then applies this to SLTH.

Result: Demonstrates exact representation of discrete neural networks and optimal overparameterization bounds.

Conclusion: The work bridges the gap between SLTH and quantization, providing theoretical foundations for efficient low-precision networks.

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [20] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura L√ºtzow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: Zono-conformal prediction introduces prediction zonotopes for efficient, data-light uncertainty quantification with coverage guarantees, outperforming interval-based methods.


<details>
  <summary>Details</summary>
Motivation: Address computational and data inefficiencies in current conformal prediction methods, and improve multi-dimensional dependency capture.

Method: Uses zonotopic uncertainty sets integrated into base predictors, identified via a single linear program; applicable to nonlinear predictors like neural networks.

Result: Less conservative than interval-based methods, achieves similar test coverage, and efficiently detects outliers.

Conclusion: Zono-conformal prediction offers a computationally efficient, data-light alternative with robust coverage guarantees.

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [21] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: The paper defines and formalizes the concept of 'confidence' in learning, distinguishing it from probability, and provides axiomatic foundations and representations for confidence-based learning.


<details>
  <summary>Details</summary>
Motivation: To clarify and formalize the often-misunderstood concept of confidence in learning, distinguishing it from probability and likelihood, and to unify related ideas like learning rates and Kalman gain under a single framework.

Method: The authors axiomatize confidence, propose two canonical measures for it, prove its representability, and derive compact representations (vector fields and loss functions) under additional assumptions.

Result: Confidence can be formally represented and measured, with Bayes Rule emerging as a special case of an optimizing learner with linear expectation loss.

Conclusion: The work provides a rigorous framework for understanding confidence in learning, unifying diverse concepts and offering practical representations for further study.

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [22] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: The paper shows that for certain non-Gaussian distributions (generalized nonparanormal), the precision matrix can still reveal conditional independence, similar to Gaussian cases. It also provides an efficient algorithm for this purpose.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of independence structures in non-Gaussian distributions, specifically those derived from Gaussian transformations, and to develop practical tools for inference.

Method: The paper introduces the generalized nonparanormal class, where transformations of Gaussian data retain conditional independence information in the precision matrix. A computationally efficient algorithm is proposed to recover this structure.

Result: The algorithm successfully infers conditional independence in synthetic and real-world data, demonstrating its effectiveness.

Conclusion: The generalized nonparanormal framework and the proposed algorithm provide a viable way to analyze conditional independence in non-Gaussian settings, bridging a gap with Gaussian methods.

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [23] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: The paper investigates the vulnerability of LIME and SHAP to adversarial manipulation, proposes a testing framework for robustness, and identifies improved configurations for bias detection.


<details>
  <summary>Details</summary>
Motivation: Post hoc explanation methods like LIME and SHAP are widely used but can be manipulated to hide biases, raising concerns about their reliability in high-stakes applications.

Method: The study replicates the COMPAS experiment, introduces a modular testing framework, and evaluates ensemble configurations of LIME/SHAP on biased models.

Result: Certain ensemble configurations significantly improve bias detection, outperforming original methods in resisting bias concealment.

Conclusion: The findings suggest that robust configurations of LIME/SHAP can enhance transparency and reliability in machine learning deployments.

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [24] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: Proposes an abundance-aware Set Transformer for microbiome sample representation, improving classification tasks by weighting sequence embeddings by abundance.


<details>
  <summary>Details</summary>
Motivation: Prior methods overlook taxa abundance in microbiome sample embeddings, limiting biological relevance and performance.

Method: Uses an abundance-aware Set Transformer to weight sequence embeddings by abundance and applies self-attention for aggregation.

Result: Outperforms average pooling and unweighted Set Transformers, achieving perfect performance in some cases.

Conclusion: Demonstrates the utility of abundance-aware aggregation for biologically informed microbiome representation.

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [25] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: The paper proposes a cost-effective predictive coding solution for instant messages using data management, feature selection, and logistic regression, with performance improved by dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: The informal nature and small size of instant messages pose challenges for predictive coding in the legal industry.

Method: A data management workflow groups messages into day chats, followed by feature selection and logistic regression, with dimensionality reduction for performance improvement.

Result: Tested on an Instant Bloomberg dataset, the method showed improved baseline model performance and demonstrated cost savings.

Conclusion: The approach provides an economically feasible and effective solution for predictive coding of instant messages.

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [26] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: A framework to debias watch time in video recommendations by comparing it to reference distributions, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Raw watch times are distorted by factors like video duration and popularity, leading to biased recommendations.

Method: Proposes a relative advantage debiasing framework using quantile-based signals and a two-stage architecture with distributional embeddings.

Result: Significant improvements in recommendation accuracy and robustness in offline and online experiments.

Conclusion: The framework effectively corrects biases in watch time, enhancing recommendation quality.

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [27] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Gir√≥-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: The paper introduces a Compressive Meta-Learning framework that improves the efficiency and accuracy of compressive learning by using neural networks to meta-learn encoding and decoding stages.


<details>
  <summary>Details</summary>
Motivation: The need for fast and efficient parameter-learning techniques due to large datasets, and the limitations of current randomized, data-independent compressive learning methods.

Method: Proposes a framework using neural networks to meta-learn encoding and decoding stages, applied to tasks like compressive PCA, ridge regression, k-means, and autoencoders.

Result: The framework provides faster and more accurate systems than current state-of-the-art approaches.

Conclusion: The Compressive Meta-Learning framework enhances compressive learning by leveraging data structure through neural networks, demonstrating effectiveness in various applications.

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [28] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: A multimodal system combining clinical notes and tabular data improves early ICD code prediction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Early forecasting of ICD codes can aid in health risk identification, treatment suggestions, and resource optimization, addressing gaps in post-discharge classification.

Method: Proposes a multimodal system using pre-trained encoders, feature pooling, and cross-modal attention to fuse clinical notes and tabular data, with a weighted temporal loss.

Result: The model outperforms current state-of-the-art systems in early ICD code prediction.

Conclusion: The proposed multimodal approach effectively enhances early prediction of ICD codes, demonstrating its potential for clinical applications.

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [29] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: FGAT, a new framework using graph neural networks and attention mechanisms, improves fashion recommendations by integrating outfit compatibility and user preferences.


<details>
  <summary>Details</summary>
Motivation: The fashion industry's rapid growth makes it hard for users to find compatible items. Existing systems often treat outfit compatibility and personalized recommendations separately, missing complex interactions.

Method: FGAT constructs a three-tier hierarchical graph (users, outfits, items) with visual and textual features, using graph attention to dynamically weight node importance.

Result: FGAT outperforms baselines like HFGN on the POG dataset, improving precision, HR, recall, NDCG, and accuracy.

Conclusion: Combining multimodal features, hierarchical graphs, and attention mechanisms enhances fashion recommendation accuracy and efficiency.

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [30] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: PAR provides a continuous optimization framework for quantization, showing high quantization in overparameterized regimes, efficient solvers, and statistical guarantees for linear regression.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimization over discrete or quantized variables by leveraging piecewise-affine regularization (PAR) for flexible modeling and computation.

Method: Theoretical analysis of PAR in supervised learning, including critical point properties, proximal mappings, and solving methods like proximal gradient and ADMM.

Result: In overparameterized regimes, PAR ensures high quantization at critical points. Efficient solvers and statistical guarantees for linear regression are derived.

Conclusion: PAR offers a robust framework for quantization with theoretical and computational advantages, applicable to various regularization types.

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [31] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenh√§usler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: CTRL is a meta-learning method improving accuracy and preserving source-level heterogeneity in ML tasks with diverse data sources.


<details>
  <summary>Details</summary>
Motivation: Address challenges like distributional shifts and varied sample sizes across data sources in ML tasks, e.g., asylum resettlement programs.

Method: Combines cross-domain residual learning and adaptive pooling/clustering (CTRL).

Result: Outperforms benchmarks on 5 datasets, including a Swiss asylum program dataset.

Conclusion: CTRL effectively balances data quantity and quality, enhancing prediction reliability across sources.

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [32] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: The paper proposes NeuralKDB, a neural version of K-dependence Bayesian classifier, using distributional representations to improve high-order feature dependency modeling in Bayesian networks.


<details>
  <summary>Details</summary>
Motivation: Bayesian network classifiers struggle with high-order feature dependencies due to parameter explosion and data sparsity. The paper aims to enhance their capability by leveraging distributional representations.

Method: The authors extend KDB into NeuralKDB, designing a neural network to learn distributional representations and parameterize conditional probabilities. A stochastic gradient descent algorithm is used for training.

Result: NeuralKDB outperforms conventional Bayesian classifiers and other competitive methods on 60 UCI datasets, excelling in capturing high-order dependencies.

Conclusion: The proposed NeuralKDB effectively addresses limitations of Bayesian networks by integrating distributional representations, achieving superior performance in complex data classification.

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [33] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: A semi-supervised generative model is proposed to handle missing views and labels in multi-view learning, outperforming existing methods in predictive and imputation tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-view learning often faces missing views and labels, and prior probabilistic approaches, while effective, are limited by the fully supervised IB framework.

Method: The method combines labeled and unlabeled data, maximizing likelihood for unlabeled samples and cross-view mutual information in a shared latent space.

Result: The model achieves superior predictive and imputation performance on image and multi-omics data with missing views and limited labels.

Conclusion: The proposed semi-supervised approach effectively leverages unlabeled data and enhances shared information extraction across views.

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [34] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper introduces the Modality Quantity and Quality Rebalanced (QQR) algorithm to address imbalance issues in Multimodal Online Federated Learning (MMO-FL) for IoT data.


<details>
  <summary>Details</summary>
Motivation: The need for efficient distributed learning paradigms in IoT due to multimodal data and edge device limitations, along with challenges like modality imbalance (QQI).

Method: Proposes the QQR algorithm, a prototype learning-based method, to rebalance modality quantity and quality during training.

Result: QQR outperforms benchmarks on real-world datasets under modality imbalance conditions.

Conclusion: QQR effectively mitigates QQI in MMO-FL, enhancing learning performance for IoT applications.

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [35] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: The paper introduces QBM-VAE, a hybrid quantum-classical model using Boltzmann distribution as a prior, outperforming Gaussian-based models in biological data tasks.


<details>
  <summary>Details</summary>
Motivation: Gaussian priors in deep learning limit model accuracy for complex data like biological datasets. The Boltzmann distribution is more expressive but computationally challenging.

Method: Proposes QBM-VAE, combining quantum processors for Boltzmann sampling with classical deep learning for a generative model.

Result: QBM-VAE excels in tasks like omics data integration and cell-type classification, surpassing traditional models.

Conclusion: Demonstrates quantum advantage in deep learning and provides a blueprint for hybrid quantum AI models.

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [36] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: The paper explores how causal abstraction theory can explain computational implementation in cognitive behavior, linking classical philosophy to modern machine learning.


<details>
  <summary>Details</summary>
Motivation: To understand how systems implement computations over representations, using causal abstraction as a framework.

Method: Uses causal abstraction theory and examples from deep learning to analyze computational implementation.

Result: Proposes an account of computational implementation grounded in causal abstraction, emphasizing representation's role.

Conclusion: Issues of computation and representation are best explored through generalization and prediction.

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [37] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF is a novel GNN framework addressing limitations in existing diffusion-based methods by unifying opinion dynamics models for adaptable, interpretable, and efficient graph learning.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based GNNs lack adaptability, depth, and theoretical understanding, prompting the need for a more flexible and interpretable framework.

Method: GODNF integrates multiple opinion dynamics models into a trainable diffusion mechanism, modeling heterogeneous patterns and dynamic influences.

Result: Theoretical analysis and experiments show GODNF outperforms state-of-the-art GNNs in tasks like node classification and influence estimation.

Conclusion: GODNF provides a robust, interpretable, and scalable solution for diffusion-based graph learning, addressing key challenges in the field.

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [38] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: A modulation-based meta-learning framework is introduced to generalize structure-preserving dynamics models across parametric families, avoiding costly retraining and enabling few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Existing structure-preserving models require fixed system configurations and costly retraining for new parameters, limiting scalability in parameter-varying scenarios.

Method: The paper proposes a modulation-based meta-learning framework that conditions models on latent representations of system parameters, eliminating the need for explicit optimization during adaptation.

Result: The approach achieves accurate predictions in few-shot settings while maintaining physical constraints, demonstrating effective generalization across parameter space.

Conclusion: The framework enables scalable and generalizable learning for parametric dynamical systems, outperforming traditional methods in stability and adaptability.

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [39] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS is a novel differentiable architecture search framework for cryptocurrency trading, integrating market regime awareness to outperform static models.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of static deep learning models in dynamic financial environments by embedding domain-specific knowledge like market regimes.

Method: Features a Bayesian search space, specialized neural modules for market conditions, and a multi-objective loss function with market-specific penalties and stability constraints.

Result: Achieves 80.3% Mean Absolute Error reduction and faster convergence (9 vs. 50+ epochs) compared to benchmarks.

Conclusion: Highlights the importance of integrating domain-specific knowledge in NAS for robust financial models.

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [40] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: The paper introduces 'Borrowing From the Future (BFF)', a contrastive multi-modal framework to improve early-stage pediatric risk assessments by leveraging later-stage data.


<details>
  <summary>Details</summary>
Motivation: Early-stage pediatric risk assessments are less precise but clinically desirable. The study aims to enhance prediction performance in early stages.

Method: BFF treats each time window as a distinct modality, training on all available data while using up-to-date information. It contrasts signals from later stages to supervise earlier stages.

Result: BFF shows consistent improvements in early risk assessments for two pediatric outcome prediction tasks.

Conclusion: BFF effectively improves early-stage risk assessments by borrowing signals from later stages, validated on real-world tasks.

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [41] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo introduces a scalable and generalizable modularizing-while-training approach for DNNs, improving module classification accuracy and reducing module size, applicable to Transformers and CNNs.


<details>
  <summary>Details</summary>
Motivation: The prohibitive costs of DNN model construction and the limitations of existing modularizing-while-training methods for diverse and large-scale models, especially Transformers, drive the need for a more scalable solution.

Method: NeMo operates at the neuron level, uses contrastive learning-based modular training, and employs a composite loss function to ensure scalability and generalizability.

Result: NeMo achieves average gains of 1.72% in module classification accuracy and 58.10% reduction in module size, outperforming state-of-the-art methods on both CNN and Transformer-based models.

Conclusion: NeMo offers a promising, scalable, and generalizable approach for DNN modularization, with demonstrated efficacy in practical scenarios.

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [42] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: The paper introduces an entropy-based mechanism to improve test-time reinforcement learning (TTRL) by balancing exploration and exploitation, achieving significant performance gains with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with adaptability in unsupervised scenarios and rely heavily on annotated data. TTRL aims to address these limitations but faces challenges like high inference costs and early-stage bias.

Method: The authors propose two strategies: Entropy-fork Tree Majority Rollout (ETMR) and Entropy-based Advantage Reshaping (EAR) to enhance TTRL's exploration-exploitation balance.

Result: Their approach improves Llama3.1-8B's performance by 68% in Pass at 1 on the AIME 2024 benchmark while using only 60% of the rollout tokens budget.

Conclusion: The method effectively optimizes inference efficiency, diversity, and robustness, advancing unsupervised reinforcement learning for open-domain reasoning.

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [43] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: A hybrid CNN-LSTM model for PM2.5 prediction outperforms traditional methods, achieving an RMSE of 5.236, but requires high computational resources.


<details>
  <summary>Details</summary>
Motivation: Accurate PM2.5 prediction is crucial for environmental protection, public health, and urban management amid global climate change.

Method: Combines CNN for spatial feature extraction and LSTM for temporal dependencies, using multivariate data from Beijing (2010-2015).

Result: Model achieves RMSE of 5.236, showing superior accuracy and generalization over traditional methods.

Conclusion: The model has strong real-world potential but needs optimization for computational efficiency and handling diverse atmospheric factors.

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [44] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM is a novel EEG decoding framework that learns personalized and shared spatio-temporal patterns for robust cross-subject decoding, outperforming state-of-the-art methods without subject-specific calibration.


<details>
  <summary>Details</summary>
Motivation: The challenge of cross-subject EEG decoding due to inter-subject variability and lack of subject-invariant representations.

Method: PTSM uses a dual-branch masking mechanism, factorized spatio-temporal masks, and information-theoretic constraints for disentangled embeddings.

Result: Achieves strong zero-shot generalization on motor imagery datasets, outperforming baselines.

Conclusion: Disentangled neural representations enable personalized and transferable decoding in non-stationary EEG settings.

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [45] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: An enhanced Interactive Voting-Based Map Matching algorithm improves GPS trajectory reconstruction accuracy, handles varying sampling rates, and integrates trajectory imputation and OpenStreetMap assets for broader applicability.


<details>
  <summary>Details</summary>
Motivation: To reconstruct GPS trajectories accurately regardless of input data quality and extend the original algorithm's capabilities to diverse real-world scenarios.

Method: Enhances the original algorithm with trajectory imputation, a distance-bounded interactive voting strategy, and OpenStreetMap integration to address missing data and reduce computational complexity.

Result: The improved algorithm maintains the original's strengths while significantly expanding its applicability and efficiency.

Conclusion: The advancements enable high-accuracy trajectory reconstruction across diverse geographic regions, leveraging OpenStreetMap's road network.

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [46] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali ƒ∞rfan Mahmutoƒüullarƒ±,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: The paper addresses gradient-based Decision-Focused Learning (DFL) for linear programs (LPs), showing that existing methods yield zero gradients. It proposes minimizing surrogate losses, even with differentiable optimization layers, achieving better regret and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based DFL methods for LPs often result in zero gradients, limiting their effectiveness. The paper aims to improve regret and training efficiency by minimizing surrogate losses.

Method: The paper proposes minimizing surrogate losses instead of direct regret minimization, even when using differentiable optimization layers. It evaluates this approach with DYS-Net, an efficient differentiable optimization technique for LPs.

Result: Experiments show that minimizing surrogate losses with differentiable optimization layers achieves comparable or better regret than surrogate-loss-based methods, while significantly reducing training time.

Conclusion: Minimizing surrogate losses with differentiable optimization layers is more effective for DFL in LPs, offering better regret and efficiency than existing approaches.

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [47] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: CHORD unifies SFT and RL via dynamic weighting, improving stability and performance by harmonizing off-policy expert data with on-policy exploration.


<details>
  <summary>Details</summary>
Motivation: Existing SFT and RL integration risks disrupting model patterns and overfitting to expert data.

Method: CHORD reframes SFT as a dynamically weighted auxiliary objective in RL, using global and token-wise controls to balance imitation and exploration.

Result: CHORD achieves stable, efficient learning and outperforms baselines by harmonizing expert data with exploration.

Conclusion: CHORD provides a unified, effective approach for refining LLMs, with potential for further research.

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [48] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: A framework for deriving fair classifiers from closed-weight LLMs via prompting, addressing fairness in high-stakes applications without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address group fairness in LLM-based classifiers for high-stakes applications, especially with closed-weight models like GPT-4, where traditional fairness methods are inapplicable.

Method: Treats LLM as a feature extractor, uses strategic prompts to elicit probabilistic predictions, and applies a fair algorithm post-hoc to train a lightweight classifier.

Result: Demonstrates strong accuracy-fairness tradeoffs on five datasets, outperforming traditional methods like head-tuning or training from scratch.

Conclusion: The proposed framework is data-efficient and effective for fair classification with both open and closed-weight LLMs.

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [49] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric G√ºnther,Bal√°zs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: The paper critiques local post-hoc explanation algorithms, showing they often fail to provide meaningful insights for complex models. It introduces a framework to define informative explanations and derives conditions under which certain algorithms become informative.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical guarantees for explanation algorithms in complex models and rigorously evaluate their informativeness.

Method: Introduces a learning-theory-based framework to define informative explanations and analyzes popular algorithms (e.g., gradient, SHAP, anchor) under this framework.

Result: Many popular explanation algorithms are non-informative for complex models. Conditions for informativeness are stronger than expected, e.g., gradient explanations fail for differentiable functions.

Conclusion: Explanation algorithms need modification to be informative, with implications for AI auditing, regulation, and high-risk applications.

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [50] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: The paper explores adversarial robustness in Spiking Neural Networks (SNNs) using temporal ensembling, proposing Robust Temporal self-Ensemble (RTE) to improve sub-network robustness and reduce adversarial transferability.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but vulnerable to adversarial attacks, with their temporal dynamics and robustness poorly understood.

Method: The authors introduce RTE, a training framework with a unified loss and stochastic sampling to enhance sub-network robustness and reduce temporal adversarial transfer.

Result: RTE outperforms existing methods in robust-accuracy trade-off and reshapes SNNs' internal robustness landscape.

Conclusion: The study emphasizes temporal structure's role in adversarial learning and provides a foundation for robust SNNs.

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [51] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle is a self-explainable GNN framework for class-level explanations, outperforming prior methods in fidelity, explainability, and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing self-explainable GNNs lack meaningful class-level explanations, prompting the need for a framework like GraphOracle.

Method: GraphOracle jointly learns a GNN classifier and discriminative subgraphs for each class, using efficient training and evaluation strategies.

Result: GraphOracle outperforms ProtGNN and PGIB in fidelity, explainability, and scalability, avoiding computational bottlenecks.

Conclusion: GraphOracle is a practical and principled solution for faithful class-level self-explainability in GNNs.

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [52] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: The paper introduces HS-GPPT, a framework for graph pre-training and prompt-tuning that ensures spectral alignment for effective knowledge transfer across diverse graph homophily levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle diverse spectral distributions in real-world graphs due to reliance on homophily-based low-frequency knowledge, limiting effective adaptation under limited supervision.

Method: Proposes HS-GPPT with a hybrid spectral filter backbone and local-global contrastive learning for spectral knowledge acquisition, and designs prompt graphs for spectral distribution alignment.

Result: Extensive experiments show HS-GPPT's effectiveness in transductive and inductive learning settings.

Conclusion: HS-GPPT bridges spectral gaps between pre-training and downstream tasks, enabling robust knowledge transfer across varying homophily levels.

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [53] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: The paper introduces Holistic Explainable AI (HXAI), a framework embedding explanations into every stage of AI workflows, tailored to users, and unifying six components into a taxonomy.


<details>
  <summary>Details</summary>
Motivation: Address the opacity of AI models by integrating explanations throughout the workflow to enhance trust and usability for diverse stakeholders.

Method: Proposes HXAI, a unified taxonomy of six components (data, analysis set-up, learning process, model output, model quality, communication channel) aligned with user needs, supported by a 112-item question bank and empirical studies.

Result: Identifies gaps in contemporary tools, provides a clear taxonomy to reduce ambiguity, and demonstrates how AI agents can translate technical details into stakeholder-specific narratives.

Conclusion: HXAI advances transparency, trustworthiness, and responsible AI deployment by integrating multidisciplinary insights and practical lessons.

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [54] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: The paper introduces Tail-Aware Conformal Prediction (TACP) to address imbalanced coverage in long-tail label distributions, ensuring better reliability for minority classes. An extension, soft TACP (sTACP), further balances coverage.


<details>
  <summary>Details</summary>
Motivation: Existing CP methods often over-cover head classes and under-cover tail classes, undermining reliability for minority classes despite marginal coverage guarantees.

Method: Proposes TACP to utilize long-tail structure and narrow the head-tail coverage gap, with an extension (sTACP) via reweighting.

Result: Theoretical analysis confirms TACP achieves a smaller head-tail gap. Experiments on benchmark datasets validate effectiveness.

Conclusion: TACP and sTACP improve coverage balance across classes, enhancing reliability for minority classes in long-tail distributions.

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [55] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: The study addresses reliability concerns in afforestation and reforestation projects by introducing a dataset with standardized location data integrity (LDIS) scores, revealing significant gaps in data quality.


<details>
  <summary>Details</summary>
Motivation: To enhance accountability in voluntary carbon markets by validating self-reported project data with satellite imagery and secondary sources.

Method: Compiled a global dataset of 1,289,068 planting sites from 45,628 projects over 33 years, using primary and secondary data, including satellite imagery, and introduced the LDIS score for location integrity.

Result: 79% of georeferenced sites failed at least 1 LDIS indicator, and 15% lacked machine-readable georeferenced data.

Conclusion: The dataset improves transparency in carbon markets and serves as valuable training data for computer vision tasks.

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [56] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: The paper introduces the Harmonized Gradient Descent (HGD) algorithm to address imbalanced data streams by balancing gradient norms across classes, achieving efficient online learning without extra resources.


<details>
  <summary>Details</summary>
Motivation: Real-world data streams are often imbalanced, and existing methods like resampling or reweighting are limited. The paper aims to solve this through gradient descent modification.

Method: Proposes HGD, which equalizes gradient norms across classes during training, requiring no additional parameters or data buffers.

Result: HGD achieves sub-linear regret bounds theoretically and outperforms existing methods in experiments on imbalanced data streams.

Conclusion: HGD is an efficient, resource-light solution for imbalanced data stream learning, validated by theory and experiments.

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [57] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA is a reinforcement learning algorithm combining individual rewards and pairwise preferences into a single update rule, outperforming SAC and RLHF baselines.


<details>
  <summary>Details</summary>
Motivation: To improve reinforcement learning by integrating both rewards and preferences, avoiding separate reward modeling and enhancing stability.

Method: DFA uses policy log-probabilities to model preference probability, incorporating human or synthesized preferences under a Bradley-Terry model.

Result: DFA matches or exceeds SAC in control environments and outperforms RLHF baselines in a stochastic GridWorld.

Conclusion: DFA effectively combines rewards and preferences, offering stable training and competitive performance.

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [58] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: The paper introduces a structural lifting strategy using Forman-Ricci curvature to enhance Graph Neural Networks (GNNs) by addressing over-squashing in higher-order graph structures.


<details>
  <summary>Details</summary>
Motivation: Real-world systems like social or biological networks require higher-order topological representations, but GNNs struggle with information distortion in long-distance message passing (over-squashing).

Method: The proposed method uses Forman-Ricci curvature to lift graph data into more expressive topologies, identifying network backbones and modeling them as hyperedges.

Result: The approach mitigates over-squashing by preserving structural properties and improving information flow across graph bottlenecks.

Conclusion: The structural lifting strategy enhances GNN performance in higher-order domains, offering a solution to information distortion in complex networks.

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [59] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD is a sequence-structure co-design framework optimizing antibody CDRs in a shared latent space, reducing query costs and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing antibody CDRs are inefficient in raw data space, requiring costly evaluations.

Method: LEAD optimizes sequences and structures in a shared latent space using a black-box guidance strategy for non-differentiable evaluators.

Result: LEAD achieves superior optimization, reduces query consumption by half, and outperforms baselines.

Conclusion: LEAD is an efficient framework for antibody CDR optimization, improving performance and reducing costs.

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [60] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: The paper proposes using contraction theory to enhance the robustness of Convolutional Neural Ordinary Differential Equations (NODEs) against input noise and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Neural networks are vulnerable to input noise and adversarial attacks, prompting the need for more robust models.

Method: The authors introduce contractive Convolutional NODEs, leveraging contraction theory to ensure exponential convergence of trajectories. Robustness is induced via Jacobian regularization or weight regularization for slope-restricted activation functions.

Result: The method is tested on MNIST and FashionMNIST datasets with corrupted images, demonstrating improved robustness.

Conclusion: Contractive Convolutional NODEs, trained with specific regularizers, offer enhanced robustness against noise and adversarial attacks.

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [61] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: The paper introduces mCOCO, a novel framework for generating connectional brain templates (CBTs) using Reservoir Computing (RC) to address limitations of existing methods like poor interpretability and high computational cost. mCOCO integrates multi-sensory inputs and outperforms GNN-based CBTs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CBT learning suffer from poor interpretability, high computational cost, and neglect of cognitive capacity. The paper aims to overcome these limitations.

Method: mCOCO leverages RC to learn functional CBTs from BOLD signals, integrating multi-sensory inputs (text, audio, visual) in two phases: deriving individual functional connectomes and incorporating cognitive traits.

Result: mCOCO outperforms GNN-based CBTs in centeredness, discriminativeness, topological soundness, and multi-sensory memory retention.

Conclusion: The mCOCO framework effectively addresses the limitations of existing CBT learning methods, offering improved interpretability, efficiency, and cognitive modeling.

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [62] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: The paper evaluates six probabilistic machine learning algorithms for uncertainty estimation, finding they are well-calibrated but struggle with out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of uncertainty quantification in complex data models like deep learning, ensuring reliable scientific discovery.

Method: Uses approximate Bayesian inference and empirical tests on synthetic datasets to assess six algorithms for class probability and uncertainty estimation.

Result: All algorithms are well-calibrated, but deep learning-based ones fail to consistently reflect uncertainty for out-of-distribution data.

Conclusion: The study clarifies the limitations of current uncertainty estimation methods, aiding future research in scientific data modeling.

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [63] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: A study uses AutoML and explainable AI to analyze crash severity in Ohio, identifying key risk factors with a Ridge Logistic Regression model achieving 84.9% AUC-ROC.


<details>
  <summary>Details</summary>
Motivation: Motor vehicle crashes are a major cause of injury and death, requiring data-driven methods to understand and reduce crash severity.

Method: Combines AutoML (JADBio platform) and SHAP for feature selection and interpretation, using a dataset of 2.3 million vehicle-level records.

Result: Achieved 84.9% AUC-ROC on test data, identifying 17 influential predictors, including location type and posted speed.

Conclusion: Provides a scalable, interpretable framework for traffic safety policy, prioritizing contextual over traditional factors like alcohol impairment.

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [64] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: A dual-space guided testing framework is proposed to balance diversity and criticality in scenario generation for safety verification of decision-making agents, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for effective safety verification in dynamic environments due to the increasing deployment of decision-making agents, with a focus on overcoming local optima entrapment in scenario generation.

Method: A dual-space framework coordinating scenario parameter space (hierarchical representation for subspace localization) and agent behavior space (feedback loop for adaptive generation mode switching).

Result: Improves critical scenario generation by 56.23% and achieves greater diversity, tested on five decision-making agents.

Conclusion: The framework effectively addresses the challenge of balancing diversity and criticality in scenario generation, outperforming state-of-the-art baselines.

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [65] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: The paper introduces Feynman diagrams to compute finite-width corrections for Neural Tangent Kernels (NTKs), simplifying algebraic manipulations and enabling recursive relations for deeper network analysis.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of infinite-width NTKs, which lack key training dynamics like NTK evolution and feature learning, by incorporating finite-width effects.

Method: Uses Feynman diagrams to compute finite-width corrections to NTK statistics, enabling recursive relations for preactivations, NTKs, and higher-derivative tensors.

Result: Extends stability results for deep networks to NTKs and proves no finite-width corrections for scale-invariant nonlinearities like ReLU on the NTK Gram matrix diagonal.

Conclusion: The framework is validated numerically and provides a feasible approach to analyze finite-width effects in NTK statistics.

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [66] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: An unsupervised anomaly detection method using a physics-informed diffusion model for multivariate time series, improving F1 scores and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance anomaly detection in time series by incorporating physics-dependent temporal distributions into diffusion models.

Method: Uses a weighted physics-informed loss during diffusion model training with a static weight schedule to learn data distribution.

Result: Improves F1 score, data diversity, and log-likelihood; outperforms baselines and prior physics-informed models on synthetic and real-world datasets.

Conclusion: Physics-informed training enhances diffusion models for unsupervised anomaly detection, achieving superior performance.

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [67] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST is a decentralized federated graph learning framework with adaptive communication, addressing the limitations of existing DFL and FGL methods by leveraging local subgraph topology for efficient model aggregation.


<details>
  <summary>Details</summary>
Motivation: Existing DFL methods lack consideration for local subgraph topology, and FGL relies on centralized architectures, missing decentralization benefits.

Method: DFed-SST uses a dual-topology adaptive communication mechanism to dynamically optimize inter-client communication based on local subgraph features.

Result: Experiments on eight datasets show DFed-SST outperforms baselines by 3.26% in average accuracy.

Conclusion: DFed-SST effectively bridges the gap in decentralized federated graph learning, offering improved performance and adaptability.

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [68] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: A nested Operator Inference (OpInf) method is introduced for learning physics-informed reduced-order models (ROMs) with improved accuracy and efficiency compared to standard OpInf.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance ROM learning by leveraging hierarchical reduced spaces and dynamic updates, addressing limitations of standard OpInf.

Method: The nested OpInf approach iteratively constructs initial guesses prioritizing dominant modes, ensuring lower reconstruction error and enabling warm-starting from existing models.

Result: Nested OpInf achieves a four times smaller error in a heat conduction problem and a 3% error with a 19,000x speed-up in a Greenland ice sheet model.

Conclusion: The nested OpInf method significantly outperforms standard OpInf in accuracy and computational efficiency, making it suitable for large-scale, dynamic applications.

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [69] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow is a server-based RL framework that decouples training from agent execution and optimizes GPU usage, ensuring stability and scalability for large-scale RL tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in industrial-scale RL, such as decoupling training from complex agent execution and maximizing GPU utilization while maintaining stability.

Method: Introduces a data plane for decoupling RL training and a tag-driven scheduling paradigm for resource abstraction. Uses a spatiotemporal multiplexing pipeline to dynamically reassign idle nodes.

Result: Achieves high throughput, eliminates pipeline bubbles, and fully exploits heterogeneous cluster resources.

Conclusion: SeamlessFlow is stable and high-performing, suitable for complex RL tasks like multi-agent and long-horizon scenarios.

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [70] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: The paper proposes a Markov game-based paradigm to analyze coalition structures in CCS projects, using multi-agent reinforcement learning with safety constraints to optimize stakeholder strategies.


<details>
  <summary>Details</summary>
Motivation: CCS projects involve diverse stakeholders with conflicting goals, making unilateral optimization unrealistic due to shared geological features and existing infrastructures.

Method: A multi-agent reinforcement learning approach with safety constraints is used, leveraging a surrogate model (E2C framework) to reduce computational costs.

Result: The framework effectively manages CO2 storage with multiple stakeholders, demonstrating optimal strategies while complying with safety regulations.

Conclusion: Collaborative coalition agreements are essential for effective CCS project management, as shown by the proposed framework.

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [71] [Allen: Rethinking MAS Design through Step-Level Policy Autonomy](https://arxiv.org/abs/2508.11294)
*Qiangong Zhou,Zhiting Wang,Mingyou Yao,Zongyang Liu*

Main category: cs.MA

TL;DR: Allen is a Multi-Agent System (MAS) designed to enhance policy autonomy and balance collaborative efficiency, task supervision, and human oversight in complex networks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in current MAS design, particularly improving policy autonomy and balancing collaboration, supervision, and oversight.

Method: Redefines the basic execution unit in MAS, enabling autonomous pattern formation. Uses a four-tier state architecture (Task, Stage, Agent, Step) for topological optimization and controllable progress.

Result: Achieves unprecedented policy autonomy while balancing collaborative structure controllability.

Conclusion: Allen successfully unifies topological optimization and progress control, with its code open-sourced for further development.

Abstract: We introduce a new Multi-Agent System (MAS) - Allen, designed to address two
core challenges in current MAS design: (1) improve system's policy autonomy,
empowering agents to dynamically adapt their behavioral strategies, and (2)
achieving the trade-off between collaborative efficiency, task supervision, and
human oversight in complex network topologies.
  Our core insight is to redefine the basic execution unit in the MAS, allowing
agents to autonomously form different patterns by combining these units. We
have constructed a four-tier state architecture (Task, Stage, Agent, Step) to
constrain system behavior from both task-oriented and execution-oriented
perspectives. This achieves a unification of topological optimization and
controllable progress.
  Allen grants unprecedented Policy Autonomy, while making a trade-off for the
controllability of the collaborative structure. The project code has been open
source at: https://github.com/motern88/Allen

</details>


### [72] [Defending a City from Multi-Drone Attacks: A Sequential Stackelberg Security Games Approach](https://arxiv.org/abs/2508.11380)
*Dolev Mutzari,Tonmoay Deb,Cristian Molinaro,Andrea Pugliese,V. S. Subrahmanian,Sarit Kraus*

Main category: cs.MA

TL;DR: The paper introduces S2D2, an algorithm for defending cities against multi-drone attacks by modeling the scenario as a Sequential Stackelberg Security Game. It outperforms greedy heuristics and approximates a Strong Stackelberg Equilibrium.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient defense strategy for cities facing multi-drone attacks, improving upon existing methods.

Method: Model the problem as a Sequential Stackelberg Security Game, where the defender commits to a mixed sequential strategy. The S2D2 algorithm is developed to compute this strategy.

Result: S2D2 outperforms greedy heuristics in experiments across 80 real cities and approximates a Strong Stackelberg Equilibrium under reasonable assumptions.

Conclusion: S2D2 provides an effective and structured defense strategy for multi-drone attacks, demonstrating significant improvements over prior approaches.

Abstract: To counter an imminent multi-drone attack on a city, defenders have deployed
drones across the city. These drones must intercept/eliminate the threat, thus
reducing potential damage from the attack. We model this as a Sequential
Stackelberg Security Game, where the defender first commits to a mixed
sequential defense strategy, and the attacker then best responds. We develop an
efficient algorithm called S2D2, which outputs a defense strategy. We
demonstrate the efficacy of S2D2 in extensive experiments on data from 80 real
cities, improving the performance of the defender in comparison to greedy
heuristics based on prior works. We prove that under some reasonable
assumptions about the city structure, S2D2 outputs an approximate Strong
Stackelberg Equilibrium (SSE) with a convenient structure.

</details>


### [73] [Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments](https://arxiv.org/abs/2508.11425)
*Jinwei Hu,Yi Dong,Youcheng Sun,Xiaowei Huang*

Main category: cs.MA

TL;DR: TAPA is a framework using LLMs to dynamically adapt modular programs for autonomous agents, improving performance in dynamic environments like cybersecurity and swarm intelligence.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents in safety-critical applications need to adapt without losing performance or reliability, requiring flexible and interpretable action spaces.

Method: TAPA leverages LLMs to synthesize, compose, and refine modular programs (logical primitives) for individual high-level actions, decoupling intent from execution.

Result: In DDoS defense, TAPA achieved 77.7% network uptime with high accuracy. In swarm control, it maintained consensus where baselines failed.

Conclusion: TAPA shifts autonomous system design from policy adaptation to dynamic action adaptation, proving effective in evolving environments.

Abstract: Autonomous agents in safety-critical applications must continuously adapt to
dynamic conditions without compromising performance and reliability. This work
introduces TAPA (Training-free Adaptation of Programmatic Agents), a novel
framework that positions large language models (LLMs) as intelligent moderators
of the symbolic action space. Unlike prior programmatic agents that typically
generate a monolithic policy program or rely on fixed symbolic action sets,
TAPA synthesizes and adapts modular programs for individual high-level actions,
referred to as logical primitives. By decoupling strategic intent from
execution, TAPA enables meta-agents to operate over an abstract, interpretable
action space while the LLM dynamically generates, composes, and refines
symbolic programs tailored to each primitive. Extensive experiments across
cybersecurity and swarm intelligence domains validate TAPA's effectiveness. In
autonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while
maintaining near-perfect detection accuracy in unknown dynamic environments. In
swarm intelligence formation control under environmental and adversarial
disturbances, TAPA consistently preserves consensus at runtime where baseline
methods fail completely. This work promotes a paradigm shift for autonomous
system design in evolving environments, from policy adaptation to dynamic
action adaptation.

</details>
