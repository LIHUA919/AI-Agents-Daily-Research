<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: A system using LLMs and AMR graphs to translate English logic puzzle descriptions into complete Answer Set Programming (ASP) code.


<details>
  <summary>Details</summary>
Motivation: Make ASP accessible to non-programmers by automatically generating code from natural language descriptions of logic puzzles.

Method: LLM simplifies natural language, identifies keywords, and generates simple facts; AMR graphs parse simplified text to systematically generate ASP constraints and rules.

Result: Successful creation of complete ASP programs that solve combinatorial logic problems from English descriptions.

Conclusion: This represents a significant step toward creating lightweight, explainable systems for natural language to complex logic problem solving.

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [2] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: The paper presents a neurosymbolic approach using Vector Symbolic Algebras (VSA) to solve the ARC-AGI benchmark, integrating System 1 intuition and System 2 reasoning through object-centric program synthesis.


<details>
  <summary>Details</summary>
Motivation: ARC-AGI is a challenging benchmark that humans solve effortlessly but remains difficult for AI systems. The authors aim to develop a cognitively plausible solver inspired by human intelligence models from neuroscience and psychology.

Method: The proposed solver uses Vector Symbolic Algebras to represent abstract objects and guide solution search through neurosymbolic methods, enabling object-centric program synthesis and sample-efficient neural learning.

Result: The solver achieved 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval, while performing well on simpler benchmarks (94.5% on Sort-of-ARC and 83.1% on 1D-ARC, outperforming GPT-4 at lower computational cost).

Conclusion: This represents the first application of VSAs to ARC-AGI and the most cognitively plausible solver developed to date, demonstrating the potential of neurosymbolic approaches for artificial general intelligence.

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [3] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: This paper proposes that enhancing LLM reasoning requires structured multi-agent pipelines enabling gradual, incremental, sequential (GIS) search, and demonstrates that recursive refinement (self-criticism + adversarial testing + feedback integration) effectively implements this approach.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' fluency, there's a need to extract stronger reasoning capabilities from them by understanding LLM computation through search-based interpretations.

Method: Compared simple linear pipeline vs complex structured pipeline with recursive refinement layer using multi-agent models representing US Founding Fathers (Hamilton, Jefferson, Madison) with RAG corpora, tested on contemporary political issues with quantitative LLM arbiter scoring and qualitative human evaluation.

Result: Complex model consistently outperformed simple model across all 9 test cases (average score 88.3 vs 71.7), showing superior analytical depth, structural nuance, and strategic framing.

Conclusion: Recursive refinement is a robust architectural feature for enhancing LLM reasoning through GIS search principles.

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [4] [Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning](https://arxiv.org/abs/2511.08749)
*Mehrdad Zakershahrak*

Main category: cs.AI

TL;DR: QDIN introduces query-specialized RL architecture that achieves near-perfect inference accuracy even with suboptimal control, showing decoupling between world knowledge and control representations.


<details>
  <summary>Details</summary>
Motivation: Traditional RL focuses only on reward maximization, but trained agents implicitly encode rich environmental knowledge that current architectures don't efficiently expose.

Method: Query Conditioned Deterministic Inference Networks (QDIN) with specialized neural modules optimized for different query types (policy, reachability, paths, comparisons).

Result: Shows fundamental decoupling: 99% reachability IoU inference accuracy vs 31% control return; query-specialized architectures outperform unified models and post-hoc methods.

Conclusion: Establishes agenda for RL systems designed as queryable knowledge bases from inception, with implications for interpretability, verification, and human-AI collaboration.

Abstract: Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.

</details>


### [5] [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)
*Elliot Meyerson,Giuseppe Paolo,Roberto Dailey,Hormoz Shahrzad,Olivier Francon,Conor F. Hayes,Xin Qiu,Babak Hodjat,Risto Miikkulainen*

Main category: cs.AI

TL;DR: MAKER system enables LLMs to perform million-step tasks with zero errors through extreme decomposition into microagents with voting-based error correction.


<details>
  <summary>Details</summary>
Motivation: Current LLMs have persistent error rates that prevent scaling to long-range tasks requiring many logical steps, despite achievements in reasoning and tool use.

Method: Extreme task decomposition into subtasks handled by focused microagents, with multi-agent voting scheme for error correction at each step.

Result: MAKER successfully solves tasks with over one million LLM steps with zero errors, demonstrating scalability beyond current LLM capabilities.

Conclusion: Massively decomposed agentic processes (MDAPs) offer a viable alternative to improving base LLMs for solving complex organizational/societal-level problems.

Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.

</details>


### [6] [Neural Value Iteration](https://arxiv.org/abs/2511.08825)
*Yang You,Ufuk Çakır,Alex Schutz,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: Proposes Neural Value Iteration, a POMDP solver using neural networks instead of α-vectors to handle large-scale problems efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional POMDP solvers using α-vectors become computationally intractable for large state spaces due to high-dimensional Bellman backups.

Method: Represents the value function as a finite set of neural networks, leveraging their generalization capability within value iteration.

Result: Achieves near-optimal solutions in extremely large POMDPs where existing offline solvers fail.

Conclusion: Neural networks offer a scalable alternative to α-vectors for POMDP planning, enabling effective solutions in previously intractable problems.

Abstract: The value function of a POMDP exhibits the piecewise-linear-convex (PWLC) property and can be represented as a finite set of hyperplanes, known as $α$-vectors. Most state-of-the-art POMDP solvers (offline planners) follow the point-based value iteration scheme, which performs Bellman backups on $α$-vectors at reachable belief points until convergence. However, since each $α$-vector is $|S|$-dimensional, these methods quickly become intractable for large-scale problems due to the prohibitive computational cost of Bellman backups. In this work, we demonstrate that the PWLC property allows a POMDP's value function to be alternatively represented as a finite set of neural networks. This insight enables a novel POMDP planning algorithm called \emph{Neural Value Iteration}, which combines the generalization capability of neural networks with the classical value iteration framework. Our approach achieves near-optimal solutions even in extremely large POMDPs that are intractable for existing offline solvers.

</details>


### [7] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: UCO introduces a reinforcement learning method with two reward functions to improve LLMs as intelligent tutors by better assessing student understanding and adapting to cognitive development.


<details>
  <summary>Details</summary>
Motivation: Current LLM fine-tuning for education lacks dynamic adaptation and cannot distinguish genuine student understanding from answer echoing.

Method: UCO uses multi-turn interactive RL with Progress Reward (measures cognitive advancement) and Scaffold Reward (identifies Zone of Proximal Development).

Result: UCO outperforms 11 baselines on BigMath and MathTutorBench, matching advanced closed-source models.

Conclusion: UCO effectively enhances adaptive teaching in LLMs by addressing key limitations in evaluating and responding to student cognition.

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [8] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine is the first open recipe for generalist agents that complete complex, hours-long missions in 3D open-world games using raw pixel input and adaptive reasoning, achieving human-level performance and zero-shot cross-game generalization.


<details>
  <summary>Details</summary>
Motivation: To develop generalist agents capable of handling complex, real-time missions in challenging 3D open-world environments with human-like efficiency, bridging perception, reasoning, and action seamlessly.

Method: Uses a vision-language model in an end-to-end framework, processing raw pixels at 5 Hz to generate 30 Hz keyboard-mouse actions, with adaptive reasoning invoked only when necessary. Trained in Genshin Impact.

Result: Completes the 5-hour Mondstadt storyline at human-level efficiency, handles diverse tasks (exploration, combat, puzzles), and achieves zero-shot generalization to Wuthering Waves and Honkai: Star Rail without fine-tuning.

Conclusion: Lumine represents a significant advancement toward generalist agents for open-ended environments, demonstrating robust performance and cross-game adaptability.

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [9] [The Double Contingency Problem: AI Recursion and the Limits of Interspecies Understanding](https://arxiv.org/abs/2511.08927)
*Graham L. Bishop*

Main category: cs.AI

TL;DR: Bioacoustic AI systems overlook the clash between AI's recursive cognition and animals' recursive communication, risking misinterpretation due to different contingencies; a shift to viewing AI as diplomatic agents is proposed.


<details>
  <summary>Details</summary>
Motivation: Current bioacoustic AI excels at cross-species pattern recognition but ignores that AI systems are recursive cognitive agents, potentially distorting animal communication due to differing contingencies, raising ethical and accuracy concerns.

Method: The paper uses philosophical analysis, referencing Yuk Hui's work on recursivity and contingency, to critique AI approaches and propose a conceptual reframing rather than technical solutions.

Result: The analysis reveals a double contingency problem where AI and animal communication systems interact problematically, suggesting current methods may obscure or misrepresent signals.

Conclusion: Bioacoustic AI should be reconceptualized as diplomatic encounters between recursive cognitions, requiring changes in model design, evaluation, and research to address the double contingency.

Abstract: Current bioacoustic AI systems achieve impressive cross-species performance by processing animal communication through transformer architectures, foundation model paradigms, and other computational approaches. However, these approaches overlook a fundamental question: what happens when one form of recursive cognition--AI systems with their attention mechanisms, iterative processing, and feedback loops--encounters the recursive communicative processes of other species? Drawing on philosopher Yuk Hui's work on recursivity and contingency, I argue that AI systems are not neutral pattern detectors but recursive cognitive agents whose own information processing may systematically obscure or distort other species' communicative structures. This creates a double contingency problem: each species' communication emerges through contingent ecological and evolutionary conditions, while AI systems process these signals through their own contingent architectural and training conditions. I propose that addressing this challenge requires reconceptualizing bioacoustic AI from universal pattern recognition toward diplomatic encounter between different forms of recursive cognition, with implications for model design, evaluation frameworks, and research methodologies.

</details>


### [10] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: This paper proposes an AI and big data integrated business process optimization model using a three-layer architecture for intelligent lifecycle management, achieving significant efficiency gains in enterprises.


<details>
  <summary>Details</summary>
Motivation: Digital transformation necessitates business process optimization to enhance enterprise competitiveness, requiring intelligent solutions for complex process management.

Method: A three-layer model with data processing, AI algorithms, and business logic layers, utilizing distributed computing and deep learning for real-time monitoring and optimization.

Result: Experimental results show 42% reduction in process time, 28% improvement in resource utilization, 35% cost reduction, and 99.9% availability under high concurrency.

Conclusion: The model offers substantial theoretical and practical value for enterprise digital transformation, providing new approaches to operational efficiency enhancement.

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [11] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast is an LLM-human co-reasoning framework that reframes time series forecasting as an interactive process, improving accuracy through collaborative preparation, generation, and verification.


<details>
  <summary>Details</summary>
Motivation: Current forecasting methods are static and lack the adaptability and reasoning of human experts, limiting their effectiveness in complex real-world scenarios.

Method: It involves an automated prediction preparation stage building a multi-source cognitive foundation (features, knowledge base, context repository, case base) and a generative reasoning stage with a meta-reasoning loop for self-correction.

Result: AlphaCast consistently outperforms state-of-the-art baselines in experiments on short- and long-term forecasting datasets.

Conclusion: The framework successfully bridges the gap between automated forecasting and human expertise, demonstrating superior performance and practical utility.

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [12] [Heterogeneous Graph Neural Networks for Assumption-Based Argumentation](https://arxiv.org/abs/2511.08982)
*Preesha Gehlot,Anna Rapberger,Fabrizio Russo,Francesca Toni*

Main category: cs.AI

TL;DR: First GNN approach for approximating credulous acceptance in Assumption-Based Argumentation, achieving state-of-the-art performance and enabling scalable approximate reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the computational intractability of exact extension computation under stable semantics for large ABA frameworks, as structured argumentation faces scalability challenges.

Method: Presented the first Graph Neural Network (GNN) approach using dependency graph representation of ABA frameworks, with two architectures: ABAGCN and ABAGAT using residual heterogeneous convolution and attention layers respectively. Trained on ICCMA 2023 benchmark with synthetic augmentation and Bayesian hyperparameter optimization.

Result: Both ABAGCN and ABAGAT outperform state-of-the-art GNN baseline with up to 0.71 node-level F1 score on ICCMA instances. Extension-reconstruction algorithm achieves F1 above 0.85 on small ABAFs and maintains about 0.58 on large frameworks.

Conclusion: This work opens new avenues for scalable approximate reasoning in structured argumentation by demonstrating the effectiveness of GNN-based approaches for approximating credulous acceptance in ABA frameworks.

Abstract: Assumption-Based Argumentation (ABA) is a powerful structured argumentation formalism, but exact computation of extensions under stable semantics is intractable for large frameworks. We present the first Graph Neural Network (GNN) approach to approximate credulous acceptance in ABA. To leverage GNNs, we model ABA frameworks via a dependency graph representation encoding assumptions, claims and rules as nodes, with heterogeneous edge labels distinguishing support, derive and attack relations. We propose two GNN architectures - ABAGCN and ABAGAT - that stack residual heterogeneous convolution or attention layers, respectively, to learn node embeddings. Our models are trained on the ICCMA 2023 benchmark, augmented with synthetic ABAFs, with hyperparameters optimised via Bayesian search. Empirically, both ABAGCN and ABAGAT outperform a state-of-the-art GNN baseline that we adapt from the abstract argumentation literature, achieving a node-level F1 score of up to 0.71 on the ICCMA instances. Finally, we develop a sound polynomial time extension-reconstruction algorithm driven by our predictor: it reconstructs stable extensions with F1 above 0.85 on small ABAFs and maintains an F1 of about 0.58 on large frameworks. Our work opens new avenues for scalable approximate reasoning in structured argumentation.

</details>


### [13] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: Argus is a runtime resilience framework that enhances autonomous driving systems by continuously monitoring for hazards and taking control when unsafe situations are detected, improving driving scores by up to 150.30% and preventing 64.38% of safety violations.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving systems face diverse driving hazards that compromise safety and performance when deployed on public roads, creating a need for resilience capabilities to monitor hazards and respond to safety violations.

Method: Argus continuously monitors trajectories generated by ADSs and when the EGO vehicle is deemed unsafe, it seamlessly takes control through a hazard mitigator. The framework was integrated with three state-of-the-art ADSs: TCP, UniAD and VAD.

Result: Argus effectively enhanced ADS resilience, improving driving scores by up to 150.30% on average, preventing up to 64.38% of violations, with minimal additional time overhead.

Conclusion: The Argus framework successfully bridges the resilience gap in autonomous driving systems by providing continuous hazard monitoring and adaptive response capabilities, significantly improving safety and driving performance with low computational overhead.

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [14] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: Review of next-gen autonomous vehicle optimization for emergency services using diffusion model-augmented RL and LLM-assisted in-context learning to overcome conventional RL limitations.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles can revolutionize emergency responses but conventional reinforcement learning struggles with poor sample efficiency and adaptability in dynamic scenarios.

Method: Analyzes shift from conventional RL to DM-augmented RL (enhancing robustness via synthetic data) and LLM-assisted ICL (enabling lightweight adaptation without retraining).

Result: Provides a critical framework comparing DM-augmented RL (higher computational cost) and LLM-assisted ICL (interpretable, efficient) for AV emergency systems.

Conclusion: Generative AI approaches like DM-augmented RL and LLM-assisted ICL are pivotal for developing robust, adaptive next-generation autonomous emergency response vehicles.

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [15] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1: A data-efficient framework for automated optimization problem solving that combines supervised fine-tuning with test-time reinforcement learning, achieving SOTA performance with only 1/10 the data of previous methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based methods for translating natural language to optimization models require large amounts of annotated data, creating cost and scalability barriers for Operations Research applications.

Method: Two-stage approach: (1) Supervised fine-tuning to learn reasoning patterns from limited labeled data, (2) Test-Time Group Relative Policy Optimization (TGRPO) to improve capability and consistency using unlabeled data.

Result: Achieves 67.7% average solving accuracy, outperforming ORLM by up to 4.2% with only 1/10 the data. TGRPO adds 3.1%-6.4% improvement and reduces Pass@1 vs Pass@8 gap from 13% to 7%. Works well with just 100 synthetic samples.

Conclusion: OR-R1 provides a robust, scalable, and cost-effective solution that lowers expertise and data barriers for industrial OR applications.

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [16] [History-Aware Reasoning for GUI Agents](https://arxiv.org/abs/2511.09127)
*Ziwei Wang,Leyang Yang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: Proposes History-Aware Reasoning (HAR) framework to enhance GUI agents' episodic reasoning by learning from past interactions, overcoming history-agnostic limitations in current methods.


<details>
  <summary>Details</summary>
Motivation: Native GUI agents exhibit weak short-term memory, treating historical interactions as discrete screens rather than connected episodes, hindering performance in long-horizon tasks.

Method: HAR framework with reflective learning scenario, tailored correction guidelines, and hybrid RL reward function to enhance short-term memory in episodic reasoning.

Result: Developed HAR-GUI-3B model that transforms reasoning from history-agnostic to history-aware, showing effectiveness across GUI benchmarks.

Conclusion: HAR framework successfully equips GUI agents with stable short-term memory and reliable screen perception, demonstrating strong generalization capabilities.

Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia Classification on Standard and Wearable ECGs](https://arxiv.org/abs/2511.08650)
*Vamsikrishna Thota,Hardik Prajapati,Yuvraj Joshi,Shubhangi Rathi*

Main category: cs.LG

TL;DR: Lightweight 1D CNN-BiLSTM model with attention for cardiac arrhythmia classification from ECGs, achieving high accuracy with minimal parameters for wearable deployment.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of cardiac arrhythmias is crucial for timely diagnosis and intervention, requiring efficient models suitable for real-time health monitoring systems.

Method: Combines 1D Convolutional Neural Networks, attention mechanisms, and Bidirectional LSTM for classifying arrhythmias from 12-lead and single-lead ECGs, using class-weighted loss to handle imbalance.

Result: Evaluated on CPSC 2018 dataset, the model shows superior accuracy and F1-scores over baselines despite having only 0.945 million parameters.

Conclusion: The lightweight model is highly suitable for real-time deployment in wearable health monitoring systems due to its efficiency and performance.

Abstract: Early and accurate detection of cardiac arrhythmias is vital for timely diagnosis and intervention. We propose a lightweight deep learning model combining 1D Convolutional Neural Networks (CNN), attention mechanisms, and Bidirectional Long Short-Term Memory (BiLSTM) for classifying arrhythmias from both 12-lead and single-lead ECGs. Evaluated on the CPSC 2018 dataset, the model addresses class imbalance using a class-weighted loss and demonstrates superior accuracy and F1- scores over baseline models. With only 0.945 million parameters, our model is well-suited for real-time deployment in wearable health monitoring systems.

</details>


### [18] [Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion](https://arxiv.org/abs/2511.08653)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.LG

TL;DR: CGAR introduces curriculum learning applied to architectural depth for efficient training of recursive reasoning models, achieving 42% faster training with minimal accuracy drop.


<details>
  <summary>Details</summary>
Motivation: Training recursive reasoning models is computationally expensive (36 GPU-hours per dataset), limiting adoption despite their ability to match larger models' performance.

Method: CGAR uses Progressive Depth Curriculum (adjusting recursion depth from shallow to deep) and Hierarchical Supervision Weighting (exponentially decaying supervision importance).

Result: On Sudoku-Extreme, CGAR achieved 1.71x training speedup (10.93 to 6.38 hours) with only 0.63% accuracy drop (86.65% to 86.02%), plus 11% fewer inference steps.

Conclusion: Principled curriculum on architectural depth enables efficient training on modest hardware while maintaining performance, showing Pareto improvement in efficiency and quality.

Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku

</details>


### [19] [Learning the Basis: A Kolmogorov-Arnold Network Approach Embedding Green's Function Priors](https://arxiv.org/abs/2511.08655)
*Rui Zhu,Yuexing Peng,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 该论文提出PhyKAN方法，将传统的矩量法中的固定RWG基函数替换为可学习的基函数，通过Kolmogorov-Arnold网络实现电磁建模的物理一致性，在标准几何形状上实现了高精度重建和雷达截面预测。


<details>
  <summary>Details</summary>
Motivation: 传统的矩量法受限于使用静态的、几何定义的基函数（如RWG基函数），限制了电磁建模的灵活性和适应性。论文旨在开发一种可学习的基函数表示方法，替代固定基函数上的系数求解。

Method: 提出PhyKAN方法，该方法基于Kolmogorov-Arnold表示定理，将RWG基函数推广为可学习和自适应的基函数族。PhyKAN将局部KAN分支与嵌入格林函数先验的全局分支相结合，从EFIE推导而来，保持物理一致性。

Result: 在标准几何形状上，PhyKAN实现了低于0.01的重建误差，并能够进行准确的无监督雷达截面预测。

Conclusion: PhyKAN提供了一座可解释的、物理一致的桥梁，连接了经典求解器和现代神经网络模型，为电磁建模提供了新的研究方向。

Abstract: The Method of Moments (MoM) is constrained by the usage of static, geometry-defined basis functions, such as the Rao-Wilton-Glisson (RWG) basis. This letter reframes electromagnetic modeling around a learnable basis representation rather than solving for the coefficients over a fixed basis. We first show that the RWG basis is essentially a static and piecewise-linear realization of the Kolmogorov-Arnold representation theorem. Inspired by this insight, we propose PhyKAN, a physics-informed Kolmogorov-Arnold Network (KAN) that generalizes RWG into a learnable and adaptive basis family. Derived from the EFIE, PhyKAN integrates a local KAN branch with a global branch embedded with Green's function priors to preserve physical consistency. It is demonstrated that, across canonical geometries, PhyKAN achieves sub-0.01 reconstruction errors as well as accurate, unsupervised radar cross section predictions, offering an interpretable, physics-consistent bridge between classical solvers and modern neural network models for electromagnetic modeling.

</details>


### [20] [TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models](https://arxiv.org/abs/2511.08667)
*Léo Grinsztajn,Klemens Flöge,Oscar Key,Felix Birkel,Philipp Jund,Brendan Roof,Benjamin Jäger,Dominik Safaric,Simone Alessi,Adrian Hayler,Mihir Manium,Rosen Yu,Felix Jablonski,Shi Bin Hoo,Anurag Garg,Jake Robertson,Magnus Bühler,Vladyslav Moroshan,Lennart Purucker,Clara Cornu,Lilly Charlotte Wehrhahn,Alessandro Bonetto,Bernhard Schölkopf,Sauraj Gambhir,Noah Hollmann,Frank Hutter*

Main category: cs.LG

TL;DR: TabPFN-2.5 is the next generation tabular foundation model that significantly scales up data handling capacity, outperforms established methods on benchmarks, and introduces efficient distillation for production deployment.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous versions (TabPFN and TabPFNv2) by scaling the model to handle much larger datasets efficiently while maintaining superior performance compared to traditional methods.

Method: Developed TabPFN-2.5 as a tabular foundation model capable of processing datasets with up to 50,000 data points and 2,000 features. Introduced a distillation engine to convert the model into compact MLP or tree ensemble formats for production use.

Result: Achieves leading performance on TabArena benchmark, substantially outperforming tuned tree-based models and matching AutoGluon 1.4. Shows 100% win rate against default XGBoost on small-medium datasets and 87% win rate on larger datasets. Distillation preserves most accuracy while dramatically reducing latency.

Conclusion: TabPFN-2.5 represents a significant advancement in tabular AI, offering superior performance, scalability, and practical deployment capabilities that will enhance existing applications built on the TabPFN ecosystem.

Abstract: The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (<=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem.

</details>


### [21] [Enabling Agents to Communicate Entirely in Latent Space](https://arxiv.org/abs/2511.09149)
*Zhuoyun Du,Runze Wang,Huiyu Bai,Zouying Cao,Xiaoyong Zhu,Bo Zheng,Wei Chen,Haochao Ying*

Main category: cs.LG

TL;DR: Interlat enables direct transmission of LLM hidden states between agents for more nuanced communication than natural language, improving collaborative problem-solving through latent space reasoning and compression.


<details>
  <summary>Details</summary>
Motivation: Natural language communication between LLM agents inherently limits information depth and nuance due to downsampling rich internal states into discrete tokens, hindering collaborative problem-solving.

Method: Proposed Interlat (Inter-agent Latent Space Communication) that uses LLM's last hidden states as mind representations for direct transmission, with additional compression via latent space reasoning.

Result: Interlat outperforms fine-tuned chain-of-thought prompting and single-agent baselines, promotes exploratory behavior, enables genuine latent information utilization, and compression accelerates inference while maintaining competitive performance.

Conclusion: The work demonstrates feasibility of entirely latent space inter-agent communication, highlighting its potential and offering valuable insights for future research in agent collaboration.

Abstract: While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.

</details>


### [22] [PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation](https://arxiv.org/abs/2511.08697)
*Can Yang,Zhenzhong Wang,Junyuan Liu,Yunpeng Gong,Min Jiang*

Main category: cs.LG

TL;DR: PEGNet is a physics-embedded graph network that uses PDE-guided message passing for stable, accurate, and physically consistent simulations of complex phenomena like airflow and drug delivery.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solvers are computationally expensive, while data-driven methods often suffer from error accumulation and poor physical consistency, especially in multiphysics scenarios with complex geometries.

Method: The proposed PEGNet embeds core PDE dynamics (e.g., convection, viscosity, diffusion) into distinct message functions within a graph neural network, uses a hierarchical architecture for multi-scale feature capture, and includes physical regularization in the loss function.

Result: Evaluation on benchmarks, including respiratory airflow and drug delivery datasets, demonstrates significant improvements in long-term prediction accuracy and physical consistency compared to existing methods.

Conclusion: PEGNet effectively integrates physical constraints into deep learning, offering a robust framework for accurate and efficient PDE-based simulations.

Abstract: Accurate and efficient simulations of physical phenomena governed by partial differential equations (PDEs) are important for scientific and engineering progress. While traditional numerical solvers are powerful, they are often computationally expensive. Recently, data-driven methods have emerged as alternatives, but they frequently suffer from error accumulation and limited physical consistency, especially in multiphysics and complex geometries. To address these challenges, we propose PEGNet, a Physics-Embedded Graph Network that incorporates PDE-guided message passing to redesign the graph neural network architecture. By embedding key PDE dynamics like convection, viscosity, and diffusion into distinct message functions, the model naturally integrates physical constraints into its forward propagation, producing more stable and physically consistent solutions. Additionally, a hierarchical architecture is employed to capture multi-scale features, and physical regularization is integrated into the loss function to further enforce adherence to governing physics. We evaluated PEGNet on benchmarks, including custom datasets for respiratory airflow and drug delivery, showing significant improvements in long-term prediction accuracy and physical consistency over existing methods. Our code is available at https://github.com/Yanghuoshan/PEGNet.

</details>


### [23] [FAIRPLAI: A Human-in-the-Loop Approach to Fair and Private Machine Learning](https://arxiv.org/abs/2511.08702)
*David Sanchez,Holly Lopez,Michelle Buraczyk,Anantaa Kotal*

Main category: cs.LG

TL;DR: FAIRPLAI is a framework that integrates human oversight into ML systems to balance accuracy, privacy, and fairness through privacy-fairness frontiers, interactive stakeholder input, and private auditing loops.


<details>
  <summary>Details</summary>
Motivation: ML systems increasingly make high-stakes decisions affecting healthcare, finance, and public services, requiring fairness, privacy protection, and accountability beyond just accuracy. Existing approaches struggle with privacy-fairness trade-offs and lack human contextual judgment.

Method: FAIRPLAI works through three mechanisms: (1) privacy-fairness frontiers showing trade-offs transparently, (2) interactive stakeholder input for selecting fairness criteria, and (3) differentially private auditing loops for human review without compromising data security.

Result: Applied to benchmark datasets, FAIRPLAI preserves strong privacy protections while reducing fairness disparities compared to automated baselines, providing an interpretable process for managing competing demands.

Conclusion: By embedding human judgment where it matters most, FAIRPLAI offers a pathway to create ML systems that are effective, responsible, and trustworthy in practice for socially impactful applications.

Abstract: As machine learning systems move from theory to practice, they are increasingly tasked with decisions that affect healthcare access, financial opportunities, hiring, and public services. In these contexts, accuracy is only one piece of the puzzle - models must also be fair to different groups, protect individual privacy, and remain accountable to stakeholders. Achieving all three is difficult: differential privacy can unintentionally worsen disparities, fairness interventions often rely on sensitive data that privacy restricts, and automated pipelines ignore that fairness is ultimately a human and contextual judgment. We introduce FAIRPLAI (Fair and Private Learning with Active Human Influence), a practical framework that integrates human oversight into the design and deployment of machine learning systems. FAIRPLAI works in three ways: (1) it constructs privacy-fairness frontiers that make trade-offs between accuracy, privacy guarantees, and group outcomes transparent; (2) it enables interactive stakeholder input, allowing decision-makers to select fairness criteria and operating points that reflect their domain needs; and (3) it embeds a differentially private auditing loop, giving humans the ability to review explanations and edge cases without compromising individual data security. Applied to benchmark datasets, FAIRPLAI consistently preserves strong privacy protections while reducing fairness disparities relative to automated baselines. More importantly, it provides a straightforward, interpretable process for practitioners to manage competing demands of accuracy, privacy, and fairness in socially impactful applications. By embedding human judgment where it matters most, FAIRPLAI offers a pathway to machine learning systems that are effective, responsible, and trustworthy in practice. GitHub: https://github.com/Li1Davey/Fairplai

</details>


### [24] [Benevolent Dictators? On LLM Agent Behavior in Dictator Games](https://arxiv.org/abs/2511.08721)
*Andreas Einwiller,Kanishka Ghosh Dastidar,Artur Romazanov,Annette Hautli-Janisz,Michael Granitzer,Florian Lemmerich*

Main category: cs.LG

TL;DR: Proposed LLM-ABS framework to address prompt sensitivity in LLM behavior studies, showing significant system prompt influence on fairness preferences and linguistic expression patterns.


<details>
  <summary>Details</summary>
Motivation: To question the robustness of recent studies on LLM agent behavior in economic games, as many overlook the role of system prompts and don't account for how sensitive results can be to slight prompt changes - a limitation that undermines studies of complex behavioral aspects of LLMs.

Method: Proposed the LLM agent behavior study (LLM-ABS) framework to explore system prompt influences, use neutral prompt variations for reliable insights, and analyze linguistic features in LLM responses to open-ended instructions.

Result: Found that LLM agents often exhibit a strong preference for fairness and that system prompts significantly impact their behavior; identified differences in how models express their responses linguistically.

Conclusion: Although prompt sensitivity remains a persistent challenge, the LLM-ABS framework demonstrates a robust foundation for LLM agent behavior studies by accounting for system prompt influences and providing more reliable insights into agent preferences.

Abstract: In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.

</details>


### [25] [Macroscopic Emission Modeling of Urban Traffic Using Probe Vehicle Data: A Machine Learning Approach](https://arxiv.org/abs/2511.08722)
*Mohammed Ali El Adlouni,Ling Jin,Xiaodan Xu,C. Anna Spurlock,Alina Lazar,Kaveh Farokhi Sadabadi,Mahyar Amirgholy,Mona Asudegi*

Main category: cs.LG

TL;DR: Machine learning applied to traffic and emission data creates data-driven emission fundamental diagrams (eMFDs) for US urban areas, aiding in monitoring and optimizing traffic to reduce congestion and emissions.


<details>
  <summary>Details</summary>
Motivation: Urban congestion causes inefficient vehicle movement and increases greenhouse gas emissions. Existing eMFD models are limited by sparse historical data, necessitating better tools for real-time emission monitoring and traffic optimization.

Method: The study uses large-scale, granular traffic and emission data from probe vehicles and applies machine learning methods to predict the relationship between network-wide emission rates and traffic variables.

Result: The framework generates data-driven eMFDs and reveals how emissions depend on network, infrastructure, land use, and vehicle characteristics, enabling real-time monitoring and optimization.

Conclusion: This approach allows transportation authorities to measure carbon emissions for given travel demands and optimize location-specific traffic management to mitigate network-wide emissions effectively.

Abstract: Urban congestions cause inefficient movement of vehicles and exacerbate greenhouse gas emissions and urban air pollution. Macroscopic emission fundamental diagram (eMFD)captures an orderly relationship among emission and aggregated traffic variables at the network level, allowing for real-time monitoring of region-wide emissions and optimal allocation of travel demand to existing networks, reducing urban congestion and associated emissions. However, empirically derived eMFD models are sparse due to historical data limitation. Leveraging a large-scale and granular traffic and emission data derived from probe vehicles, this study is the first to apply machine learning methods to predict the network wide emission rate to traffic relationship in U.S. urban areas at a large scale. The analysis framework and insights developed in this work generate data-driven eMFDs and a deeper understanding of their location dependence on network, infrastructure, land use, and vehicle characteristics, enabling transportation authorities to measure carbon emissions from urban transport of given travel demand and optimize location specific traffic management and planning decisions to mitigate network-wide emissions.

</details>


### [26] [Gromov-Wasserstein Graph Coarsening](https://arxiv.org/abs/2511.08733)
*Carlos A. Taveras,Santiago Segarra,César A. Uribe*

Main category: cs.LG

TL;DR: Proposes two graph coarsening algorithms (GPC and KGPC) using Gromov-Wasserstein geometry, showing improved performance over existing methods on large datasets.


<details>
  <summary>Details</summary>
Motivation: To address the problem of graph coarsening by leveraging a novel representation of distortion induced by merging nodes within the Gromov-Wasserstein framework.

Method: Greedy Pair Coarsening (GPC) iteratively merges node pairs minimizing local distortion; k-means Greedy Pair Coarsening (KGPC) merges clusters based on pairwise distortion metrics.

Result: Validated on six large-scale datasets and a downstream clustering task, showing superior performance across various parameters and scenarios compared to existing approaches.

Conclusion: The methods provide effective graph coarsening with theoretical guarantees, demonstrating robustness and advantages in practical applications.

Abstract: We study the problem of graph coarsening within the Gromov-Wasserstein geometry. Specifically, we propose two algorithms that leverage a novel representation of the distortion induced by merging pairs of nodes. The first method, termed Greedy Pair Coarsening (GPC), iteratively merges pairs of nodes that locally minimize a measure of distortion until the desired size is achieved. The second method, termed $k$-means Greedy Pair Coarsening (KGPC), leverages clustering based on pairwise distortion metrics to directly merge clusters of nodes. We provide conditions guaranteeing optimal coarsening for our methods and validate their performance on six large-scale datasets and a downstream clustering task. Results show that the proposed methods outperform existing approaches on a wide range of parameters and scenarios.

</details>


### [27] [Hey Pentti, We Did (More of) It!: A Vector-Symbolic Lisp With Residue Arithmetic](https://arxiv.org/abs/2511.08767)
*Connor Hanley,Eilene Tomkins-Flanaganm,Mary Alexandria Kelly*

Main category: cs.LG

TL;DR: Extends Vector-Symbolic Architecture with FHRRs and RHC to encode Lisp 1.5 syntax for arithmetic, enabling neural networks to handle structured, interpretable representations for more general AI.


<details>
  <summary>Details</summary>
Motivation: To increase neural network expressivity by enabling them to contain arbitrarily structured, inherently interpretable representations, aiming for more generally intelligent agents.

Method: Uses Frequency-domain Holographic Reduced Representations (FHRRs) and Residue Hyperdimensional Computing (RHC) to extend a VSA encoding of Lisp 1.5 with arithmetic primitives.

Result: Achieves a Turing-complete syntax encoded over high-dimensional vector space, enhancing the capability of neural network states.

Conclusion: The approach shows promise for machine learning applications, emphasizing the importance of structured representations and network sensitivity to structure for advancing AI generality.

Abstract: Using Frequency-domain Holographic Reduced Representations (FHRRs), we extend a Vector-Symbolic Architecture (VSA) encoding of Lisp 1.5 with primitives for arithmetic operations using Residue Hyperdimensional Computing (RHC). Encoding a Turing-complete syntax over a high-dimensional vector space increases the expressivity of neural network states, enabling network states to contain arbitrarily structured representations that are inherently interpretable. We discuss the potential applications of the VSA encoding in machine learning tasks, as well as the importance of encoding structured representations and designing neural networks whose behavior is sensitive to the structure of their representations in virtue of attaining more general intelligent agents than exist at present.

</details>


### [28] [A Generalized Bias-Variance Decomposition for Bregman Divergences](https://arxiv.org/abs/2511.08789)
*David Pfau*

Main category: cs.LG

TL;DR: Extends bias-variance decomposition from squared error to Bregman divergences for maximum likelihood with exponential families, providing clear derivation and literature references.


<details>
  <summary>Details</summary>
Motivation: To address the gap in clear, standalone derivations for the bias-variance decomposition generalization beyond squared error, particularly for exponential families in maximum likelihood estimation.

Method: The authors present a standalone derivation of the bias-variance decomposition generalization, where prediction error is expressed as a Bregman divergence, specifically relevant for maximum likelihood estimation with exponential families.

Result: A pedagogical derivation is provided that generalizes the classical bias-variance decomposition to Bregman divergences, connecting it to maximum likelihood estimation with exponential families.

Conclusion: The work provides a formal derivation of the bias-variance decomposition for general exponential family maximum likelihood estimation using Bregman divergences, establishing proper pedagogical context and literature references.

Abstract: The bias-variance decomposition is a central result in statistics and machine learning, but is typically presented only for the squared error. We present a generalization of the bias-variance decomposition where the prediction error is a Bregman divergence, which is relevant to maximum likelihood estimation with exponential families. While the result is already known, there was not previously a clear, standalone derivation, so we provide one for pedagogical purposes. A version of this note previously appeared on the author's personal website without context. Here we provide additional discussion and references to the relevant prior literature.

</details>


### [29] [BayesQ: Uncertainty-Guided Bayesian Quantization](https://arxiv.org/abs/2511.08821)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.LG

TL;DR: First uncertainty-guided PTQ framework optimizing quantization under posterior expected loss, achieving significant improvements over baselines with comparable preprocessing overhead.


<details>
  <summary>Details</summary>
Motivation: To optimize quantization under posterior expected loss rather than deterministic loss, enabling more uncertainty-aware and risk-minimizing post-training quantization.

Method: Fits a lightweight Gaussian posterior over weights (diagonal Laplace by default), whitens by posterior covariance, designs codebooks to minimize posterior-expected distortion, allocates mixed precision via greedy knapsack algorithm maximizing marginal expected-loss reduction per bit under global budget, and uses optional calibration-only distillation.

Result: At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over GPTQ by +1.5/+0.7/+0.3 top-1 percentage points on ResNet-50 and +1.1/+0.4/+0.2 GLUE points on BERT-base, while requiring one-time preprocessing comparable to GPTQ.

Conclusion: BayesQ reframes low-bit quantization as uncertainty-aware risk minimization and demonstrates superior performance over strong PTQ baselines across different bit budgets.

Abstract: We present BayesQ, an uncertainty-guided post-training quantization framework that is the first to optimize quantization under the posterior expected loss. BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion, and allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. For scalar quantizers, posterior-expected MSE yields closed-form tables; task-aware proxies are handled by short Monte Carlo on a small calibration set. An optional calibration-only distillation aligns the quantized model with the posterior predictive teacher. At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) e.g., vs. GPTQ by $+1.5/+0.7/+0.3$ top-1 percentage points on RN50 and $+1.1/+0.4/+0.2$ GLUE points on BERT, while requiring one-time preprocessing comparable to a GPTQ pass. BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.

</details>


### [30] [Physics-Informed Machine Learning for Characterizing System Stability](https://arxiv.org/abs/2511.08831)
*Tomoki Koike,Elizabeth Qian*

Main category: cs.LG

TL;DR: Proposes LyapInf, a physics-informed machine learning method that infers a Lyapunov function from trajectory data without needing system equations, to estimate stability regions.


<details>
  <summary>Details</summary>
Motivation: Many dynamical systems lack a priori known stability regions, and existing Lyapunov-based methods require explicit system equations, which may be unavailable.

Method: Assumes a quadratic Lyapunov function form and fits it by minimizing the residual of the Zubov equation using trajectory data, treating the system as a black box.

Result: Numerical tests show LyapInf successfully infers quadratic Lyapunov functions characterizing near-maximal ellipsoidal stability regions without system equations.

Conclusion: LyapInf provides an effective data-driven approach for stability analysis when system models are unknown, applicable to real-world black-box systems.

Abstract: In the design and operation of complex dynamical systems, it is essential to ensure that all state trajectories of the dynamical system converge to a desired equilibrium within a guaranteed stability region. Yet, for many practical systems -- especially in aerospace -- this region cannot be determined a priori and is often challenging to compute. One of the most common methods for computing the stability region is to identify a Lyapunov function. A Lyapunov function is a positive function whose time derivative along system trajectories is non-positive, which provides a sufficient condition for stability and characterizes an estimated stability region. However, existing methods of characterizing a stability region via a Lyapunov function often rely on explicit knowledge of the system governing equations. In this work, we present a new physics-informed machine learning method of characterizing an estimated stability region by inferring a Lyapunov function from system trajectory data that treats the dynamical system as a black box and does not require explicit knowledge of the system governing equations. In our presented Lyapunov function Inference method (LyapInf), we propose a quadratic form for the unknown Lyapunov function and fit the unknown quadratic operator to system trajectory data by minimizing the average residual of the Zubov equation, a first-order partial differential equation whose solution yields a Lyapunov function. The inferred quadratic Lyapunov function can then characterize an ellipsoidal estimate of the stability region. Numerical results on benchmark examples demonstrate that our physics-informed stability analysis method successfully characterizes a near-maximal ellipsoid of the system stability region associated with the inferred Lyapunov function without requiring knowledge of the system governing equations.

</details>


### [31] [TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations](https://arxiv.org/abs/2511.08832)
*Nikunj Gupta,Ludwika Twardecka,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: TIGER enhances MARL by modeling evolving inter-agent coordination through dynamic temporal graphs and temporal attention, achieving better performance than static graph methods.


<details>
  <summary>Details</summary>
Motivation: Most MARL approaches rely on static or per-step relational graphs, overlooking the temporal evolution of coordination structures that naturally occur as agents adapt and reorganize cooperation strategies.

Method: TIGER constructs dynamic temporal graphs connecting agents' current and historical interactions, and employs a temporal attention-based encoder to aggregate information across structural and temporal neighborhoods for time-aware agent embeddings.

Result: TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency on coordination-intensive benchmarks.

Conclusion: The paper concludes that TIGER effectively models evolving coordination structures in multi-agent environments, demonstrating superior performance and sample efficiency over existing methods through extensive experiments and ablation studies.

Abstract: In this paper, we propose capturing and utilizing \textit{Temporal Information through Graph-based Embeddings and Representations} or \textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.

</details>


### [32] [Enhancing DPSGD via Per-Sample Momentum and Low-Pass Filtering](https://arxiv.org/abs/2511.08841)
*Xincheng Xu,Thilina Ranbaduge,Qing Wang,Thierry Rakotoarivelo,David Smith*

Main category: cs.LG

TL;DR: DP-PMLF improves DPSGD by using per-sample momentum and low-pass filtering to reduce both noise and bias, enhancing privacy-utility trade-off.


<details>
  <summary>Details</summary>
Motivation: DPSGD suffers from accuracy degradation due to DP noise and clipping bias; existing methods only address one issue, creating a need for a holistic solution.

Method: Integrates per-sample momentum to smooth gradients before clipping and applies a post-processing low-pass filter to attenuate DP noise without extra privacy cost.

Result: Theoretical analysis shows improved convergence under DP guarantees; empirical evaluations demonstrate better performance than state-of-the-art DPSGD variants.

Conclusion: DP-PMLF effectively mitigates DP noise and clipping bias simultaneously, offering a superior privacy-utility balance.

Abstract: Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to train deep neural networks with formal privacy guarantees. However, the addition of differential privacy (DP) often degrades model accuracy by introducing both noise and bias. Existing techniques typically address only one of these issues, as reducing DP noise can exacerbate clipping bias and vice-versa. In this paper, we propose a novel method, \emph{DP-PMLF}, which integrates per-sample momentum with a low-pass filtering strategy to simultaneously mitigate DP noise and clipping bias. Our approach uses per-sample momentum to smooth gradient estimates prior to clipping, thereby reducing sampling variance. It further employs a post-processing low-pass filter to attenuate high-frequency DP noise without consuming additional privacy budget. We provide a theoretical analysis demonstrating an improved convergence rate under rigorous DP guarantees, and our empirical evaluations reveal that DP-PMLF significantly enhances the privacy-utility trade-off compared to several state-of-the-art DPSGD variants.

</details>


### [33] [On topological descriptors for graph products](https://arxiv.org/abs/2511.08846)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: This paper analyzes topological descriptors (Euler characteristic and persistent homology) for graph products, showing that persistent homology captures more information than individual graphs while Euler characteristic does not, and provides algorithms for efficient computation.


<details>
  <summary>Details</summary>
Motivation: Topological descriptors are increasingly used to capture multiscale structural information in relational data, and this work explores how different filtrations on graph products affect these descriptors to enhance their expressive power.

Method: The authors establish theoretical characterizations of Euler characteristic's expressive power on color-based filtrations and develop algorithms to compute persistent homology diagrams for vertex- and edge-level filtrations on graph products.

Result: Persistent homology descriptors of graph products contain strictly more information than those from individual graphs, while Euler characteristic does not. The paper also provides empirical validation through runtime analysis, expressivity tests, and graph classification performance.

Conclusion: The work establishes that product filtrations enable more powerful graph persistent descriptors, paving the way for enhanced topological analysis of relational data structures.

Abstract: Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at https://github.com/Aalto-QuML/tda_graph_product.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [Introduction to Automated Negotiation](https://arxiv.org/abs/2511.08659)
*Dave de Jonge*

Main category: cs.MA

TL;DR: Introductory textbook on automated negotiation for computer science beginners with accompanying Python framework for hands-on learning


<details>
  <summary>Details</summary>
Motivation: To provide accessible introduction to automated negotiation for students with minimal prerequisites

Method: Textbook approach with practical Python-based negotiation framework for algorithm implementation

Result: Self-contained educational resource enabling students to understand and experiment with negotiation algorithms

Conclusion: Comprehensive beginner-friendly resource that balances theoretical foundations with practical implementation

Abstract: This book is an introductory textbook targeted towards computer science students who are completely new to the topic of automated negotiation. It does not require any prerequisite knowledge, except for elementary mathematics and basic programming skills.
  This book comes with an simple toy-world negotiation framework implemented in Python that can be used by the readers to implement their own negotiation algorithms and perform experiments with them. This framework is small and simple enough that any reader who does not like to work in Python should be able to re-implement it very quickly in any other programming language of their choice.

</details>


### [35] [Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives](https://arxiv.org/abs/2511.08710)
*Romain Cosentino,Sarath Shekkizhar,Adam Earle*

Main category: cs.MA

TL;DR: Studies gradient-based interactions between language model agents with misaligned objectives, characterizing biased equilibrium and asymmetric convergence conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how misaligned objectives in multi-agent interactions affect convergence and lead to biased outcomes.

Method: Theoretical framework analyzing iterative gradient updates between two agents, with validation using transformer models and GPT-5 for in-context linear regression.

Result: Shows misaligned objectives result in biased equilibrium with residual errors predictable from objective gap and prompt geometry; asymmetric convergence conditions established.

Conclusion: Provides a framework to study and defend multi-agent systems, linking prompt design to stability, bias, and robustness.

Abstract: We develop a theoretical framework for agent-to-agent interactions in multi-agent scenarios. We consider the setup in which two language model based agents perform iterative gradient updates toward their respective objectives in-context, using the output of the other agent as input. We characterize the generation dynamics associated with the interaction when the agents have misaligned objectives, and show that this results in a biased equilibrium where neither agent reaches its target - with the residual errors predictable from the objective gap and the geometry induced by the prompt of each agent. We establish the conditions for asymmetric convergence and provide an algorithm that provably achieves an adversarial result, producing one-sided success. Experiments with trained transformer models as well as GPT$5$ for the task of in-context linear regression validate the theory. Our framework presents a setup to study, predict, and defend multi-agent systems; explicitly linking prompt design and interaction setup to stability, bias, and robustness.

</details>


### [36] [Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2511.08926)
*Zhuhui Li,Chunbo Luo,Liming Huang,Luyu Qi,Geyong Min*

Main category: cs.MA

TL;DR: Proposes AA-MAMORL, an attention-based framework for multi-agent multi-objective systems that learns agents' utility functions during centralized training to approximate Bayesian Nash Equilibrium in decentralized execution, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MAMOS optimization methods struggle with heterogeneous objectives and utility functions, intensifying training non-stationarity due to private utilities and policies.

Method: Uses an Agent-Attention framework to implicitly learn joint beliefs over other agents' utility functions and policies during centralized training, mapping global states/utilities to individual policies for decentralized execution.

Result: AA-MAMORL significantly improves performance over state-of-the-art methods in both custom MAMO Particle environment and MOMALand benchmark, demonstrating effective approximation of BNE.

Conclusion: Access to global preferences and the proposed AA-MAMORL framework enables effective decentralized execution in MAMOS while handling heterogeneous utility settings, providing scalable optimization.

Abstract: Multi-agent multi-objective systems (MAMOS) have emerged as powerful frameworks for modelling complex decision-making problems across various real-world domains, such as robotic exploration, autonomous traffic management, and sensor network optimisation. MAMOS offers enhanced scalability and robustness through decentralised control and more accurately reflects inherent trade-offs between conflicting objectives. In MAMOS, each agent uses utility functions that map return vectors to scalar values. Existing MAMOS optimisation methods face challenges in handling heterogeneous objective and utility function settings, where training non-stationarity is intensified due to private utility functions and the associated policies. In this paper, we first theoretically prove that direct access to, or structured modeling of, global utility functions is necessary for the Bayesian Nash Equilibrium under decentralised execution constraints. To access the global utility functions while preserving the decentralised execution, we propose an Agent-Attention Multi-Agent Multi-Objective Reinforcement Learning (AA-MAMORL) framework. Our approach implicitly learns a joint belief over other agents' utility functions and their associated policies during centralised training, effectively mapping global states and utilities to each agent's policy. In execution, each agent independently selects actions based on local observations and its private utility function to approximate a BNE, without relying on inter-agent communication. We conduct comprehensive experiments in both a custom-designed MAMO Particle environment and the standard MOMALand benchmark. The results demonstrate that access to global preferences and our proposed AA-MAMORL significantly improve performance and consistently outperform state-of-the-art methods.

</details>


### [37] [Learning Efficient Communication Protocols for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.09171)
*Xinren Zhang,Jiadong Yu,Zixin Zhong*

Main category: cs.MA

TL;DR: A new framework for multi-round communication protocols in Multi-Agent Reinforcement Learning improves efficiency and cooperation.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in Multi-Agent Reinforcement Learning communication by optimizing communication protocols, which are inadequately explored in prior work.

Method: Developed a generalized framework for learning multi-round communication protocols with three Communication Efficiency Metrics (Information Entropy Efficiency Index, Specialization Efficiency Index, and Topology Efficiency Index).

Result: The approach improves communication efficiency and achieves better cooperation with higher success rates.

Conclusion: The framework significantly boosts communication efficiency and cooperation performance.

Abstract: Multi-Agent Systems (MAS) have emerged as a powerful paradigm for modeling complex interactions among autonomous entities in distributed environments. In Multi-Agent Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since agents may generate redundant or non-essential messages. While prior work has focused on boosting task performance with information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of communication protocols (communication topology and message). To fill this gap, we introduce a generalized framework for learning multi-round communication protocols that are both effective and efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the learning process: the Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) for efficiency-augmented optimization, and the Topology Efficiency Index (TEI) for explicit evaluation. We integrate IEI and SEI as the adjusted loss functions to promote informative messaging and role specialization, while using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments, we demonstrate that our learned communication protocol can significantly enhance communication efficiency and achieves better cooperation performance with improved success rates.

</details>


### [38] [Enhancing PIBT via Multi-Action Operations](https://arxiv.org/abs/2511.09193)
*Egor Yukhnevich,Anton Andreychuk*

Main category: cs.MA

TL;DR: Enhanced PIBT algorithm improves MAPF solver by adding multi-action operations to handle rotation actions efficiently, maintaining speed while boosting performance in orientation-sensitive scenarios.


<details>
  <summary>Details</summary>
Motivation: PIBT's short-horizon design causes poor performance when agents require time-consuming rotation actions due to orientations, limiting its effectiveness in such MAPF settings.

Method: Modify PIBT to incorporate multi-action operations, integrate with graph-guidance technique, and apply large neighborhood search optimization for enhanced planning.

Result: The enhanced PIBT achieves state-of-the-art performance in online LMAPF-T settings by addressing rotation inefficiencies while preserving computational speed.

Conclusion: The proposed enhancements successfully overcome PIBT's limitation in orientation-rich environments, demonstrating robust efficiency and superior results in complex MAPF tasks.

Abstract: PIBT is a rule-based Multi-Agent Path Finding (MAPF) solver, widely used as a low-level planner or action sampler in many state-of-the-art approaches. Its primary advantage lies in its exceptional speed, enabling action selection for thousands of agents within milliseconds by considering only the immediate next timestep. However, this short-horizon design leads to poor performance in scenarios where agents have orientation and must perform time-consuming rotation actions. In this work, we present an enhanced version of PIBT that addresses this limitation by incorporating multi-action operations. We detail the modifications introduced to improve PIBT's performance while preserving its hallmark efficiency. Furthermore, we demonstrate how our method, when combined with graph-guidance technique and large neighborhood search optimization, achieves state-of-the-art performance in the online LMAPF-T setting.

</details>
