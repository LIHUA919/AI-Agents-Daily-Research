{"id": "2602.16714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16714", "abs": "https://arxiv.org/abs/2602.16714", "authors": ["Renato Marcelo", "Ana Rodrigues", "Cristiana Palmela Pereira", "Ant\u00f3nio Figueiras", "Rui Santos", "Jos\u00e9 Rui Figueira", "Alexandre P Francisco", "C\u00e1tia Vaz"], "title": "AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment", "comment": null, "summary": "Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.", "AI": {"tldr": "AIdentifyAGE ontology standardizes forensic dental age assessment for consistency, transparency, and explainability in medico-legal contexts.", "motivation": "Age assessment is critical in forensic cases involving undocumented individuals and minors, but current methods lack standardization, interoperability, and reproducibility, worsened by AI adoption.", "method": "Develops the domain-specific AIdentifyAGE ontology with experts, integrating manual and AI-assisted workflows, medical, dental, machine learning ontologies, and FAIR principles.", "result": "Provides a semantically coherent framework linking observations, methods, reference data, and outcomes, modeling medico-legal workflows for decision support.", "conclusion": "The ontology enhances consistency and explainability, serving as a foundation for ontology-driven systems in forensic and judicial applications."}}
{"id": "2602.16715", "categories": ["cs.AI", "cs.CL", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16715", "abs": "https://arxiv.org/abs/2602.16715", "authors": ["H. Sinan Bank", "Daniel R. Herber"], "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems", "comment": "26 pages, 10 figures", "summary": "We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.", "AI": {"tldr": "The paper tests LLMs, RAG, and GraphRAG for automated Design Structure Matrix generation, evaluating on power screwdriver and CubeSat use cases.", "motivation": "To explore the potential of automated methods for generating DSMs to enhance design process efficiency and support architectural analysis.", "method": "Testing LLMs, RAG, and GraphRAG on two use cases (power screwdriver and CubeSat) with known references; measuring performance on component relationship determination and component identification tasks.", "result": "Identified opportunities for automated DSM generation despite design and computational challenges, with code made publicly available for reproducibility and expert feedback.", "conclusion": "The study demonstrates feasibility of using AI techniques for DSM generation, offering a foundation for further development and domain expert collaboration."}}
{"id": "2602.16716", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.16716", "abs": "https://arxiv.org/abs/2602.16716", "authors": ["Song-Ju Kim"], "title": "Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence", "comment": "This paper addresses contextuality from a representation-theoretic and information-theoretic perspective in adaptive systems. It is conceptually and technically distinct from the authors' earlier arXiv works (QTOW/QTOW2), which pursue different formulations of contextuality", "summary": "Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.", "AI": {"tldr": "Contextuality in classical models arises from single-state reuse, requiring an information-theoretic cost, while nonclassical frameworks avoid it by relaxing joint probability assumptions.", "motivation": "To understand the representational consequences of reusing a fixed internal state across multiple contexts in adaptive systems, common in natural and artificial intelligence.", "method": "Modeling contexts as interventions on a shared internal state, proving that classical models reproducing contextual outcomes incur an irreducible information-theoretic cost, and providing a minimal constructive example.", "result": "Contextuality is shown to be inevitable in classical probabilistic representations due to single-state reuse, with dependence on context not mediated solely by the internal state.", "conclusion": "Contextuality is a general representational constraint on adaptive intelligence, independent of physical implementation, and nonclassical frameworks can avoid it by relaxing the assumption of a single global joint probability space."}}
{"id": "2602.16726", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16726", "abs": "https://arxiv.org/abs/2602.16726", "authors": ["Hua Yan", "Heng Tan", "Yu Yang"], "title": "Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data", "comment": null, "summary": "Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets.", "AI": {"tldr": "This paper proposes M2LSimu, a framework for large-scale human mobility simulation using LLMs with multi-prompt adjustments guided by mobility measures to generate realistic collective behaviors.", "motivation": "Existing LLM-based mobility simulation methods generate individual trajectories independently, lacking population-level coordination and failing to capture collective behaviors.", "method": "The authors design M2LSimu, which uses mobility measures from shared data to guide multi-prompt adjustments, applying coarse-grained strategies and fine-grained individual-level adaptations under budget constraints.", "result": "Experiments on two public datasets show that M2LSimu significantly outperforms state-of-the-art LLM-based methods.", "conclusion": "M2LSimu effectively addresses the limitation of independent trajectory generation by incorporating population-level guidance, enhancing the realism of large-scale human mobility simulations."}}
{"id": "2602.16727", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16727", "abs": "https://arxiv.org/abs/2602.16727", "authors": ["Hua Yan", "Heng Tan", "Yingxue Zhang", "Yu Yang"], "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation", "comment": null, "summary": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.", "AI": {"tldr": "MobCache is a mobility-aware cache framework that uses reconstructible caches and latent-space reasoning to enable efficient, scalable human mobility simulations while maintaining performance comparable to LLM-based methods.", "motivation": "Large-scale human mobility simulation is essential for urban planning, epidemiology, and transportation, but LLM-based approaches are computationally expensive, limiting scalability.", "method": "MobCache includes a reasoning component that encodes reasoning steps as latent embeddings for reuse, and a decoding component that uses a lightweight decoder with mobility law-constrained distillation to translate reasoning into natural language efficiently.", "result": "Experiments show MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.", "conclusion": "MobCache offers an effective solution for scalable human mobility simulations by reducing computational costs without sacrificing fidelity, making it practical for real-world applications."}}
{"id": "2602.16738", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16738", "abs": "https://arxiv.org/abs/2602.16738", "authors": ["Rebin Saleh", "Khanh Pham Dinh", "Bal\u00e1zs Vill\u00e1nyi", "Truong-Son Hy"], "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance", "comment": null, "summary": "Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints.", "AI": {"tldr": "SEMAS is a self-evolving hierarchical multi-agent system for industrial IoT predictive maintenance, enabling real-time anomaly detection with interpretability and low resource usage across Edge, Fog, and Cloud tiers.", "motivation": "Traditional predictive maintenance systems use static models that fail to adapt to changing conditions, while LLM-based systems are too resource-intensive for edge deployment, necessitating a solution that balances real-time performance, adaptability, and interpretability.", "method": "SEMAS distributes specialized agents across Edge, Fog, and Cloud: Edge agents handle lightweight feature extraction, Fog agents use ensemble detection with dynamic consensus voting, and Cloud agents optimize policies via Proximal Policy Optimization (PPO), with LLM-based response generation and federated knowledge aggregation.", "result": "Evaluation on Boiler Emulator and Wind Turbine benchmarks shows SEMAS achieves superior anomaly detection performance, high stability, sustained accuracy in evolving contexts, and significant latency improvements for real-time deployment, with ablation studies confirming the contributions of PPO, consensus voting, and federated aggregation.", "conclusion": "Resource-aware, self-evolving multi-agent coordination is crucial for practical industrial IoT predictive maintenance, addressing latency and explainability constraints effectively with SEMAS's hierarchical architecture."}}
{"id": "2602.16730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16730", "abs": "https://arxiv.org/abs/2602.16730", "authors": ["Lei Han", "Mohamed Abdel-Aty", "Younggun Kim", "Yang-Jun Joo", "Zubayer Islam"], "title": "MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior", "comment": null, "summary": "Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.", "AI": {"tldr": "The paper proposes MMCAformer, a model that integrates micro driving behavior features from Connected Vehicle data with macro traffic features for improved speed prediction using cross-attention, outperforming baselines and reducing uncertainty.", "motivation": "Existing traffic speed prediction relies on aggregated macroscopic data, but traffic dynamics are also influenced by individual microscopic human driving behaviors. Recent Connected Vehicle data offer rich driving behavior features, creating an opportunity to incorporate these into speed prediction for better accuracy and proactive management.", "method": "The authors propose the Macro-Micro Cross-Attention Transformer (MMCAformer), which uses self-attention to learn dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. It is optimized with a Student-t negative log-likelihood loss for point-wise prediction and uncertainty estimation.", "result": "Experiments on four Florida freeways show MMCAformer outperforms baselines. Introducing micro driving behavior features improves prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) and reduces model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across freeways). Hard braking and acceleration frequencies are the most influential features, with improvements more pronounced under congested, low-speed conditions.", "conclusion": "Incorporating micro driving behavior features from CV data with macro traffic features through MMCAformer significantly enhances speed prediction accuracy and reduces uncertainty, especially in congested traffic, demonstrating the value of behavioral insights for proactive traffic management."}}
{"id": "2602.16763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16763", "abs": "https://arxiv.org/abs/2602.16763", "authors": ["Mubashara Akhtar", "Anka Reuel", "Prajna Soni", "Sanchit Ahuja", "Pawan Sasanka Ammanamanchi", "Ruchit Rawal", "Vil\u00e9m Zouhar", "Srishti Yadav", "Chenxi Whitehouse", "Dayeon Ki", "Jennifer Mickel", "Leshem Choshen", "Marek \u0160uppa", "Jan Batzner", "Jenny Chim", "Jeba Sania", "Yanan Long", "Hossein A. Rahmani", "Christina Knight", "Yiyang Nan", "Jyoutir Raj", "Yu Fan", "Shubham Singh", "Subramanyam Sahoo", "Eliya Habba", "Usman Gohar", "Siddhesh Pawar", "Robert Scholz", "Arjun Subramonian", "Jingwei Ni", "Mykel Kochenderfer", "Sanmi Koyejo", "Mrinmaya Sachan", "Stella Biderman", "Zeerak Talat", "Avijit Ghosh", "Irene Solaiman"], "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation", "comment": null, "summary": "Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.", "AI": {"tldr": "Analysis of 60 LLM benchmarks shows that nearly half are saturated, with factors like expert curation and no test data hiding affecting longevity.", "motivation": "AI benchmarks often become saturated quickly, losing their ability to differentiate top-performing models, which reduces their long-term utility in guiding AI development and deployment.", "method": "Examine 60 Large Language Model benchmarks from technical reports, characterize them along 14 properties in task design, data construction, and evaluation format, and test five hypotheses on factors driving saturation.", "result": "Nearly 50% of benchmarks are saturated, with saturation rates increasing over time; hiding test data does not prevent saturation, but expert-curated benchmarks resist it better than crowdsourced ones.", "conclusion": "Design choices such as expert curation extend benchmark longevity, providing insights for creating more durable evaluation frameworks to sustain progress measurement in AI."}}
{"id": "2602.16873", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16873", "abs": "https://arxiv.org/abs/2602.16873", "authors": ["Geunbin Yu"], "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence", "comment": "21 pages, 10 figures, 6 tables", "summary": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.", "AI": {"tldr": "AdaptOrch is a framework for dynamic multi-agent orchestration that selects optimal topologies (parallel, sequential, hierarchical, hybrid) based on task graphs, outperforming static single-topology baselines by 12-23% with identical models.", "motivation": "Traditional model selection yields diminishing returns as LLMs converge in performance; orchestration topology now dominates system-level performance over individual model capability.", "method": "Developed AdaptOrch with a Performance Convergence Scaling Law, Topology Routing Algorithm for O(|V| + |E|) mapping, and Adaptive Synthesis Protocol for output consistency and termination guarantees.", "result": "Validated across coding, reasoning, and retrieval-augmented generation tasks, showing improvements of 12-23% over static baselines.", "conclusion": "Orchestration design is established as a first-class optimization target independent of model scaling, with topology-aware approaches crucial for future AI systems."}}
{"id": "2602.16735", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16735", "abs": "https://arxiv.org/abs/2602.16735", "authors": ["Saud Alghumayjan", "Ming Yi", "Bolun Xu"], "title": "A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets", "comment": null, "summary": "This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.", "AI": {"tldr": "A few-shot LLM framework for predicting electricity price spikes using natural-language prompts from system data, achieving performance comparable to supervised models with less data.", "motivation": "To address the challenge of predicting electricity price spikes with limited historical data, leveraging the data-efficiency and prompt-based reasoning of Large Language Models.", "method": "Aggregate system state information (e.g., demand, renewables, weather, recent prices) into statistical features, format them as natural-language prompts, and feed these with instructions to an LLM for classification and confidence scoring.", "result": "The approach achieves performance comparable to supervised models like SVM and XGBoost in the Texas market and outperforms them when historical data is limited.", "conclusion": "LLMs show potential as a data-efficient tool for electricity price spike classification in data-scarce settings."}}
{"id": "2602.16805", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16805", "abs": "https://arxiv.org/abs/2602.16805", "authors": ["Yonatan Gideoni", "Sebastian Risi", "Yarin Gal"], "title": "Simple Baselines are Competitive with Code Evolution", "comment": null, "summary": "Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.", "AI": {"tldr": "Simple baselines match or exceed sophisticated code evolution methods in three domains: mathematical bounds, agentic scaffolds, and ML competitions, highlighting issues in evaluation and search space design.", "motivation": "Many code evolution pipelines show impressive performance but are often not compared to simpler baselines, raising questions about their effectiveness and evaluation rigor.", "method": "Testing two simple baselines across three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions, and analyzing results to identify shortcomings.", "result": "Simple baselines performed similarly or better than more complex methods in all domains. For mathematical bounds, search space design and domain knowledge in prompts are primary, not the evolution pipeline. High variance in scaffolds and small datasets lead to suboptimal selections, with hand-designed majority vote scaffolds performing best.", "conclusion": "The primary challenges are designing good search spaces and improving evaluation methods to reduce stochasticity. Proposed avenues and best practices aim to enable more rigorous code evolution in future work, emphasizing the need for domain expertise and better evaluation over complex pipelines."}}
{"id": "2602.17078", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17078", "abs": "https://arxiv.org/abs/2602.17078", "authors": ["Xuefeng Wang", "Lei Zhang", "Henglin Pu", "Husheng Li", "Ahmed H. Qureshi"], "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form", "comment": "Accepted by ICLR 2026. 27 pages, 15 figures", "summary": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel physics-informed neural network (PINN)-based actor-critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method.", "AI": {"tldr": "A continuous-time constrained MARL framework using PINN-based actor-critic method improves safety and performance in multi-agent systems.", "motivation": "Traditional discrete-time MARL with fixed intervals often underperforms in complex, high-frequency settings, and existing CT-MARL methods rarely handle safety constraints like collisions, creating a need for new approaches.", "method": "Proposes a continuous-time constrained MDP (CT-CMDP) formulation via epigraph-based reformulation and a physics-informed neural network (PINN)-based actor-critic method for stable optimization in continuous time.", "result": "Evaluation on continuous-time safe multi-particle environments and MuJoCo benchmarks shows smoother value approximations, more stable training, and improved performance compared to safe MARL baselines.", "conclusion": "The method effectively integrates safety constraints into continuous-time MARL, demonstrating robustness and enhanced performance in complex multi-agent dynamics."}}
{"id": "2602.16739", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16739", "abs": "https://arxiv.org/abs/2602.16739", "authors": ["Lei Han", "Mohamed Abdel-Aty", "Zubayer Islam", "Chenzhu Wang"], "title": "Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features", "comment": null, "summary": "Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.", "AI": {"tldr": "A hybrid framework for real-time secondary crash likelihood prediction without post-crash features, using dynamic spatiotemporal windows and ensemble learning.", "motivation": "Existing approaches rely on post-crash features unavailable in real time, limiting practical applicability for active traffic management.", "method": "Designs dynamic spatiotemporal windows to extract real-time traffic and environmental features, includes three models (primary crash and two secondary crash models) with ensemble learning of six algorithms and a voting mechanism.", "result": "Identifies 91% of secondary crashes with a false alarm rate of 0.20, improves AUC from 0.654-0.902 for individual models to 0.952 for the hybrid model.", "conclusion": "The framework effectively predicts secondary crashes in real time, outperforming prior studies and enhancing traffic management."}}
{"id": "2602.16807", "categories": ["cs.AI", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.16807", "abs": "https://arxiv.org/abs/2602.16807", "authors": ["Duncan Soiffer", "Nathaniel Itty", "Christopher D. Rosin", "Blake Bruell", "Mason DiCicco", "G\u00e1bor N. S\u00e1rk\u00f6zy", "Ryan Offstein", "Daniel Reichman"], "title": "Improved Upper Bounds for Slicing the Hypercube", "comment": null, "summary": "A collection of hyperplanes $\\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\\{-1,1\\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \\leq \\lceil \\frac{4n}{5} \\rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \\leq \\frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \\leq \\lceil\\frac{5n}{6} \\rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.", "AI": {"tldr": "The paper establishes an improved upper bound for the minimum number of hyperplanes required to slice all edges of an n-dimensional hypercube, using an automated search tool to construct a key example.", "motivation": "The study aims to improve the known upper bound of S(n) \u2264 \u23085n/6\u2309 from 1971 for the hyperplane edge-slicing problem in hypercubes, leveraging computational tools for construction.", "method": "The proof involves constructing 8 hyperplanes that slice all edges of the 10-dimensional hypercube Q_10, assisted by the CPro1 tool that uses reasoning LLMs and hyperparameter tuning for search algorithms.", "result": "It is proved that S(n) \u2264 \u23084n/5\u2309 for most n, with S(n) \u2264 4n/5 + 1 when n is an odd multiple of 5. New lower bounds are also obtained for edges sliced by k<n hyperplanes.", "conclusion": "The results significantly advance the understanding of hyperplane slicing in hypercubes, providing tighter bounds and demonstrating the utility of automated tools in mathematical discovery."}}
{"id": "2602.17100", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17100", "abs": "https://arxiv.org/abs/2602.17100", "authors": ["Siyu Wang", "Ruotian Lu", "Zhihao Yang", "Yuchao Wang", "Yanzhou Zhang", "Lei Xu", "Qimin Xu", "Guojun Yin", "Cailian Chen", "Xinping Guan"], "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation", "comment": null, "summary": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.", "AI": {"tldr": "AgentConductor is a reinforcement learning-optimized multi-agent system with an LLM-based orchestrator that dynamically generates interaction topologies for code generation tasks, achieving superior accuracy and efficiency over existing methods.", "motivation": "Existing multi-agent systems for code generation use fixed interaction topologies, leading to redundant communication and performance issues, especially when tasks vary in difficulty.", "method": "Propose AgentConductor, which uses an LLM-based orchestrator to infer agent roles and task difficulty, constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology with a novel topological density function and difficulty interval partitioning.", "result": "AgentConductor outperforms the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction across three competition-level and two foundational code datasets.", "conclusion": "AgentConductor demonstrates that dynamic, feedback-driven topology generation can significantly enhance the performance and efficiency of LLM-driven multi-agent systems in complex code generation tasks."}}
