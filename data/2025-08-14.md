<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit adapts value function initialization to deep reinforcement learning (DRL) by reusing tabular Q-values from prior tasks, improving early learning efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Extending value function initialization (VFI) to DRL is challenging due to continuous state-action spaces, neural network noise, and impractical storage of past models.

Method: DQInit uses compact tabular Q-values from prior tasks, integrating them softly into underexplored regions and gradually shifting to learned estimates.

Result: Experiments show DQInit improves early learning efficiency, stability, and overall performance in continuous control tasks.

Conclusion: DQInit effectively transfers knowledge in DRL using value estimates, combining jumpstart RL and policy distillation strengths while avoiding their drawbacks.

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: The Othello AI Arena is a novel benchmark framework designed to evaluate AI systems' rapid adaptation to unseen environments, focusing on meta-learning and generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing AI benchmarks lack assessment of flexibility and generalization in novel environments, a key aspect of AGI.

Method: The Arena poses a meta-learning challenge where AI systems must adapt to unique Othello board configurations within 60 seconds, separating meta-level intelligence from task-level performance.

Result: Initial tests show diverse adaptation approaches, from parameter tuning to environmental model learning.

Conclusion: The Othello AI Arena serves as both an educational tool and research benchmark for evaluating rapid adaptation in AI.

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: Proposes an automated multi-modal evaluation framework using LLMs and multi-agent collaboration to address challenges in current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for multi-modal AI assistants are costly, inconsistent, and subjective.

Method: Uses a three-tier agent architecture (interaction, semantic, experience) with supervised fine-tuning on Qwen3-8B.

Result: Achieves high accuracy matching human experts and effectively predicts user satisfaction and defects.

Conclusion: The framework is effective for automated evaluation of multi-modal AI assistants.

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: EvoCurr is a self-evolve framework where a curriculum-generation LLM creates progressively harder problem instances for a solver LLM, improving complex decision-making performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with highly complex problems requiring deep reasoning; direct solving lacks structured guidance.

Method: A curriculum-generation LLM dynamically adjusts problem difficulty based on solver LLM's progress, using Python decision-tree scripts.

Result: EvoCurr significantly boosts task success rates and efficiency over direct-solving baselines.

Conclusion: LLM-driven curriculum learning enhances automated reasoning in high-complexity domains.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: The paper proposes a method to decompose uncertainty in SHAP values into aleatoric, epistemic, and entanglement components, enhancing interpretability in high-stakes domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: SHAP values are treated as point estimates, ignoring uncertainty in models and data, which is critical for reliable decision-making.

Method: Integrates Dempster-Shafer theory and Dirichlet processes over tree ensembles to decompose uncertainty.

Result: Features with high SHAP values aren't always stable; epistemic uncertainty can be reduced with better data and model techniques.

Conclusion: The approach improves SHAP-based attributions' reliability, aiding robust decision-making in high-stakes applications.

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO enhances RLVR by using diverse expert prompts and mutual learning, improving LLM reasoning performance by 4.89-11.33%.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR struggles with reward sparsity, limiting learning signals in challenging tasks.

Method: Proposes MEML-GRPO, leveraging diverse expert prompts and inter-expert mutual learning to broaden responses and share knowledge.

Result: Achieves performance gains of 4.89% (Qwen) and 11.33% (Llama) on reasoning benchmarks.

Conclusion: MEML-GRPO effectively addresses RLVR limitations, significantly boosting LLM reasoning capabilities.

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: UDA (Unsupervised Debiasing Alignment) reduces preference bias in pairwise LLM evaluations by dynamically adjusting the Elo rating system, improving consistency and reliability.


<details>
  <summary>Details</summary>
Motivation: Addressing preference bias in pairwise LLM evaluations, which leads to inconsistent rankings due to systematic favoritism by judges.

Method: Proposes UDA, a framework using a compact neural network to adaptively adjust the Elo system's K-factor and refine win probabilities, minimizing judge disagreement.

Result: UDA reduces inter-judge rating standard deviation by up to 63.4% and improves correlation with human judgments by 24.7%.

Conclusion: UDA enhances evaluation robustness by aligning judges towards consensus, reducing bias, and improving reliability.

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: PacifAIst benchmark evaluates LLM alignment in scenarios where instrumental goals conflict with human safety, revealing performance gaps among top models.


<details>
  <summary>Details</summary>
Motivation: Current safety benchmarks lack systematic evaluation of LLM decision-making in goal conflicts, necessitating tools to measure emergent misaligned behaviors.

Method: Introduced PacifAIst, a benchmark with 700 scenarios testing self-preferential behavior via a taxonomy of Existential Prioritization (EP).

Result: Google's Gemini 2.5 Flash scored highest (90.31%), while GPT-5 scored lowest (79.49%), highlighting alignment challenges.

Conclusion: Standardized tools like PacifAIst are crucial to ensure AI systems prioritize human safety over instrumental goals.

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: The paper analyzes Public Observation Logic (POL), a variant of public announcement logic, proving its satisfiability problem is 2EXPTIME-complete.


<details>
  <summary>Details</summary>
Motivation: To address knowledge updates in multi-agent systems, particularly in epistemic planning, where observations play a key role.

Method: Extends public announcement logic by equipping states in epistemic models with expected observations, evolving states as observations match expectations.

Result: The satisfiability problem of POL is proven to be 2EXPTIME-complete.

Conclusion: POL provides a formal framework for reasoning about knowledge updates via observations, with a computationally complex but decidable satisfiability problem.

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL is a new reinforcement learning framework for human-aligned AI in PCGRL, using text, level, and sketch inputs to improve control and human-likeness.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems in PCGRL lack human-centered behavior, limiting their practical use in design workflows.

Method: VIPCGRL integrates text, level, and sketch modalities via quadruple contrastive learning and aligns policies using embedding similarity rewards.

Result: VIPCGRL outperforms baselines in human-likeness, validated by metrics and human evaluations.

Conclusion: VIPCGRL enhances human-AI collaboration in PCGRL, with code and dataset to be released.

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: A dynamic Multi-Agent System (MAS) architecture with supervision and maneuvering mechanisms improves reliability and accuracy in tool-augmented LLMs, outperforming single-agent systems.


<details>
  <summary>Details</summary>
Motivation: Challenges like extended contexts and noisy tool outputs in LLMs necessitate enhanced stability in agent-based systems.

Method: Introduces dynamic supervision and maneuvering mechanisms within the AWorld framework, using Execution and Guard Agents to verify and correct reasoning.

Result: The system significantly improves solution effectiveness and stability, achieving top performance on the GAIA leaderboard.

Conclusion: Collaborative agent roles enhance reliability and trustworthiness in intelligent systems.

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: A multi-agent framework combines Knowledge Graphs and Retrieval-Augmented Generation for precise regulatory QA, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of precise, verifiable QA in regulatory compliance using LLMs.

Method: Builds a KG from regulatory documents, embeds triplets in a vector database, and uses an orchestrated agent pipeline for QA.

Result: Outperforms conventional methods in complex regulatory queries with factual correctness and traceability.

Conclusion: Provides a robust solution for compliance-driven applications with enhanced understanding and traceability.

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: The study evaluates four LLMs (OpenAI GPT-4o, o1, DeepSeek-V3, R1) on challenging math tasks, identifying step-level errors. OpenAI o1 performed best, with dual-agent setups improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess LLM accuracy in math education, focusing on error-prone tasks to improve AI-driven instruction and assessment.

Method: Tested LLMs on arithmetic, algebra, and number theory tasks, analyzing answer accuracy and step-level errors in single- and dual-agent setups.

Result: OpenAI o1 showed highest accuracy; procedural slips were common, while conceptual errors were rare. Dual-agent setups improved performance.

Conclusion: Findings guide LLM enhancement and effective integration into math education, improving AI-driven assessment precision.

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: The paper investigates vulnerabilities in collaborative multi-agent reinforcement learning (c-MARL) under realistic adversarial conditions, proposing efficient perturbation algorithms to misalign agents' perceptions.


<details>
  <summary>Details</summary>
Motivation: The lack of thorough investigation into c-MARL's vulnerabilities to adversarial attacks, especially under realistic constraints, motivates this work.

Method: The study assumes adversaries can only perturb observations of deployed agents or have no access, proposing simple yet effective perturbation algorithms.

Result: Empirical validation on three benchmarks and 22 environments shows the approach's effectiveness and sample efficiency (1,000 samples vs. millions).

Conclusion: The paper highlights new vulnerabilities in c-MARL and demonstrates the practicality of adversarial attacks under constrained conditions.

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [15] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: A Transformer model with feature tokenization is proposed for real-time ETA prediction of airborne aircraft, outperforming XGBoost in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Real-time ETA prediction is critical for efficient arrival management in aviation, requiring both accuracy and computational efficiency.

Method: A feature tokenization-based Transformer model is used to process raw data (e.g., aircraft position, speed, weather) and predict ETA at 1Hz frequency.

Result: The model improves accuracy by 7% over XGBoost and reduces computing time by 61%, with inference times as low as 51.7 microseconds for 40 aircraft.

Conclusion: The proposed method is highly efficient and accurate, making it suitable for real-time arrival management systems.

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [16] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN is a framework for multimodal sentiment analysis that dynamically edits noise in modality features, preserving critical information while suppressing irrelevant data. MoLAN+ builds on this to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat entire modalities as independent units, risking loss of critical information while suppressing noise. MoLAN addresses this by fine-grained noise editing.

Method: MoLAN divides modality features into blocks, dynamically assigning denoising strength based on noise level and semantic relevance. MoLAN+ extends this framework for sentiment analysis.

Result: MoLAN+ outperforms existing methods, achieving state-of-the-art performance across five models and four datasets.

Conclusion: MoLAN provides a flexible, unified framework for noise suppression in multimodal data, with MoLAN+ demonstrating superior performance in sentiment analysis.

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [17] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: The paper proposes an LLM transformer-based in-context learning (ICL) approach to optimize WiFi 7 channel access, addressing the limitations of traditional backoff schemes and model-based methods under dynamic node densities.


<details>
  <summary>Details</summary>
Motivation: Current binary exponential backoff schemes and model-based approaches (e.g., non-persistent and p-persistent CSMA) perform poorly under dynamic channel conditions due to inaccurate node density estimation. This paper aims to improve throughput by leveraging transformer-based ICL.

Method: The authors design a transformer-based ICL optimizer that uses pre-collected collision-threshold data and query cases as prompts to predict contention window thresholds (CWT). They also develop an efficient training algorithm and extend the approach to handle erroneous data.

Result: The proposed method achieves near-optimal CWT prediction and throughput with minimal deviations, outperforming existing model-based and DRL-based approaches in NS-3 simulations.

Conclusion: The transformer-based ICL approach effectively optimizes channel access in WiFi 7, demonstrating fast convergence and near-optimal performance under unknown node densities.

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [18] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B is a 2.6B-parameter LLM designed to balance performance and efficiency, featuring innovations like Differential Attention and PolyNorm, outperforming similar models.


<details>
  <summary>Details</summary>
Motivation: To democratize advanced LLM capabilities for emerging research groups by addressing the challenge of balancing performance and computational efficiency.

Method: Incorporates Differential Attention and PolyNorm activation functions, tested through extensive experimentation to optimize architecture.

Result: Outperforms similarly sized state-of-the-art models in benchmarks, demonstrating effectiveness, scalability, and real-world applicability.

Conclusion: Motif-2.6B advances efficient, scalable foundational LLMs, providing insights and a foundation for future research and deployment.

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [19] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: The paper questions the necessity of complex sequence mixers in time series analysis (TSA) and proposes JustDense, a method replacing them with dense layers. Results show comparable or better performance, challenging assumptions about complexity in TSA.


<details>
  <summary>Details</summary>
Motivation: Recent studies suggest simpler architectures may outperform complex sequence mixers in TSA, prompting the question of their necessity.

Method: JustDense replaces sequence mixers in TSA models with dense layers, isolating mixing operations for theoretical clarity.

Result: Experiments on 29 benchmarks show dense layers match or outperform complex mixers, questioning the need for intricate designs.

Conclusion: JustDense demonstrates that simpler dense layers can suffice in TSA, challenging the belief in the superiority of complex architectures.

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [20] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI is a deep learning framework using I-G transformation and 2SRI to address simultaneous feedback and unobserved confounders in peer effect estimation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately estimate peer effects due to ignoring simultaneous feedback or relying on restrictive linear assumptions.

Method: DIG2RSI uses I-G transformation to disentangle peer influences and 2SRI with neural networks to handle unobserved confounders, incorporating adversarial debiasing.

Result: The framework proves consistent under standard conditions and outperforms existing methods on benchmarks and real-world data.

Conclusion: DIG2RSI effectively addresses feedback and confounding, providing accurate peer effect estimation in complex networks.

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [21] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: AdaPO is an online RL framework for LMMs that adaptively adjusts training objectives to mitigate reward hacking, enhancing self-evaluation and reasoning.


<details>
  <summary>Details</summary>
Motivation: Self-evaluation is critical for LMMs but lacks in foundation models. Fixed RL rewards lead to reward hacking and model collapse.

Method: AdaPO uses an Adaptive Reward Model (ARM) and Reward Aware Dynamic KL Regularization to adjust objectives dynamically.

Result: Improves reasoning and self-evaluation across 8 benchmarks.

Conclusion: AdaPO effectively addresses reward hacking and enhances model performance without manual intervention.

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [22] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: A framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems, ensuring physical consistency and accurate parameter recovery.


<details>
  <summary>Details</summary>
Motivation: To address ill-posed inverse problems in scientific systems by combining generative modeling with physics-aware constraints, enabling data-efficient and physically valid solutions.

Method: Differentiable post-training procedure minimizes weak-form PDE residuals, augmented with a learnable latent parameter predictor for joint optimization.

Result: Improved PDE constraint satisfaction and accurate recovery of hidden parameters, validated on canonical PDE benchmarks.

Conclusion: Bridges generative modeling and scientific inference, offering new possibilities for simulation-augmented discovery and efficient physical system modeling.

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [23] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive introduces a multi-objective reinforcement learning framework for autonomous driving, enabling iterative trajectory refinement via adversarial optimization, outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current methods isolate trajectory generation from evaluation or collapse multi-dimensional preferences into scalar rewards, limiting iterative refinement and obscuring trade-offs.

Method: EvaDrive uses adversarial optimization with a hierarchical generator (autoregressive intent modeling + diffusion-based refinement) and a multi-objective critic, guided by Pareto frontier selection.

Result: Achieves 94.9 PDMS on NAVSIM v1 and 64.96 Driving Score on Bench2Drive, surpassing existing methods.

Conclusion: EvaDrive offers a scalarization-free, human-like iterative decision-making framework for autonomous driving.

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [24] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: The paper integrates 15 datasets to create a large database for T1D research, addressing data scarcity. It includes 2510 subjects with frequent glucose measurements, analyzes data quality, and explores correlations between glucose levels and heart rate before hypoglycemia.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of small datasets in diabetes and hypoglycemia research by creating a comprehensive database to improve predictive models for glucose levels and hypoglycemia alarms.

Method: The work systematically integrates 15 datasets, resulting in a database of 2510 subjects with glucose measurements every 5 minutes. Two sub-databases (demographics and heart rate data) are extracted, and data quality is assessed. A correlation study between glucose and heart rate is conducted.

Result: The integrated database includes 149 million measurements (4% hypoglycemic). Data imbalance and missing values are identified as challenges. A correlation between glucose levels and heart rate 15-55 minutes before hypoglycemia is found.

Conclusion: The study provides a valuable resource for T1D research, highlighting data quality issues and revealing a temporal relationship between heart rate and hypoglycemia, aiding future predictive models.

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [25] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: The paper introduces a Physics-Guided Memory Network (PgMN) to combine deep learning and physics-based models for accurate energy consumption forecasting, especially in scenarios with limited or no historical data.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of deep learning (dependency on historical data) and physics-based models (extensive parameter requirements) in energy forecasting for buildings.

Method: PgMN integrates deep learning and physics-based predictions using Parallel Projection Layers, a Memory Unit, and a Memory Experience Module.

Result: PgMN shows accuracy in diverse scenarios, including new buildings, missing data, and dynamic changes, validated through theoretical and experimental evaluation.

Conclusion: PgMN offers a robust solution for energy forecasting in dynamic environments, overcoming data limitations and enhancing applicability.

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [26] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised explainable AI framework to detect and characterize replay attacks in nuclear reactor data, achieving high accuracy on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Ensuring data integrity against deception attacks is critical for safe nuclear reactor operation, but current methods lack root-cause analysis and rely on synthetic data or oversimplified assumptions.

Method: An unsupervised XAI framework combining autoencoder and customized windowSHAP algorithm to detect, identify, and characterize replay attacks in real-time multivariate time series data.

Result: The framework achieved 95% or better accuracy in detecting, identifying sources, and timing of replay attacks on real datasets from Purdue's nuclear reactor.

Conclusion: The proposed XAI framework effectively addresses replay attacks in nuclear cyber-physical systems, offering explainability and high accuracy on real-world data.

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [27] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: ASL introduces mixed-precision to SC NNs, reducing energy/latency by 60% with minimal accuracy loss, validated by theoretical and practical analysis.


<details>
  <summary>Details</summary>
Motivation: To improve energy efficiency in SC NNs for IoT by exploring mixed-precision layer-wise implementations, addressing unexplored truncation noise propagation.

Method: Proposes ASL with operator-norm-based modeling, RF regression for sensitivity analysis, and two truncation strategies (coarse/fine-grained).

Result: ASL reduces energy/latency by over 60% with negligible accuracy loss in a 32nm SC MLP.

Conclusion: ASL is feasible for IoT, showcasing mixed-precision truncation advantages in SC designs.

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [28] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: A novel diffusion model-based method for population synthesis is proposed, outperforming VAEs and GANs in balancing feasibility and diversity by recovering missing sampling zeros and minimizing structural zeros.


<details>
  <summary>Details</summary>
Motivation: Population synthesis is crucial for agent-based modeling in transportation systems, but sparse survey data and high dimensionality make accurate modeling challenging. Deep generative models can help but risk generating infeasible attribute combinations.

Method: A diffusion model-based approach is introduced to estimate the joint distribution of population attributes, recovering missing sampling zeros while minimizing structural zeros.

Result: The proposed method outperforms VAE and GAN approaches in metrics like marginal distribution similarity, feasibility, and diversity.

Conclusion: The diffusion model-based method effectively balances feasibility and diversity in population synthesis, offering a superior solution for high-dimensional tabular data.

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [29] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG is a framework for adaptive learning of ECG signals with layout variations, achieving robust arrhythmia detection.


<details>
  <summary>Details</summary>
Motivation: ECG signals vary in layout across hospitals, causing asynchronous lead time and blackout loss, challenging existing models.

Method: PatchECG uses a masking training strategy to focus on key patches and collaborative dependencies between leads.

Result: Achieved AUROC of 0.835 on PTB-XL, 0.778 on real ECG images, and 0.893 on 12x1 layouts, outperforming baselines.

Conclusion: PatchECG is robust to layout variations and superior to existing methods, enhancing arrhythmia diagnosis.

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [30] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M dataset and SVGen model enable efficient, accurate SVG generation from natural language, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of turning creative ideas into precise vector graphics efficiently.

Method: Introduces SVG-1M dataset with aligned text-SVG pairs and proposes SVGen, an end-to-end model using curriculum and reinforcement learning.

Result: SVGen outperforms general large models and traditional rendering methods in effectiveness and efficiency.

Conclusion: The approach provides a scalable solution for generating SVGs from text, with publicly available resources.

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [31] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: A lightweight, training-free method using Retrieval-Augmented Generation (RAG) to bridge the modality gap in large multimodal models (LMMs) by mapping images to textual descriptions, improving performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the modality gap in LMMs and the high cost of fine-tuning, the paper proposes a cost-effective solution.

Method: Utilizes RAG with a linear mapping to align visual and textual embeddings, generating synthetic descriptions for optimization.

Result: Shows significant improvements on two benchmark multimodal datasets.

Conclusion: The proposed approach effectively bridges the modality gap without expensive fine-tuning, offering practical benefits.

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [32] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP improves federated learning (FL) in non-IID scenarios by using stochastic feature manifold completion and class-prototypes to align feature manifolds, enhancing model convergence and performance, especially in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Non-IID data in FL, common in medical imaging, hinders model convergence and performance. FedMP addresses this challenge.

Method: FedMP uses stochastic feature manifold completion and class-prototypes to align feature manifolds across clients, improving decision boundaries.

Result: FedMP outperforms existing FL algorithms on medical imaging and multi-domain datasets, with analysis on manifold dimensionality, communication, and privacy.

Conclusion: FedMP effectively enhances FL performance in non-IID settings, validated by experiments and analyses.

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [33] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: DQT introduces a dynamic quantization framework with nested integer representation and bit-shift operations, enabling efficient mixed-precision without costly dequantization cycles, outperforming existing methods on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic quantization methods require expensive dequantize-requantize cycles, breaking integer-only hardware efficiency. DQT aims to eliminate this bottleneck.

Method: DQT uses a nested integer representation and custom integer-only arithmetic for on-the-fly bit-width switching via bit-shift operations, avoiding dequantization.

Result: DQT achieves 77.00% top-1 accuracy on ImageNet with ResNet50, outperforming static (76.70%) and dynamic (76.94%) methods, with minimal bit-shift costs (28.3M vs. 56.6M MACs).

Conclusion: DQT enables efficient, adaptive AI by removing dequantization bottlenecks, offering superior accuracy-efficiency trade-offs for resource-constrained devices.

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [34] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC is a novel single-cell clustering method that learns adaptive cell graphs with contrastive guidance, outperforming existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods struggle with high dimensionality and noise in scRNA-seq data, while existing graph-based methods rely on static structures that fail to capture long-tailed distributions.

Method: scAGC uses a topology-adaptive graph autoencoder with Gumbel-Softmax sampling and integrates ZINB loss for robust feature reconstruction, alongside contrastive learning for stability.

Result: scAGC achieves the best NMI and ARI scores on 9 and 7 real scRNA-seq datasets, respectively.

Conclusion: scAGC effectively addresses limitations of existing methods, providing a robust and adaptive solution for single-cell clustering.

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [35] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: The paper proposes LCSFLA, a federated learning method for IoV, addressing non-IID data challenges via truthful auction-based client selection and long-term data quality assessment.


<details>
  <summary>Details</summary>
Motivation: Federated learning in IoV faces issues like non-IID data, resource wastage, and information asymmetry in client selection, impacting model performance.

Method: Introduces LCSFLA, combining long-term client selection with truthful auction to ensure data quality and truthful client participation.

Result: The method improves model performance by mitigating non-IID effects and ensures truthful client participation through incentive mechanisms.

Conclusion: LCSFLA effectively addresses non-IID challenges in federated learning for IoV, enhancing model accuracy and resource efficiency.

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [36] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: A survey on breath analysis methods, comparing contact-based and contactless approaches, with a focus on machine learning and deep learning applications, challenges, and future trends.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional contact-based breath analysis methods by exploring noninvasive, contactless techniques enabled by advanced technologies like machine learning.

Method: Examines contactless methods (e.g., Wi-Fi CSI, acoustic sensing) and details preprocessing, feature extraction, and classification techniques, comparing ML/DL models.

Result: Highlights the potential of contactless methods for accurate respiratory monitoring and discusses applications like disease detection and multi-user scenarios.

Conclusion: The survey provides a framework for future research, emphasizing Explainable AI, federated learning, and hybrid modeling to advance breath analysis in healthcare.

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [37] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: The paper proposes Fine-Grained Safety Neurons (FGSN) with a training-free continual projection method to mitigate safety risks in fine-tuned LLMs, balancing safety and utility by focusing on fine-grained neurons and safety layers.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs introduces safety risks by disrupting alignment mechanisms, and existing post-fine-tuning defenses are limited by coarse-grained approaches.

Method: FGSN integrates multi-scale interactions between safety layers and neurons, localizes precise safety neurons, and projects parameters onto safety directions.

Result: Experiments show FGSN reduces harmfulness and attack success rates with minimal parameter changes, preserving model utility.

Conclusion: FGSN offers a continual defense mechanism against emerging safety concerns while maintaining alignment with human preferences.

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [38] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast is an LLM-driven framework for context-aware time series forecasting by unifying numerical and textual data through symbolic representations.


<details>
  <summary>Details</summary>
Motivation: Improving forecasting accuracy by integrating historical numerical sequences with unstructured contextual features like text.

Method: Uses a discrete tokenizer to convert numerical sequences into tokens, aligns them with textual inputs in a shared LLM-embedded space, and fine-tunes the LLM for forecasting.

Result: Effective and generalizable performance on diverse real-world datasets with contextual features.

Conclusion: TokenCast successfully bridges the gap between numerical and textual data for enhanced forecasting.

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [39] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: D2F enhances dLLMs with block-wise autoregressive generation and inter-block parallel decoding, achieving faster inference than AR LLMs and vanilla dLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing dLLMs lack superior inference speed over AR LLMs, prompting the need for a hybrid AR-diffusion approach.

Method: D2F introduces block-wise autoregressive generation and inter-block parallel decoding, implemented via asymmetric distillation and pipelined parallel decoding.

Result: D2F dLLMs achieve 2.5x faster inference than LLaMA3/Qwen2.5 and 50x acceleration over vanilla dLLMs with comparable quality.

Conclusion: D2F successfully bridges the efficiency gap between dLLMs and AR LLMs, offering a practical hybrid paradigm.

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [40] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL improves controllability in procedural content generation by leveraging sentence embeddings for multi-objective instructions, achieving a 13.8% improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with leveraging textual input for complex, multi-objective instructions in content generation, limiting controllability.

Method: MIPCGRL uses sentence embeddings, multi-label classification, and multi-head regression to train a multi-objective embedding space.

Result: The method achieves up to a 13.8% improvement in controllability with multi-objective instructions.

Conclusion: MIPCGRL enables more expressive and flexible content generation by effectively processing complex instructions.

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [41] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: A meta-learning framework is introduced to automate the selection of optimal inference acceleration methods in decentralized AI systems, improving efficiency and performance over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: The high computational costs and challenges of scalability and data security in deploying large-scale models like LLMs motivate the shift to decentralized systems, necessitating efficient acceleration schemes.

Method: The paper proposes a meta-learning-based framework that learns from historical performance data of various acceleration techniques to automate the selection process for specific tasks.

Result: The framework outperforms traditional methods, streamlining decision-making and enhancing efficiency and performance in decentralized AI systems.

Conclusion: The study demonstrates the potential of meta-learning for inference acceleration, contributing to more cost-effective and scalable AI solutions in decentralized settings.

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [42] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: The paper introduces ADT4Coupons, a framework for optimizing coupon distribution in online platforms by leveraging sequential user interactions and long-term revenue goals.


<details>
  <summary>Details</summary>
Motivation: Existing coupon distribution strategies fail to utilize sequential user-platform interactions effectively, leading to performance stagnation despite available e-commerce data.

Method: Proposes ADT4Coupons, a framework integrating general scenarios, sequential modeling with historical data, and iterative updates for optimized coupon distribution.

Result: Empirical results on real-world, public, and synthetic datasets show ADT4Coupons' superiority in boosting long-term revenue.

Conclusion: ADT4Coupons effectively addresses the limitations of current strategies, offering a robust solution for dynamic coupon distribution in online marketing.

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [43] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: The paper introduces the Construction Safety Dataset (CSDataset), a comprehensive multi-level dataset integrating structured and unstructured data to address limitations in construction safety research. It includes incidents, inspections, and violations from OSHA, enabling machine learning and large language model applications. Preliminary benchmarking shows a 17.3% reduction in incidents with complaint-driven inspections.


<details>
  <summary>Details</summary>
Motivation: The limited volume and diversity of existing construction safety datasets hinder in-depth analyses, prompting the creation of CSDataset to fill this gap.

Method: The paper presents CSDataset, integrating structured and unstructured data from OSHA, and conducts preliminary benchmarking and cross-level analyses.

Result: Complaint-driven inspections were found to reduce the likelihood of subsequent incidents by 17.3%. The dataset enables diverse machine learning applications.

Conclusion: CSDataset addresses research gaps in construction safety, offering a valuable resource for future studies and practical applications.

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [44] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE is a quantization inference framework using Mixture-of-Experts to improve model performance by dynamically routing inputs to specialized quantization experts, reducing accuracy degradation without adding significant latency.


<details>
  <summary>Details</summary>
Motivation: Quantization reduces model efficiency and deployment costs but often degrades accuracy. MoQE aims to mitigate this by leveraging multiple quantization experts.

Method: MoQE combines multiple quantization variants of a full-precision model as experts, using a lightweight router to dynamically assign inputs to the best expert.

Result: MoQE matches SOTA quantization performance on benchmarks (ImageNet, WikiText, etc.) without notable latency increase.

Conclusion: MoQE effectively addresses quantization-induced accuracy loss by specializing experts and dynamic routing, offering a practical solution for efficient deployment.

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [45] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: A novel repair algorithm for laser-enabled selective transfer in microLED fabrication reduces transfer steps by 50% and achieves fast planning times, outperforming local proximity and RL-based methods.


<details>
  <summary>Details</summary>
Motivation: The need for efficient computational models to optimize shift sequences in microLED fabrication, minimizing motion and adapting to varying objectives.

Method: A differentiable transfer module for modeling discrete shifts, trainable via gradient-based optimization, eliminating handcrafted features and enabling flexible objectives.

Result: 50% reduction in transfer steps and sub-2-minute planning time on 2000x2000 arrays.

Conclusion: The method offers a scalable, adaptable solution for accelerating microLED repair in AR/VR and display fabrication.

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [46] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec introduces hierarchical adaptive networks with task vectors for dynamic test-time adaptation, improving robustness and handling diverse distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Standard methods for test-time adaptation rely on single-dimensional linear layers, which struggle with complex shifts. Hi-Vec aims to address this limitation.

Method: Hi-Vec uses hierarchical layers for dynamic adaptation, featuring layer selection, weight merging, and linear layer agreement to prevent noisy fine-tuning.

Result: Hi-Vec outperforms state-of-the-art methods, enhancing robustness, uncertainty handling, and performance with limited batch sizes and outliers.

Conclusion: Hi-Vec effectively advances test-time adaptation by dynamically adjusting to diverse shifts and improving model robustness.

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [47] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: Proposes GSMT, a hybrid model combining GAT and RNN with a task corrector for bus trajectory prediction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate bus trajectory prediction is vital for intelligent transportation, especially in regions with limited multimodal data.

Method: Integrates GAT and RNN, uses a task corrector to refine predictions by clustering historical trajectories.

Result: Outperforms existing methods in short- and long-term prediction on real-world data from Kuala Lumpur.

Conclusion: GSMT effectively addresses trajectory prediction challenges in dense urban environments.

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [48] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: A novel Quantum-Inspired Graph Neural Network (QI-GNN) with a Canonical Polyadic (CP) decomposition layer is proposed for detecting illicit blockchain transactions, outperforming classical methods with a 74.8% F2 score.


<details>
  <summary>Details</summary>
Motivation: The need for robust solutions to detect illicit transactions in blockchain networks drives the exploration of quantum-inspired techniques for financial security.

Method: Combines QI-GNN with an Ensemble Model (QBoost or Random Forest) and introduces a CP decomposition layer for efficient complex data analysis.

Result: Achieves a 74.8% F2 score in detecting fraudulent transactions, surpassing classical machine learning methods.

Conclusion: Quantum-inspired algorithms, enhanced by structural advancements like the CP layer, show promise for financial fraud detection and warrant further exploration.

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [49] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: A novel LLM-based prototype estimation framework for tabular learning, using example-free prompts to generate feature values, enabling zero-shot and few-shot learning without training.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively utilizing advanced LLMs in few-shot and zero-shot scenarios for tabular data modeling.

Method: Proposes a framework that queries LLMs to generate feature values via example-free prompts, creating zero-shot prototypes, which can be enhanced with few-shot samples.

Result: Demonstrates effectiveness in zero and few-shot tabular learning, bypassing constraints of example-based prompts.

Conclusion: The framework provides a scalable and robust solution for tabular learning without requiring classifier training or LLM fine-tuning.

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [50] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: A study proposes a deep learning ensemble (ResCNN and AttentionCNN) for robust single-trial odor detection using olfactory bulb LFPs, achieving high accuracy (86.6%) and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current odor detection methods struggle with complex mixtures and lack single-trial fidelity, necessitating a general system for reliable detection.

Method: An ensemble of 1D convolutional networks (ResCNN and AttentionCNN) decodes odor presence from multichannel olfactory bulb LFPs.

Result: The model achieved 86.6% accuracy, 81.0% F1-score, and 0.9247 AUC, outperforming benchmarks and capturing biologically significant signatures.

Conclusion: The study confirms the feasibility of robust single-trial odor detection from LFPs and highlights deep learning's potential for understanding olfactory representations.

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [51] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: The paper addresses over-squashing in GNNs, proposes a metric to assess it, and evaluates rewiring strategies, finding benefits vary by dataset and task.


<details>
  <summary>Details</summary>
Motivation: Over-squashing in GNNs limits expressivity, but existing rewiring techniques lack clear empirical metrics to assess their impact.

Method: Proposes a topology-focused method to measure over-squashing using mutual sensitivity decay and extends it to graph-level statistics. Evaluates rewiring strategies on graph- and node-classification benchmarks.

Result: Rewiring mitigates over-squashing in graph classification but varies by dataset; node classification shows less impact, and aggressive rewiring can harm performance.

Conclusion: Rewiring is beneficial when over-squashing is substantial and corrected moderately; a diagnostic tool helps practitioners decide its utility pre-training.

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [52] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: The paper proposes an explainable framework for automated Knowledge Component (KC) discovery in CS education using pattern-based KCs derived from student code.


<details>
  <summary>Details</summary>
Motivation: Personalized learning in CS education requires accurate modeling of student knowledge, but current KC extraction methods lack explainability and struggle with the variability of student solutions.

Method: A Variational Autoencoder generates representative patterns from student code, guided by an attention-based model. These patterns are clustered into KCs and evaluated using learning curve analysis and Deep Knowledge Tracing (DKT).

Result: The framework shows meaningful learning trajectories and improves DKT predictive performance over traditional methods.

Conclusion: The work provides an automated, scalable, and explainable approach to identifying essential programming patterns for student learning.

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [53] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: A decentralized weather forecasting framework combining Federated Learning and blockchain improves accuracy, privacy, and scalability while addressing security vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Centralized forecasting systems face security risks, scalability issues, and single points of failure, necessitating a decentralized solution.

Method: Integrates Federated Learning for privacy-preserving model training and Ethereum blockchain for transparent verification, with IPFS for storage and a reputation-based voting mechanism for security.

Result: The framework enhances forecasting accuracy, resilience, and scalability, proving suitable for real-world, security-critical applications.

Conclusion: The proposed decentralized approach effectively addresses the limitations of centralized systems, offering a secure and scalable solution for weather forecasting.

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [54] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: Dataset distillation compresses large datasets into small synthetic ones, enabling efficient learning in one gradient step. It generalizes to tasks like RL, transforming RL into supervised learning.


<details>
  <summary>Details</summary>
Motivation: To explore dataset distillation's ability to compress and transform learning modalities, particularly from RL to supervised learning.

Method: Uses a novel extension of proximal policy optimization for meta-learning to distill RL environments into one-batch supervised datasets.

Result: Successfully distilled complex RL environments (e.g., MuJoCo, Atari) into one-step supervised learning tasks, showing generalizability across architectures.

Conclusion: Dataset distillation effectively compresses and transforms RL tasks into supervised learning, demonstrating versatility and efficiency.

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [55] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper introduces GNNev, an exact verification method for GNNs to ensure robustness against adversarial attacks, supporting sum, max, and mean aggregation functions.


<details>
  <summary>Details</summary>
Motivation: GNNs are used in critical applications but lack robust verification methods for common aggregation functions, necessitating a reliable solution.

Method: The method uses constraint solving with bound tightening and incremental solving to verify GNNs against attribute and structural perturbations.

Result: GNNev outperforms existing tools on sum-aggregated tasks and shows effectiveness on benchmarks and real-world datasets.

Conclusion: GNNev provides a versatile and efficient solution for verifying GNN robustness, supporting multiple aggregation functions.

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [56] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: A biologically inspired synaptic pruning method replaces dropout in neural networks, improving efficiency and performance in time series forecasting.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between biological synaptic pruning and artificial neural network regularization by proposing a more biologically plausible method.

Method: The method uses magnitude-based synaptic pruning, progressively removing low-importance connections during training with a cubic sparsity schedule and permanent pruning masks.

Result: Experiments show significant improvements, with up to 20% lower Mean Absolute Error in financial forecasting and 52% in select transformer models.

Conclusion: The method offers a practical, biologically inspired alternative to dropout, enhancing performance in diverse architectures, especially for financial time series forecasting.

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [57] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec is a geometric recommendation framework for financial graphs, using Ricci curvature and flow to analyze stress and shock propagation, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance financial decision support by modeling dynamic interactions among stocks, indicators, and news, and attributing root causes of stress.

Method: Uses discrete Ricci curvature to quantify local stress and Ricci flow to trace shock propagation, revealing causal substructures for risk-aware ranking.

Result: Preliminary results on S&P 500 data with FinBERT-based sentiment show improved robustness and interpretability under perturbations.

Conclusion: RicciFlowRec introduces geometric flow-based reasoning to financial recommendations, with potential for portfolio optimization and forecasting.

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [58] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: Text-to-image models can align dementia-related speech with generated images, achieving 75% accuracy in dementia detection from images alone.


<details>
  <summary>Details</summary>
Motivation: To explore if text-to-image models can align pathological speech (e.g., dementia) with generated images and explain this alignment.

Method: Examine alignment of dementia-related speech with images, develop explainability methods, and test detection accuracy on the ADReSS dataset.

Result: Dementia detection from generated images achieved 75% accuracy. Explainability methods identified contributing language parts.

Conclusion: Text-to-image models can effectively align and detect dementia-related speech, with potential for explainable AI applications.

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [59] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML is a meta-learning algorithm combining gradient matching and sharpness-aware minimization for better generalization with limited data.


<details>
  <summary>Details</summary>
Motivation: To improve model adaptability and robustness in few-shot learning scenarios.

Method: Combines gradient matching and sharpness-aware minimization in a bi-level optimization framework.

Result: Outperforms existing methods in accuracy and generalization on benchmark datasets.

Conclusion: Effective for few-shot learning; source code is publicly available.

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [60] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: Domain-specific sparse autoencoders (SAEs) improve feature interpretability and reconstruction fidelity in large language models by focusing training on well-defined domains like medical text.


<details>
  <summary>Details</summary>
Motivation: Conventional SAEs trained on broad data distributions struggle with capturing domain-specific features, leading to fragmented or absorbed latents and high residual error.

Method: Train JumpReLU SAEs on layer-20 activations of Gemma-2 models using 195k clinical QA examples, confining the training to medical text.

Result: Domain-confined SAEs explain 20% more variance, achieve higher loss recovery, and reduce linear residual error compared to broad-domain SAEs. Features align with clinically meaningful concepts.

Conclusion: Domain-confinement mitigates limitations of broad-domain SAEs, enabling more interpretable latent decompositions and questioning the need for general-purpose SAEs in foundation models.

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [61] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: IHGNN introduces an implicit equilibrium formulation for hypergraphs, enabling stable and efficient global propagation without deep architectures, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Real-world interactions often involve groups (e.g., co-authors, joint user-item engagement), but existing hypergraph neural networks struggle with long-range dependencies and training instability due to fixed message-passing layers.

Method: IHGNN replaces explicit layers with a nonlinear fixed-point equation for representation computation, includes a well-posed training scheme, and employs implicit-gradient training with stabilization.

Result: IHGNN outperforms traditional graph/hypergraph neural networks in accuracy and robustness, showing resilience to initialization and hyperparameter variation.

Conclusion: IHGNN offers strong generalization and practical value for higher-order relational learning, addressing limitations of existing methods.

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [62] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT unifies contrastive and generative SSL for time series, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Explore the complementary potential of contrastive and generative SSL paradigms for multivariate time series.

Method: Propose CoGenT, a framework combining contrastive and generative optimization to address limitations of both approaches.

Result: Achieves up to 59.2% and 14.27% F1 gains over standalone methods on six datasets.

Conclusion: CoGenT establishes a foundation for hybrid SSL in temporal domains, balancing discriminative and generative strengths.

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [63] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: A federated learning-based framework for cross-institution financial risk analysis ensures data privacy by avoiding raw data sharing, using feature attention and temporal modeling, and outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy challenges and enabling collaborative modeling in financial risk analysis without sharing raw data.

Method: Proposes a federated learning framework with feature attention, temporal modeling, and differential privacy for distributed optimization and global model aggregation.

Result: Outperforms centralized and existing federated learning methods in communication efficiency, accuracy, risk detection, and generalization.

Conclusion: The framework enhances risk identification efficiency and data sovereignty, offering a secure solution for financial risk analysis.

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [64] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: DeepFeatIoT, a deep learning model, combines local, global, randomized convolutional, and LLM features to improve IoT time series data classification, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Challenges like metadata loss, data heterogeneity, and irregular timestamps hinder IoT time series data interpretation, reducing smart system effectiveness.

Method: Proposes DeepFeatIoT, integrating learned (local/global) and non-learned (randomized convolutional, LLM) features for enhanced classification.

Result: Consistently outperforms state-of-the-art models across diverse IoT datasets, even with limited labeled data.

Conclusion: DeepFeatIoT advances IoT analytics, supporting next-gen smart systems.

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [65] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: Proposes an unsupervised anomaly detection method for distributed backend services using dynamic graphs and Transformers, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges like complex dependencies, evolving behaviors, and lack of labeled data in anomaly detection for distributed systems.

Method: Constructs dynamic graphs for structural dependencies, uses graph convolution and Transformers for temporal behavior, and fuses features for anomaly scoring.

Result: Outperforms existing models in capturing anomaly paths and dynamic behaviors, showing high expressiveness and stability.

Conclusion: The method is effective, unsupervised, and suitable for practical deployment in cloud monitoring.

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [66] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune is a fine-grained framework for optimizing safety and utility in LLMs by dynamically modulating sparse neurons, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current techniques for safety alignment in LLMs lack robustness against attacks, refuse benign queries, and degrade performance, necessitating a better solution.

Method: NeuronTune identifies safety-critical and utility-preserving neurons via attribution, then uses meta-learning to adaptively modulate their activations.

Result: NeuronTune achieves superior safety and maintains excellent utility, outperforming state-of-the-art methods.

Conclusion: Fine-grained neuron-level intervention, as in NeuronTune, effectively balances safety and utility in LLMs.

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [67] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: A framework for Federated Continual Learning (FCL) bridges small models and Foundation Models (FMs) to enhance task adaptation and knowledge retention.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of FMs underperforming on local tasks due to inability to use private data and their complexity in continual learning.

Method: Proposes a collaborative framework with lightweight local models, Small Model Continual Fine-tuning, and One-by-One Distillation for knowledge fusion.

Result: Superior performance demonstrated, even with heterogeneous small models.

Conclusion: The framework effectively bridges small models and FMs, improving continual learning in federated settings.

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [68] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: A hierarchical federated fine-tuning framework for IoV systems uses LoRA and a novel UCB-DUAL algorithm to optimize resource-aware, mobility-resilient learning, reducing latency by 24% and improving accuracy by 2.5%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in IoV systems like client mobility, heterogeneous resources, and intermittent connectivity for efficient multi-task adaptation.

Method: Proposes a hierarchical federated fine-tuning framework with LoRA and a decentralized, energy-aware rank adaptation mechanism (UCB-DUAL algorithm).

Result: Achieves best accuracy-efficiency trade-off, reducing latency by 24% and improving accuracy by 2.5%.

Conclusion: The framework effectively supports dynamic IoV scenarios with proven efficiency and accuracy gains.

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [69] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA is a novel algorithm to identify highway segments causing traffic slowdowns, using event-based time series analysis, probabilistic modeling, and binary classification, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing road congestion by identifying causal highway segments to efficiently allocate resources for slowdown reduction.

Method: Uses time series of road speeds, focusing on event presence/absence, probabilistic modeling for slowdown probabilities, and a binary classifier for cause/effect identification.

Result: Tested on Los Angeles highway data, NEXICA outperforms state-of-the-art baselines in accuracy and computation speed.

Conclusion: NEXICA effectively identifies causal highway segments, offering a superior solution for traffic congestion analysis.

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [70] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: The paper introduces GDCC, a framework for efficient exploration in reinforcement learning by measuring causality (causal capacity) and identifying critical decision points as subgoals.


<details>
  <summary>Details</summary>
Motivation: Causal inference enhances exploration in reinforcement learning, but measuring causality in complex environments is challenging.

Method: Proposes GDCC: measures causal capacity, identifies critical points (subgoals) using Monte Carlo, and optimizes for continuous spaces.

Result: States with high causal capacity align with subgoals; GDCC outperforms baselines in success rates.

Conclusion: GDCC effectively guides exploration by leveraging causality and subgoals, improving performance in multi-objective tasks.

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [71] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG is a multimodal framework that integrates variable semantics and numerical observations for time series modeling, improving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional models ignore semantic information in variable names, missing critical domain knowledge. TimeMKG addresses this gap.

Method: Uses large language models to interpret semantics, constructs knowledge graphs, and employs a dual-modality encoder with cross-modality attention.

Result: Incorporating variable-level knowledge enhances predictive performance and generalization across datasets.

Conclusion: TimeMKG bridges signal processing and knowledge-informed inference, offering robust and interpretable time series modeling.

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [72] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: A novel open-set fault diagnosis model (FGCRN) is proposed to classify known health states and identify unknown faults in multimode processes, using advanced feature extraction and unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of constructing compact and accurate decision boundaries for health states in multimode processes, where samples of the same state show multiple cluster distributions.

Method: Combines multiscale depthwise convolution, bidirectional gated recurrent unit, and temporal attention for feature extraction. Uses a distance-based loss for intra-class compactness and unsupervised learning for fine-grained feature representations. Employs extreme value theory for unknown fault identification.

Result: Demonstrates superior performance in classifying known states and identifying unknown faults.

Conclusion: FGCRN effectively addresses the challenges of fault diagnosis in multimode processes, offering accurate and compact decision boundaries.

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [73] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: The paper explores using clinical datasets and AI tools for disease understanding, focusing on improving Concept Bottleneck Models (CBMs) with contextual data from clinical notes, achieving a 10% performance boost.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of AI tools in labeling incomplete clinical datasets and improve the interpretability and performance of CBMs.

Method: Incorporates contextual information from clinical notes using a Large Language Model (LLM) to generate additional concepts for CBMs.

Result: A 10% performance gain over existing methods and more comprehensive concept learning, reducing reliance on spurious shortcuts.

Conclusion: Leveraging clinical notes with LLMs enhances CBM performance and interpretability, improving disease characterization like ARDS.

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [74] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS is a novel Meta-NAS framework that combines global and local search strategies to discover high-performing neural architectures beyond predefined search spaces, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Meta-NAS methods suffer from poor generalization, limited search spaces, or high computational costs, limiting their real-world applicability.

Method: GraB-NAS models architectures as graphs and uses a hybrid search strategy: global search via Bayesian Optimization and local exploration via gradient ascent in latent space.

Result: GraB-NAS outperforms state-of-the-art Meta-NAS baselines, achieving better generalization and search effectiveness.

Conclusion: GraB-NAS successfully addresses limitations of existing Meta-NAS methods, offering a scalable and effective solution for task-aware architecture discovery.

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [75] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H combines closed-form weight computation with gradient-based optimization of synthetic inputs, soft labels, and hidden activations, achieving high accuracy quickly on MNIST and Fashion-MNIST.


<details>
  <summary>Details</summary>
Motivation: To develop a fast training paradigm that shifts trainable degrees of freedom from weight space to data/activation space, improving efficiency and performance.

Method: Uses closed-form weight computation via ridge-regularized pseudo-inverse solves and updates prototypes with Adam. Includes multi-layer extensions, learnable ridge parameters, and optional PCA/PLS projections.

Result: Achieves 97.8% and 89.3% test accuracy on MNIST and Fashion-MNIST, respectively, in 3.9s--4.5s with 130k parameters and 250 epochs.

Conclusion: Proto-PINV+H offers superior accuracy-speed-size trade-offs compared to ELM, random-feature ridge, and shallow MLPs trained by backpropagation.

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [76] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for in-context learning (ICL) in LLMs, explaining how transformers perform factual-recall tasks via vector arithmetic, with empirical validation.


<details>
  <summary>Details</summary>
Motivation: Despite empirical evidence of latent task vectors and the role of QA data in ICL, a theoretical explanation for factual-recall capabilities in transformers is lacking.

Method: The authors develop an optimization theory for nonlinear residual transformers trained via gradient descent, analyzing their performance in factual-recall ICL tasks.

Result: The framework proves 0-1 loss convergence and strong generalization, including robustness to concept recombination and distribution shifts.

Conclusion: The study clarifies the superiority of transformers over static embedding methods, supported by empirical simulations.

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [77] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP is a structured pruning method using expander graphs to reduce LLM size and computation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational and memory challenges of deploying large LLMs by developing efficient model variants.

Method: Leverages expander graph theory to guide N:M structured pruning, ensuring information flow in pruned networks.

Result: Achieves significant acceleration, memory savings, and outperforms existing pruning techniques in accuracy.

Conclusion: EGGS-PTP is an effective solution for efficient LLM deployment with structured sparsity.

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [78] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: The paper proposes a data-efficient distillation framework (DED) to optimize reasoning in LLMs, achieving state-of-the-art results with minimal curated data.


<details>
  <summary>Details</summary>
Motivation: Current methods for improving LLM reasoning involve high computational costs and scaling challenges. The paper aims to address this by focusing on efficient distillation.

Method: DED selects optimal teacher models, uses a curated smaller corpus for balanced performance, and employs diverse reasoning trajectories.

Result: DED achieves top results on mathematical reasoning and code generation tasks with only 0.8k examples, outperforming existing methods.

Conclusion: DED provides a practical, efficient approach to enhance reasoning in LLMs while maintaining general capabilities.

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [79] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: The paper establishes a lower bound on dataset size to confirm anomaly presence, linking it to contamination rate and an algorithm-dependent constant.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored question of when anomalies can be conclusively detected in datasets.

Method: Conducted over three million statistical tests across various anomaly detection tasks and algorithms to derive a relationship between dataset size, contamination rate, and an algorithm-dependent constant.

Result: Found that for a dataset of size N and contamination rate ν, anomalies can be confirmed if N ≥ α_algo/ν², setting a limit on anomaly rarity.

Conclusion: The study provides a practical threshold for confirming anomaly existence, highlighting constraints due to rarity.

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [80] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: The paper introduces four strategies (ReDP, CorDP, IC-DP, RouteDP) to enhance zero-shot capabilities of LLMs in context-aided forecasting, improving interpretability, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of LLMs in context-aided forecasting beyond naive prompting, addressing gaps in interpretability, applicability, accuracy, and resource efficiency.

Method: Four strategies: ReDP (explicit reasoning traces), CorDP (context refinement), IC-DP (historical example embedding), RouteDP (task difficulty routing). Evaluated on CiK benchmark tasks.

Result: Demonstrated distinct benefits over naive prompting across LLMs of varying sizes and families, improving interpretability, accuracy, and efficiency.

Conclusion: The strategies offer simple yet effective improvements for LLM-based context-aided forecasting, paving the way for further advancements.

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [81] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: MiCo framework optimizes layer-wise mixed-precision quantization (MPQ) for edge AI, balancing accuracy and latency with hardware-aware models and direct deployment.


<details>
  <summary>Details</summary>
Motivation: Existing MPQ methods lack flexibility, efficiency, and an end-to-end framework for optimization and deployment.

Method: MiCo uses a novel optimization algorithm to search for optimal MPQ schemes, integrates hardware-aware latency models, and enables direct deployment from PyTorch to C.

Result: Achieves high accuracy with minimal drops while meeting latency constraints, enabling end-to-end speedup.

Conclusion: MiCo provides a comprehensive solution for MPQ exploration and deployment, addressing gaps in current methods.

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [82] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: Residual Reservoir Memory Networks (ResRMNs) combine linear and non-linear reservoirs with residual connections for improved long-term input propagation, outperforming conventional RC models.


<details>
  <summary>Details</summary>
Motivation: To enhance long-term input propagation in Reservoir Computing (RC) by integrating residual orthogonal connections in a novel RNN architecture.

Method: ResRMN combines a linear memory reservoir with a non-linear reservoir using residual orthogonal connections. Linear stability analysis is used to study reservoir dynamics.

Result: Empirical tests on time-series and 1-D classification tasks show ResRMNs outperform conventional RC models.

Conclusion: ResRMNs offer a promising advancement in RC by improving long-term input propagation and performance.

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [83] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: CGAD, a Causal Graph-based Anomaly Detection framework, outperforms traditional methods by leveraging causal structures for robust cyberattack detection in critical infrastructures.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional anomaly detection methods in handling distribution shifts and class imbalance in multivariate time series, especially for critical infrastructure protection.

Method: CGAD uses a two-phase supervised framework: causal profiling (learning causal invariant graphs via Dynamic Bayesian Networks) and anomaly scoring (detecting anomalies through structural divergence in causal graphs).

Result: CGAD achieves higher precision, adaptability, and accuracy in non-stationary environments, with significant improvements in F1 and ROC-AUC scores over baselines.

Conclusion: CGAD redefines robustness in anomaly detection, proving effective for complex cyberattack scenarios where traditional methods fail.

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [84] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: Gauss-Tin, a hybrid method combining replay strategy and Gaussian mixture model, improves LLM retention by 6% over traditional methods, mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in LLMs, where new learning erases prior knowledge, by enhancing retention through strategic replay and instructional guidance.

Method: Integrates replay-based techniques with a Gaussian mixture model for better sample selection and uses instructional guidance to reinforce past learnings.

Result: Shows a 6% improvement in retention metrics compared to traditional methods.

Conclusion: Gauss-Tin effectively mitigates catastrophic forgetting, highlighting the potential of hybrid models for robust LLMs in dynamic environments.

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [85] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: The paper proposes an interpretable GNN framework for PBPM, addressing gaps in localized vs. global modeling, temporal relevance, and transition semantics, achieving competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based PBPM models lack temporal relevance and transition semantics, limiting their effectiveness.

Method: The framework compares prefix-based GCNs and full trace GATs, introduces a time decay attention mechanism, and embeds transition semantics into edge features.

Result: The models achieve competitive Top-k accuracy and DL scores on five benchmarks without per-dataset tuning.

Conclusion: The work provides a robust, generalizable, and explainable solution for next event prediction in PBPM.

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [86] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G uses LLMs to generate synthetic data for rare event classification, then propagates labels semi-supervisedly on a similarity graph, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Label scarcity for rare events limits ML model training. SYNAPSE-G addresses this by generating synthetic data to kickstart learning.

Method: Leverages LLMs for synthetic data generation, constructs a similarity graph for semi-supervised label propagation, and refines labels via an oracle.

Result: Outperforms baselines like nearest neighbor search on SST2 and MHS datasets, improving rare event classification.

Conclusion: SYNAPSE-G effectively tackles the cold-start problem by combining synthetic data generation and semi-supervised learning.

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [87] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: The paper explores Edge General Intelligence (EGI) using world models for autonomous decision-making in edge computing, covering architecture, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in integrating world models into wireless edge computing for proactive AI systems.

Method: Analyzes architectural foundations of world models (latent representation, dynamics modeling, imagination-based planning) and their applications in EGI scenarios.

Result: Highlights proactive applications in vehicular networks, UAVs, IoT, and network functions, and synergies with foundation models and digital twins.

Conclusion: Identifies open challenges (safety, training, deployment) and provides a roadmap for next-gen autonomous edge systems.

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [88] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: The paper introduces Prediction with Limited Selectivity (PLS), a model where forecasters can only predict on a subset of the time horizon, and analyzes optimal prediction error both instance-by-instance and on average.


<details>
  <summary>Details</summary>
Motivation: Existing selective prediction models allow forecasters to predict at any time, but this may not reflect real-world constraints. PLS addresses this by limiting prediction windows.

Method: The study introduces PLS, analyzes optimal prediction error, and proposes a complexity measure for instance-dependent bounds.

Result: Instance-dependent bounds on optimal error are derived, and these bounds align with high probability for randomly-generated PLS instances.

Conclusion: PLS provides a more realistic framework for selective prediction, with theoretical guarantees on prediction error.

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [89] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: The paper introduces a Physics- and Geometry-Aware Spatio-Spectral Graph Neural Operator (πG-Sp²GNO) for solving PDEs efficiently, improving upon existing methods by incorporating geometry awareness and physics-informed learning.


<details>
  <summary>Details</summary>
Motivation: Efficiently solving PDEs, especially for complex geometries and limited labeled data, is a critical challenge in science and engineering.

Method: The proposed πG-Sp²GNO enhances Sp²GNO with geometry awareness and physics-informed learning, using a spatio-spectral structure for multiscale learning and a hybrid loss function for time-dependent problems.

Result: The method outperforms state-of-the-art physics-informed neural operator algorithms on benchmark examples involving regular and complex domains, geometry variations, and time-dependent problems.

Conclusion: The πG-Sp²GNO is effective for solving PDEs, demonstrating superior performance in accuracy and efficiency compared to existing methods.

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [90] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks is a Python framework using Gaussian Process models to analyze protein thermal stability data, overcoming limitations of traditional TPP methods.


<details>
  <summary>Details</summary>
Motivation: Existing TPP workflows assume sigmoidal melting curves and rely on empirical null distributions, limiting significant hits. Thermal Tracks addresses these constraints.

Method: Uses Gaussian Process models with squared-exponential kernels to flexibly model melting curves and generate unbiased null distributions.

Result: Enables analysis of proteome-wide perturbations and unconventional melting profiles, such as phase-separating or membrane proteins.

Conclusion: Thermal Tracks is a versatile, accessible tool for proteome-wide thermal profiling, available on GitHub.

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [91] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: The paper shows that vanilla gradient descent without regularization can effectively solve asymmetric low-rank matrix completion, achieving linear convergence and implicit regularization.


<details>
  <summary>Details</summary>
Motivation: To challenge the necessity of regularization in gradient descent for matrix completion by proving its implicit properties suffice.

Method: Uses vanilla gradient descent with spectral initialization and leave-one-out technique for theoretical analysis.

Result: Demonstrates linear convergence and implicit regularization, with lower computational cost and comparable performance.

Conclusion: Regularization terms are unnecessary for convergence, as gradient descent inherently balances the problem.

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [92] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: The paper presents an operator-theoretic framework for temporal anchoring in embedding spaces, with proofs for contraction lemmas, convergence theorems, and robustness. It formalizes a Manuscript Computer and analyzes attention layers, including Lipschitz properties of softmax.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical framework for temporal anchoring in embedding spaces, ensuring robustness and convergence in computational models.

Method: Uses drift maps, event-indexed blocks, and affine projections, with proofs for contraction and convergence. Formalizes a Manuscript Computer and analyzes attention layers.

Result: Proves contraction lemmas, convergence theorems, and robustness. Shows softmax is 1/2-Lipschitz and derives layer-contraction conditions.

Conclusion: The framework is robust and mathematically rigorous, with applications in attention layers and computational models.

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [93] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: The paper proposes a Dynamic Connection Masking (DCM) mechanism for MLPs and KANs to improve robustness against noisy labels by adaptively masking less important edges during training.


<details>
  <summary>Details</summary>
Motivation: Noisy labels degrade model performance, and existing solutions focus on loss functions and sample selection, neglecting regularization in model architecture.

Method: Introduces DCM to evaluate and mask less important edges in MLPs and KANs, reducing gradient error and enhancing noise robustness.

Result: DCM outperforms SOTA methods in experiments on synthetic and real-world benchmarks, and KANs show superior robustness over MLPs.

Conclusion: DCM is effective for noise robustness and can be integrated with other methods, with KANs emerging as strong classifiers for noisy labels.

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [94] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: GTG is a subtree-centric generative model for brain connectomes, addressing limitations of current methods by focusing on local motifs, edge-weight prediction, and scalability.


<details>
  <summary>Details</summary>
Motivation: Current graph generative models for brain connectomes have limitations like blurred local details, reliance on node attributes, poor edge-weight prediction, and high computational costs.

Method: GTG decomposes connectomes into entropy-guided k-hop trees, encodes them with a shared GCN, and uses a bipartite message-passing layer and dual-branch decoder for reconstruction.

Result: GTG outperforms baselines in self-supervised tasks, achieves competitive supervised performance, and offers higher structural fidelity with lower memory usage.

Conclusion: GTG provides an efficient, accurate solution for connectome synthesis, with potential for extensions like super-resolution and cross-modality synthesis.

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [95] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: GenCO is a novel framework for optimizing creative combinations in e-commerce ads using generative modeling and multi-instance reward learning, significantly boosting ad revenue.


<details>
  <summary>Details</summary>
Motivation: Existing methods evaluate creative elements individually, missing the complexity of combinations, leading to suboptimal ad performance.

Method: A two-stage approach: (1) generative model produces diverse creative combinations, optimized via reinforcement learning; (2) multi-instance learning attributes combination-level rewards to individual elements for better feedback.

Result: Deployed on a leading e-commerce platform, GenCO increased advertising revenue. A large-scale dataset was also released.

Conclusion: GenCO effectively navigates the creative combination space, improving ad performance and offering practical and research value.

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [96] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: Proposes Hereditary Knowledge Transfer (HKT), a biologically inspired method to enhance small models by selectively transferring task-relevant features from larger models, outperforming standard distillation.


<details>
  <summary>Details</summary>
Motivation: Addresses the trade-off between model performance and deployability by improving small models' capabilities without increasing size.

Method: Introduces HKT with Extraction, Transfer, and Mixture (ETM) stages and a Genetic Attention mechanism for selective feature integration.

Result: HKT improves performance on vision tasks (optical flow, classification, segmentation) while maintaining model compactness, outperforming standard distillation.

Conclusion: HKT offers a scalable, interpretable solution for deploying high-performance models in resource-constrained settings.

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [97] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: A machine learning pipeline was developed to predict aging trajectories using longitudinal biomarker data, outperforming static models by incorporating rate-of-change features.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting aging trajectories dynamically, moving beyond static biomarker snapshots.

Method: Developed a LightGBM model using longitudinal data (2019-2022) with engineered slope features capturing biomarker changes over time.

Result: High accuracy in predicting age (R²=0.515 for males, R²=0.498 for females), with slope features being key predictors.

Conclusion: Dynamic tracking of health trajectories improves age prediction, enabling personalized prevention strategies.

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [98] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [99] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: The paper introduces TriForecaster, a framework for Multi-Region Electric Load Forecasting (MRELF), addressing regional, contextual, and temporal variations using Mixture of Experts (MoE) and Multi-Task Learning (MTL). It achieves a 22.4% error reduction and is deployed in eastern China.


<details>
  <summary>Details</summary>
Motivation: The rise of smart grids and meters has provided detailed load data, revealing similar patterns across cities in eastern China, motivating the need for accurate short-term load forecasting for multiple sub-regions.

Method: Proposes TriForecaster, featuring RegionMixer and Context-Time Specializer (CTSpecializer) layers, leveraging MoE within MTL to handle regional, contextual, and temporal variations.

Result: TriForecaster reduces forecast error by 22.4% on four real-world datasets and is deployed on the eForecaster platform, supporting 17 cities with over 100 gigawatt-hours daily usage.

Conclusion: TriForecaster demonstrates flexibility and broad applicability, proving effective for large-scale, short-term load forecasting in practical scenarios.

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [100] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: A method for optimizing Matérn kernel temporal Gaussian processes via recursive Bayesian estimation outperforms marginal likelihood maximization and Hamiltonian Monte Carlo in runtime and accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the optimization of hyperparameters in Gaussian processes, particularly for Matérn kernel temporal models.

Method: Casts the optimization problem as a recursive Bayesian estimation procedure for autoregressive model parameters.

Result: Outperforms marginal likelihood maximization and Hamiltonian Monte Carlo in runtime and root mean square error.

Conclusion: The proposed recursive Bayesian estimation method is more efficient and accurate for hyperparameter optimization in Gaussian processes.

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [101] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: The study uses machine learning to analyze biomechanical features in long jump performances, identifying key factors like knee angle for males and landing pose for females, alongside velocity.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-based methods struggle to analyze complex biomechanical relationships, prompting the use of machine learning for deeper insights.

Method: Quantile regression models were applied to biomechanical data from World Championships, with SHAP, PDPs, and ICE plots for interpretation.

Result: Key findings include knee angle (>169°) for male athletes and landing pose/approach technique for female athletes as critical for top 10% performance.

Conclusion: The study provides a data-driven framework for analyzing athletic performance, highlighting technical aspects beyond velocity.

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [102] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: RankList, a listwise preference learning framework, improves global ranking consistency over pairwise methods like RankNet, achieving better performance in tasks like speech emotion recognition and image aesthetic assessment.


<details>
  <summary>Details</summary>
Motivation: Pairwise preference learning methods (e.g., RankNet) are limited to local comparisons and struggle with global ranking consistency, motivating the need for a listwise approach.

Method: RankList generalizes RankNet with list-level supervision, modeling local and non-local ranking constraints probabilistically. It uses a log-sum-exp approximation for efficiency and skip-wise comparisons for global fidelity.

Result: RankList outperforms baselines in Kendall's Tau and ranking accuracy on SER datasets (MSP-Podcast, IEMOCAP, BIIC Podcast) and aesthetic image ranking (Artistic Image Aesthetics dataset).

Conclusion: RankList provides a unified, extensible framework for ordered preference learning, improving in-domain performance and cross-domain generalization.

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [103] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: FedShard is a federated unlearning algorithm ensuring efficiency and performance fairness, addressing challenges in convergence and unlearning fairness, and introducing new fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Current federated unlearning methods lack focus on efficiency and performance fairness among clients, leaving gaps in addressing fairness during unlearning.

Method: FedShard adaptively balances convergence, unlearning efficiency, and fairness, with novel metrics to assess fairness.

Result: FedShard improves unlearning speed (1.3-6.2x faster than retraining, 4.9x faster than state-of-the-art) and mitigates unfairness risks like cascaded leaving and poisoning attacks.

Conclusion: FedShard effectively ensures fairness in federated unlearning, offering balanced costs and faster unlearning while addressing key challenges.

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [104] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: Modern artificial neural networks (ANNs) outperform classical machine learning methods in predicting soil properties at field-scale, with TabPFN emerging as the top performer.


<details>
  <summary>Details</summary>
Motivation: The study aims to challenge the dominance of classical machine learning methods (e.g., Random Forest, Partial Least Squares Regression) in field-scale predictive soil modeling (PSM) by evaluating the suitability of modern ANNs.

Method: A comprehensive benchmark evaluates state-of-the-art ANN architectures, including MLP-based models, transformer variants, retrieval-augmented approaches, and the foundation model TabPFN, across 31 datasets with small sample sizes.

Result: Modern ANNs consistently outperform classical methods, with TabPFN showing the strongest overall performance and robustness.

Conclusion: The study recommends adopting modern ANNs, particularly TabPFN, as the new default for field-scale PSM, marking a shift from classical methods.

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [105] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: The paper introduces Prototype Diffusion Model (PDM), an efficient alternative to retrieval-augmented diffusion models, using dynamic visual prototypes for semantic conditioning without external memory.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally intensive, and retrieval-based methods introduce storage and adaptability issues. PDM aims to address these drawbacks.

Method: PDM integrates prototype learning into diffusion, constructing dynamic visual prototypes from clean image features via contrastive learning to guide denoising.

Result: PDM maintains high generation quality while reducing computational and storage costs compared to retrieval-based methods.

Conclusion: PDM offers a scalable and efficient solution for semantic conditioning in diffusion models, eliminating the need for external memory.

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [106] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: The paper proposes a Noise Hypernetwork to replace reward-guided test-time noise optimization in diffusion models, reducing computational overhead while preserving quality gains.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling improves model performance but increases computation time, making it impractical for many applications. The goal is to retain benefits without the overhead.

Method: Introduces a Noise Hypernetwork to modulate initial input noise, using a tractable noise-space objective to optimize for desired characteristics while maintaining fidelity to the base model.

Result: The approach recovers significant quality gains from test-time optimization at a fraction of the computational cost.

Conclusion: The proposed method effectively integrates test-time scaling knowledge into models post-training, balancing performance and efficiency.

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [107] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: The paper introduces DyMoE, a dynamic mixture-of-experts approach for graph incremental learning, addressing catastrophic forgetting by adding specialized experts and using a regularization loss. It achieves a 4.92% accuracy boost over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing graph incremental learning methods suffer from catastrophic forgetting and fail to account for varying contributions of prior knowledge.

Method: Proposes DyMoE, adding new expert networks for incoming data and a regularization loss to maintain old task performance. Introduces sparse MoE to reduce computational cost.

Result: Achieves a 4.92% relative accuracy increase over baselines in class incremental learning.

Conclusion: DyMoE effectively handles incremental learning by dynamically adapting experts and maintaining prior knowledge, outperforming existing methods.

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [108] [Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.09230)
*Yutong Wu,Jie Zhang,Yiming Li,Chao Zhang,Qing Guo,Nils Lukas,Tianwei Zhang*

Main category: cs.MA

TL;DR: Cowpox is a defense approach to enhance robustness in multi-agent systems by limiting infection spread and immunizing agents.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems lack robustness, allowing attacks on one agent to compromise the entire system.

Method: Cowpox uses a distributed mechanism to generate and distribute a cure sample, immunizing agents pre-exposure and aiding recovery.

Result: Empirical and theoretical results show Cowpox effectively improves system robustness.

Conclusion: Cowpox provides a provable defense against adversarial attacks in multi-agent systems.

Abstract: Vision Language Model (VLM)-based agents are stateful, autonomous entities
capable of perceiving and interacting with their environments through vision
and language. Multi-agent systems comprise specialized agents who collaborate
to solve a (complex) task. A core security property is robustness, stating that
the system should maintain its integrity under adversarial attacks. However,
the design of existing multi-agent systems lacks the robustness consideration,
as a successful exploit against one agent can spread and infect other agents to
undermine the entire system's assurance. To address this, we propose a new
defense approach, Cowpox, to provably enhance the robustness of multi-agent
systems. It incorporates a distributed mechanism, which improves the recovery
rate of agents by limiting the expected number of infections to other agents.
The core idea is to generate and distribute a special cure sample that
immunizes an agent against the attack before exposure and helps recover the
already infected agents. We demonstrate the effectiveness of Cowpox empirically
and provide theoretical robustness guarantees.

</details>


### [109] [Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective](https://arxiv.org/abs/2508.09541)
*Gang Chen,Guoxin Wang,Anton van Beek,Zhenjun Ming,Yan Yan*

Main category: cs.MA

TL;DR: The paper explores how dependency hierarchies emerge in multi-agent self-organizing systems (MASOS) during task execution, using MARL to analyze inter-agent dependencies and hierarchy evolution.


<details>
  <summary>Details</summary>
Motivation: To understand the unpredictable emergent behaviors in MASOS, specifically how dependency hierarchies form and evolve dynamically during collaborative tasks.

Method: Multi-agent reinforcement learning (MARL) is used to train MASOS for a box-pushing task, quantifying inter-agent dependencies and analyzing hierarchy emergence.

Result: Hierarchies emerge dynamically and organically, influenced by task environment and network initialization, shaped by agents' 'Talent' and 'Effort.'

Conclusion: Dependency hierarchies in MASOS arise from collective pursuit of objectives, evolving dynamically without pre-configured rules, and are shaped by agents' inherent traits and environmental interactions.

Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.

</details>


### [110] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: The paper extends the OWASP MAS Threat Modeling Guide to address gaps in modeling failures for LLM-driven multi-agent systems, introducing new threat classes and evaluation strategies.


<details>
  <summary>Details</summary>
Motivation: To improve security in complex, autonomous multi-agent systems by addressing gaps in existing threat models.

Method: Introduces additional threat classes and scenarios, along with evaluation strategies like robustness testing and emergent behavior monitoring.

Result: Expands OWASP's framework to better cover risks in LLM-driven multi-agent architectures.

Conclusion: The work enhances security and resilience in real-world deployments of multi-agent systems.

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>
