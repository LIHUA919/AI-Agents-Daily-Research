<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix HÃ¤hnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: This paper introduces multimodal AI agents that can perform life cycle assessments (LCA) for electronic devices in under one minute, achieving carbon footprint estimates within 19% of expert LCAs by leveraging online text and images from repair communities and government certifications.


<details>
  <summary>Details</summary>
Motivation: Traditional LCA requires extensive expert time (weeks to months) and often lacks necessary data for mapping materials and processes to environmental impacts. There's a growing need for sustainability information, but data availability gaps prevent efficient carbon footprint calculations for electronic devices.

Method: The researchers developed multimodal AI agents that emulate interactions between LCA experts and stakeholders, using custom data abstraction and software tools to extract information from online text and images. They also created: (1) a direct estimation method comparing products to clusters with similar descriptions, (2) a data-driven approach to generate emission factors using weighted sums of similar materials, and (3) scaling analysis for future LCA workflows.

Result: The AI system reduces LCA time from weeks/months to under one minute while achieving carbon footprint estimates within 19% accuracy of expert LCAs using zero proprietary data. The direct estimation method runs in 3ms with 12.28% MAPE on electronic products. The emission factor generation method improves MAPE by 120.26% compared to human experts selecting closest database entries.

Conclusion: The multimodal AI approach successfully addresses data availability gaps in LCA while dramatically reducing time requirements. The system demonstrates high accuracy across multiple methods and shows potential for transforming LCA workflows, making sustainability assessments more accessible and efficient for electronic device manufacturing.

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [2] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: This paper introduces Agent Identity Evals (AIE), a framework for measuring how well language model agents maintain stable and reliable identity over time, addressing issues like inconsistency and state perturbations that can undermine their trustworthiness and utility.


<details>
  <summary>Details</summary>
Motivation: Language model agents (LMAs) inherit pathologies from large language models including statelessness, stochasticity, and sensitivity to prompts, which can undermine their identifiability, continuity, persistence and consistency. This identity attrition erodes their reliability, trustworthiness and utility by interfering with core agentic capabilities like reasoning, planning and action.

Method: The authors introduce Agent Identity Evals (AIE), a rigorous, statistically-driven, empirical framework that comprises novel metrics to measure the degree to which LMA systems exhibit and maintain their agentic identity over time. The framework includes formal definitions and methods applicable at each stage of the LMA life-cycle, with worked examples for practical application.

Result: AIE provides a comprehensive set of metrics that can measure LMA identity maintenance including their capabilities, properties and ability to recover from state perturbations. The framework can integrate with other performance, capability and robustness measures to assist in designing optimal LMA infrastructure and scaffolding such as memory and tools.

Conclusion: The AIE framework addresses critical challenges in LMA identity maintenance by providing systematic evaluation methods. This contributes to building more reliable, trustworthy and useful language model agents by ensuring they maintain stable identity over time, which is central to their agentic capability and trustworthiness.

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [3] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: This paper improves the EECBS algorithm for Multi-Agent Path Finding by proposing new flex distribution mechanisms (Conflict-Based, Delay-Based, and Mixed-Strategy) that distribute computational flexibility based on collision counts and constraint delays, outperforming the original greedy approach while maintaining bounded-suboptimal guarantees.


<details>
  <summary>Details</summary>
Motivation: The original EECBS algorithm with flex distribution can become inefficient when increased thresholds push the sum of costs beyond the bound, forcing the algorithm to switch between different path sets instead of resolving collisions on a particular set, reducing overall efficiency in finding bounded-suboptimal solutions for multi-agent path finding.

Method: The paper proposes three new flex distribution mechanisms: (1) Conflict-Based Flex Distribution that distributes flex proportional to collision numbers, (2) Delay-Based Flex Distribution that estimates delays needed to satisfy constraints, and (3) Mixed-Strategy Flex Distribution that combines both approaches in a hierarchical framework while maintaining completeness and bounded-suboptimal guarantees.

Result: Experimental results demonstrate that all three proposed flex distribution approaches outperform the original greedy flex distribution method in EECBS, with the algorithms maintaining their theoretical guarantees of completeness and bounded-suboptimality.

Conclusion: The new flex distribution mechanisms successfully address the efficiency issues in EECBS by intelligently distributing computational flexibility based on problem structure (collisions and delays), leading to improved performance while preserving the algorithm's theoretical properties for bounded-suboptimal multi-agent path finding.

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [4] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: This paper shows that using LoRA (Low-Rank Adaptation) for safety fine-tuning can align LLMs for safety without degrading their reasoning abilities, avoiding the "Safety Tax" problem that occurs with full-model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Safety alignment fine-tuning is necessary to prevent LLMs from assisting with harmful requests, but it significantly degrades reasoning abilities (known as "Safety Tax"). The authors aim to find a way to maintain safety alignment while preserving reasoning capabilities.

Method: The authors use LoRA (Low-Rank Adaptation) for supervised fine-tuning (SFT) on refusal datasets instead of full-model fine-tuning. They restrict safety weight updates to a low-rank space to minimize interference with reasoning weights. They also explore regularization methods and weight merging techniques to further reduce weight overlap.

Result: Extensive experiments across four benchmarks (math, science, and coding) demonstrate that LoRA-based safety fine-tuning achieves safety levels comparable to full-model fine-tuning while preserving reasoning abilities. LoRA produces smaller weight overlap with initial weights compared to full-model fine-tuning, and additional regularization/weight merging methods show some improvement on certain tasks.

Conclusion: LoRA effectively solves the reasoning-safety trade-off by maintaining high safety alignment without compromising reasoning capabilities. This approach avoids the "Safety Tax" and provides a promising direction for designing better safety alignment methods that don't harm model performance.

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [5] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: This paper introduces HySAFE-AI, a hybrid framework that adapts traditional safety analysis methods (FMEA and FTA) to evaluate the safety of modern end-to-end AI systems like LLMs and VLMs in safety-critical applications such as autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems are increasingly using end-to-end monolithic architectures (LLMs, VLMs) in safety-critical areas like autonomous driving and robotics, but traditional safety analysis methods are inadequate for evaluating these complex systems, particularly their intricate latent representations and failure modes.

Method: The paper reviews different architectural solutions and evaluates traditional safety analysis techniques (FMEA and FTA). It then develops HySAFE-AI, a hybrid framework that adapts these traditional methods specifically for AI systems, focusing on how foundational models form and utilize latent representations.

Result: The authors demonstrate how traditional safety analysis techniques can be improved and adapted for modern AI architectures, resulting in the HySAFE-AI framework that better addresses the unique safety challenges of end-to-end AI systems in critical applications.

Conclusion: The paper provides a comprehensive framework for AI safety analysis and offers guidance for future AI safety standards development, addressing the gap between traditional safety methods and modern AI system architectures in safety-critical domains.

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [6] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: This paper introduces GraphPile, a large-scale dataset for Graph Problem Reasoning (GPR), and trains GraphMind models to enhance general reasoning capabilities of LLMs, achieving significant improvements in both mathematical and non-mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with novel and complex reasoning problems, and domain-specific continued pretraining methods lack transferability to broader reasoning tasks. The authors aim to bridge the gap between domain-specific pretraining and universal reasoning capabilities by leveraging graph problem reasoning.

Method: The authors create GraphPile, a 10.9 billion token corpus spanning 23 graph tasks including pathfinding, network analysis, numerical computation, and topological reasoning. The dataset incorporates chain-of-thought, program-of-thought, trace of execution, and real-world graph data. They then use this corpus for continued pretraining on base models (Llama 3, 3.1, and Gemma 2) to train GraphMind.

Result: GraphMind achieves up to 4.9% higher accuracy in mathematical reasoning and up to 21.2% improvement in non-mathematical reasoning tasks such as logical and commonsense reasoning compared to base models.

Conclusion: The work successfully demonstrates that Graph Problem Reasoning can enhance general reasoning capabilities of LLMs, bridging the gap between domain-specific pretraining and universal reasoning. This represents the first work to harness GPR for enhancing reasoning patterns and introduces the first dataset of its kind, advancing LLM adaptability and robustness.

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [7] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: This paper proposes integrating AI copilots into vehicles to enable proactive maintenance through intelligent sensing platforms that can communicate with both machines and drivers.


<details>
  <summary>Details</summary>
Motivation: The need to transform vehicle maintenance from reactive (fixing problems after they occur) to proactive (preventing problems before they happen) by leveraging AI technology and turning vehicles into intelligent sensing platforms.

Method: Integration of AI copilots in vehicles that can interface with both machine systems and human drivers, enabling bidirectional communication and intelligent interpretation of vehicle data for predictive maintenance purposes.

Result: The paper presents a conceptual and technical framework for intelligent vehicle systems that can facilitate proactive maintenance through AI-powered sensing and communication capabilities.

Conclusion: AI copilots that bridge machine-driver communication represent a promising approach for advancing intelligent vehicle systems, predictive maintenance, and AI-powered user interaction, requiring interdisciplinary collaboration for future development.

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [8] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: Researchers developed SCRIPT, a ChatGPT-4o-mini based chatbot for programming education, and tested it with 136 students to analyze feedback preferences and interaction patterns in novice programming learning.


<details>
  <summary>Details</summary>
Motivation: To support novice programming learners by developing an AI-based tool that provides both open-ended interactions and structured guidance, addressing the need for effective GenAI applications in programming education.

Method: Developed SCRIPT chatbot using ChatGPT-4o-mini with predefined prompts for structured guidance and open-ended interaction capabilities. Conducted an experiment with 136 students from an introductory programming course at a German university, analyzing student interactions and feedback preferences while solving programming tasks.

Result: Students' feedback requests followed a specific sequence pattern. The chatbot responses aligned well with students' requested feedback types in 75% of cases and successfully adhered to system prompt constraints. The study provided insights into how students interact with AI-based learning tools.

Conclusion: The findings inform the design of GenAI-based learning support systems and reveal challenges in balancing guidance and flexibility in AI-assisted educational tools. SCRIPT demonstrates potential for supporting novice programming learners while highlighting important design considerations for future AI tutoring systems.

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [9] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: This paper introduces Compliance Brain Assistant (CBA), an AI assistant that uses a smart routing system to handle compliance tasks efficiently by choosing between fast simple responses and complex agentic processing, achieving significant performance improvements over standard LLMs.


<details>
  <summary>Details</summary>
Motivation: Enterprise compliance personnel need an efficient AI assistant that can handle both simple and complex compliance queries while balancing response quality and latency. Existing out-of-the-box LLMs are insufficient for specialized compliance tasks that require contextual knowledge retrieval and multi-step reasoning.

Method: The paper designs a user query router that intelligently selects between two modes: (1) FastTrack mode for simple requests requiring only knowledge retrieval from corpora, and (2) FullAgentic mode for complex requests needing composite actions, tool invocations, API calls, and proactive context discovery across compliance artifacts.

Result: CBA significantly outperformed vanilla LLMs on real-world privacy/compliance queries, achieving 83.7% vs 41.7% average keyword match rate and 82.0% vs 20.0% LLM-judge pass rate. The routing-based design maintained better performance metrics while keeping runtime approximately the same as individual modes.

Conclusion: The routing mechanism successfully achieves an optimal trade-off between response quality and latency, validating the hypothesis that intelligent mode selection between fast and comprehensive processing leads to superior performance in enterprise compliance assistance tasks.

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [10] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: The paper proposes Ctx2TrajGen, a context-aware trajectory generation framework using GAIL with PPO and WGAN-GP to synthesize realistic urban driving behaviors by conditioning on surrounding vehicles and road geometry, achieving superior performance on the DRIFT dataset.


<details>
  <summary>Details</summary>
Motivation: The need for precise modeling of microscopic vehicle trajectories for traffic behavior analysis and autonomous driving systems, addressing challenges of nonlinear interdependencies, training instability, data scarcity, and domain shift in microscopic traffic settings.

Method: Ctx2TrajGen framework that combines Generative Adversarial Imitation Learning (GAIL) with Proximal Policy Optimization (PPO) and Wasserstein GAN with Gradient Penalty (WGAN-GP), explicitly conditioning trajectory generation on surrounding vehicles and road geometry context.

Result: Superior performance compared to existing methods on the drone-captured DRIFT dataset in terms of realism, behavioral diversity, and contextual fidelity, providing a robust solution without requiring simulation.

Conclusion: Ctx2TrajGen successfully generates interaction-aware trajectories that align with real-world context, offering an effective approach to address data scarcity and domain shift challenges in microscopic vehicle trajectory modeling for traffic analysis and autonomous driving applications.

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [11] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: The paper proposes UDASA, an uncertainty-driven framework that automatically aligns large language models with human intent and safety norms without requiring human annotations, achieving better performance across multiple alignment tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to achieve high-quality alignment with human intent and safety norms without human annotations, which represents a fundamental challenge in developing safe and helpful AI systems.

Method: UDASA generates multiple responses for each input, quantifies uncertainty across three dimensions (semantics, factuality, and value alignment), constructs preference pairs based on uncertainty scores, categorizes training samples into conservative/moderate/exploratory stages, and optimizes the model progressively across these stages.

Result: UDASA outperforms existing alignment methods across multiple tasks including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, showing significant improvements in model performance.

Conclusion: The uncertainty-driven adaptive self-alignment framework successfully improves LLM alignment in a fully automated manner, demonstrating that uncertainty quantification can effectively guide the alignment process without human supervision.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [12] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: LTLZinc is a benchmarking framework that generates temporal reasoning and continual learning datasets from linear temporal logic specifications, revealing limitations in current neuro-symbolic AI methods when dealing with time-dependent tasks.


<details>
  <summary>Details</summary>
Motivation: Most neuro-symbolic AI approaches only work in static scenarios and struggle with temporal reasoning. There's a lack of benchmarking frameworks to evaluate neuro-symbolic and continual learning methods on time-dependent tasks that require both symbolic reasoning and learning over time.

Method: The authors developed LTLZinc, a framework that generates datasets by combining linear temporal logic (LTL) specifications with MiniZinc constraints and arbitrary image classification datasets. The framework creates temporal reasoning tasks and continual learning scenarios with fine-grained annotations for multiple training settings.

Result: Experiments on six neuro-symbolic sequence classification tasks and four class-continual learning tasks revealed the challenging nature of temporal learning and reasoning, exposing significant limitations in current state-of-the-art neuro-symbolic methods when handling temporal dimensions.

Conclusion: The study demonstrates that existing neuro-symbolic AI methods struggle with temporal reasoning tasks. The authors release LTLZinc generator and ten ready-to-use tasks to encourage research towards unified temporal learning and reasoning frameworks in the neuro-symbolic and continual learning communities.

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [13] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: This paper investigates Controlled Query Evaluation (CQE) over ontologies using epistemic dependencies to regulate information disclosure, focusing on answering Boolean unions of conjunctive queries with optimal security guarantees and efficient computation.


<details>
  <summary>Details</summary>
Motivation: The need to control information disclosure in ontology-based systems while maintaining query answering capabilities, specifically addressing security concerns when revealing information that could lead to unauthorized inferences through epistemic dependencies.

Method: Combines epistemic dependencies (EDs) with optimal GA censors (maximal sets of safely revealable ground atoms) and uses intersection-based approach for answering Boolean unions of conjunctive queries (BUCQs). Develops a first-order rewriting algorithm for DL-Lite_R ontologies.

Result: Characterizes security of intersection-based approach, identifies full EDs as a safe class, proves AC^0 data complexity for BUCQ answering in DL-Lite_R ontologies, and demonstrates practical feasibility through experimental evaluation in two scenarios.

Conclusion: The intersection-based CQE approach with epistemic dependencies provides strong security guarantees while maintaining computational efficiency, particularly for full EDs and DL-Lite_R ontologies, making it practically feasible for controlled information disclosure in ontology systems.

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [14] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: This paper proposes automated hybrid grounding for Answer Set Programming that intelligently combines standard bottom-up grounding with body-decoupled grounding using data-structural heuristics to determine the optimal grounding strategy.


<details>
  <summary>Details</summary>
Motivation: The grounding bottleneck is a major challenge preventing widespread adoption of Answer Set Programming in industry. While hybrid grounding techniques exist that combine standard bottom-up grounding with body-decoupled grounding, it was unclear when to use which approach optimally.

Method: The authors develop a splitting algorithm based on data-structural heuristics that automatically detects when to use body-decoupled grounding versus standard bottom-up grounding. The heuristics consider rule structure and incorporate instance data through an estimation procedure.

Result: Experimental results on a prototypical implementation show promising performance improvements on hard-to-ground scenarios, while achieving performance comparable to state-of-the-art on hard-to-solve instances.

Conclusion: The automated hybrid grounding approach successfully addresses the decision problem of when to apply different grounding techniques, demonstrating improvements in grounding efficiency while maintaining competitive solving performance.

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [15] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: This paper systematically investigates multi-domain reasoning in Reinforcement Learning with Verifiable Rewards (RLVR), examining how mathematical reasoning, code generation, and logical puzzle solving interact when combined in LLM training, revealing insights for optimizing RL methodologies across multiple cognitive domains.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR research focuses on isolated reasoning domains, but real-world scenarios require integrated application of multiple cognitive skills. The interplay among these reasoning skills under reinforcement learning remains poorly understood, creating a need to bridge this gap through systematic multi-domain investigation.

Method: The study uses GRPO algorithm and Qwen-2.5-7B model family to conduct a comprehensive four-component analysis: (1) evaluating in-domain improvements and cross-domain generalization on single-domain datasets, (2) examining interactions during combined cross-domain training, (3) comparing base vs instruct models under identical RL configurations, and (4) exploring curriculum learning strategies, reward design variations, and language-specific factors.

Result: The extensive experiments reveal significant insights into domain interaction dynamics, identifying key factors that influence both specialized and generalizable reasoning performance. The results show mutual enhancements and conflicts that emerge during cross-domain training, and demonstrate the impact of various RL training details on multi-domain reasoning capabilities.

Conclusion: The findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs, offering crucial understanding of how different reasoning domains interact and can be effectively combined in reinforcement learning frameworks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [16] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: This paper presents the TAI Scan Tool, a RAG-based self-assessment system that helps determine AI system risk levels under the EU AI Act with minimal user input, using a two-step process of pre-screening and assessment to provide risk classification and relevant compliance articles.


<details>
  <summary>Details</summary>
Motivation: The need for a practical tool to help organizations assess their AI systems' compliance with the EU AI Act, particularly determining risk levels and understanding regulatory obligations with minimal input requirements.

Method: A RAG-based (Retrieval-Augmented Generation) approach with a two-step process: (1) pre-screening phase and (2) assessment phase, designed to classify AI systems according to AI Act risk levels while retrieving relevant regulatory articles for compliance guidance.

Result: Qualitative evaluation using use-case scenarios showed promising results with correct risk level predictions and successful retrieval of relevant articles across three distinct semantic groups. The tool demonstrated reasoning capability by comparing systems with high-risk AI system settings.

Conclusion: The TAI Scan Tool successfully provides AI Act compliance assessment with minimalistic input, correctly identifying risk levels and retrieving relevant regulatory articles. The tool's reasoning process focuses on high-risk system comparisons, reflecting the careful consideration required for such deployments as emphasized in the AI Act.

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [17] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: This paper introduces FundusExpert, an ophthalmology-specific multimodal large language model that integrates positioning and diagnostic reasoning capabilities, achieving superior performance in ophthalmic tasks through a novel dataset construction approach and clinically-aligned cognitive reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models face critical challenges in specialized medical domains like ophthalmology, including fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding and accurate diagnosis.

Method: The authors develop FundusExpert using FundusGen dataset constructed through the Fundus-Engine system, which automates localization and uses MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis. They construct clinically aligned cognitive chains to guide interpretable reasoning paths and fine-tune the model with instruction data.

Result: FundusExpert achieves best performance in ophthalmic question-answering tasks, surpassing 40B MedRegA by 26.6% average accuracy. In zero-shot report generation, it achieves 77.0% clinical consistency, significantly outperforming GPT-4o's 47.6%. The study reveals a scaling law between data quality and model capability (L â N^0.068).

Conclusion: The work successfully develops a scalable, clinically-aligned MLLM for ophthalmology by integrating region-level localization with diagnostic reasoning chains. The cognitive alignment annotations in FundusGen enhance data utilization efficiency, and the approach explores a pathway toward bridging the visual-language gap in specialized medical MLLMs.

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [18] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: The paper presents HoPeS, a framework using LLM-powered agents to simulate different stakeholder perspectives in socio-ecological systems, allowing users to experience and understand diverse viewpoints through role-playing simulations.


<details>
  <summary>Details</summary>
Motivation: Understanding socio-ecological systems requires insights from diverse stakeholder perspectives that are often difficult to access. Traditional methods may not adequately capture the complexity of different viewpoints and their interactions in these systems.

Method: Developed the HoPeS (Human-Oriented Perspective Shifting) modeling framework that employs LLM-powered agents to represent various stakeholders. Users can step into agent roles to experience perspectival differences, supported by a simulation protocol that serves as a "scaffold" for multiple perspective-taking simulations.

Result: A prototype system was demonstrated in the context of institutional dynamics and land use change. In an illustrative experiment, a user adopted perspectives of a system observer and researcher, revealing discrepancies between policy recommendations and implementation due to competing stakeholder advocacies. The user experienced frustration as a researcher but showed high motivation to experiment with alternative strategies.

Conclusion: The HoPeS framework shows potential for exploring different perspectives in socio-ecological systems, mirroring real-world misalignments between researchers and policymakers. Further refinement of the system and protocol could enable new forms of interdisciplinary collaboration in socio-ecological simulations.

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [19] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: This paper introduces symbiotic agents that combine Large Language Models (LLMs) with real-time optimization algorithms for 6G networks, achieving a fivefold reduction in decision errors compared to standalone LLM agents while enabling trustworthy AI-driven network management.


<details>
  <summary>Details</summary>
Motivation: The need to transition from specialized AI algorithms handling isolated tasks to artificial general intelligence (AGI)-driven networks where agents can manage diverse network functions with broader reasoning capabilities, while ensuring trustworthy AI for real-time decision-making in 6G network management and service provisioning.

Method: A novel symbiotic agent paradigm that combines LLMs with real-time optimization algorithms at two levels: input-level optimizers providing bounded uncertainty steering for numerically precise tasks, and output-level optimizers supervised by LLMs for adaptive real-time control. Two agent types were implemented: Radio Access Network optimizers and multi-agent negotiators for Service-Level Agreements.

Result: Symbiotic agents reduced decision errors fivefold compared to standalone LLM-based agents. Smaller language models achieved similar accuracy with 99.9% reduction in GPU resource overhead and near-real-time loops of 82 ms. Multi-agent demonstration showed 44% reduction in RAN over-utilization and significant flexibility in service-level agreement and resource allocation.

Conclusion: The symbiotic paradigm provides a foundation for next-generation AGI-driven network systems that remain adaptable, efficient, and trustworthy as LLMs advance, demonstrating the potential for combining LLMs with optimization algorithms to achieve superior performance in 6G network management.

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [20] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: This paper challenges recent claims that reasoning in Large Reasoning Models (LRMs) is ineffective by showing that LRMs consistently outperform standard LLMs when augmented with tools like Python interpreters and scratchpads across all complexity levels.


<details>
  <summary>Details</summary>
Motivation: Recent empirical studies suggested that explicit step-by-step reasoning in LRMs may not actually enhance reasoning ability, with some findings showing that LLMs without explicit reasoning outperform LRMs on various complexity tasks. This created a narrative that reasoning might be an illusion, prompting the need to investigate whether tool augmentation could change these limitations.

Method: The researchers incorporated two types of tools - Python interpreters and scratchpads - and evaluated three representative LLMs alongside their LRM counterparts on Apple's benchmark reasoning puzzles across different levels of task complexity.

Result: With proper tool use, LRMs consistently outperformed their non-reasoning counterparts across all levels of task complexity, contradicting previous findings that suggested reasoning processes were ineffective.

Conclusion: The findings challenge the recent narrative that reasoning is an illusion and demonstrate the potential of tool-augmented LRMs for solving complex problems, suggesting that the perceived limitations of LRMs can be overcome through appropriate tool integration.

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [21] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: This paper presents an automated online competition system that streamlines the submission and evaluation process for research competitions, addressing the operational burden on organizers and compatibility issues from diverse participant environments.


<details>
  <summary>Details</summary>
Motivation: Research communities struggle to track progress across domains due to scattered publications and state-of-the-art claims. While periodic competitions help evaluate algorithms, they create significant operational burdens for organizers who must manage large submission volumes and deal with compatibility issues from participants' diverse development environments.

Method: The authors developed an online competition system that automates submission and evaluation processes. The system uses isolated environments to evaluate submissions, allowing organizers to efficiently manage large numbers of submissions while eliminating compatibility issues.

Result: The system has been successfully deployed and tested in several real competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition, demonstrating its practical effectiveness in managing research competitions.

Conclusion: The automated online competition system effectively addresses the operational challenges of research competitions by streamlining submission management and evaluation processes through isolated environments, as evidenced by successful real-world deployments.

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: This study develops AI algorithms to standardize transtibial prosthetic socket design by predicting socket shapes from 3D limb scans, with random forest achieving the best performance when predicting prosthetist adaptations rather than final socket shapes directly.


<details>
  <summary>Details</summary>
Motivation: The quality of transtibial prosthetic sockets currently depends heavily on individual prosthetist skills and manual fitting expertise, creating variability in outcomes. The study aims to use AI to help standardize the socket design process and reduce this dependency on individual practitioner expertise.

Method: The researchers collected 3D scan data from 118 patients including residual limb scans and corresponding prosthetist-designed sockets. They applied preprocessing steps for alignment, standardization, and compression using Morphable Models and PCA. Three AI algorithms were tested: 3D neural networks, feedforward neural networks, and random forest, each designed to either predict final socket shapes directly or predict prosthetist adaptations to generate socket shapes.

Result: All algorithms performed better when predicting required adaptations rather than directly predicting final socket shapes. The random forest model for adaptation prediction achieved the best performance with a median surface-to-surface distance error of 1.24mm (Q1: 1.03mm, Q3: 1.54mm) when compared to prosthetist-designed sockets.

Conclusion: AI approaches, particularly random forest models predicting prosthetist adaptations, can effectively assist in standardizing transtibial prosthetic socket design. The adaptation-based prediction approach consistently outperforms direct socket shape prediction across all tested algorithms, offering a promising path toward reducing variability in prosthetic socket quality.

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [23] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: This paper develops an automated workflow to detect and correct noisy features in self-driving laboratories for materials discovery, studying how dataset size, noise intensity, and feature distributions affect the ability to recover corrupted data.


<details>
  <summary>Details</summary>
Motivation: Self-driving laboratories (SDLs) for materials discovery face data quality issues where errors in input parameter capture can corrupt features used for modeling, compromising both current and future experimental campaigns. There is a need for systematic approaches to identify and correct these noisy features to maintain data integrity.

Method: The authors developed an automated workflow that: (1) systematically detects noisy features, (2) determines which sample-feature pairings can be corrected, and (3) recovers correct feature values. They conducted a systematic study examining the effects of dataset size, noise intensity, and feature value distribution on detectability and recoverability using k-nearest neighbors (kNN) imputation.

Result: High-intensity noise and large training datasets facilitate better detection and correction of noisy features. Low-intensity noise reduces detection and recovery performance but can be compensated by larger clean training datasets. Features with continuous and dispersed distributions show greater recoverability compared to discrete or narrow distributions. The framework provides a model-agnostic approach for data recovery.

Conclusion: The study demonstrates a practical framework for rational data recovery in materials discovery that is robust across different noise conditions, dataset sizes, and feature distributions. The work establishes benchmarks for kNN imputation in materials datasets and aims to enhance data quality and experimental precision in automated materials discovery systems.

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [24] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan NiÄkoviÄ,Radu Grosu*

Main category: cs.LG

TL;DR: TD-Interpreter is a visual question-answering tool that helps engineers understand complex timing diagrams by fine-tuning LLaVA multimodal language model and using synthetic data generation to overcome training data limitations.


<details>
  <summary>Details</summary>
Motivation: Engineers need assistance in understanding complex timing diagrams from third parties during design and verification processes, but existing tools lack the capability to provide interactive question-answering support for visual timing diagram analysis.

Method: Fine-tuned LLaVA (7B Multimodal Large Language Model) for timing diagram interpretation, developed a synthetic data generation workflow to align visual information with textual interpretation, and created a visual question-answer environment for interactive timing diagram analysis.

Result: TD-Interpreter significantly outperformed untuned GPT-4o by a large margin on evaluated benchmarks, demonstrating effective understanding and interpretation of timing diagrams through the multimodal learning approach.

Conclusion: The paper successfully demonstrates that fine-tuning lightweight multimodal models with synthetic data can create effective tools for timing diagram interpretation, providing engineers with valuable assistance in design and verification workflows.

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [25] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: This paper proposes a novel framework that integrates hyperbolic Transformers into reinforcement learning to improve multi-step reasoning tasks, achieving significant improvements in accuracy (32-45%) and computational efficiency (16-32%) compared to vanilla transformer-based RL methods.


<details>
  <summary>Details</summary>
Motivation: Conventional RL methods struggle with complex multi-step reasoning tasks due to credit assignment problems, high-dimensional state representations, and stability issues. While RL shows promise for multi-step reasoning by optimizing long-term rewards, there's a need for better approaches that can handle hierarchical structures in reasoning tasks more effectively.

Method: The paper introduces a framework that integrates hyperbolic Transformers into reinforcement learning. The approach leverages hyperbolic embeddings to model hierarchical structures effectively, combining recent advancements in Transformer architectures with hyperbolic geometry to address the limitations of conventional RL methods.

Result: The hyperbolic RL framework achieved substantial improvements over vanilla transformer-based RL: 32-44% accuracy improvement on FrontierMath benchmark, 43-45% on nonlinear optimal control benchmark, while reducing computational time by 16-32% on FrontierMath and 16-17% on nonlinear optimal control tasks.

Conclusion: The work demonstrates that hyperbolic Transformers have significant potential in reinforcement learning, especially for multi-step reasoning tasks involving hierarchical structures. The framework successfully addresses key challenges in RL-based reasoning while providing both accuracy and efficiency improvements.

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [26] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: DiffCarl is a diffusion-modeled reinforcement learning algorithm that optimizes multi-microgrid energy scheduling while considering carbon emissions and operational risks, achieving 2.3-30.1% lower operational costs and 28.7% lower carbon emissions compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Multi-microgrid systems face significant challenges in real-time energy scheduling and optimization under uncertainty due to growing renewable energy integration and increasing system complexity, requiring solutions that can handle uncertainty while considering environmental impact and operational risks.

Method: DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework, learning action distributions through a denoising generation process to enable adaptive energy scheduling that explicitly accounts for carbon emissions and operational risk under uncertainty.

Result: DiffCarl outperforms classic algorithms and state-of-the-art DRL solutions with 2.3-30.1% lower operational costs, achieves 28.7% lower carbon emissions compared to carbon-unaware variants, and reduces performance variability in dynamic microgrid environments.

Conclusion: DiffCarl represents a practical and forward-looking solution for intelligent microgrid operation, with flexible design enabling efficient adaptation to different system configurations and objectives, making it suitable for real-world deployment in evolving energy systems.

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [27] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe FrÃ©,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: This paper develops the mathematical foundations for Cartan Neural Networks, which use non-compact symmetric spaces U/H to create geometrically consistent and interpretable neural networks with group-theoretic structures.


<details>
  <summary>Details</summary>
Motivation: To develop a geometrically consistent theory of neural networks that is both covariant and interpretable by exploiting the mathematical structures of non-compact symmetric spaces and group theory.

Method: The paper expands on mathematical structures underpinning Cartan Neural Networks by detailing geometric properties of layers and analyzing how maps between layers interact with these structures using non-compact symmetric spaces U/H and group-theoretic frameworks.

Result: The work establishes the theoretical foundation that makes Cartan Neural Networks covariant and geometrically interpretable, demonstrating how group-theoretic structures can be successfully integrated into neural network architectures.

Conclusion: This paper, together with its twin paper, represents a first step towards a fully geometrically interpretable theory of neural networks that exploits group-theoretic structures, providing both mathematical rigor and practical feasibility for geometric neural networks.

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [28] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: The paper proposes Confidence optimization Probabilistic Encoding (CPE) to improve neural network classification by addressing distance measurement distortions caused by Gaussian noise in probabilistic encoding through confidence-aware mechanisms and L2 regularization.


<details>
  <summary>Details</summary>
Motivation: Probabilistic encoding with Gaussian noise enhances generalization but distorts point-based distance measurements in classification tasks, creating reliability issues that need to be addressed to maintain effective representation learning.

Method: The CPE method introduces two key strategies: (1) a confidence-aware mechanism to adjust distance calculations for consistency and reliability in probabilistic encoding classification, and (2) replacing KL divergence-based variance regularization with simpler L2 regularization to directly constrain variance without unreliable prior assumptions.

Result: Extensive experiments on natural language classification tasks show that CPE significantly improves performance and generalization on both BERT and RoBERTa models, demonstrating the method's model-agnostic effectiveness.

Conclusion: CPE successfully addresses the distance measurement reliability issues in probabilistic encoding while maintaining the benefits of uncertainty modeling, providing a model-agnostic solution that enhances both performance and generalization in neural network classification tasks.

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [29] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: SplitMeanFlow introduces a novel algebraic identity called Interval Splitting Consistency to learn average velocity fields for fast generative sampling, eliminating computational overhead and achieving 20x speedups in production speech synthesis.


<details>
  <summary>Details</summary>
Motivation: Generative models like Flow Matching suffer from computationally expensive iterative sampling processes. While MeanFlow addresses this by learning average velocity fields through differential identities, this approach is computationally demanding and requires complex JVP computations, limiting practical deployment.

Method: The paper derives a purely algebraic identity called Interval Splitting Consistency based on the additivity property of definite integrals. This establishes a self-referential relationship for average velocity fields across different time intervals without differential operators. SplitMeanFlow framework enforces this algebraic consistency as a direct learning objective.

Result: SplitMeanFlow eliminates JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step models achieve 20x speedups and have been successfully deployed in large-scale speech synthesis products like Doubao.

Conclusion: The algebraic approach proves to be a more general and efficient foundation for learning average velocity fields, with MeanFlow's differential identity being recovered as a limiting case. This enables practical deployment of fast generative models in production systems.

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [30] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: This paper presents a simple quantization-aware training approach for large language models that achieves superior accuracy compared to existing methods while adding less than 0.1% to training cost and requiring no additional operations beyond quantization itself.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of quantizing large language models to reduce inference latency, model size, and energy consumption while maintaining accuracy and compatibility with specialized inference accelerators, all within reasonable training time.

Method: The paper proposes a simple, end-to-end quantization-aware training approach that can be applied to activations, cache, and weights without introducing additional operations to the model beyond the quantization itself.

Result: The method outperforms leading published quantization methods by large margins on several modern benchmarks for both base and instruct model variants, while increasing total model training budget by less than 0.1%. The approach generalizes across different model architectures.

Conclusion: The proposed quantization-aware training approach provides an effective solution for model quantization that achieves superior performance with minimal training overhead and broad applicability across model architectures without requiring incompatible operations for inference accelerators.

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [31] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: This paper develops a Hierarchical Reinforcement Learning approach for adaptive lower-limb exoskeleton control that uses predictive sensor information to improve terrain adaptation and decision-making during walking across varied environments.


<details>
  <summary>Details</summary>
Motivation: Current exoskeleton control systems struggle with adaptive decision-making across varied terrains, particularly during uncertain conditions where terrain misclassification occurs frequently. There is a need for control strategies that can enhance mobility and autonomy for individuals with motor impairments by better adapting to different walking environments.

Method: The researchers implemented a Hierarchical Reinforcement Learning framework with two levels: a higher-level terrain strategy adaptation component and a lower-level predictive information provider using General Value Functions (GVFs). GVFs continuously learn to predict future sensor values from multiple wearable sensors (electromyography, pressure insoles, goniometers). Two methods were investigated for incorporating actual and predicted sensor signals into the policy network to improve exoskeleton control decisions.

Result: The addition of GVF-generated predictions significantly increased overall network accuracy. Performance improvements were observed across multiple terrain types including even ground, uneven ground, ramps (up and down), and turns - terrains that are frequently misclassified without predictive information. The predictive information particularly aided decision-making during uncertain conditions with high misclassification rates.

Conclusion: Predictive information from GVFs enhances exoskeleton control decision-making, especially in uncertain terrain conditions. This HRL approach provides new insights for developing safer exoskeleton systems that can better facilitate transitions and navigation across different walking environments, ultimately improving mobility assistance for individuals with motor impairments.

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [32] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,BlaÅ¾ StojanoviÄ,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: PyG 2.0 is a major update to the PyTorch Geometric framework that introduces significant improvements in scalability and real-world application capabilities for Graph Neural Networks, with enhanced support for heterogeneous/temporal graphs and large-scale graph learning problems.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for scalable graph neural network frameworks that can handle large-scale, real-world graph learning problems and support diverse application areas including heterogeneous and temporal graphs.

Method: Enhanced PyG framework architecture with support for heterogeneous and temporal graphs, scalable feature/graph stores, various optimizations, and comprehensive updates to enable efficient large-scale graph learning.

Result: PyG 2.0 successfully supports graph learning across a large variety of application areas, with particular strength in relational deep learning and large language modeling, while providing improved scalability for real-world applications.

Conclusion: PyG 2.0 represents a significant evolution of the PyTorch Geometric framework, establishing it as a leading solution for Graph Neural Networks with enhanced capabilities for handling diverse, large-scale graph learning challenges across multiple application domains.

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [33] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: This paper proposes a novel framework that strategically leverages bias instead of eliminating it to improve out-of-distribution domain adaptation, showing superior performance over existing invariant representation learning methods.


<details>
  <summary>Details</summary>
Motivation: Existing OOD domain adaptation methods focus on eliminating bias through invariant representation learning, but this paper questions whether bias should always be eliminated and explores when and how bias can be beneficial for model adaptation.

Method: The framework has two key components: (1) using invariance as guidance to extract predictive ingredients from bias, and (2) exploiting identified bias to estimate environmental conditions and explore appropriate bias-aware predictors to bridge environment gaps.

Result: Experiments on synthetic datasets and standard domain generalization benchmarks consistently show that the proposed method outperforms existing approaches, demonstrating robustness and adaptability.

Conclusion: The paper establishes that bias can be strategically leveraged rather than eliminated for better OOD domain adaptation, providing both theoretical foundations and practical improvements over current invariant representation learning methods.

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [34] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,BÃ¡lint MucsÃ¡nyi,Lenard Rommel,Thomas Christie,Lars KasÃ¼schke,Marvin PfÃ¶rtner,Philipp Hennig*

Main category: cs.LG

TL;DR: The paper introduces laplax, an open-source Python package for Laplace approximations in deep neural networks using JAX, designed to facilitate Bayesian neural network research and uncertainty quantification with a modular, functional architecture.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and efficient methods to quantify weight-space uncertainty in deep neural networks, enabling Bayesian tools like predictive uncertainty and model selection, while providing researchers with a flexible framework for rapid prototyping and experimentation in Bayesian deep learning.

Method: Development of laplax, a Python package built on JAX with a modular and purely functional architecture, minimal external dependencies, designed specifically for performing Laplace approximations on neural networks.

Result: A researcher-friendly, open-source software package that provides flexible tools for Laplace approximations, enabling efficient uncertainty quantification and Bayesian analysis in deep neural networks.

Conclusion: The laplax package successfully provides an accessible and efficient platform for Bayesian neural network research, uncertainty quantification, and development of improved Laplace approximation techniques, contributing to the advancement of uncertainty-aware deep learning methods.

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [35] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: This paper introduces CGF-LLM, a novel architecture that combines GPT-2 with fuzzy time series and causal graphs to predict multivariate time series by converting numerical data into interpretable textual representations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage Large Language Models for time series forecasting by addressing the challenge of making numerical time series data interpretable for LLMs, enabling both semantic understanding and structural insight through fuzzification and causal analysis.

Method: The method involves a novel CGF-LLM framework that uses GPT-2 combined with fuzzy time series (FTS) and causal graph analysis to convert numerical time series into interpretable textual representations through parallel fuzzification and causal analysis processes.

Result: The proposed LLM-based time series forecasting model demonstrated effectiveness across four different multivariate time series datasets, showing that the textual representation provides a more interpretable view of complex time series dynamics.

Conclusion: CGF-LLM represents the first architecture combining GPT-2, fuzzy time series, and causal graphs for multivariate time series forecasting, successfully enabling interpretable predictions and opening promising future research directions in LLM-based time series forecasting.

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [36] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: This paper extends Bilevel Local Operator Learning (BiLO) to Bayesian inference for PDE-constrained optimization, using gradient-based MCMC and low-rank adaptation to efficiently sample PDE parameters from posterior distributions while avoiding high-dimensional neural network weight sampling.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification and inverse problems governed by PDEs are crucial in scientific and engineering applications, but existing Bayesian neural network methods face challenges in high-dimensional weight sampling and require prior distribution specification on neural network solutions.

Method: The approach uses a bilevel framework: lower level trains a network to approximate local solution operators by minimizing local operator loss, while upper level samples PDE parameters from posterior distribution using gradient-based MCMC methods and low-rank adaptation (LoRA). The method enforces strong PDE constraints and allows uncertainty to propagate naturally from data through PDE constraints.

Result: The method delivers accurate parameter inference and uncertainty quantification with high computational efficiency across various PDE models. Analysis shows direct links between lower-level problem tolerance and uncertainty quantification accuracy, with improved accuracy for both parameter inference and uncertainty quantification compared to existing methods.

Conclusion: The proposed BiLO extension to Bayesian inference successfully bypasses high-dimensional neural network weight sampling challenges while maintaining strong PDE constraint enforcement, resulting in accurate and computationally efficient uncertainty quantification for PDE-constrained optimization problems.

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [37] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: This paper proposes an interpretable alternative to offline reinforcement learning for deriving treatment policies by using tree-based models to identify the most frequently chosen actions in patient states, addressing interpretability and evaluation challenges in safety-critical healthcare domains.


<details>
  <summary>Details</summary>
Motivation: Offline RL faces significant challenges in safety-critical domains due to black-box policy interpretability issues and unreliable off-policy evaluation methods that are sensitive to deviations from data-collecting behavior policies, especially when using importance sampling techniques.

Method: The authors propose using tree-based models to estimate behavior policies and derive treatment policies from the most frequently chosen actions in each patient state. The tree structure provides natural state grouping and interpretability by design, while controlling the number of actions considered enables reliable off-policy evaluation by maintaining overlap with the behavior policy.

Result: The proposed approach was validated on real-world healthcare datasets for rheumatoid arthritis and sepsis care, demonstrating that the derived policies can outperform current clinical practice while maintaining interpretability and standardizing frequent treatment patterns that capture collective clinical judgment.

Conclusion: The framework offers a pragmatic alternative to offline RL that addresses key limitations in healthcare applications by providing interpretable treatment policies that can be reliably evaluated and potentially improve upon existing clinical practices, making it suitable for safety-critical medical decision-making.

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [38] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: This paper benchmarks foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) for synthetic tabular data generation in low-data settings, revealing significant privacy risks from verbatim seed row repetition, and proposes prompt tweaks to improve the privacy-utility tradeoff.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art generative models require large datasets and can overfit in low-data settings where synthetic data is most needed. While foundation models using in-context learning (ICL) avoid retraining, they introduce new privacy risks by repeating seed rows verbatim - a problem understudied in tabular synthesis where single rows may identify individuals.

Method: The authors conduct the first comprehensive benchmark comparing three foundation models against four baselines on 35 real-world tables from health, finance, and policy domains. They evaluate statistical fidelity, downstream utility, and membership inference leakage, then perform a factorial study testing prompt modifications (batch size, temperature, summary statistics) to improve privacy-utility tradeoffs.

Result: Foundation models consistently show the highest privacy risk, with LLaMA 3.3 70B reaching 54 percentage points higher true-positive rate than the safest baseline. However, three zero-cost prompt tweaks can reduce worst-case AUC by 14 points and rare-class leakage by up to 39 points while maintaining over 90% statistical fidelity.

Conclusion: The study provides a practical guide for safer low-data synthesis with foundation models, demonstrating that while these models pose significant privacy risks, simple prompt modifications can substantially improve the privacy-utility frontier without sacrificing statistical quality.

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [39] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik RÃ¶Ãle,Daniel Cremers,Torsten SchÃ¶n*

Main category: cs.LG

TL;DR: This paper proposes an ensemble-based defense architecture to protect Deep Reinforcement Learning models in autonomous driving from adversarial attacks, achieving significant improvements in robustness with over 213% increase in mean reward and 82% reduction in collision rates.


<details>
  <summary>Details</summary>
Motivation: While DRL has shown success across various domains including autonomous driving, there is a critical research gap regarding the robustness of DRL models against adversarial attacks, particularly the integration of multiple defense mechanisms in autonomous driving scenarios.

Method: The authors propose a novel ensemble-based defense architecture that integrates multiple defense mechanisms to mitigate adversarial attacks in autonomous driving scenarios, building upon existing defenses like adversarial training and distillation.

Result: Under FGSM attacks, the ensemble method achieved a 213% improvement in mean reward (from 5.87 to 18.38) and an 82% reduction in mean collision rate (from 0.50 to 0.09) in highway and merge scenarios, outperforming all standalone defense strategies.

Conclusion: The proposed ensemble-based defense architecture significantly enhances the robustness of DRL models in autonomous driving against adversarial attacks, demonstrating superior performance compared to individual defense mechanisms.

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [40] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: This paper proposes Knowledge Distillation (KD) as a novel method for electronic nose sensor drift compensation, achieving up to 18% improvement in accuracy over existing methods through rigorous statistical validation on the UCI Gas Sensor Array Drift Dataset.


<details>
  <summary>Details</summary>
Motivation: Electronic nose systems suffer from sensor drift due to environmental changes and sensor aging, which degrades gas classification performance in real-world deployment. Previous studies lacked robust statistical validation and may overcompensate for drift, losing important class-related variance.

Method: The authors designed two domain adaptation tasks using the UCI Gas Sensor Array Drift Dataset and systematically tested three methods across 30 random test set partitions: (1) a novel Knowledge Distillation (KD) method, (2) Domain Regularized Component Analysis (DRCA) as benchmark, and (3) a hybrid KD-DRCA method.

Result: Knowledge Distillation consistently outperformed both DRCA and KD-DRCA methods, achieving up to 18% improvement in accuracy and 15% improvement in F1-score for sensor drift compensation across the experimental validation.

Conclusion: This is the first application of Knowledge Distillation for electronic nose drift mitigation, significantly outperforming the previous state-of-the-art DRCA method and enhancing the reliability of sensor drift compensation in real-world environments.

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [41] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: The paper proposes ZORMS-LfD, a zeroth-order optimization method for learning optimal control problems from expert demonstrations that doesn't require gradient computation and works for both continuous and discrete time systems with constraints.


<details>
  <summary>Details</summary>
Motivation: Existing first-order methods for learning from demonstrations require gradient computation of costs, constraints, dynamics, and learning loss, which assumes smoothness of the learning-loss landscape. Most methods are also limited to discrete time systems, with constrained continuous-time problems receiving insufficient attention.

Method: ZORMS-LfD (Zeroth-Order Random Matrix Search for Learning from Demonstrations) uses gradient-free optimization to learn costs, constraints, and dynamics of constrained optimal control problems from expert demonstrations, working for both continuous and discrete time systems without requiring smoothness assumptions.

Result: ZORMS-LfD matches or exceeds state-of-the-art methods in learning loss and compute time. On unconstrained continuous-time problems, it achieves similar performance with over 80% reduction in compute time. On constrained continuous-time problems, it outperforms the commonly used gradient-free Nelder-Mead method.

Conclusion: ZORMS-LfD provides an effective gradient-free alternative for learning from demonstrations in optimal control, particularly excelling in constrained continuous-time problems where specialized methods are lacking, while offering significant computational efficiency improvements.

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [42] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: This paper discovers that RL fine-tuning of large language models naturally creates sparse parameter updates, modifying only 5-30% of weights while leaving most parameters unchanged, and shows this sparse subnetwork is sufficient for achieving full model performance.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the common assumption that RL fine-tuning requires updating most of a model's parameters, investigating whether parameter updates during RL alignment are actually sparse and what this means for understanding how RL adapts LLMs.

Method: The authors analyze parameter update patterns across multiple RL algorithms (PPO, DPO, SimPO, PRIME) and model families (OpenAI, Meta, open-source LLMs), examining which weights change during fine-tuning and testing whether fine-tuning only the sparse subnetwork can recover full performance.

Result: RL fine-tuning consistently modifies only 5-30% of model weights across different algorithms and models, with substantial overlap in updated subnetworks across different seeds, datasets, and algorithms. Fine-tuning only this sparse subnetwork achieves performance nearly identical to full fine-tuning.

Conclusion: RL adapts models by focusing on a small, consistently updated subnetwork rather than shifting all weights, likely because RL operates near the model's original distribution. This finding enables more efficient RL methods and provides new insights into model adaptation through the lens of the lottery ticket hypothesis.

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [43] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: This tutorial provides a comprehensive introduction to probabilistic graphical modeling, covering the theoretical foundations, learning algorithms, and inference methods that combine probability theory with graph theory for machine learning applications.


<details>
  <summary>Details</summary>
Motivation: To provide a concise educational introduction to probabilistic graphical modeling, which bridges probability theory and graph theory to create powerful tools for machine learning under uncertainty. The motivation is to make this elegant mathematical framework accessible through a structured tutorial approach.

Method: The tutorial employs a structured educational approach covering: (1) foundational review of probability and graph theory, (2) representation of multivariate distributions using intuitive graph visualizations, (3) algorithms for learning model parameters and graphical structures from data, and (4) both exact and approximate inference algorithms.

Result: The tutorial successfully presents the three dominant themes of probabilistic graphical modeling: visual representation of probability distributions through graphs, data-driven learning algorithms for parameters and structures, and comprehensive inference methods for probabilistic reasoning.

Conclusion: Probabilistic graphical modeling provides a powerful and elegant framework that combines probability distributions with graph theory to create compact yet expressive representations for generative modeling, enabling effective probabilistic reasoning and decision-making under uncertainty.

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [44] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-LÃ³pez,Ricardo A. Morell-Rodriguez,SebastiÃ¡n O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: An AI-powered monkeypox diagnostic tool was developed using MobileNetV2 on NVIDIA Jetson Orin Nano, achieving 93.07% F1-Score while optimized with TensorRT for reduced power consumption and faster inference, deployed with Wi-Fi hotspot for mobile access in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: The need for rapid diagnosis of infectious diseases like monkeypox in resource-constrained environments where effective containment and treatment are crucial, but traditional diagnostic methods may be limited or unavailable.

Method: Developed an AI diagnostic tool using pre-trained MobileNetV2 architecture for binary classification, trained on the open-source Monkeypox Skin Lesion Dataset, optimized with TensorRT framework for FP32, FP16, and INT8 formats, and deployed on NVIDIA Jetson Orin Nano with Wi-Fi Access Point and web-based interface.

Result: Achieved 93.07% F1-Score with well-balanced precision and recall. TensorRT optimization reduced model size, increased inference speed, and lowered power consumption by approximately a factor of two while maintaining original accuracy. The system enables direct image upload and analysis through mobile devices via Wi-Fi connectivity.

Conclusion: The diagnostic tool represents an efficient, scalable, and energy-conscious solution for monkeypox diagnosis in underserved regions, with significant potential for broader adoption in low-resource healthcare settings due to its optimized performance and accessible deployment architecture.

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [45] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-LÃ³pez,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: This study develops an AI-driven skin cancer diagnostic tool using MobileNetV2 and TensorRT optimization for deployment on NVIDIA Jetson Orin Nano, achieving 87.18% F1-score while maintaining energy efficiency for resource-limited healthcare settings.


<details>
  <summary>Details</summary>
Motivation: Skin cancer is prevalent and preventable but early detection remains challenging in resource-limited settings with scarce access to specialized healthcare. There is a need for accessible AI diagnostic tools that can operate efficiently on edge devices in underserved regions.

Method: The study employs transfer learning with MobileNetV2 architecture for binary classification of skin lesions into "Skin Cancer" and "Other" categories. TensorRT framework is used to compress and optimize the model for deployment on NVIDIA Jetson Orin Nano, balancing performance with energy efficiency.

Result: The optimized model achieved an F1-Score of 87.18% with 93.18% precision and 81.91% recall. Post-compression results showed model size reduction up to 0.41, improved inference speed and throughput, and decreased energy consumption up to 0.93 in INT8 precision while maintaining diagnostic performance.

Conclusion: The study validates the feasibility of deploying high-performing, energy-efficient diagnostic tools on resource-constrained edge devices. The methodologies have broader applications beyond skin cancer detection and demonstrate the potential of optimized AI systems to revolutionize healthcare diagnostics, bridging the gap between advanced technology and underserved regions.

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [46] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: ARIA is an LLM agent framework that continuously learns updated domain knowledge at test time through self-assessment, human interaction, and knowledge repository management, achieving better adaptability in dynamic environments like regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents struggle in environments with frequently changing rules and domain knowledge (e.g., regulatory compliance, user risk screening) because offline fine-tuning and standard prompting cannot effectively adapt to new knowledge during actual operation.

Method: ARIA uses structured self-dialogue to assess uncertainty and identify knowledge gaps, proactively requests targeted explanations from human experts, maintains a timestamped knowledge repository, and resolves conflicting knowledge through comparisons and clarification queries.

Result: ARIA shows significant improvements in adaptability and accuracy compared to baselines using offline fine-tuning and existing self-improving agents on customer due diligence name screening and dynamic knowledge tasks. Successfully deployed in TikTok Pay serving 150+ million monthly active users.

Conclusion: ARIA demonstrates practical effectiveness for operational use in rapidly evolving environments, proving that continuous test-time learning through human-agent interaction can significantly improve LLM agent performance in dynamic domains.

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [47] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: This paper proposes SADA (Stability-guided Adaptive Diffusion Acceleration), a novel method that accelerates diffusion model sampling by adaptively applying sparsity decisions based on stability criteria, achieving 1.8x+ speedups with minimal quality loss across multiple models and modalities.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high computational costs due to iterative sampling and quadratic attention costs. Existing training-free acceleration methods reduce computation but show low faithfulness because they don't consider varying denoising trajectories for different prompts and ignore the underlying ODE formulation and numerical solutions.

Method: SADA unifies step-wise and token-wise sparsity decisions through a single stability criterion. It adaptively allocates sparsity based on sampling trajectory and introduces principled approximation schemes that leverage precise gradient information from numerical ODE solvers to accelerate ODE-based generative models including diffusion and flow-matching.

Result: Comprehensive evaluations on SD-2, SDXL, and Flux with EDM and DPM++ solvers show consistent â¥1.8x speedups with minimal fidelity degradation (LPIPS â¤0.10, FID â¤4.5). SADA significantly outperforms prior methods and adapts seamlessly to ControlNet and MusicLDM, achieving 1.8x speedup on MusicLDM with ~0.01 spectrogram LPIPS.

Conclusion: SADA provides an effective solution for accelerating diffusion model sampling while maintaining high fidelity by considering both prompt-specific denoising trajectories and ODE numerical solutions. The method demonstrates strong generalization across different models, solvers, and modalities.

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [48] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore is an unsupervised coreset selection framework that identifies the most informative training samples for neural operators without requiring ground-truth PDE solutions, achieving up to 78% increase in training efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Neural operators for solving PDEs face two main bottlenecks: they require significant amounts of training data and expensive labeled data from numerical simulations. Current methods need both large datasets and costly ground-truth solutions to train effectively.

Method: PICore uses a physics-informed loss function to select unlabeled inputs based on their potential contribution to operator learning. It identifies a compact subset of the most informative samples, then only simulates those selected samples using numerical solvers to generate labels, before training the neural operator on this reduced labeled dataset.

Result: Across four diverse PDE benchmarks and multiple coreset selection strategies, PICore achieves up to 78% average increase in training efficiency compared to supervised coreset selection methods while maintaining minimal changes in accuracy. The method significantly reduces both annotation costs and training time.

Conclusion: PICore successfully addresses both major bottlenecks in neural operator training by providing an unsupervised approach to identify the most valuable training samples, leading to substantial efficiency gains without sacrificing model performance. This makes neural operator training more practical and cost-effective for PDE solving applications.

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [49] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: This paper proposes a novel diffusion-based counterfactual explanation framework for network intrusion detection systems (NIDS) that provides actionable explanations to improve understanding of deep learning-based attack detection decisions and enable effective countermeasures.


<details>
  <summary>Details</summary>
Motivation: Modern NIDS use complex deep learning models that act as "black boxes," making it difficult to understand detection decisions, build trust in the system, and develop timely countermeasures against attacks. Existing explainable AI methods don't provide explanations that can be easily converted into actionable countermeasures.

Method: The authors developed a diffusion-based counterfactual explanation framework that generates minimal and diverse counterfactual explanations for network intrusion attacks. They also create global rules by summarizing counterfactual explanations that work at both instance and global levels.

Result: The proposed method outperformed existing counterfactual explanation algorithms on 3 modern network intrusion datasets by providing minimal, diverse explanations more efficiently with reduced generation time. The global counterfactual rules effectively filtered out incoming attack queries, demonstrating practical utility for intrusion detection and defense.

Conclusion: The diffusion-based counterfactual explanation framework successfully addresses the opacity problem in deep learning-based NIDS by providing actionable explanations that can be converted into effective defense mechanisms. The method shows superior performance in generating efficient counterfactual explanations and demonstrates practical value through global rule creation for attack filtering.

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [50] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: This paper proposes Met2Net, an implicit two-stage training method for weather prediction that uses separate encoders/decoders for each weather variable and a translator to capture inter-variable interactions, achieving state-of-the-art performance with 28.82% and 23.39% MSE reduction for temperature and humidity predictions respectively.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end weather prediction methods face representation inconsistency in multivariable integration and struggle to capture dependencies between variables in complex weather systems. Existing two-stage approaches from multimodal models are suboptimal due to inconformity in training tasks between stages, creating a need for better multivariable weather prediction methods.

Method: The paper proposes an implicit two-stage training approach with separate encoders and decoders for each weather variable. Stage 1: Translator is frozen while Encoders and Decoders learn a shared latent space. Stage 2: Encoders and Decoders are frozen while the Translator captures inter-variable interactions for prediction. A self-attention mechanism is introduced for multivariable fusion in the latent space.

Result: The method achieves state-of-the-art performance in weather prediction experiments. Specifically, it reduces Mean Squared Error (MSE) by 28.82% for near-surface air temperature predictions and by 23.39% for relative humidity predictions compared to existing methods.

Conclusion: The proposed implicit two-stage training method with separate encoders/decoders and a translator component effectively addresses representation inconsistency and variable dependency issues in weather prediction, demonstrating significant improvements over existing approaches and establishing new state-of-the-art performance.

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [51] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: This paper presents a method to efficiently deploy multimodal large language models (MLLMs) for video content moderation by transforming generative MLLMs into classifiers and using a router-ranking cascade system to reduce computational costs while improving moderation performance.


<details>
  <summary>Details</summary>
Motivation: Traditional video classification models struggle with complex content moderation scenarios like implicit harmful content and contextual ambiguity. While MLLMs offer better cross-modal reasoning and contextual understanding, their high computational cost and the challenge of adapting generative models for classification tasks prevent industrial adoption.

Method: The authors propose two key components: (1) An efficient method to transform generative MLLMs into multimodal classifiers using minimal discriminative training data, and (2) A router-ranking cascade system that integrates MLLMs with a lightweight router model to enable industry-scale deployment while managing computational costs.

Result: Offline experiments show 66.50% improvement in F1 score over traditional classifiers while using only 2% of the fine-tuning data. Online evaluations demonstrate 41% increase in automatic content moderation volume, with the cascade system reducing computational cost to only 1.5% of direct full-scale deployment.

Conclusion: The proposed MLLM-based approach successfully addresses the limitations of traditional video content moderation by significantly improving performance while maintaining computational efficiency through the router-ranking cascade system, making it viable for industrial-scale deployment.

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [52] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: This paper proposes a joint rate-utility optimization method for dataset distillation that achieves up to 170Ã greater compression than standard methods while maintaining comparable accuracy by using optimizable latent codes and lightweight decoders.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods either focus on maximizing performance under fixed storage budgets or pursue suitable synthetic data representations for redundancy removal, but fail to jointly optimize both rate (compression) and utility (performance) objectives simultaneously.

Method: The authors parameterize synthetic samples as optimizable latent codes that are decoded by extremely lightweight networks. They estimate Shannon entropy of quantized latents as the rate measure and use existing distillation loss as utility measure, trading them off via a Lagrange multiplier. They also introduce "bits per class" (bpc) as a precise storage metric.

Result: On CIFAR-10, CIFAR-100, and ImageNet-128 datasets, the method achieves up to 170Ã greater compression than standard distillation methods at comparable accuracy. The approach consistently establishes better rate-utility trade-offs across diverse bpc budgets, distillation losses, and backbone architectures.

Conclusion: The proposed joint rate-utility optimization approach successfully addresses the limitations of existing dataset distillation methods by simultaneously optimizing compression and performance, demonstrating significant improvements in storage efficiency while maintaining model accuracy across multiple datasets and experimental settings.

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [53] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: P3SL is a personalized privacy-preserving split learning framework that enables heterogeneous edge devices to participate in federated machine learning while maintaining customized privacy protection and optimizing resource usage through personalized split points determined via bi-level optimization.


<details>
  <summary>Details</summary>
Motivation: Existing split learning frameworks for heterogeneous environments fail to address personalized privacy requirements and local model customization under varying environmental conditions, while also neglecting the diverse computational resources and privacy needs of different edge devices.

Method: The paper proposes P3SL with two key components: (1) a personalized sequential split learning pipeline that allows customized privacy protection and personalized local models, and (2) a bi-level optimization technique that enables clients to determine optimal split points without sharing sensitive information with the server.

Result: P3SL was implemented and evaluated on a testbed with 7 heterogeneous devices (4 Jetson Nano P3450, 2 Raspberry Pis, 1 laptop) using diverse model architectures and datasets under varying environmental conditions, demonstrating the framework's effectiveness in balancing energy consumption, privacy protection, and model accuracy.

Conclusion: P3SL successfully addresses the limitations of existing split learning approaches by providing personalized privacy-preserving capabilities for heterogeneous edge devices while maintaining high model accuracy and optimizing resource utilization through intelligent split point determination.

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


### [54] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: This paper proposes a data-centric approach to Green Federated Learning that reduces environmental impact by optimizing data selection and federated node choices, demonstrating promising results in time series classification tasks.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of AI/ML has significant environmental impact through energy consumption and carbon emissions. While Federated Learning offers advantages in data privacy and transmission costs, it introduces challenges related to data heterogeneity and environmental impact that need to be addressed.

Method: The methodology involves: (1) analyzing characteristics of federated datasets, (2) selecting optimal data subsets based on quality metrics, (3) choosing federated nodes with lowest environmental impact, and (4) developing an interactive recommendation system that optimizes FL configurations through data reduction.

Result: The approach demonstrated promising results in reducing environmental impact when applied to time series classification tasks, showing that data-centric factors like quality and volume significantly influence both FL training performance and carbon emissions.

Conclusion: A data-centric approach to Green Federated Learning can effectively minimize environmental impact by optimizing data selection and node choices, contributing to the advancement of Green AI through reduced carbon emissions while maintaining training performance.

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [55] [DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs](https://arxiv.org/abs/2507.17245)
*Haolin Jin,Mengbai Xiao,Yuan Yuan,Xiao Zhang,Dongxiao Yu,Guanghui Zhang,Haoliang Wang*

Main category: cs.LG

TL;DR: DistrAttention is a novel self-attention mechanism that addresses the quadratic complexity problem in Transformers by grouping data on embedding dimensions using locality-sensitive hashing, achieving 37% speedup over FlashAttention-2 while maintaining full contextual information and flexibility.


<details>
  <summary>Details</summary>
Motivation: The core self-attention mechanism in Transformers has quadratic time complexity relative to input sequence length, which limits scalability. Existing optimization approaches either sacrifice full-contextual information or lack flexibility, creating a need for an efficient attention mechanism that preserves both full context and flexibility.

Method: DistrAttention groups data on embedding dimensionality using a lightweight sampling and fusion method that exploits locality-sensitive hashing to group similar data. A block-wise grouping framework is designed to limit errors from locality-sensitive hashing, and block size optimization enables integration with FlashAttention-2 for high GPU performance.

Result: DistrAttention achieves 37% faster computation than FlashAttention-2 for self-attention calculation. In ViT inference, it demonstrates the fastest speed and highest accuracy among approximate self-attention mechanisms. For Llama3-1B, it achieves the lowest inference time with only 1% accuracy loss.

Conclusion: DistrAttention successfully addresses the scalability limitations of Transformer self-attention by providing an efficient mechanism that maintains full contextual information and flexibility while delivering significant performance improvements across different model architectures and tasks.

Abstract: The Transformer architecture has revolutionized deep learning, delivering the
state-of-the-art performance in areas such as natural language processing,
computer vision, and time series prediction. However, its core component,
self-attention, has the quadratic time complexity relative to input sequence
length, which hinders the scalability of Transformers. The exsiting approaches
on optimizing self-attention either discard full-contextual information or lack
of flexibility. In this work, we design DistrAttention, an effcient and
flexible self-attention mechanism with the full context. DistrAttention
achieves this by grouping data on the embedding dimensionality, usually
referred to as $d$. We realize DistrAttention with a lightweight sampling and
fusion method that exploits locality-sensitive hashing to group similar data. A
block-wise grouping framework is further designed to limit the errors
introduced by locality sensitive hashing. By optimizing the selection of block
sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining
high-performance on modern GPUs. We evaluate DistrAttention with extensive
experiments. The results show that our method is 37% faster than
FlashAttention-2 on calculating self-attention. In ViT inference,
DistrAttention is the fastest and the most accurate among approximate
self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the
lowest inference time with only 1% accuray loss.

</details>


### [56] [Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions](https://arxiv.org/abs/2507.17255)
*Songxuan Shi*

Main category: cs.LG

TL;DR: This paper investigates the generative capabilities of Autoencoders and proposes a new VAE-like training method that connects VAEs and VQ-VAEs through clustering centers, demonstrating smooth interpolation but revealing limitations when multiple vectors lead to discrete representations without semantic learning.


<details>
  <summary>Details</summary>
Motivation: Standard Autoencoders have limited generative potential due to undefined regions in their encoding space. The authors aim to enhance AE generative capabilities and establish theoretical connections between VAEs and VQ-VAEs through a unified framework that addresses encoding space compactness issues.

Method: The authors propose a reformulated VAE-like training method that introduces clustering centers to enhance data compactness in latent space without using traditional KL divergence or reparameterization techniques. They extend this approach to multiple learnable vectors and analyze the progression toward VQ-VAE-like models in continuous space.

Result: Experiments on MNIST, CelebA, and FashionMNIST datasets demonstrate smooth interpolative transitions, though blurriness persists. The method shows natural progression toward VQ-VAE-like behavior when extended to multiple vectors. However, when encoders output multiple vectors, the model degenerates into a discrete Autoencoder that combines image fragments without learning proper semantic representations.

Conclusion: The study reveals that encoding space compactness and dispersion play critical roles in generative modeling. The proposed framework provides new insights into the intrinsic connections between VAEs and VQ-VAEs, offering fresh perspectives on their design principles and fundamental limitations in generative tasks.

Abstract: This paper explores the generative capabilities of Autoencoders (AEs) and
establishes connections between Variational Autoencoders (VAEs) and Vector
Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training
framework. We demonstrate that AEs exhibit generative potential via latent
space interpolation and perturbation, albeit limited by undefined regions in
the encoding space. To address this, we propose a new VAE-like training method
that introduces clustering centers to enhance data compactness and ensure
well-defined latent spaces without relying on traditional KL divergence or
reparameterization techniques. Experimental results on MNIST, CelebA, and
FashionMNIST datasets show smooth interpolative transitions, though blurriness
persists. Extending this approach to multiple learnable vectors, we observe a
natural progression toward a VQ-VAE-like model in continuous space. However,
when the encoder outputs multiple vectors, the model degenerates into a
discrete Autoencoder (VQ-AE), which combines image fragments without learning
semantic representations. Our findings highlight the critical role of encoding
space compactness and dispersion in generative modeling and provide insights
into the intrinsic connections between VAEs and VQ-VAEs, offering a new
perspective on their design and limitations.

</details>


### [57] [Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance](https://arxiv.org/abs/2507.17273)
*Rishi Parekh,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.LG

TL;DR: This paper presents a framework that combines Knowledge Graphs (KGs) and Large Language Model (LLM)-based agents to automatically analyze complex warehouse simulation data, identifying bottlenecks and inefficiencies through iterative reasoning and self-correction, achieving near-perfect performance in operational analysis.


<details>
  <summary>Details</summary>
Motivation: Analyzing large, complex output datasets from Discrete Event Simulations (DES) of warehouse operations to identify bottlenecks and inefficiencies is a critical yet challenging task that often demands significant manual effort or specialized analytical tools, creating a need for more automated and intuitive analysis methods.

Method: The framework transforms raw DES data into semantically rich Knowledge Graphs capturing relationships between simulation events and entities. An LLM-based agent performs iterative reasoning by generating interdependent sub-questions, creating Cypher queries for KG interaction, extracting information, and using self-reflection to correct errors in an adaptive and iterative process.

Result: The approach outperforms baseline methods for warehouse bottleneck identification when tested with equipment breakdowns and process irregularities. It achieves near-perfect pass rates for operational questions in pinpointing inefficiencies and demonstrates superior diagnostic ability for complex investigative questions in uncovering subtle, interconnected issues.

Conclusion: This work successfully bridges simulation modeling and AI (KG+LLM), offering a more intuitive method for actionable insights that reduces time-to-insight and enables automated warehouse inefficiency evaluation and diagnosis, providing a significant improvement over traditional manual analysis approaches.

Abstract: Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.

</details>


### [58] [Decentralized Federated Learning of Probabilistic Generative Classifiers](https://arxiv.org/abs/2507.17285)
*Aritz PÃ©rez,Carlos Echegoyen,GuzmÃ¡n SantafÃ©*

Main category: cs.LG

TL;DR: This paper proposes a decentralized federated learning approach for probabilistic generative classifiers where nodes share local statistics with neighbors to collaboratively learn a global model without a central server.


<details>
  <summary>Details</summary>
Motivation: Current federated learning relies on central servers, but decentralized architectures are needed where users can collaborate directly without centralized coordination while preserving data privacy and handling heterogeneous data distributions.

Method: A decentralized framework where nodes in a communication network share local statistics with neighboring nodes, aggregate neighbors' information, and iteratively update local probabilistic generative classifiers that progressively converge to a global model.

Result: Extensive experiments show the algorithm consistently converges to globally competitive models across various network topologies, network sizes, local dataset sizes, and extreme non-i.i.d. data distributions.

Conclusion: The proposed decentralized federated learning approach successfully enables collaborative learning of probabilistic generative classifiers without central coordination, demonstrating robust performance across diverse experimental conditions.

Abstract: Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.

</details>


### [59] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch is a token-level hybrid decoding framework that accelerates Chain-of-Thought reasoning by dynamically switching between small and large language models based on confidence thresholds, achieving up to 85% latency reduction with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in large language models introduces substantial computational overhead due to autoregressive decoding over long sequences. Existing acceleration methods like speculative decoding have limited speedup when model agreement is low and fail to exploit small models' advantages in producing concise reasoning.

Method: R-Stitch uses a confidence-based hybrid approach where a small language model generates tokens by default, and delegates to a large language model only when confidence falls below a threshold. This avoids full-sequence rollback and selectively uses the LLM for uncertain steps while maintaining efficiency.

Result: Experiments on math reasoning benchmarks show R-Stitch achieves up to 85% reduction in inference latency with negligible accuracy drop, demonstrating practical effectiveness in accelerating CoT reasoning.

Conclusion: R-Stitch provides an effective, model-agnostic, training-free solution for accelerating Chain-of-Thought inference that is compatible with standard decoding pipelines and preserves both efficiency and answer quality through selective model switching.

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [60] [Confounded Causal Imitation Learning with Instrumental Variables](https://arxiv.org/abs/2507.17309)
*Yan Zeng,Shenglan Nie,Feng Xie,Libo Huang,Peng Wu,Zhi Geng*

Main category: cs.LG

TL;DR: This paper proposes C2L (Confounded Causal Imitation Learning), a two-stage framework that uses instrumental variables to address unmeasured confounders in imitation learning, enabling more accurate policy estimation from demonstrations.


<details>
  <summary>Details</summary>
Motivation: Imitation learning from demonstrations suffers from confounding effects of unmeasured variables that influence both states and actions, leading to biased policy estimation. Existing methods fail to handle confounders that affect actions across multiple timesteps rather than just immediate dependencies.

Method: A two-stage framework: (1) IV identification stage using a testing criterion based on pseudo-variables to identify valid instrumental variables with sufficient and necessary conditions, (2) Policy optimization stage offering two approaches - simulator-based and offline learning - using the identified IV to learn unbiased policies.

Result: Extensive experiments demonstrated the effectiveness of both valid IV identification and policy learning components of the proposed framework, showing improved performance over existing imitation learning methods.

Conclusion: The C2L model successfully addresses confounding issues in imitation learning by leveraging instrumental variables, providing a principled approach to handle multi-timestep confounders and enabling more accurate policy estimation from demonstration data.

Abstract: Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.

</details>


### [61] [EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents](https://arxiv.org/abs/2507.17311)
*Zijie Guo,Jiong Wang,Xiaoyu Yue,Wangxu Wei,Zhe Jiang,Wanghan Xu,Ben Fei,Wenlong Zhang,Xinyu Gu,Lijing Cheng,Jing-Jia Luo,Chao Li,Yaqiang Wang,Tao Chen,Wanli Ouyang,Fenghua Ling,Lei Bai*

Main category: cs.LG

TL;DR: EarthLink is the first AI agent designed as an interactive copilot for Earth scientists that automates end-to-end research workflows and demonstrates analytical competency comparable to human junior researchers in climate change studies.


<details>
  <summary>Details</summary>
Motivation: Modern Earth science faces significant bottlenecks due to vast, fragmented, and complex Earth system data coupled with increasingly sophisticated analytical demands, which hinders rapid scientific discovery and requires automated solutions.

Method: Development of EarthLink, an AI agent with interactive copilot capabilities that automates research workflows from planning and code generation to multi-scenario analysis, featuring dynamic learning from user interactions and continuous capability refinement through feedback loops.

Result: EarthLink successfully performed core climate change scientific tasks including model-observation comparisons and complex phenomena diagnosis. Multi-expert evaluation showed it produced scientifically sound analyses with analytical competency rated comparable to specific aspects of human junior researcher workflows.

Conclusion: EarthLink represents a pivotal advancement towards an efficient, trustworthy, and collaborative paradigm for Earth system research, enabling scientists to shift from manual execution to strategic oversight and hypothesis generation in an era of accelerating global change.

Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change.

</details>


### [62] [A Learning-based Domain Decomposition Method](https://arxiv.org/abs/2507.17328)
*Rui Wu,Nikola Kovachki,Burigede Liu*

Main category: cs.LG

TL;DR: A learning-based domain decomposition method (L-DDM) that uses pre-trained neural operators to efficiently solve PDEs on complex geometries, outperforming traditional methods while maintaining resolution-invariance and strong generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods like Finite Element Method struggle with computational cost and scalability for large, geometrically complex problems in engineering. Existing neural network approaches are limited to simple domains, making them inadequate for real-world PDEs with complex geometries.

Method: The paper proposes L-DDM which combines a single pre-trained neural operator (trained on simple domains) with domain decomposition schemes. They use physics-pretrained neural operators (PPNO) as surrogate models within the decomposition framework to handle complex geometries efficiently.

Result: The method successfully approximates solutions to elliptic PDEs with discontinuous microstructures in complex geometries. It outperforms current state-of-the-art methods on challenging problems while demonstrating resolution-invariance and strong generalization to unseen microstructural patterns.

Conclusion: L-DDM effectively bridges the gap between neural network efficiency and complex geometry handling by leveraging domain decomposition. The approach provides both theoretical foundations and practical advantages, offering a scalable solution for large-scale engineering problems with complex domains.

Abstract: Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.

</details>


### [63] [DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD](https://arxiv.org/abs/2507.17346)
*Rongwei Lu,Jingyan Jiang,Chunyang Li,Haotian Dong,Xingguang Wei,Delin Cai,Zhi Wang*

Main category: cs.LG

TL;DR: This paper proposes DeCo-SGD, an adaptive distributed SGD optimizer that dynamically adjusts gradient compression and staleness parameters based on real-time network conditions to achieve better training performance in challenging network environments with high latency and low bandwidth.


<details>
  <summary>Details</summary>
Motivation: Distributed machine learning suffers severe throughput degradation in high-latency, low-bandwidth networks. Existing approaches use gradient compression and delayed aggregation but rely on static heuristic strategies due to lack of theoretical guidance, creating a complex three-way trade-off among compression ratio, staleness, and convergence rate that prevents optimal performance under varying network conditions.

Method: The authors introduce a new theoretical framework that decomposes the joint optimization problem into traditional convergence rate analysis with multiple analyzable noise terms. They reveal that staleness exponentially amplifies the negative impact of gradient compression. Based on this theory, they propose DeCo-SGD which integrates convergence rate analysis with network-aware time minimization to dynamically adjust compression ratio and staleness parameters according to real-time network conditions.

Result: DeCo-SGD achieves significant performance improvements: up to 5.07Ã speed-up over standard distributed SGD (D-SGD) and 1.37Ã speed-up over static strategies in high-latency and low, varying bandwidth network environments respectively.

Conclusion: The study successfully fills the theoretical gap in understanding how compressed and delayed gradients affect distributed training, revealing the exponential amplification effect of staleness on gradient compression. The proposed DeCo-SGD algorithm demonstrates substantial improvements over existing methods by adaptively balancing compression and staleness based on network conditions and training requirements.

Abstract: Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.

</details>


### [64] [TOC-UCO: a comprehensive repository of tabular ordinal classification datasets](https://arxiv.org/abs/2507.17348)
*Rafael AyllÃ³n-GavilÃ¡n,David Guijo-Rubio,Antonio Manuel GÃ³mez-Orellana,David Guijo-Rubio,Francisco BÃ©rchez-Moreno,VÃ­ctor Manuel Vargas-Yun,Pedro A. GutiÃ©rrez*

Main category: cs.LG

TL;DR: The University of CÃ³rdoba introduces TOC-UCO, a comprehensive repository of 46 preprocessed tabular datasets specifically designed for benchmarking ordinal classification methods, addressing the field's lack of standardized evaluation datasets.


<details>
  <summary>Details</summary>
Motivation: The ordinal classification field lacks a comprehensive set of standardized datasets for benchmarking novel approaches, which hinders the development and validation of new methodologies in this important area of machine learning.

Method: Created TOC-UCO repository by collecting, preprocessing, and standardizing 46 tabular ordinal datasets under a common framework, ensuring reasonable pattern numbers and appropriate class distributions, while providing detailed sources, preprocessing steps, and 30 randomized train-test partitions for reproducible experiments.

Result: Successfully established a publicly available repository containing 46 high-quality tabular ordinal datasets with standardized preprocessing, complete documentation, and predefined evaluation protocols to enable consistent benchmarking of ordinal classification approaches.

Conclusion: TOC-UCO provides the ordinal classification community with a much-needed standardized benchmarking resource that will facilitate robust validation of novel approaches and improve reproducibility in ordinal classification research.

Abstract: An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.

</details>


### [65] [DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2507.17365)
*Chuzhan Hao,Wenfeng Feng,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: DynaSearcher is a multi-step search agent that uses dynamic knowledge graphs and multi-reward reinforcement learning to improve factual consistency and efficiency in complex information retrieval tasks, achieving state-of-the-art performance with smaller models.


<details>
  <summary>Details</summary>
Motivation: Multi-step agentic retrieval systems face challenges in generating factually inconsistent intermediate queries and inefficient search trajectories, leading to reasoning deviations and redundant computations in practical applications.

Method: The paper proposes DynaSearcher, which combines dynamic knowledge graphs as external structured knowledge to guide search processes and ensure factual consistency, with a multi-reward reinforcement learning framework for fine-grained control over retrieval accuracy, efficiency, and response quality.

Result: DynaSearcher achieves state-of-the-art answer accuracy on six multi-hop question answering datasets, matching frontier LLMs performance while using only small-scale models and limited computational resources, with strong generalization across diverse retrieval environments.

Conclusion: The approach successfully addresses key challenges in multi-step retrieval systems by leveraging structured knowledge and multi-objective optimization, demonstrating broad applicability and robustness while maintaining computational efficiency.

Abstract: Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.

</details>


### [66] [ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning](https://arxiv.org/abs/2507.17368)
*Hao Dai,Chong Tang,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: ViRN is a continual learning framework that combines variational inference with distributional trilateration to handle long-tailed data distributions, achieving 10.24% accuracy improvement over existing methods on classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: Continual learning with long-tailed data distributions is challenging for real-world AI systems because models must sequentially adapt to new classes while retaining old knowledge despite severe class imbalance. Existing methods struggle to balance stability and plasticity, often failing under extreme sample scarcity conditions.

Method: The paper proposes ViRN, which integrates two key components: (1) A Variational Autoencoder to model class-conditional distributions and mitigate bias toward head classes, and (2) Wasserstein distance-based neighborhood retrieval with geometric fusion to reconstruct tail-class distributions for sample-efficient alignment of tail-class representations.

Result: ViRN was evaluated on six long-tailed classification benchmarks including speech tasks (rare acoustic events, accents) and image tasks, achieving an average accuracy gain of 10.24% compared to state-of-the-art continual learning methods.

Conclusion: ViRN successfully addresses the challenge of continual learning with long-tailed distributions by effectively combining variational inference with distributional trilateration, demonstrating significant performance improvements across diverse classification tasks and establishing a robust framework for handling class imbalance in sequential learning scenarios.

Abstract: Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.

</details>


### [67] [Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective](https://arxiv.org/abs/2507.17382)
*Hao Dai,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: This paper proposes VB-CGCD, a variational Bayesian framework for continual generalized category discovery that addresses catastrophic forgetting by aligning class covariances and using stochastic variational updates to learn new classes from unlabeled data while preserving old knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing continual generalized category discovery methods struggle with catastrophic forgetting when incrementally learning new classes from unlabeled data streams that mix known and novel categories, particularly due to covariance misalignment between old and new classes.

Method: The authors propose Variational Bayes C-GCD (VB-CGCD), which integrates variational inference with covariance-aware nearest-class-mean classification. The framework adaptively aligns class distributions while suppressing pseudo-label noise through stochastic variational updates.

Result: VB-CGCD outperforms prior methods by +15.21% overall accuracy in final sessions on standard benchmarks. On a new challenging benchmark with only 10% labeled data, VB-CGCD achieves 67.86% final accuracy compared to 38.55% for state-of-the-art methods.

Conclusion: The paper demonstrates that analyzing forgetting dynamics through a Bayesian lens and addressing covariance misalignment leads to significant improvements in continual generalized category discovery, with VB-CGCD showing robust performance across diverse scenarios including challenging low-data regimes.

Abstract: Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD

</details>


### [68] [A Comprehensive Evaluation on Quantization Techniques for Large Language Models](https://arxiv.org/abs/2507.17417)
*Yutong Liu,Cairong Zhao,Guosheng Hu*

Main category: cs.LG

TL;DR: This paper provides a comprehensive review and fair comparison of post-training quantization methods for large language models, decoupling existing methods into pre-quantization transformation and quantization error mitigation components, and evaluating their performance including the latest MXFP4 format.


<details>
  <summary>Details</summary>
Motivation: The quantization field lacks fair comparisons since different methods contain multiple components and are evaluated on different grounds. Additionally, theoretical connections among existing methods need better understanding for in-depth analysis of quantization techniques for LLMs.

Method: The authors decouple quantization methods into two steps: (1) pre-quantization transformation - preprocessing to reduce outlier impact and flatten data distribution, and (2) quantization error mitigation - techniques to offset quantization errors. They conduct comprehensive evaluations on the same ground and analyze the MXFP4 data format.

Result: Optimized rotation and scaling provide the best performance for pre-quantization transformation. Combining low-rank compensation with GPTQ occasionally outperforms GPTQ alone for error mitigation. The optimal pre-quantization transformation strategy for INT4 does not generalize well to MXFP4 format.

Conclusion: The study establishes a framework for understanding quantization methods through two key components and provides fair performance comparisons. The findings reveal that optimal strategies for INT4 quantization may not apply to newer formats like MXFP4, suggesting the need for format-specific optimization approaches.

Abstract: For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.

</details>


### [69] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: This paper introduces BGM-HAN, an enhanced hierarchical attention network that uses byte-pair encoding and gated multi-head attention to improve university admissions decision-making by modeling semi-structured applicant data more effectively than existing ML and LLM baselines.


<details>
  <summary>Details</summary>
Motivation: Human decision-making in high-stakes domains like university admissions is vulnerable to hard-to-detect cognitive biases that threaten fairness and long-term outcomes, despite relying on expertise and heuristics. There's a need for AI systems that can augment decision-making while maintaining interpretability and fairness.

Method: The authors propose BGM-HAN (Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network) that integrates hierarchical learning with various enhancements to model semi-structured applicant data. The method captures multi-level representations using byte-pair encoding and gated multi-head attention mechanisms for nuanced assessment.

Result: Experimental results on real admissions data show that BGM-HAN significantly outperforms state-of-the-art baselines including traditional machine learning methods and large language models, while improving both interpretability and predictive performance.

Conclusion: BGM-HAN offers a promising framework for augmenting decision-making in high-stakes domains where structure, context, and fairness are critical considerations, successfully addressing the challenge of cognitive bias in human decision-making through enhanced AI assistance.

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [70] [Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition](https://arxiv.org/abs/2507.17450)
*Arsha Niksa,Hooman Zare,Ali Shahrabi,Hanieh Hatami,Mohammadreza Razvan*

Main category: cs.LG

TL;DR: This paper proposes a topological approach using persistent homology to analyze eye-tracking data for multiclass emotion recognition, achieving 75.6% accuracy on four emotion classes from the Circumplex Model of Affect.


<details>
  <summary>Details</summary>
Motivation: Traditional emotion recognition methods may not capture the complex geometric and topological patterns in gaze trajectories. The authors aim to explore whether topological data analysis, specifically persistent homology, can better encode discriminative gaze dynamics for emotion classification.

Method: The pipeline uses delay embeddings of gaze trajectories, applies persistent homology analysis to generate persistence diagrams, extracts shape-based features (mean persistence, maximum persistence, and entropy) from these diagrams, and trains a random forest classifier on the extracted topological features.

Result: The random forest classifier achieved up to 75.6% accuracy on four emotion classes corresponding to the quadrants of the Circumplex Model of Affect. The persistence diagram geometry successfully encoded discriminative patterns in gaze dynamics.

Conclusion: Topological approaches using persistent homology show promise for affective computing and human behavior analysis. The geometry of persistence diagrams effectively captures discriminative gaze dynamics, suggesting this method could be valuable for emotion recognition applications.

Abstract: We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.

</details>


### [71] [HOTA: Hamiltonian framework for Optimal Transport Advection](https://arxiv.org/abs/2507.17513)
*Nazar Buzun,Daniil Shlenskii,Maxim Bobrin,Dmitry V. Dylov*

Main category: cs.LG

TL;DR: HOTA is a new method for optimal transport that uses Hamilton-Jacobi-Bellman equations to optimize trajectories through Kantorovich potentials, avoiding density estimation and working with non-smooth cost functions while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current generative models using optimal transport assume trivial geometry (like Euclidean space) and rely on strong density-estimation assumptions, resulting in trajectories that don't respect true optimality principles in the underlying manifold.

Method: Hamiltonian Optimal Transport Advection (HOTA) - a Hamilton-Jacobi-Bellman based approach that directly tackles the dual dynamical optimal transport problem through Kantorovich potentials, enabling efficient and scalable trajectory optimization without explicit density modeling.

Result: HOTA outperforms all baseline methods on standard benchmarks and custom datasets with non-differentiable costs, demonstrating superior performance in both feasibility and optimality metrics. The method works effectively even when cost functionals are non-smooth.

Conclusion: HOTA successfully addresses limitations of existing optimal transport methods by avoiding density estimation requirements and handling non-smooth cost functions, while achieving better performance than current approaches across various benchmarks.

Abstract: Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.

</details>


### [72] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: This paper introduces Oliva, a novel verification framework that improves the efficiency of neural network verification by intelligently prioritizing sub-problems in branch-and-bound methods based on their likelihood of containing counterexamples, achieving up to 80X speedup over state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Existing branch-and-bound (BaB) verification methods explore sub-problems in a naive "first-come-first-serve" manner, leading to inefficiency in reaching verification conclusions. The paper aims to address this inefficiency by introducing a smarter exploration strategy that prioritizes sub-problems more likely to contain counterexamples.

Method: The authors propose Oliva framework with two variants: (1) Oliva^GR - a greedy strategy that always prioritizes sub-problems more likely to find counterexamples, and (2) Oliva^SA - a balanced strategy inspired by simulated annealing that gradually shifts from exploration to exploitation. The method introduces an ordering over sub-problems based on their likelihood of containing counterexamples.

Result: Experimental evaluation on 690 verification problems across 5 models with MNIST and CIFAR10 datasets shows significant performance improvements: up to 25X speedup on MNIST and up to 80X speedup on CIFAR10 compared to state-of-the-art approaches. The method maintains correctness even when no counterexamples are found.

Conclusion: Oliva successfully addresses the inefficiency problem in neural network verification by introducing intelligent sub-problem prioritization strategies. The framework demonstrates substantial speedup improvements while maintaining verification correctness, making it a significant advancement in formal verification of neural networks against adversarial perturbations.

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [73] [Federated Majorize-Minimization: Beyond Parameter Aggregation](https://arxiv.org/abs/2507.17534)
*Aymeric Dieuleveut,Gersende Fort,Mahmoud Hegazy,Hoi-To Wai*

Main category: cs.LG

TL;DR: This paper proposes a unified framework for federated learning algorithms based on Majorize-Minimization (MM) problems, introducing SSMM for centralized settings and QSMM for federated settings that aggregates surrogate function information rather than parameters.


<details>
  <summary>Details</summary>
Motivation: To develop a unified approach for designing stochastic optimization algorithms that can robustly scale to federated learning environments while addressing common challenges like data heterogeneity, partial participation, and communication constraints.

Method: The authors study Majorize-Minimization (MM) problems with linearly parameterized majorizing surrogate functions, develop a unifying algorithm called Stochastic Approximation Stochastic Surrogate MM (SSMM), and extend it to federated settings as QSMM. The key innovation is aggregating information about surrogate majorizing functions instead of original parameters.

Result: The framework encompasses various existing algorithms including proximal gradient methods, Expectation Maximization, and variational surrogate MM as special cases. QSMM successfully handles federated learning bottlenecks and demonstrates flexibility by being applied to optimal transport map computation in federated settings.

Conclusion: The proposed MM-based framework provides a unified theoretical foundation for federated optimization algorithms, with QSMM offering a novel approach to federated learning by aggregating surrogate function characteristics rather than model parameters, showing promise for various applications including optimal transport problems.

Abstract: This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.

</details>


### [74] [C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning](https://arxiv.org/abs/2507.17454)
*Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.LG

TL;DR: C3RL is a novel representation learning framework for multivariate time series forecasting that combines channel-mixing (CM) and channel-independence (CI) strategies using contrastive learning, achieving significant performance improvements across multiple models.


<details>
  <summary>Details</summary>
Motivation: Existing multivariate time series forecasting approaches face a trade-off: channel-mixing (CM) strategies capture inter-variable dependencies but miss variable-specific patterns, while channel-independence (CI) strategies capture variable-specific patterns but fail to exploit cross-variable dependencies. Hybrid feature fusion methods have limited generalization and interpretability.

Method: C3RL employs a siamese network architecture inspired by contrastive learning from computer vision. It treats CM and CI strategy inputs as transposed views, uses one strategy as backbone while the other complements it, and jointly optimizes contrastive and prediction losses with adaptive weighting to balance representation learning and forecasting performance.

Result: Extensive experiments on seven models demonstrate that C3RL significantly improves performance: boosting best-case performance rate to 81.4% for CI-based models and 76.3% for CM-based models, showing strong generalization across different model architectures.

Conclusion: C3RL successfully addresses the limitations of existing approaches by effectively combining CM and CI strategies through contrastive learning, demonstrating superior performance and generalization capabilities for multivariate time series forecasting tasks.

Abstract: Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.

</details>


### [75] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: This paper proposes a Quantum Federated Learning (QFL) algorithm that uses Fisher information to identify critical parameters in quantum models, enabling better performance and robustness in decentralized training while preserving data privacy across heterogeneous client partitions.


<details>
  <summary>Details</summary>
Motivation: Traditional Federated Learning faces challenges including high communication costs, heterogeneous client data, prolonged processing times, and privacy vulnerabilities. While quantum federated learning shows promise, there's a need to address these challenges by leveraging Fisher information's ability to quantify information content in quantum states under parameter changes.

Method: The authors develop a Quantum Federated Learning algorithm that computes Fisher information on local client models with heterogeneously distributed data. The method identifies critical parameters that significantly influence quantum model performance and ensures these parameters are preserved during the model aggregation process.

Result: Experimental evaluation on ADNI and MNIST datasets shows that the proposed Fisher information-based QFL approach achieves better performance and demonstrates improved robustness compared to quantum federated averaging methods and other QFL variants.

Conclusion: The incorporation of Fisher information in Quantum Federated Learning effectively addresses key challenges in federated learning by preserving critical parameters during aggregation, leading to enhanced model performance and robustness while maintaining the privacy-preserving benefits of federated learning in quantum computing applications.

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [76] [How Should We Meta-Learn Reinforcement Learning Algorithms?](https://arxiv.org/abs/2507.17668)
*Alexander David Goldie,Zilin Wang,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.LG

TL;DR: This paper empirically compares different meta-learning approaches (evolution, LLMs) for automatically discovering reinforcement learning algorithms, evaluating their performance, interpretability, and efficiency to provide guidelines for future meta-learned RL algorithm development.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in meta-learning algorithms from data rather than manual design to improve ML systems, especially for RL where algorithms are often suboptimally adapted from supervised learning. However, there has been a severe lack of systematic comparison between different meta-learning approaches for discovering RL algorithms.

Method: The authors conduct an empirical comparison of different meta-learning approaches (including evolutionary methods for black-box optimization and LLMs for code generation) applied to meta-learning algorithms targeting various parts of the RL pipeline. They evaluate multiple factors including meta-train/meta-test performance, interpretability, sample cost, and training time.

Result: The paper provides a comprehensive empirical analysis comparing different meta-learning approaches for RL algorithm discovery across multiple evaluation criteria including performance, interpretability, sample efficiency, and computational cost. The results reveal the relative strengths and weaknesses of different meta-learning methods.

Conclusion: Based on their empirical findings, the authors propose several guidelines for meta-learning new RL algorithms that will help ensure future learned algorithms achieve optimal performance. These guidelines provide practical recommendations for researchers working on automated RL algorithm discovery.

Abstract: The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.

</details>


### [77] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper introduces Deeply Normalized Transformer (DNT) that enables effective training with vanilla momentum SGD instead of requiring adaptive optimizers like AdamW, achieving comparable performance through strategic normalization placement.


<details>
  <summary>Details</summary>
Motivation: Transformers typically require advanced adaptive optimizers like AdamW rather than simple momentum SGD due to heavy-tailed gradient distributions, limiting training flexibility and efficiency.

Method: DNT strategically integrates normalization techniques at specific positions within Transformer architectures to modulate Jacobian matrices, balance weight and activation influences, and concentrate gradient distributions for effective vanilla momentum SGD training.

Result: DNT outperforms standard Transformer variants (ViT and GPT) and can be effectively trained with vanilla momentum SGD while achieving comparable performance to AdamW-trained Transformers, validated through extensive empirical evaluation.

Conclusion: The strategic placement of normalization in DNT successfully addresses gradient distribution issues in Transformers, enabling the use of simpler optimizers without performance degradation, supported by both theoretical justification and empirical validation.

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [78] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,AntÃ´nio H. Ribeiro,Umut ÅimÅekli,Tolga Birdal*

Main category: cs.LG

TL;DR: This paper investigates the fundamental trade-off between neural network compressibility and adversarial robustness, showing that compressed models create sensitive directions that adversaries can exploit, regardless of the compression method used.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks need to balance multiple competing objectives including accuracy, generalization, efficiency, and robustness. While compression and robustness have been studied separately, there lacks a unified understanding of how they interact with each other, particularly how different compression techniques affect adversarial vulnerability.

Method: The authors develop a principled theoretical framework to analyze how different forms of compressibility (neuron-level sparsity and spectral compressibility) affect adversarial robustness. They derive robustness bounds that connect compression-induced sensitive directions in representation space to adversarial vulnerabilities, and validate their theoretical predictions through empirical evaluations on synthetic and realistic tasks.

Result: The study reveals that compression creates highly sensitive directions in representation space that adversaries can exploit, leading to a fundamental tension between compressibility and robustness. These vulnerabilities persist across different compression methods (regularization, architectural bias, implicit dynamics), under adversarial training, and in transfer learning scenarios. The findings also explain the emergence of universal adversarial perturbations.

Conclusion: There exists a fundamental trade-off between structured compressibility and adversarial robustness in neural networks. The vulnerabilities arise regardless of how compression is achieved, suggesting that designing models that are both efficient and secure requires new approaches that account for this inherent tension.

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [79] [Flow Matching Meets Biology and Life Science: A Survey](https://arxiv.org/abs/2507.17731)
*Zihao Li,Zhichen Zeng,Xiao Lin,Feihao Fang,Yanru Qu,Zhe Xu,Zhining Liu,Xuying Ning,Tianxin Wei,Ge Liu,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: This paper presents the first comprehensive survey of flow matching methods and their applications in biological domains, covering biological sequence modeling, molecule generation, and protein generation, along with commonly used datasets and tools.


<details>
  <summary>Details</summary>
Motivation: Flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling with growing interest in biological applications. However, there was a need for a comprehensive survey to systematically review flow matching foundations and categorize its biological applications.

Method: The authors conduct a systematic literature review, first covering the foundations and variants of flow matching, then categorizing applications into three major biological areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. They also compile datasets and software tools used in the field.

Result: The survey provides an in-depth review of recent progress in flow matching applications across biological domains, summarizes commonly used datasets and software tools, and identifies potential future research directions. A curated resource repository is made available online.

Conclusion: Flow matching represents a significant advancement in generative modeling for biological applications, offering efficient alternatives to diffusion models. The comprehensive survey establishes a foundation for understanding current capabilities and guides future research directions in this rapidly evolving field.

Abstract: Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

</details>


### [80] [Generalized Low-Rank Matrix Contextual Bandits with Graph Information](https://arxiv.org/abs/2507.17528)
*Yao Wang,Jiannan Li,Yue Kang,Shanxing Gao,Zhenxin Xiao*

Main category: cs.LG

TL;DR: This paper proposes a novel matrix contextual bandit framework that combines low-rank structure with graph information using UCB algorithm, achieving better cumulative regret bounds than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing matrix contextual bandit methods only utilize low-rank structure but ignore valuable graph information that captures similarity relationships among users/items in real-world scenarios like online advertising and recommender systems, leading to suboptimal decision-making policies.

Method: The authors develop a matrix CB framework based on upper confidence bound (UCB) that integrates both low-rank structure and graph information by solving a joint nuclear norm and matrix Laplacian regularization problem, followed by implementing a graph-based generalized linear UCB algorithm.

Result: Theoretical analysis shows the proposed method achieves better cumulative regret bounds compared to popular alternatives due to effective graph information utilization. Synthetic and real-world experiments demonstrate the superior performance of the approach.

Conclusion: The proposed graph-enhanced matrix contextual bandit framework successfully combines low-rank structure with graph information, outperforming existing methods in both theoretical guarantees and empirical performance for sequential decision-making problems.

Abstract: The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.

</details>


### [81] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: The paper introduces "Rubrics as Rewards" (RaR), a framework that uses structured, checklist-style rubrics as interpretable reward signals for reinforcement learning in language models, achieving up to 28% improvement on HealthBench-1k compared to traditional preference-based methods.


<details>
  <summary>Details</summary>
Motivation: Real-world reinforcement learning tasks often lack single, unambiguous ground truth and require balancing objective and subjective criteria. Traditional preference-based methods rely on opaque reward functions that are difficult to interpret and prone to spurious correlations, making it challenging to define reliable reward signals for post-training language models.

Method: The authors propose Rubrics as Rewards (RaR), a framework that employs structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO (Group Relative Policy Optimization). This approach treats rubrics as structured reward signals to enable better alignment with human preferences.

Result: The best RaR method achieves up to 28% relative improvement on HealthBench-1k compared to simple Likert-based approaches. The method matches or surpasses the performance of reward signals derived from expert-written references and enables smaller-scale judge models to maintain robust performance across different model scales.

Conclusion: RaR successfully addresses the limitations of traditional preference-based methods by providing interpretable and structured reward signals. The framework demonstrates that smaller-scale judge models can better align with human preferences and maintain consistent performance across model scales when using structured rubrics instead of opaque reward functions.

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


### [82] [Generalized Advantage Estimation for Distributional Policy Gradients](https://arxiv.org/abs/2507.17530)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: This paper proposes Distributional Generalized Advantage Estimation (DGAE), which extends traditional GAE to handle value distributions in distributional reinforcement learning using optimal transport theory and a Wasserstein-like directional metric for improved robustness to system noise.


<details>
  <summary>Details</summary>
Motivation: Traditional GAE is not designed to handle value distributions in distributional RL, which can better capture system stochasticity and provide more robustness to system noises compared to point estimates.

Method: The authors utilize optimal transport theory to introduce a Wasserstein-like directional metric that measures both distance and directional discrepancies between probability distributions. They then apply exponentially weighted estimation with this metric to derive Distributional GAE (DGAE).

Result: DGAE was integrated into three different policy gradient methods and evaluated across various OpenAI Gym environments, showing performance improvements compared to baselines using traditional GAE.

Conclusion: DGAE successfully extends GAE to distributional RL while maintaining low-variance advantage estimates with controlled bias, making it suitable for policy gradient algorithms and demonstrating improved performance over traditional GAE methods.

Abstract: Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.

</details>


### [83] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: This paper shows that using large learning rates can simultaneously achieve robustness to spurious correlations and network compressibility, while also producing beneficial representation properties like invariant feature utilization and activation sparsity.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning models need to be both robust and resource-efficient, but achieving these two desirable properties jointly remains challenging. The authors aim to find a solution that can address both requirements simultaneously.

Method: The authors investigate the use of high learning rates as a facilitator for achieving both robustness and compressibility. They analyze the effect of large learning rates on representation properties including invariant feature utilization, class separation, and activation sparsity across diverse spurious correlation datasets, models, and optimizers.

Result: Large learning rates consistently produce desirable properties including robustness to spurious correlations, network compressibility, invariant feature utilization, class separation, and activation sparsity. The method compares favorably to other hyperparameters and regularization methods in satisfying these properties together.

Conclusion: High learning rates serve as an effective approach for jointly achieving robustness and resource-efficiency in machine learning models. The previously documented success of large learning rates in standard classification tasks is likely attributed to their ability to address hidden or rare spurious correlations in training datasets.

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


### [84] [XStacking: Explanation-Guided Stacked Ensemble Learning](https://arxiv.org/abs/2507.17650)
*Moncef Garouani,Ayah Barhrhouj,Olivier Teste*

Main category: cs.LG

TL;DR: XStacking is an explainable ensemble machine learning framework that combines multiple base models while maintaining interpretability through dynamic feature transformation and Shapley additive explanations, achieving improved predictive performance across 29 datasets.


<details>
  <summary>Details</summary>
Motivation: Ensemble Machine Learning techniques like stacking improve predictive performance but lack interpretability, creating a need for frameworks that can maintain both accuracy and explainability for responsible ML applications.

Method: The paper proposes XStacking, which integrates dynamic feature transformation with model-agnostic Shapley additive explanations to create an inherently explainable stacking framework that combines multiple base models.

Result: XStacking was evaluated on 29 datasets and demonstrated improvements in both predictive effectiveness of the learning space and interpretability of the resulting models compared to traditional ensemble methods.

Conclusion: XStacking provides a practical and scalable solution for responsible machine learning by successfully addressing the interpretability limitation of ensemble methods while retaining their predictive accuracy advantages.

Abstract: Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.

</details>


### [85] [Generalized Dual Discriminator GANs](https://arxiv.org/abs/2507.17684)
*Penukonda Naga Chandana,Tejas Srivastava,Gowtham R. Kurri,V. Lalitha*

Main category: cs.LG

TL;DR: This paper introduces dual discriminator Î±-GANs (D2Î±-GANs) that combine dual discriminators with tunable Î±-loss functions to address mode collapse in GANs, and generalizes this to a broader class of models with theoretical analysis showing the optimization reduces to minimizing linear combinations of f-divergences.


<details>
  <summary>Details</summary>
Motivation: To mitigate the mode collapse problem in generative adversarial networks by extending dual discriminator GANs (D2-GANs) with more flexible loss functions and providing a theoretical framework for a broader class of generalized dual discriminator models.

Method: The authors introduce D2Î±-GANs that combine dual discriminators with tunable Î±-loss functions, then generalize this approach to arbitrary functions on positive reals, creating generalized dual discriminator GANs. They provide theoretical analysis showing the min-max optimization reduces to minimizing linear combinations of f-divergences and reverse f-divergences.

Result: Theoretical analysis demonstrates that the proposed models' objectives reduce to linear combinations of f-divergences and reverse f-divergences, generalizing the known result for D2-GANs (KL and reverse KL divergences). Experiments on 2D synthetic data validate the approach using multiple performance metrics.

Conclusion: The proposed generalized dual discriminator framework successfully extends D2-GANs with more flexible loss functions while maintaining theoretical guarantees, offering a broader class of models that can better address mode collapse through the combination of f-divergences and reverse f-divergences.

Abstract: Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.

</details>


### [86] [Towards Effective Open-set Graph Class-incremental Learning](https://arxiv.org/abs/2507.17687)
*Jiazhen Chen,Zheng Ma,Sichao Fu,Mingbin Feng,Tony S. Wirjanto,Weihua Ou*

Main category: cs.LG

TL;DR: This paper proposes OGCIL, a framework for open-set graph class-incremental learning that enables GNNs to learn new classes while detecting unknown classes and preventing catastrophic forgetting through pseudo-sample generation and prototypical hypersphere classification.


<details>
  <summary>Details</summary>
Motivation: Existing graph class-incremental learning methods assume a closed-set scenario where test samples belong to known classes, limiting their real-world applicability where unknown classes naturally emerge during inference. This creates two challenges: catastrophic forgetting of old classes and inadequate open-set recognition.

Method: The OGCIL framework uses: (1) a prototypical conditional variational autoencoder to synthesize node embeddings for old classes without storing raw data, (2) a mixing-based strategy to generate out-of-distribution samples from pseudo in-distribution and current embeddings, and (3) a prototypical hypersphere classification loss that anchors known class embeddings to prototypes while repelling unknown embeddings.

Result: Extensive experiments on five benchmarks demonstrate that OGCIL outperforms existing graph class-incremental learning and open-set GNN methods, effectively handling both catastrophic forgetting and unknown class detection.

Conclusion: The proposed OGCIL framework successfully addresses the challenging open-set graph class-incremental learning scenario by combining pseudo-sample generation for knowledge replay and prototype-aware rejection regions for robust open-set recognition, advancing the field beyond closed-set assumptions.

Abstract: Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.

</details>


### [87] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: The paper proposes Joint Asymmetric Loss (JAL), a novel robust loss framework that extends asymmetric losses to complex optimization scenarios by introducing Asymmetric Mean Square Error (AMSE) as a passive loss, addressing the underfitting issues of symmetric losses in noisy label learning.


<details>
  <summary>Details</summary>
Motivation: Existing symmetric losses for noisy label learning suffer from underfitting due to overly strict constraints. While asymmetric losses have superior theoretical properties, they are incompatible with advanced optimization frameworks like Active Passive Loss (APL), limiting their applicability and potential.

Method: The authors extend asymmetric loss to the passive loss scenario by proposing Asymmetric Mean Square Error (AMSE) with rigorously established necessary and sufficient conditions for asymmetric properties. They then substitute the traditional symmetric passive loss in APL with AMSE to create the Joint Asymmetric Loss (JAL) framework.

Result: Extensive experiments demonstrate the effectiveness of JAL in mitigating label noise, showing improved performance over existing methods in learning with noisy labels for deep neural networks.

Conclusion: The proposed JAL framework successfully bridges the gap between theoretically superior asymmetric losses and practical optimization frameworks, providing an effective solution for robust learning with noisy labels while addressing the underfitting issues of symmetric losses.

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [88] [HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging](https://arxiv.org/abs/2507.17706)
*Taha Ceritli,Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: HydraOpt is a novel model merging technique that reduces storage requirements for low-rank adapters in LLMs by 48% while maintaining competitive performance, outperforming existing merging methods.


<details>
  <summary>Details</summary>
Motivation: Large language models using adapters for downstream tasks face significant memory storage challenges, especially in resource-constrained environments like mobile devices, as storing separate adapters for each task increases memory requirements substantially.

Method: HydraOpt leverages inherent similarities between matrices of low-rank adapters to create a flexible model merging technique that allows navigation across the spectrum of storage efficiency and performance trade-offs, unlike fixed trade-off methods.

Result: HydraOpt achieves 48% storage size reduction compared to storing all adapters while experiencing only 0.2-1.8% performance drop, and outperforms existing merging techniques at equivalent or slightly worse storage efficiency levels.

Conclusion: HydraOpt successfully addresses the storage-performance trade-off challenge in adapter-based LLMs by providing a flexible merging solution that significantly reduces memory requirements while maintaining competitive performance across downstream tasks.

Abstract: Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [89] [Budget Allocation Policies for Real-Time Multi-Agent Path Finding](https://arxiv.org/abs/2507.16874)
*Raz Beck,Roni Stern*

Main category: cs.MA

TL;DR: This paper addresses Real-Time Multi-Agent Pathfinding (RT-MAPF) by exploring different policies for allocating planning budgets in windowed MAPF algorithms, finding that distributing budgets across agents performs better than shared budget approaches in constrained scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing RT-MAPF solutions use windowed MAPF algorithms without explicitly considering planning budget allocation. Real-world scenarios require agents to move while planning continues, making budget allocation crucial for performance in over-constrained situations.

Method: The authors explore different planning budget allocation policies within windowed versions of standard MAPF algorithms, specifically Prioritized Planning (PrP) and MAPF-LNS2. They compare baseline shared budget approaches against policies that distribute planning budgets across individual agents.

Result: The baseline approach where all agents share a common planning budget pool proves ineffective in over-constrained situations. Budget distribution policies that allocate planning time across agents can solve more problems and achieve smaller makespan compared to shared budget approaches.

Conclusion: Distributing planning budgets over individual agents rather than using a shared budget pool significantly improves RT-MAPF performance, enabling better problem-solving capabilities and reduced makespan in constrained environments.

Abstract: Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of
agents such that each agent reaches its desired destination while avoiding
collisions with the other agents. Many MAPF solvers are designed to run
offline, that is, first generate paths for all agents and then execute them.
Real-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot
wait until a complete path for each agent has been found before they start to
move. Instead, planning and execution are interleaved, where the agents must
commit to a fixed number of steps in a constant amount of computation time,
referred to as the planning budget. Existing solutions to RT-MAPF iteratively
call windowed versions of MAPF algorithms in every planning period, without
explicitly considering the size of the planning budget. We address this gap and
explore different policies for allocating the planning budget in windowed
versions of standard MAPF algorithms, namely Prioritized Planning (PrP) and
MAPF-LNS2. Our exploration shows that the baseline approach in which all agents
draw from a shared planning budget pool is ineffective in over-constrained
situations. Instead, policies that distribute the planning budget over the
agents are able to solve more problems with a smaller makespan.

</details>


### [90] [Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems](https://arxiv.org/abs/2507.17061)
*Chengxuan Xia,Qianye Wu,Sixuan Tian,Yilun Hao*

Main category: cs.MA

TL;DR: This paper proposes a coordination framework for LLM agents that enables adaptive collaboration through dynamic task routing, bidirectional feedback, and parallel agent evaluation, showing improved performance over static multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM frameworks rely on static workflows, fixed roles, and limited inter-agent communication, which reduces their effectiveness in open-ended, high-complexity domains requiring more flexible and adaptive coordination.

Method: The framework incorporates three core mechanisms: (1) dynamic task routing that allows agents to reallocate tasks based on confidence and workload, (2) bidirectional feedback for exchanging structured critiques to improve outputs iteratively, and (3) parallel agent evaluation where agents compete on high-ambiguity subtasks with evaluator-driven selection.

Result: The framework demonstrates substantial improvements in factual coverage, coherence, and efficiency compared to static and partially adaptive baselines when implemented in a modular architecture.

Conclusion: The study highlights the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems, showing that dynamic coordination mechanisms significantly enhance collaborative task completion performance.

Abstract: Large language model (LLM) agents have shown increasing promise for
collaborative task completion. However, existing multi-agent frameworks often
rely on static workflows, fixed roles, and limited inter-agent communication,
reducing their effectiveness in open-ended, high-complexity domains. This paper
proposes a coordination framework that enables adaptiveness through three core
mechanisms: dynamic task routing, bidirectional feedback, and parallel agent
evaluation. The framework allows agents to reallocate tasks based on confidence
and workload, exchange structured critiques to iteratively improve outputs, and
crucially compete on high-ambiguity subtasks with evaluator-driven selection of
the most suitable result. We instantiate these principles in a modular
architecture and demonstrate substantial improvements in factual coverage,
coherence, and efficiency over static and partially adaptive baselines. Our
findings highlight the benefits of incorporating both adaptiveness and
structured competition in multi-agent LLM systems.

</details>


### [91] [Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating LLMs and Blockchain for Transparent Coordination](https://arxiv.org/abs/2507.17134)
*Mariam ALMutairi,Hyungmin Kim*

Main category: cs.MA

TL;DR: This paper proposes a hybrid framework combining blockchain technology with LLM-powered multi-agent systems to improve medical supply chain resilience during health emergencies like COVID-19, enabling autonomous agents to negotiate resource allocation while ensuring transparency and accountability through smart contracts.


<details>
  <summary>Details</summary>
Motivation: Global health emergencies like COVID-19 have revealed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions during crises.

Method: A novel hybrid framework integrating blockchain technology with decentralized LLM-powered multi-agent negotiation systems, where autonomous agents representing manufacturers, distributors, and healthcare institutions engage in structured negotiations, supported by off-chain adaptive reasoning and on-chain smart contract enforcement with formal cross-layer communication protocols.

Result: Simulation environment testing during pandemic scenarios demonstrated improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability compared to traditional approaches.

Conclusion: The research successfully develops an innovative approach that synergizes blockchain trust guarantees with adaptive intelligence of LLM-driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty during health emergencies.

Abstract: Global health emergencies, such as the COVID-19 pandemic, have exposed
critical weaknesses in traditional medical supply chains, including
inefficiencies in resource allocation, lack of transparency, and poor
adaptability to dynamic disruptions. This paper presents a novel hybrid
framework that integrates blockchain technology with a decentralized, large
language model (LLM) powered multi-agent negotiation system to enhance the
resilience and accountability of medical supply chains during crises. In this
system, autonomous agents-representing manufacturers, distributors, and
healthcare institutions-engage in structured, context-aware negotiation and
decision-making processes facilitated by LLMs, enabling rapid and ethical
allocation of scarce medical resources. The off-chain agent layer supports
adaptive reasoning and local decision-making, while the on-chain blockchain
layer ensures immutable, transparent, and auditable enforcement of decisions
via smart contracts. The framework also incorporates a formal cross-layer
communication protocol to bridge decentralized negotiation with institutional
enforcement. A simulation environment emulating pandemic scenarios evaluates
the system's performance, demonstrating improvements in negotiation efficiency,
fairness of allocation, supply chain responsiveness, and auditability. This
research contributes an innovative approach that synergizes blockchain trust
guarantees with the adaptive intelligence of LLM-driven agents, providing a
robust and scalable solution for critical supply chain coordination under
uncertainty.

</details>


### [92] [Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach](https://arxiv.org/abs/2507.17433)
*Hugh Adams,Srijoni Majumdar,Evangelos Pournaras*

Main category: cs.MA

TL;DR: This paper proposes a multi-agent deep reinforcement learning approach with branching neural networks to support decision-making in participatory budgeting, helping voters develop better voting strategies and policymakers design fairer elections by optimizing for greater representation of voter preferences.


<details>
  <summary>Details</summary>
Motivation: Participatory budgeting suffers from "choice overload" where citizens struggle to make decisions on numerous projects when voting on budget allocation. There's a need for decision support tools that can help voters make better choices while ensuring fair distribution of public funds and enabling policymakers to design more equitable election processes.

Method: The authors develop a multi-agent deep reinforcement learning model using a novel branching neural network architecture to overcome scalability challenges in a decentralized manner. The approach optimizes voter actions to achieve greater representation of voter preferences in the winning project set, focusing on fair compromise through strategic voting behavior modeling.

Result: Experimental evaluation using real-world participatory budgeting data revealed that fair compromises in participatory budgeting can be achieved through projects with smaller costs. The approach successfully identified voting strategies that increase the winning proportion of votes and highlighted design aspects that enable fair project compromises.

Conclusion: The multi-agent reinforcement learning approach with branching neural networks provides an effective and ethically aligned solution for participatory budgeting decision support. It benefits both voters by reducing choice overload and improving voting outcomes, and policymakers by revealing election design principles that promote fairness, with smaller-cost projects emerging as a key pattern for achieving fair compromises.

Abstract: Participatory budgeting is a method of collectively understanding and
addressing spending priorities where citizens vote on how a budget is spent, it
is regularly run to improve the fairness of the distribution of public funds.
Participatory budgeting requires voters to make decisions on projects which can
lead to ``choice overload". A multi-agent reinforcement learning approach to
decision support can make decision making easier for voters by identifying
voting strategies that increase the winning proportion of their vote. This
novel approach can also support policymakers by highlighting aspects of
election design that enable fair compromise on projects. This paper presents a
novel, ethically aligned approach to decision support using multi-agent deep
reinforcement learning modelling. This paper introduces a novel use of a
branching neural network architecture to overcome scalability challenges of
multi-agent reinforcement learning in a decentralized way. Fair compromises are
found through optimising voter actions towards greater representation of voter
preferences in the winning set. Experimental evaluation with real-world
participatory budgeting data reveals a pattern in fair compromise: that it is
achievable through projects with smaller cost.

</details>
