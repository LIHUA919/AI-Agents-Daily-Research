<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: Researchers developed a multi-agent AI tutoring platform that goes beyond reactive assistance to provide structured, personalized mathematics learning through adaptive feedback, course generation, and textbook knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: Current AI tutoring systems are primarily reactive, providing direct answers without encouraging deep reflection or incorporating structured pedagogical strategies. This limitation is particularly evident in mathematics education where AI tutoring systems remain underdeveloped, prompting the need for more structured and individualized learning experiences.

Method: The researchers introduced a novel multi-agent AI tutoring platform that integrates three key components: adaptive and personalized feedback systems, structured course generation capabilities, and textbook knowledge retrieval mechanisms to create modular, tool-assisted learning processes.

Result: The system enables students to learn new mathematical topics while identifying and targeting their specific weaknesses, provides effective exam revision support, and offers unlimited personalized exercises for practice, creating a comprehensive learning environment.

Conclusion: The research contributes a novel platform that combines pedagogical agents with AI-driven components, advancing the field of AI in education by providing modular and effective systems specifically designed for mathematics teaching and learning.

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [2] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: The paper proposes a game-theoretic model for tactical decision-making in highway merging scenarios, improving payoff functions and lag actions to simulate realistic driver behavior.


<details>
  <summary>Details</summary>
Motivation: To enhance simulation environments for autonomous vehicle development by replicating real-world driver behavior, particularly in highway merging scenarios.

Method: A game-theoretic model for tactical decision-making with improved payoff functions and lag actions, coupled with an underlying dynamics model for unified decision and dynamics simulation.

Result: The model demonstrated good reproducibility of complex interactions in validation and was efficiently integrated into a high-fidelity simulation environment.

Conclusion: The proposed model effectively simulates realistic highway merging interactions and is suitable for large-scale autonomous vehicle development simulations.

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [3] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: The paper introduces a taxonomy for eXplainable Reinforcement Learning (XRL) based on 'What' and 'How' questions, reviews 250+ papers, and highlights future needs for the field.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of AI models, particularly in reinforcement learning, by providing a structured approach to explainability.

Method: Proposes a taxonomy for XRL, categorizing methods by 'What' (target of explanation) and 'How' (method of explanation), and reviews 250+ papers.

Result: A comprehensive taxonomy and state-of-the-art review of XRL, identifying gaps and related domains.

Conclusion: The taxonomy aids in understanding XRL methods, and future work should address identified needs in the field.

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [4] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: An automated framework combines RL for playtesting and LMMs for game design iteration, improving AI-assisted game development.


<details>
  <summary>Details</summary>
Motivation: Modern generative systems fail to capture dynamic player behavior from static rules, necessitating a better approach.

Method: Uses RL agents to playtest and LMMs to analyze play traces (metrics/image strips) and revise game configurations iteratively.

Result: LMMs effectively refine game mechanics by reasoning over RL-supplied behavioral traces.

Conclusion: The framework offers scalable, practical tools for AI-assisted game design.

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [5] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: The paper evaluates deception probes in AI assistants, comparing white-box and black-box monitoring to measure their effectiveness and resistance to evasion.


<details>
  <summary>Details</summary>
Motivation: To assess how well deception probes detect deceptive responses and whether they can be evaded by deceptive AI assistants.

Method: Comparison of white-box (access to token-level probe activations) and black-box monitoring (no such access) to benchmark deception probes.

Result: Weak but encouraging black-to-white performance boosts from existing deception probes.

Conclusion: Deception probes show promise but require further improvement for practical effectiveness.

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [6] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: Study develops an AI Agent for peer learning in English composition, assuming peers at the same proficiency level enhance effectiveness.


<details>
  <summary>Details</summary>
Motivation: Peer learning has limitations; AI can provide consistent, same-level companions to improve effectiveness.

Method: Focus on English composition, assuming peers at the same level make similar mistakes.

Result: AI Agent designed to enable peer learning anytime, anywhere.

Conclusion: AI can address peer learning limitations by simulating same-level companions.

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [7] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval is an open-source framework for automated, scalable evaluation of LLM agents, addressing limitations of static benchmarks and manual efforts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating LLM agents are limited by static benchmarks and labor-intensive processes, necessitating a more robust and scalable solution.

Method: MCPEval uses a Model Context Protocol (MCP) to automate task generation and evaluation, standardizing metrics and integrating with native agent tools.

Result: Empirical results across five domains demonstrate MCPEval's effectiveness in revealing nuanced, domain-specific performance.

Conclusion: MCPEval is released publicly to promote reproducible and standardized evaluation of LLM agents.

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [8] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: The paper presents a solution for NLPCC 2025 Task 8 on Emotional Support Conversation (ESC), using large-scale language models with prompt engineering and fine-tuning, achieving second place.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for mental health support by improving empathetic and effective emotional assistance through dialogue.

Method: Leverage large-scale language models enhanced by prompt engineering and fine-tuning, including Low-Rank Adaptation and full-parameter fine-tuning.

Result: The best model ranked second in the competition, demonstrating the effectiveness of combining LLMs with adaptation methods for ESC.

Conclusion: Future work will focus on enhancing emotional understanding and response personalization for more practical and reliable ESC systems.

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [9] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: The paper advocates for a new evaluation framework for AI world models, inspired by human adaptability, using novel games to assess rapid learning and adaptation.


<details>
  <summary>Details</summary>
Motivation: Current AI world models lack the efficiency and adaptability of human learning, focusing instead on static representations from large datasets.

Method: Proposes a benchmarking paradigm using novel games with deep, refreshing novelty to evaluate rapid world model induction in AI.

Result: Introduces key desiderata for game design and metrics to challenge AI's adaptability, aiming to inspire future AI evaluation efforts.

Conclusion: The framework aims to advance AI towards human-like rapid adaptation and robust generalization, a key step towards artificial general intelligence.

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [10] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: The paper proposes moving human judgement outside AI simulations to explore ethical decision-making efficiently, focusing on dynamically weighting ethical attributes during simulations.


<details>
  <summary>Details</summary>
Motivation: Human involvement in every ethical decision within AI simulations is impractical and slows down scenario exploration. The paper aims to streamline this by designing ethical metrics upfront and automating weight calculations during simulations.

Method: Human commanders design ethical metrics, while simulations explore the space. The paper leverages multi-criteria decision-making literature to dynamically weight ethical attributes during simulations.

Result: The approach allows efficient exploration of ethical scenarios without constant human input, presenting commanders with select options for final judgement.

Conclusion: Automating ethical weight calculations in simulations balances efficiency and human oversight, addressing the challenge of ethical decision-making in AI-driven scenarios.

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [11] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: The paper highlights the risks of AI systems manipulating humans, proposes a safety framework to assess and mitigate these threats, and provides actionable guidelines for AI companies.


<details>
  <summary>Details</summary>
Motivation: The growing threat of AI systems manipulating human behavior, especially in cybersecurity and corporate settings, lacks systematic assessment and mitigation strategies.

Method: The authors introduce a safety case framework with three core arguments (inability, control, trustworthiness), detailing evidence requirements and evaluation methodologies.

Result: A systematic methodology is presented for integrating manipulation risk into AI safety governance, offering practical tools for AI companies.

Conclusion: The paper provides a foundational approach for AI companies to assess and mitigate manipulation risks before deployment, addressing a critical gap in AI safety.

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [12] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: The paper introduces VAR-MATH, a symbolic evaluation framework to assess true reasoning in LLMs, revealing performance drops in RL-trained models on variabilized benchmarks.


<details>
  <summary>Details</summary>
Motivation: To determine if RL improvements in LLMs reflect genuine reasoning or benchmark overfitting, addressing issues like benchmark contamination and evaluation fragility.

Method: Developed VAR-MATH, converting fixed numerical problems into symbolic templates to test reasoning consistency across variants.

Result: RL-trained models showed significant performance drops (48.0% on AMC23, 58.3% on AIME24) on variabilized benchmarks.

Conclusion: VAR-MATH provides a robust, contamination-resistant evaluation method, highlighting superficial heuristics in current RL approaches.

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [13] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: The paper bridges Probabilistic Event Calculus (PEC) and Markov Decision Processes (MDPs) to enable goal-directed reasoning, preserving interpretability while leveraging MDP tools.


<details>
  <summary>Details</summary>
Motivation: PEC lacks mechanisms for goal-directed reasoning, limiting its practical applications despite its interpretability and expressiveness for narrative reasoning.

Method: Develops a formal translation of PEC domains into MDPs, introducing 'action-taking situations' to preserve PEC's flexible action semantics.

Result: The PEC-MDP formalism enables the use of MDP algorithms for temporal reasoning and planning, with methods to map policies back to human-readable PEC.

Conclusion: The translation extends PEC's capabilities while maintaining interpretability, supporting both temporal reasoning and objective-driven planning.

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [14] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: X-MILP is a domain-agnostic method for generating contrastive explanations for MILPs using constraint reasoning and IIS to create a "graph of reasons."


<details>
  <summary>Details</summary>
Motivation: Address the need for trustworthy AI by developing techniques to explain MILP solutions, aiding user understanding.

Method: Encodes user queries as constraints, computes IIS to identify reasons, and represents explanations as a graph.

Result: Tested on optimization problems to assess computational feasibility.

Conclusion: X-MILP effectively provides structured explanations for MILP solutions, enhancing interpretability.

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [15] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: The paper proposes MLR and RF models for highway traffic flow prediction using California data, finding optimal performance at 10-minute intervals.


<details>
  <summary>Details</summary>
Motivation: To address global traffic congestion by improving traffic flow prediction using AI algorithms.

Method: Utilized MLR and RF on 30-second interval traffic data from California Highway 78, testing intervals from 30 seconds to 15 minutes.

Result: Both MLR and RF models performed best with 10-minute data intervals, as measured by R^2, MAE, and RMSE.

Conclusion: The findings can aid in traffic congestion solutions and efficient traffic management.

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [16] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: The paper introduces a dynamic reinforcement learning framework to improve tree-structured reasoning in language models, addressing limitations of static methods like ProbTree by enabling adaptive reasoning and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current tree-structured reasoning methods (e.g., ProbTree) suffer from static reasoning trees and computational inefficiency, limiting their adaptability and performance in real-world question answering.

Method: The proposed framework uses dynamic reinforcement learning to incrementally build reasoning trees based on real-time confidence estimates, optimizing action selection (decomposition, retrieval, aggregation).

Result: The approach enhances solution quality and computational efficiency by selectively expanding the reasoning tree and focusing resource allocation.

Conclusion: This work advances tree-structured reasoning by combining probabilistic rigor with flexibility, offering a more effective solution for complex question answering.

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [17] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: The paper proposes new ethical criteria for evaluating LLM-based artificial moral agents (AMAs), replacing outdated frameworks due to LLMs' opacity.


<details>
  <summary>Details</summary>
Motivation: Traditional ethical criteria for AMAs assume transparent architectures, which LLMs lack, necessitating revised evaluation standards.

Method: The paper introduces ten functional criteria (e.g., moral concordance, trustworthiness) and applies them to hypothetical scenarios (e.g., autonomous public bus).

Result: A revised framework (SMA-LLS) is proposed to align LLM-based AMAs with societal benefits.

Conclusion: The new criteria aim to improve the ethical evaluation and integration of LLM-based AMAs in morally complex contexts.

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


### [18] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: The paper proposes a unification algorithm for higher-order patterns with fuzzy logic, ensuring termination, soundness, and completeness.


<details>
  <summary>Details</summary>
Motivation: Combining higher-order theories and fuzzy logic aids decision-making where exact matches are rare. Efficient reasoning for this combination is challenging.

Method: Integrates higher-order patterns with fuzzy equivalences using minimum T-norm. Proposes a unification algorithm for these patterns.

Result: The algorithm is unitary, computes a most general unifier with the highest degree of approximation.

Conclusion: The approach successfully unifies higher-order patterns with fuzzy logic, providing a robust solution for reasoning tasks.

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [19] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: The paper introduces GEA, an arena for evaluating LLMs that includes energy consumption data, showing users prefer energy-efficient models when aware of their consumption.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation methods (automated benchmarks or human studies) have limitations like poor human correlation or scalability issues. Public arenas like LM arena offer scalability but lack energy awareness.

Method: GEA (Generative Energy Arena) incorporates energy consumption data into model evaluations, allowing users to rank models based on both response quality and energy efficiency.

Result: Preliminary results show users favor smaller, energy-efficient models when energy data is provided, suggesting top-performing models' extra cost isn't justified for most interactions.

Conclusion: Energy awareness influences user preference toward efficient models, indicating a need to balance performance and sustainability in LLM deployment.

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


### [20] [FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming](https://arxiv.org/abs/2507.13337)
*Gal Beniamini,Yuval Dor,Alon Vinnikov,Shir Granot Peled,Or Weinstein,Or Sharir,Noam Wies,Tomer Nussbaum,Ido Ben Shaul,Tomer Zekharya,Yoav Levine,Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: The paper introduces FormulaOne, a benchmark for evaluating AI models on complex, real-life research problems in graph theory, logic, and algorithms, revealing their limitations compared to human expertise.


<details>
  <summary>Details</summary>
Motivation: To assess the true expertise of frontier AI models by moving beyond contrived puzzles and focusing on demanding real-world research problems.

Method: Constructed FormulaOne, a benchmark derived from Monadic Second-Order (MSO) logic on graphs, featuring highly challenging problems with commercial and theoretical relevance.

Result: State-of-the-art models like OpenAI's o3 perform poorly, solving less than 1% of problems, indicating a significant gap from expert-level understanding.

Conclusion: FormulaOne highlights the limitations of current AI models and provides a framework for future research, including a simpler warmup dataset (FormulaOne-Warmup).

Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close
are they to true human -- or superhuman -- expertise? Genuine experts can
tackle the hardest problems and push the boundaries of scientific
understanding. To illuminate the limits of frontier model capabilities, we turn
away from contrived competitive programming puzzles, and instead focus on
real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph
theory, logic, and algorithms, all well within the training distribution of
frontier models. Our problems are incredibly demanding, requiring an array of
reasoning steps. The dataset has three key properties. First, it is of
commercial interest and relates to practical large-scale optimisation problems,
such as those arising in routing, scheduling, and network design. Second, it is
generated from the highly expressive framework of Monadic Second-Order (MSO)
logic on graphs, paving the way toward automatic problem generation at scale;
ideal for building RL environments. Third, many of our problems are intimately
related to the frontier of theoretical computer science, and to central
conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As
such, any significant algorithmic progress on our dataset, beyond known
results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on
FormulaOne, solving less than 1% of the questions, even when given 10 attempts
and explanatory fewshot examples -- highlighting how far they remain from
expert-level understanding in some domains. To support further research, we
additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from
the same distribution. We release the full corpus along with a comprehensive
evaluation framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: Scaling test-time computation with chain-of-thought reasoning and iterative exploration improves complex tasks like math and coding. Prolonged RL on small models, using verifiable rewards and GRPO enhancements, yields significant gains.


<details>
  <summary>Details</summary>
Motivation: Investigate the effects of prolonged RL on small language models across diverse reasoning domains to identify key training ingredients.

Method: Uses verifiable reward tasks, enhances GRPO, and introduces controlled KL regularization, clipping ratio, and periodic reference policy resets.

Result: Achieves +14.7% on math, +13.9% on coding, and +54.8% on logic puzzles over baselines.

Conclusion: Key techniques like verifiable rewards and GRPO enhancements unlock long-term performance gains, with the model released for further research.

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


### [22] [The Serial Scaling Hypothesis](https://arxiv.org/abs/2507.12549)
*Yuxi Liu,Konpat Preechakul,Kananart Kuwaranancharoen,Yutong Bai*

Main category: cs.LG

TL;DR: The paper highlights the limitations of parallel-centric architectures in solving inherently sequential problems and emphasizes the need to scale serial computation for AI progress.


<details>
  <summary>Details</summary>
Motivation: To address the blind spot in machine learning where some problems are fundamentally sequential and cannot be parallelized, impacting tasks like mathematical reasoning and decision-making.

Method: Formalizes the distinction between parallel and serial problems using complexity theory and evaluates current architectures' limitations.

Result: Demonstrates that parallel-centric architectures are fundamentally limited for inherently serial tasks.

Conclusion: Scaling serial computation, not just parallel, is crucial for advancing AI in complex reasoning tasks.

Abstract: While machine learning has advanced through massive parallelization, we
identify a critical blind spot: some problems are fundamentally sequential.
These "inherently serial" problems-from mathematical reasoning to physical
simulations to sequential decision-making-require dependent computational steps
that cannot be parallelized. Drawing from complexity theory, we formalize this
distinction and demonstrate that current parallel-centric architectures face
fundamental limitations on such tasks. We argue that recognizing the serial
nature of computation holds profound implications on machine learning, model
design, hardware development. As AI tackles increasingly complex reasoning,
deliberately scaling serial computation-not just parallel computation-is
essential for continued progress.

</details>


### [23] [Can Mental Imagery Improve the Thinking Capabilities of AI Systems?](https://arxiv.org/abs/2507.12555)
*Slimane Larabi*

Main category: cs.LG

TL;DR: A machine thinking framework integrating mental imagery is proposed to enhance autonomous reasoning and knowledge integration in AI agents.


<details>
  <summary>Details</summary>
Motivation: Existing AI models lack autonomous reasoning and struggle with cross-domain knowledge integration, unlike humans who use mental imagery.

Method: Proposes a framework with a Cognitive Thinking Unit and three auxiliary units (Input Data, Needs, and Mental Imagery Units), using natural language or sketches for data representation.

Result: Validation tests were conducted, with results presented and discussed.

Conclusion: The framework shows potential to improve AI reasoning by incorporating mental imagery.

Abstract: Although existing models can interact with humans and provide satisfactory
responses, they lack the ability to act autonomously or engage in independent
reasoning. Furthermore, input data in these models is typically provided as
explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform
tasks and make decisions autonomously based on their programming, data inputs,
and learned knowledge, have shown significant progress. However, they struggle
with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process,
which involves performing tasks based on internal multisensory data, planned
actions, needs, and reasoning capabilities. In this paper, we investigate how
to integrate mental imagery into a machine thinking framework and how this
could be beneficial in initiating the thinking process. Our proposed machine
thinking framework integrates a Cognitive thinking unit supported by three
auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery
Unit. Within this framework, data is represented as natural language sentences
or drawn sketches, serving both informative and decision-making purposes. We
conducted validation tests for this framework, and the results are presented
and discussed.

</details>


### [24] [IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift](https://arxiv.org/abs/2507.12573)
*Eduardo V. L. Barboza,Paulo R. Lisboa de Almeida,Alceu de Souza Britto Jr.,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: The paper proposes IncA-DES, a method for handling concept drift in data streams by fusing classifiers and using a drift detector, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Concept drift in data streams complicates ML tasks, requiring adaptive methods to handle changing data distributions.

Method: IncA-DES combines local expert training, a drift detector, and an overlap-based filter, with an Online K-d tree for efficiency.

Result: The framework outperforms seven state-of-the-art methods in accuracy and processing time, with minimal accuracy loss when using the Online K-d tree.

Conclusion: IncA-DES is effective for concept drift in data streams, balancing accuracy and efficiency, and is made publicly available.

Abstract: Data streams pose challenges not usually encountered in batch-based ML. One
of them is concept drift, which is characterized by the change in data
distribution over time. Among many approaches explored in literature, the
fusion of classifiers has been showing good results and is getting growing
attention. DS methods, due to the ensemble being instance-based, seem to be an
efficient choice under drifting scenarios. However, some attention must be paid
to adapting such methods for concept drift. The training must be done in order
to create local experts, and the commonly used neighborhood-search DS may
become prohibitive with the continuous arrival of data. In this work, we
propose IncA-DES, which employs a training strategy that promotes the
generation of local experts with the assumption that different regions of the
feature space become available with time. Additionally, the fusion of a concept
drift detector supports the maintenance of information and adaptation to a new
concept. An overlap-based classification filter is also employed in order to
avoid using the DS method when there is a consensus in the neighborhood, a
strategy that we argue every DS method should employ, as it was shown to make
them more applicable and quicker. Moreover, aiming to reduce the processing
time of the kNN, we propose an Online K-d tree algorithm, which can quickly
remove instances without becoming inconsistent and deals with unbalancing
concerns that may occur in data streams. Experimental results showed that the
proposed framework got the best average accuracy compared to seven
state-of-the-art methods considering different levels of label availability and
presented the smaller processing time between the most accurate methods.
Additionally, the fusion with the Online K-d tree has improved processing time
with a negligible loss in accuracy. We have made our framework available in an
online repository.

</details>


### [25] [Assay2Mol: large language model-based drug design using BioAssay context](https://arxiv.org/abs/2507.12574)
*Yifan Deng,Spencer S. Ericksen,Anthony Gitter*

Main category: cs.LG

TL;DR: Assay2Mol uses a language model to leverage unstructured biochemical assay data for drug discovery, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Unstructured text in biochemical assays contains untapped information for drug discovery.

Method: Assay2Mol retrieves similar assay records and generates candidate molecules using in-context learning.

Result: It outperforms other machine learning approaches and improves synthesizability of molecules.

Conclusion: Assay2Mol effectively utilizes unstructured assay data for early-stage drug discovery.

Abstract: Scientific databases aggregate vast amounts of quantitative data alongside
descriptive text. In biochemistry, molecule screening assays evaluate the
functional responses of candidate molecules against disease targets.
Unstructured text that describes the biological mechanisms through which these
targets operate, experimental screening protocols, and other attributes of
assays offer rich information for new drug discovery campaigns but has been
untapped because of that unstructured format. We present Assay2Mol, a large
language model-based workflow that can capitalize on the vast existing
biochemical screening assays for early-stage drug discovery. Assay2Mol
retrieves existing assay records involving targets similar to the new target
and generates candidate molecules using in-context learning with the retrieved
assay screening data. Assay2Mol outperforms recent machine learning approaches
that generate candidate ligand molecules for target protein structures, while
also promoting more synthesizable molecule generation.

</details>


### [26] [Ranking Vectors Clustering: Theory and Applications](https://arxiv.org/abs/2507.12583)
*Ali Fattahi,Ali Eshragh,Babak Aslani,Meysam Rabiee*

Main category: cs.LG

TL;DR: The paper addresses the k-centroids ranking vectors clustering problem (KRC), proving its NP-hardness and proposing efficient algorithms (KRCA and BnB) to solve it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to cluster ranking vectors, which represent preferences, for applications like personalization and decision-making, where classical k-means is inadequate.

Method: The authors derive a closed-form solution for the single-cluster case and develop KRCA (an approximation algorithm) and BnB (a branch-and-bound algorithm) for efficient clustering.

Result: KRCA outperforms baseline solutions in experiments, showing improved quality and speed. Theoretical error bounds are established.

Conclusion: KRC is practically significant for large-scale decision-making, with KRCA and BnB offering efficient solutions and insights for future research.

Abstract: We study the problem of clustering ranking vectors, where each vector
represents preferences as an ordered list of distinct integers. Specifically,
we focus on the k-centroids ranking vectors clustering problem (KRC), which
aims to partition a set of ranking vectors into k clusters and identify the
centroid of each cluster. Unlike classical k-means clustering (KMC), KRC
constrains both the observations and centroids to be ranking vectors. We
establish the NP-hardness of KRC and characterize its feasible set. For the
single-cluster case, we derive a closed-form analytical solution for the
optimal centroid, which can be computed in linear time. To address the
computational challenges of KRC, we develop an efficient approximation
algorithm, KRCA, which iteratively refines initial solutions from KMC, referred
to as the baseline solution. Additionally, we introduce a branch-and-bound
(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a
decision tree framework to reduce computational time while incorporating a
controlling parameter to balance solution quality and efficiency. We establish
theoretical error bounds for KRCA and BnB. Through extensive numerical
experiments on synthetic and real-world datasets, we demonstrate that KRCA
consistently outperforms baseline solutions, delivering significant
improvements in solution quality with fast computational times. This work
highlights the practical significance of KRC for personalization and
large-scale decision making, offering methodological advancements and insights
that can be built upon in future studies.

</details>


### [27] [Second-Order Bounds for [0,1]-Valued Regression via Betting Loss](https://arxiv.org/abs/2507.12584)
*Yinan Li,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: The paper explores regression on [0,1]-valued data, showing the log loss minimizer achieves a first-order bound. It introduces the betting loss for variance-dependent (second-order) bounds, adapting without prior variance knowledge.


<details>
  <summary>Details</summary>
Motivation: To improve generalization bounds in [0,1]-valued regression by moving beyond first-order bounds and achieving variance-adaptive results without explicit variance modeling.

Method: Analyzes log loss minimizer for first-order bounds, then proposes the betting loss for variance-dependent bounds.

Result: The betting loss achieves a variance-dependent bound, improving upon first-order bounds without requiring variance knowledge.

Conclusion: The betting loss provides a novel, variance-adaptive solution for regression, outperforming traditional methods like squared or log loss.

Abstract: We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a
related problem called cost-sensitive classification, \citet{foster21efficient}
have shown that the log loss minimizer achieves an improved generalization
bound compared to that of the squared loss minimizer in the sense that the
bound scales with the cost of the best classifier, which can be arbitrarily
small depending on the problem at hand. Such a result is often called a
first-order bound. For $[0,1]$-valued regression, we first show that the log
loss minimizer leads to a similar first-order bound. We then ask if there
exists a loss function that achieves a variance-dependent bound (also known as
a second order bound), which is a strict improvement upon first-order bounds.
We answer this question in the affirmative by proposing a novel loss function
called the betting loss. Our result is ``variance-adaptive'' in the sense that
the bound is attained \textit{without any knowledge about the variance}, which
is in contrast to modeling label (or reward) variance or the label distribution
itself explicitly as part of the function class such as distributional
reinforcement learning.

</details>


### [28] [Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?](https://arxiv.org/abs/2507.12604)
*Antoni Zajko,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: The paper proposes two novel methods for tabular representation learning tailored to warm-starting Bayesian Hyperparameter Optimization (HPO), evaluating their alignment with landmarkers and performance impact.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous tabular datasets lack effective representations for meta-learning, especially for specific tasks like HPO warm-starting.

Method: Two methods are introduced: deep metric learning and landmarkers reconstruction, both designed to capture landmarker properties.

Result: The encoders learn representations aligned with landmarkers but show limited performance gains in HPO warm-starting.

Conclusion: While the methods meet the proposed requirement, their direct impact on the meta-task is not significant.

Abstract: Effectively representing heterogeneous tabular datasets for meta-learning
purposes is still an open problem. Previous approaches rely on representations
that are intended to be universal. This paper proposes two novel methods for
tabular representation learning tailored to a specific meta-task -
warm-starting Bayesian Hyperparameter Optimization. Both follow the specific
requirement formulated by ourselves that enforces representations to capture
the properties of landmarkers. The first approach involves deep metric
learning, while the second one is based on landmarkers reconstruction. We
evaluate the proposed encoders in two ways. Next to the gain in the target
meta-task, we also use the degree of fulfillment of the proposed requirement as
the evaluation metric. Experiments demonstrate that while the proposed encoders
can effectively learn representations aligned with landmarkers, they may not
directly translate to significant performance gains in the meta-task of HPO
warm-starting.

</details>


### [29] [Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning](https://arxiv.org/abs/2507.12612)
*Prateek Chanda,Saral Sureka,Parth Pratim Chatterjee,Krishnateja Killamsetty,Nikhil Shivakumar Nayak,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: TASKPGM is a framework for optimizing training mixtures for LLMs by minimizing an energy function over an MRF, balancing task representativeness and diversity.


<details>
  <summary>Details</summary>
Motivation: Current methods for selecting task mixtures in LLM finetuning are manual and heuristic, lacking principled optimization.

Method: TASKPGM uses behavioral divergences (e.g., Jensen Shannon Divergence) to model task relationships and provides a closed-form solution under simplex constraints.

Result: Empirical improvements on models like Llama 2 and Mistral across benchmarks (MMLU, BIGBench), with theoretical guarantees.

Conclusion: TASKPGM is a scalable, interpretable tool for robust LLM finetuning, offering insights into task influence and mixture composition.

Abstract: The performance of finetuned large language models (LLMs) hinges critically
on the composition of the training mixture. However, selecting an optimal blend
of task datasets remains a largely manual, heuristic driven process, with
practitioners often relying on uniform or size based sampling strategies. We
introduce TASKPGM, a principled and scalable framework for mixture optimization
that selects continuous task proportions by minimizing an energy function over
a Markov Random Field (MRF). Task relationships are modeled using behavioral
divergences such as Jensen Shannon Divergence and Pointwise Mutual Information
computed from the predictive distributions of single task finetuned models. Our
method yields a closed form solution under simplex constraints and provably
balances representativeness and diversity among tasks. We provide theoretical
guarantees, including weak submodularity for budgeted variants, and demonstrate
consistent empirical improvements on Llama 2 and Mistral across evaluation
suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers
interpretable insights into task influence and mixture composition, making it a
powerful tool for efficient and robust LLM finetuning.

</details>


### [30] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: The paper addresses the startup overhead in training large language models (LLMs), introduces Bootseer to reduce it by 50%, and analyzes its impact using real production data.


<details>
  <summary>Details</summary>
Motivation: Startup overhead in LLM training wastes significant GPU time (3.5% in one cluster) and hampers efficiency in large-scale industrial settings.

Method: Bootseer optimizes three bottlenecks: container image loading, runtime dependency installation, and model checkpoint resumption, using techniques like hot block prefetch, dependency snapshotting, and striped HDFS-FUSE.

Result: Bootseer reduces startup overhead by 50% in real LLM training workloads.

Conclusion: Addressing startup overhead is critical for industrial-scale LLM training, and Bootseer provides an effective solution.

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [31] [Reasoning-Finetuning Repurposes Latent Representations in Base Models](https://arxiv.org/abs/2507.12638)
*Jake Ward,Chuqiao Lin,Constantin Venhoff,Neel Nanda*

Main category: cs.LG

TL;DR: Backtracking in reasoning models is driven by repurposed directions in base model activations, not learned from scratch.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism behind backtracking in reasoning models and how base model representations are repurposed.

Method: Identified a direction in base Llama-3.1-8B's residual stream that induces backtracking in the distilled reasoning model, DeepSeek-R1-Distill-Llama-8B.

Result: The identified direction induces backtracking in the distilled model but not the base model, suggesting repurposing of pre-existing representations.

Conclusion: Reasoning-finetuned models repurpose base model representations rather than learning new capabilities from scratch.

Abstract: Backtracking, an emergent behavior elicited by reasoning fine-tuning, has
been shown to be a key mechanism in reasoning models' enhanced capabilities.
Prior work has succeeded in manipulating this behavior via steering vectors,
but the underlying mechanism remains poorly understood. In this work, we show
that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part
driven by a repurposed direction already present in base model activations.
Specifically, we identify a direction in base Llama-3.1-8B's residual stream
which systematically induces backtracking when used to steer the distilled
reasoning model, and find that the effects of steering with this direction
cannot be trivially explained by token-level attributes. We further find that
this direction does not induce backtracking in the base model, suggesting that
the reasoning finetuning process repurposes pre-existing representations to
form new behavioral circuits. Additionally, we hypothesize that this direction
is one of several which may work together to mediate backtracking. Our findings
offer a compelling picture that reasoning-finetuned models repurpose
pre-existing base model representations, rather than learn new capabilities
from scratch.

</details>


### [32] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: The paper explores federated learning (FL) for privacy-preserving neural decoding, showing its superiority in open-loop scenarios but challenges in closed-loop, real-time applications.


<details>
  <summary>Details</summary>
Motivation: Neural signals contain sensitive data, requiring privacy-conscious methods for decoder training. FL offers a solution but is untested in closed-loop neural interfaces.

Method: The study evaluates FL-based neural decoding using high-dimensional electromyography signals in open- and closed-loop scenarios.

Result: FL outperformed local learning in open-loop but required adaptation for closed-loop, where local learning performed better but posed higher privacy risks.

Conclusion: The study reveals a performance-privacy tradeoff in real-time applications, calling for FL methods tailored for single-user, co-adaptive use.

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [33] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: The paper introduces a transfer learning method and an adaptive activation function to improve the extrapolation performance of Physics-Informed Neural Networks (PINNs), achieving significant error reductions without added computational cost.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with poor extrapolation and sensitivity to activation functions, limiting their practical utility.

Method: The authors propose transfer learning within an extended training domain and an adaptive activation function combining standard functions.

Result: Experiments show a 40% reduction in relative L2 error and 50% reduction in mean absolute error in extrapolation.

Conclusion: The method enhances PINN robustness and accuracy for extrapolation tasks, with minimal computational overhead.

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [34] [Data Transformation Strategies to Remove Heterogeneity](https://arxiv.org/abs/2507.12677)
*Sangbong Yoo,Jaeyoung Lee,Chanyoung Yoon,Geonyeong Son,Hyein Hong,Seongbum Seo,Soobin Yim,Chanyoung Jung,Jungsoo Park,Misuk Kim,Yun Jang*

Main category: cs.LG

TL;DR: The paper discusses data heterogeneity challenges, emphasizing the importance of data transformation for AI efficiency and the lack of comprehensive reviews on modern transformation techniques.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity complicates AI utilization due to format disparities, requiring expert intervention. Current methods overlook data transformation's critical role in AI efficiency.

Method: The survey systematically categorizes and presents strategies to address data format heterogeneity, highlighting associated challenges.

Result: The study sheds light on the complexities of data heterogeneity and the need for tailored transformation techniques to preserve data details.

Conclusion: A comprehensive review of contemporary data transformation approaches is needed to streamline AI data preparation and enhance learning efficiency.

Abstract: Data heterogeneity is a prevalent issue, stemming from various conflicting
factors, making its utilization complex. This uncertainty, particularly
resulting from disparities in data formats, frequently necessitates the
involvement of experts to find resolutions. Current methodologies primarily
address conflicts related to data structures and schemas, often overlooking the
pivotal role played by data transformation. As the utilization of artificial
intelligence (AI) continues to expand, there is a growing demand for a more
streamlined data preparation process, and data transformation becomes
paramount. It customizes training data to enhance AI learning efficiency and
adapts input formats to suit diverse AI models. Selecting an appropriate
transformation technique is paramount in preserving crucial data details.
Despite the widespread integration of AI across various industries,
comprehensive reviews concerning contemporary data transformation approaches
are scarce. This survey explores the intricacies of data heterogeneity and its
underlying sources. It systematically categorizes and presents strategies to
address heterogeneity stemming from differences in data formats, shedding light
on the inherent challenges associated with each strategy.

</details>


### [35] [PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform](https://arxiv.org/abs/2507.12704)
*Xiangyi Chen,Kousik Rajesh,Matthew Lawhon,Zelun Wang,Hanyu Li,Haomiao Li,Saurabh Vishwas Joshi,Pong Eksombatchai,Jaewon Yang,Yi-Ping Hsu,Jiajing Xu,Charles Rosenberg*

Main category: cs.LG

TL;DR: PinFM is a foundational transformer model for user activity sequences in recommender systems, pretrained on 20B+ parameters and fine-tuned for applications, addressing scalability and latency challenges with innovative techniques like DCAT.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying pretraining-and-fine-tuning in industrial recommender systems, such as scalability, latency, and handling new items.

Method: Pretrain a transformer model (PinFM) with 20B+ parameters, fine-tune for specific applications, and use techniques like DCAT for optimization.

Result: 600% throughput improvement, 20% increase in engagement with new items, and deployment for 500M+ users.

Conclusion: PinFM successfully addresses industrial recommender system challenges, improving scalability, latency, and user engagement.

Abstract: User activity sequences have emerged as one of the most important signals in
recommender systems. We present a foundational model, PinFM, for understanding
user activity sequences across multiple applications at a billion-scale visual
discovery platform. We pretrain a transformer model with 20B+ parameters using
extensive user activity data, then fine-tune it for specific applications,
efficiently coupling it with existing models. While this
pretraining-and-fine-tuning approach has been popular in other domains, such as
Vision and NLP, its application in industrial recommender systems presents
numerous challenges. The foundational model must be scalable enough to score
millions of items every second while meeting tight cost and latency constraints
imposed by these systems. Additionally, it should capture the interactions
between user activities and other features and handle new items that were not
present during the pretraining stage.
  We developed innovative techniques to address these challenges. Our
infrastructure and algorithmic optimizations, such as the Deduplicated
Cross-Attention Transformer (DCAT), improved our throughput by 600% on
Pinterest internal data. We demonstrate that PinFM can learn interactions
between user sequences and candidate items by altering input sequences, leading
to a 20% increase in engagement with new items. PinFM is now deployed to help
improve the experience of more than a half billion users across various
applications.

</details>


### [36] [From SGD to Spectra: A Theory of Neural Network Weight Dynamics](https://arxiv.org/abs/2507.12709)
*Brian Richard Olsen,Sam Fatehmanesh,Frank Xiao,Adarsh Kumarappan,Anirudh Gajula*

Main category: cs.LG

TL;DR: A continuous-time SDE framework connects SGD dynamics to singular-value spectra in weight matrices, explaining the 'bulk+tail' spectral structure in deep networks.


<details>
  <summary>Details</summary>
Motivation: To theoretically clarify the training dynamics of deep neural networks, which remain unclear despite their success.

Method: Develop a matrix-valued SDE framework linking SGD dynamics to singular-value spectra, deriving exact SDEs for squared singular values.

Result: Squared singular values follow Dyson Brownian motion with repulsion; stationary distributions are gamma-type with power-law tails.

Conclusion: The framework provides a rigorous foundation for understanding deep learning's success, validated by experiments on transformers and MLPs.

Abstract: Deep neural networks have revolutionized machine learning, yet their training
dynamics remain theoretically unclear-we develop a continuous-time,
matrix-valued stochastic differential equation (SDE) framework that rigorously
connects the microscopic dynamics of SGD to the macroscopic evolution of
singular-value spectra in weight matrices. We derive exact SDEs showing that
squared singular values follow Dyson Brownian motion with eigenvalue repulsion,
and characterize stationary distributions as gamma-type densities with
power-law tails, providing the first theoretical explanation for the
empirically observed 'bulk+tail' spectral structure in trained networks.
Through controlled experiments on transformer and MLP architectures, we
validate our theoretical predictions and demonstrate quantitative agreement
between SDE-based forecasts and observed spectral evolution, providing a
rigorous foundation for understanding why deep learning works.

</details>


### [37] [Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning](https://arxiv.org/abs/2507.12750)
*Suorong Yang,Peijia Li,Yujie Liu,Zhiming Xu,Peng Ye,Wanli Ouyang,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: A dynamic dataset pruning framework adaptively selects training samples using task-driven difficulty and cross-modality semantic consistency, leveraging pretrained multimodal models for robust sample selection.


<details>
  <summary>Details</summary>
Motivation: Existing dataset pruning methods rely on static heuristics or task-specific metrics, lacking robustness and generalizability across domains.

Method: Introduces a dynamic pruning framework that uses task-driven difficulty and cross-modality semantic consistency, guided by pretrained multimodal foundation models.

Result: Improves training efficiency and model performance by filtering uninformative samples.

Conclusion: Demonstrates the potential of cross-modality alignment for robust sample selection, advancing data-centric learning practices.

Abstract: Modern deep models are trained on large real-world datasets, where data
quality varies and redundancy is common. Data-centric approaches such as
dataset pruning have shown promise in improving training efficiency and model
performance. However, most existing methods rely on static heuristics or
task-specific metrics, limiting their robustness and generalizability across
domains. In this work, we introduce a dynamic dataset pruning framework that
adaptively selects training samples based on both task-driven difficulty and
cross-modality semantic consistency. By incorporating supervision from
pretrained multimodal foundation models, our approach captures training
dynamics while effectively filtering out uninformative samples. Our work
highlights the potential of integrating cross-modality alignment for robust
sample selection, advancing data-centric learning toward more efficient and
robust practices across application domains.

</details>


### [38] [Layer Separation Deep Learning Model with Auxiliary Variables for Partial Differential Equations](https://arxiv.org/abs/2507.12766)
*Yaru Liu,Yiqi Gu*

Main category: cs.LG

TL;DR: The paper introduces the LySep model, a layer separation framework for optimizing deep learning in solving PDEs, addressing issues like local minima and gradient problems by decomposing networks into shallow architectures with auxiliary variables.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for PDEs face challenges like non-convex loss functions, leading to suboptimal solutions, gradient explosion, or vanishing gradients. The LySep model aims to mitigate these issues.

Method: The LySep model uses auxiliary variables to separate deep neural network layers, representing outputs and derivatives per layer. It decomposes the network into shallow architectures and employs alternating direction algorithms for optimization.

Result: Theoretical consistency with original deep models is proven, and numerical results show LySep's effectiveness in minimizing loss and reducing solution errors in high-dimensional PDEs.

Conclusion: The LySep model offers a robust optimization framework for deep learning in PDEs, improving performance by addressing key optimization challenges.

Abstract: In this paper, we propose a new optimization framework, the layer separation
(LySep) model, to improve the deep learning-based methods in solving partial
differential equations. Due to the highly non-convex nature of the loss
function in deep learning, existing optimization algorithms often converge to
suboptimal local minima or suffer from gradient explosion or vanishing,
resulting in poor performance. To address these issues, we introduce auxiliary
variables to separate the layers of deep neural networks. Specifically, the
output and its derivatives of each layer are represented by auxiliary
variables, effectively decomposing the deep architecture into a series of
shallow architectures. New loss functions with auxiliary variables are
established, in which only variables from two neighboring layers are coupled.
Corresponding algorithms based on alternating directions are developed, where
many variables can be updated optimally in closed forms. Moreover, we provide
theoretical analyses demonstrating the consistency between the LySep model and
the original deep model. High-dimensional numerical results validate our theory
and demonstrate the advantages of LySep in minimizing loss and reducing
solution error.

</details>


### [39] [A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models](https://arxiv.org/abs/2507.12774)
*Weijieying Ren,Jingxi Zhu,Zehao Liu,Tianxiang Zhao,Vasant Honavar*

Main category: cs.LG

TL;DR: A survey on AI-driven EHR modeling, covering deep learning and LLMs, with a taxonomy of five design dimensions and emerging trends like foundation models and LLM-driven agents.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of EHR data (heterogeneity, temporal irregularity, domain-specificity) and advance AI applications in healthcare.

Method: Introduces a unified taxonomy with five dimensions: data-centric approaches, neural architecture design, learning strategies, multimodal learning, and LLM-based systems. Reviews methods for data quality, representation, self-supervised learning, and clinical knowledge integration.

Result: Highlights advancements like foundation models, LLM-driven agents, and EHR-to-text translation. Identifies open challenges in benchmarking, explainability, and clinical alignment.

Conclusion: Provides a structured roadmap for AI-driven EHR modeling and clinical decision support, aiming to guide future research.

Abstract: Artificial intelligence (AI) has demonstrated significant potential in
transforming healthcare through the analysis and modeling of electronic health
records (EHRs). However, the inherent heterogeneity, temporal irregularity, and
domain-specific nature of EHR data present unique challenges that differ
fundamentally from those in vision and natural language tasks. This survey
offers a comprehensive overview of recent advancements at the intersection of
deep learning, large language models (LLMs), and EHR modeling. We introduce a
unified taxonomy that spans five key design dimensions: data-centric
approaches, neural architecture design, learning-focused strategies, multimodal
learning, and LLM-based modeling systems. Within each dimension, we review
representative methods addressing data quality enhancement, structural and
temporal representation, self-supervised learning, and integration with
clinical knowledge. We further highlight emerging trends such as foundation
models, LLM-driven clinical agents, and EHR-to-text translation for downstream
reasoning. Finally, we discuss open challenges in benchmarking, explainability,
clinical alignment, and generalization across diverse clinical settings. This
survey aims to provide a structured roadmap for advancing AI-driven EHR
modeling and clinical decision support. For a comprehensive list of EHR-related
methods, kindly refer to https://survey-on-tabular-data.github.io/.

</details>


### [40] [Multi-Channel Graph Neural Network for Financial Risk Prediction of NEEQ Enterprises](https://arxiv.org/abs/2507.12787)
*Jianyu Zhu*

Main category: cs.LG

TL;DR: A multi-channel deep learning framework for predicting financial risk in NEEQ-listed SMEs, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address the elevated financial risks faced by NEEQ-listed SMEs due to their limited scale and resilience.

Method: Proposes a Triple-Channel Graph Isomorphism Network (GIN) integrating financial indicators, textual data, and enterprise relationships, fused with attention and gating.

Result: Outperforms traditional methods and single-modality baselines in AUC, Precision, Recall, and F1 Score on 7,731 NEEQ companies.

Conclusion: Provides a robust, data-driven tool for financial risk prediction, aiding regulators and investors.

Abstract: With the continuous evolution of China's multi-level capital market, the
National Equities Exchange and Quotations (NEEQ), also known as the "New Third
Board," has become a critical financing platform for small and medium-sized
enterprises (SMEs). However, due to their limited scale and financial
resilience, many NEEQ-listed companies face elevated risks of financial
distress. To address this issue, we propose a multi-channel deep learning
framework that integrates structured financial indicators, textual disclosures,
and enterprise relationship data for comprehensive financial risk prediction.
Specifically, we design a Triple-Channel Graph Isomorphism Network (GIN) that
processes numeric, textual, and graph-based inputs separately. These
modality-specific representations are fused using an attention-based mechanism
followed by a gating unit to enhance robustness and prediction accuracy.
Experimental results on data from 7,731 real-world NEEQ companies demonstrate
that our model significantly outperforms traditional machine learning methods
and single-modality baselines in terms of AUC, Precision, Recall, and F1 Score.
This work provides theoretical and practical insights into risk modeling for
SMEs and offers a data-driven tool to support financial regulators and
investors.

</details>


### [41] [FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction](https://arxiv.org/abs/2507.12803)
*Qianru Zhang,Chenglei Yu,Haixin Wang,Yudong Yan,Yuansheng Cao,Siu-Ming Yiu,Tailin Wu,Hongzhi Yin*

Main category: cs.LG

TL;DR: FLDmamba combines Fourier and Laplace transforms with Mamba for efficient long-term time series prediction, outperforming Transformer and Mamba-based models.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of Transformer and Mamba models in capturing multi-scale periodicity, transient dynamics, and noise robustness in time series data.

Method: Proposes FLDmamba, integrating Fourier and Laplace transforms with Mamba to enhance periodicity and dynamics capture while improving noise robustness.

Result: FLDmamba achieves superior performance on benchmarks compared to Transformer and other Mamba-based models.

Conclusion: FLDmamba effectively addresses key challenges in time series prediction, offering a robust and efficient solution.

Abstract: Time series prediction, a crucial task across various domains, faces
significant challenges due to the inherent complexities of time series data,
including non-stationarity, multi-scale periodicity, and transient dynamics,
particularly when tackling long-term predictions. While Transformer-based
architectures have shown promise, their quadratic complexity with sequence
length hinders their efficiency for long-term predictions. Recent advancements
in State-Space Models, such as Mamba, offer a more efficient alternative for
long-term modeling, but they cannot capture multi-scale periodicity and
transient dynamics effectively. Meanwhile, they are susceptible to data noise
issues in time series. This paper proposes a novel framework, FLDmamba (Fourier
and Laplace Transform Decomposition Mamba), addressing these limitations.
FLDmamba leverages the strengths of both Fourier and Laplace transforms to
effectively capture both multi-scale periodicity, transient dynamics within
time series data, and improve the robustness of the model to the data noise
issue. Our extensive experiments demonstrate that FLDmamba achieves superior
performance on time series prediction benchmarks, outperforming both
Transformer-based and other Mamba-based architectures. To promote the
reproducibility of our method, we have made both the code and data accessible
via the following
URL:{\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\model}.

</details>


### [42] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: PMKLC, a parallel multi-knowledge learning-based compressor, improves compression ratio, throughput, and robustness for genomic data, outperforming 14 baselines.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based lossless compressors for genomic data suffer from inadequate compression ratio, low throughput, and poor robustness, limiting their adoption.

Method: PMKLC introduces a multi-knowledge learning framework, GPU-accelerated encoding, parallel mechanisms, and two compression modes (single-GPU and multi-GPU).

Result: PMKLC achieves up to 73.6% better compression ratio and 10.7× higher throughput than baselines, with strong robustness and memory efficiency.

Conclusion: PMKLC is a robust, efficient solution for genomic data compression, suitable for diverse scenarios and resource constraints.

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [43] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: The paper introduces RONOM, a framework combining reduced-order modeling (ROM) and neural operators, to address computational challenges in solving time-dependent PDEs. It provides error bounds and demonstrates superior performance in spatial super-resolution and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of ROM (fixed discretization) and neural operators (lack of error quantification) in solving PDEs, the paper aims to bridge these approaches for better flexibility and accuracy.

Method: The RONOM framework integrates ROM and neural operator concepts, establishing discretization error bounds and analyzing convergence and robustness.

Result: RONOM achieves comparable input generalization and superior spatial super-resolution and robustness compared to existing neural operators, with insights into temporal super-resolution.

Conclusion: RONOM successfully combines ROM and operator learning, offering improved performance and theoretical guarantees for solving PDEs in many-query scenarios.

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>


### [44] [From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning](https://arxiv.org/abs/2507.12815)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: ReLOAD introduces a reward annotation framework for offline RL using RND to generate intrinsic rewards from expert demonstrations, eliminating the need for handcrafted rewards.


<details>
  <summary>Details</summary>
Motivation: Offline RL often requires costly or impractical reward annotations, limiting its adoption. ReLOAD aims to address this by automating reward generation.

Method: ReLOAD uses RND to train a predictor network on expert state transitions, with prediction errors serving as intrinsic rewards for the static dataset.

Result: Experiments on D4RL show ReLOAD achieves competitive performance without handcrafted rewards.

Conclusion: ReLOAD provides a practical solution for offline RL by automating reward annotation, enabling robust policy learning.

Abstract: Offline Reinforcement Learning (RL) aims to learn effective policies from a
static dataset without requiring further agent-environment interactions.
However, its practical adoption is often hindered by the need for explicit
reward annotations, which can be costly to engineer or difficult to obtain
retrospectively. To address this, we propose ReLOAD (Reinforcement Learning
with Offline Reward Annotation via Distillation), a novel reward annotation
framework for offline RL. Unlike existing methods that depend on complex
alignment procedures, our approach adapts Random Network Distillation (RND) to
generate intrinsic rewards from expert demonstrations using a simple yet
effective embedding discrepancy measure. First, we train a predictor network to
mimic a fixed target network's embeddings based on expert state transitions.
Later, the prediction error between these networks serves as a reward signal
for each transition in the static dataset. This mechanism provides a structured
reward signal without requiring handcrafted reward annotations. We provide a
formal theoretical construct that offers insights into how RND prediction
errors effectively serve as intrinsic rewards by distinguishing expert-like
transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables
robust offline policy learning and achieves performance competitive with
traditional reward-annotated methods.

</details>


### [45] [Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability](https://arxiv.org/abs/2507.12837)
*Kaiqi Jiang,Jeremy Cohen,Yuanzhi Li*

Main category: cs.LG

TL;DR: This paper investigates the behavior of Neural Tangent Kernel (NTK) eigenvectors during the Edge of Stability (EoS) phenomenon in Gradient Descent (GD), revealing that larger learning rates improve alignment between NTK eigenvectors and training targets.


<details>
  <summary>Details</summary>
Motivation: Despite prior research on NTK eigenvalue behavior during EoS, the dynamics of NTK eigenvectors remain unexplored. This study aims to fill that gap.

Method: The paper examines NTK eigenvector dynamics across architectures and provides a theoretical analysis for a two-layer linear network.

Result: Larger learning rates enhance alignment between leading NTK eigenvectors and training targets.

Conclusion: The findings deepen the understanding of GD training dynamics in deep learning, particularly regarding NTK eigenvector behavior during EoS.

Abstract: The study of Neural Tangent Kernels (NTKs) in deep learning has drawn
increasing attention in recent years. NTKs typically actively change during
training and are related to feature learning. In parallel, recent work on
Gradient Descent (GD) has found a phenomenon called Edge of Stability (EoS), in
which the largest eigenvalue of the NTK oscillates around a value inversely
proportional to the step size. However, although follow-up works have explored
the underlying mechanism of such eigenvalue behavior in depth, the
understanding of the behavior of the NTK eigenvectors during EoS is still
missing. This paper examines the dynamics of NTK eigenvectors during EoS in
detail. Across different architectures, we observe that larger learning rates
cause the leading eigenvectors of the final NTK, as well as the full NTK
matrix, to have greater alignment with the training target. We then study the
underlying mechanism of this phenomenon and provide a theoretical analysis for
a two-layer linear network. Our study enhances the understanding of GD training
dynamics in deep learning.

</details>


### [46] [A Kernel Distribution Closeness Testing](https://arxiv.org/abs/2507.12843)
*Zhijian Zhou,Liuhua Peng,Xunye Tian,Feng Liu*

Main category: cs.LG

TL;DR: The paper introduces Norm-Adaptive Maximum Mean Discrepancy (NAMMD) to improve distribution closeness testing (DCT) by addressing MMD's limitations in assessing closeness levels for multiple distribution pairs. NAMMD scales MMD values using RKHS norms, leading to higher test power and bounded type-I error, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Existing DCT methods are limited to discrete one-dimensional spaces, restricting their use for complex data like images. MMD, while powerful, lacks informativeness for multiple distribution pairs due to identical values for distributions with different RKHS norms.

Method: The authors propose NAMMD, a new discrepancy measure scaling MMD by RKHS norms. They derive its asymptotic distribution and design NAMMD-based DCT, theoretically proving its superiority over MMD-based DCT.

Result: NAMMD-based DCT shows higher test power and bounded type-I error, validated on synthetic and real-world data (e.g., images). NAMMD also improves two-sample testing, outperforming MMD-based tests.

Conclusion: NAMMD enhances distribution closeness and two-sample testing by addressing MMD's limitations, offering a more informative and powerful tool for complex data analysis.

Abstract: The distribution closeness testing (DCT) assesses whether the distance
between a distribution pair is at least $\epsilon$-far. Existing DCT methods
mainly measure discrepancies between a distribution pair defined on discrete
one-dimensional spaces (e.g., using total variation), which limits their
applications to complex data (e.g., images). To extend DCT to more types of
data, a natural idea is to introduce maximum mean discrepancy (MMD), a powerful
measurement of the distributional discrepancy between two complex
distributions, into DCT scenarios. However, we find that MMD's value can be the
same for many pairs of distributions that have different norms in the same
reproducing kernel Hilbert space (RKHS), making MMD less informative when
assessing the closeness levels for multiple distribution pairs. To mitigate the
issue, we design a new measurement of distributional discrepancy, norm-adaptive
MMD (NAMMD), which scales MMD's value using the RKHS norms of distributions.
Based on the asymptotic distribution of NAMMD, we finally propose the
NAMMD-based DCT to assess the closeness levels of a distribution pair.
Theoretically, we prove that NAMMD-based DCT has higher test power compared to
MMD-based DCT, with bounded type-I error, which is also validated by extensive
experiments on many types of data (e.g., synthetic noise, real images).
Furthermore, we also apply the proposed NAMMD for addressing the two-sample
testing problem and find NAMMD-based two-sample test has higher test power than
the MMD-based two-sample test in both theory and experiments.

</details>


### [47] [Transformer-Based Person Identification via Wi-Fi CSI Amplitude and Phase Perturbations](https://arxiv.org/abs/2507.12854)
*Danilo Avola,Andrea Bernardini,Francesco Danese,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: A transformer-based method identifies individuals from Wi-Fi CSI while stationary, achieving 99.82% accuracy.


<details>
  <summary>Details</summary>
Motivation: Wi-Fi sensing offers privacy-preserving human identification, but stationary identification is underexplored.

Method: Uses a dual-branch transformer to process CSI amplitude and phase, with preprocessing for signal quality.

Result: Achieves 99.82% accuracy, outperforming baselines.

Conclusion: Demonstrates feasibility of passive, device-free identification using low-cost Wi-Fi hardware.

Abstract: Wi-Fi sensing is gaining momentum as a non-intrusive and privacy-preserving
alternative to vision-based systems for human identification. However, person
identification through wireless signals, particularly without user motion,
remains largely unexplored. Most prior wireless-based approaches rely on
movement patterns, such as walking gait, to extract biometric cues. In
contrast, we propose a transformer-based method that identifies individuals
from Channel State Information (CSI) recorded while the subject remains
stationary. CSI captures fine-grained amplitude and phase distortions induced
by the unique interaction between the human body and the radio signal. To
support evaluation, we introduce a dataset acquired with ESP32 devices in a
controlled indoor environment, featuring six participants observed across
multiple orientations. A tailored preprocessing pipeline, including outlier
removal, smoothing, and phase calibration, enhances signal quality. Our
dual-branch transformer architecture processes amplitude and phase modalities
separately and achieves 99.82\% classification accuracy, outperforming
convolutional and multilayer perceptron baselines. These results demonstrate
the discriminative potential of CSI perturbations, highlighting their capacity
to encode biometric traits in a consistent manner. They further confirm the
viability of passive, device-free person identification using low-cost
commodity Wi-Fi hardware in real-world settings.

</details>


### [48] [Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)](https://arxiv.org/abs/2507.12856)
*Chongli Qin,Jost Tobias Springenberg*

Main category: cs.LG

TL;DR: The paper connects Behavior Cloning (BC) with Reinforcement Learning (RL), proposing an importance weighted variant (iw-SFT) that improves performance and aligns closer to RL objectives.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between supervised fine-tuning (SFT) and RL, showing SFT as a lower bound on RL objectives and improving it.

Method: Introduces iw-SFT, a modified SFT method with importance weighting, optimizing a tighter RL bound and generalizing to quality-scored data.

Result: iw-SFT outperforms standard SFT, achieving 66.7% on the AIME 2024 dataset, and competes with advanced RL methods.

Conclusion: iw-SFT is a simple yet effective enhancement to SFT, aligning it closer to RL and improving performance in language models and control tasks.

Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant
paradigm for supervised fine-tuning (SFT) of large language models; as well as
for imitation learning of control policies. Here, we draw on a connection
between this successful strategy and the theory and practice of finding optimal
policies via Reinforcement Learning (RL). Building on existing literature, we
clarify that SFT can be understood as maximizing a lower bound on the RL
objective in a sparse reward setting. Giving support to its often observed good
performance. From this viewpoint, we realize that a small modification to SFT
leads to an importance weighted variant that behaves closer to training with RL
as it: i) optimizes a tighter bound to the RL objective and, ii) can improve
performance compared to SFT on curated data. We refer to this variant as
importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to
implement and can be further generalized to training with quality scored data.
The resulting SFT variants are competitive with more advanced RL algorithms for
large language models and for training policies in continuous control tasks.
For example achieving 66.7% on the AIME 2024 dataset.

</details>


### [49] [An Investigation of Ear-EEG Signals for a Novel Biometric Authentication System](https://arxiv.org/abs/2507.12873)
*Danilo Avola,Giancarlo Crocetti,Gian Luca Foresti,Daniele Pannone,Claudio Piciarelli,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: The paper proposes using ear-EEG signals for biometric authentication, achieving 82% accuracy with a deep neural network.


<details>
  <summary>Details</summary>
Motivation: Traditional EEG-based biometric systems are secure but cumbersome; ear-EEG offers a user-friendly alternative.

Method: Extracts temporal and spectral features from ear-EEG signals and uses a deep neural network for identification.

Result: Achieved 82% accuracy in subject identification using the only available ear-EEG dataset.

Conclusion: Ear-EEG is a promising and practical direction for real-world biometric systems.

Abstract: This work explores the feasibility of biometric authentication using EEG
signals acquired through in-ear devices, commonly referred to as ear-EEG.
Traditional EEG-based biometric systems, while secure, often suffer from low
usability due to cumbersome scalp-based electrode setups. In this study, we
propose a novel and practical framework leveraging ear-EEG signals as a
user-friendly alternative for everyday biometric authentication. The system
extracts an original combination of temporal and spectral features from ear-EEG
signals and feeds them into a fully connected deep neural network for subject
identification. Experimental results on the only currently available ear-EEG
dataset suitable for different purposes, including biometric authentication,
demonstrate promising performance, with an average accuracy of 82\% in a
subject identification scenario. These findings confirm the potential of
ear-EEG as a viable and deployable direction for next-generation real-world
biometric systems.

</details>


### [50] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: MC$^2$A is an algorithm-hardware co-design framework for accelerating MCMC algorithms, achieving significant speedups over existing solutions.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of MCMC algorithms limits their scalability and real-world applicability, necessitating efficient acceleration solutions.

Method: MC$^2$A extends the roofline model, proposes a parametrized hardware accelerator, and introduces a novel Gumbel sampler to optimize MCMC workloads.

Result: MC$^2$A achieves speedups of 307.6×, 1.4×, 2.0×, and 84.2× over CPU, GPU, TPU, and state-of-the-art MCMC accelerators, respectively.

Conclusion: The framework demonstrates the feasibility of general hardware acceleration for MCMC, enabling broader adoption in diverse applications.

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [51] [Topology-Aware Activation Functions in Neural Networks](https://arxiv.org/abs/2507.12874)
*Pavel Snopov,Oleg R. Musin*

Main category: cs.LG

TL;DR: The paper introduces novel activation functions, SmoothSplit and ParametricSplit, to enhance neural networks' ability to manipulate data topology, outperforming traditional functions like ReLU in low-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: Traditional activation functions like ReLU have limitations in handling data topology. The study aims to improve neural networks' ability to transform complex data manifolds.

Method: Proposes SmoothSplit and ParametricSplit, activation functions with topology "cutting" capabilities, tested on synthetic and real-world datasets.

Result: ParametricSplit outperforms traditional activations in low-dimensional settings while remaining competitive in higher-dimensional ones.

Conclusion: Topology-aware activation functions like ParametricSplit show promise for advancing neural network architectures.

Abstract: This study explores novel activation functions that enhance the ability of
neural networks to manipulate data topology during training. Building on the
limitations of traditional activation functions like $\mathrm{ReLU}$, we
propose $\mathrm{SmoothSplit}$ and $\mathrm{ParametricSplit}$, which introduce
topology "cutting" capabilities. These functions enable networks to transform
complex data manifolds effectively, improving performance in scenarios with
low-dimensional layers. Through experiments on synthetic and real-world
datasets, we demonstrate that $\mathrm{ParametricSplit}$ outperforms
traditional activations in low-dimensional settings while maintaining
competitive performance in higher-dimensional ones. Our findings highlight the
potential of topology-aware activation functions in advancing neural network
architectures. The code is available via
https://github.com/Snopoff/Topology-Aware-Activations.

</details>


### [52] [A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints](https://arxiv.org/abs/2507.12979)
*Youssef Tawfilis,Hossam Amer,Minar El-Aasser,Tallal Elshabrawy*

Main category: cs.LG

TL;DR: Proposes a decentralized GAN training method using federated learning to leverage distributed data and low-capability devices without raw data sharing.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of training generative models with limited resources and privacy concerns by utilizing underutilized devices and distributed data.

Method: Combines KLD-weighted Clustered Federated Learning for data heterogeneity and Heterogeneous U-Shaped split learning for device heterogeneity, ensuring no raw data sharing.

Result: Achieves 1.1x-2.2x higher image generation scores, 10% average boost in classification metrics (up to 50% in non-IID settings), and lower latency.

Conclusion: The approach effectively trains GANs in decentralized settings, improving performance while preserving privacy.

Abstract: Federated Learning has gained increasing attention for its ability to enable
multiple nodes to collaboratively train machine learning models without sharing
their raw data. At the same time, Generative AI -- particularly Generative
Adversarial Networks (GANs) -- have achieved remarkable success across a wide
range of domains, such as healthcare, security, and Image Generation. However,
training generative models typically requires large datasets and significant
computational resources, which are often unavailable in real-world settings.
Acquiring such resources can be costly and inefficient, especially when many
underutilized devices -- such as IoT devices and edge devices -- with varying
capabilities remain idle. Moreover, obtaining large datasets is challenging due
to privacy concerns and copyright restrictions, as most devices are unwilling
to share their data. To address these challenges, we propose a novel approach
for decentralized GAN training that enables the utilization of distributed data
and underutilized, low-capability devices while not sharing data in its raw
form. Our approach is designed to tackle key challenges in decentralized
environments, combining KLD-weighted Clustered Federated Learning to address
the issues of data heterogeneity and multi-domain datasets, with Heterogeneous
U-Shaped split learning to tackle the challenge of device heterogeneity under
strict data sharing constraints -- ensuring that no labels or raw data, whether
real or synthetic, are ever shared between nodes. Experimental results shows
that our approach demonstrates consistent and significant improvements across
key performance metrics, where it achieves 1.1x -- 2.2x higher image generation
scores, an average 10% boost in classification metrics (up to 50% in
multi-domain non-IID settings), in much lower latency compared to several
benchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.

</details>


### [53] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR is a two-stage framework using video diffusion pre-training and masked inverse dynamics for bimanual robotic manipulation, achieving strong generalization with minimal human demonstrations.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and embodiment heterogeneity hinder scaling in bimanual robotic manipulation. VIDAR aims to address these challenges.

Method: VIDAR combines large-scale video diffusion pre-training on 750K multi-view videos with a masked inverse dynamics model for action prediction, avoiding pixel-level labels.

Result: With only 20 minutes of human demonstrations (1% of typical data), VIDAR generalizes to unseen tasks and backgrounds, outperforming state-of-the-art methods.

Conclusion: Video foundation models with masked action prediction can enable scalable and generalizable robotic manipulation in diverse real-world settings.

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [54] [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
*Nikita Koriagin,Yaroslav Aksenov,Daniil Laptev,Gleb Gerasimov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: A residual learning approach improves Sparse Autoencoders (SAEs) by capturing domain-specific features missed by primary models, enhancing interpretability without full retraining.


<details>
  <summary>Details</summary>
Motivation: Address the feature blindness of SAEs in capturing domain-specific features not prevalent in their training data.

Method: Train a secondary SAE to model reconstruction errors of a pretrained SAE on domain-specific texts, combining outputs during inference.

Result: Significant improvements in LLM cross-entropy and explained variance metrics across specialized domains, while maintaining general task performance.

Conclusion: Enables selective enhancement of SAE interpretability for specific domains, advancing targeted mechanistic interpretability of LLMs.

Abstract: Sparse Autoencoders have emerged as powerful tools for interpreting the
internal representations of Large Language Models, yet they often fail to
capture domain-specific features not prevalent in their training corpora. This
paper introduces a residual learning approach that addresses this feature
blindness without requiring complete retraining. We propose training a
secondary SAE specifically to model the reconstruction error of a pretrained
SAE on domain-specific texts, effectively capturing features missed by the
primary model. By summing the outputs of both models during inference, we
demonstrate significant improvements in both LLM cross-entropy and explained
variance metrics across multiple specialized domains. Our experiments show that
this method efficiently incorporates new domain knowledge into existing SAEs
while maintaining their performance on general tasks. This approach enables
researchers to selectively enhance SAE interpretability for specific domains of
interest, opening new possibilities for targeted mechanistic interpretability
of LLMs.

</details>


### [55] [Learning to Reject Low-Quality Explanations via User Feedback](https://arxiv.org/abs/2507.12900)
*Luca Stradiotti,Dario Pesenti,Stefano Teso,Jesse Davis*

Main category: cs.LG

TL;DR: The paper introduces a framework (LtX) for classifiers to reject inputs with low-quality explanations, proposing ULER to assess and mirror human judgments of explanation quality.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications like credit scoring require trustworthy ML predictions, but poor explanations can hinder user trust and decision-making.

Method: ULER, a rejector, is trained using human ratings and per-feature relevance judgments to evaluate explanation quality.

Result: ULER outperforms existing methods on eight benchmarks and a new human-annotated dataset.

Conclusion: The framework enables classifiers to reject low-quality explanations, improving trust and usability in high-stakes applications.

Abstract: Machine Learning predictors are increasingly being employed in high-stakes
applications such as credit scoring. Explanations help users unpack the reasons
behind their predictions, but are not always "high quality''. That is,
end-users may have difficulty interpreting or believing them, which can
complicate trust assessment and downstream decision-making. We argue that
classifiers should have the option to refuse handling inputs whose predictions
cannot be explained properly and introduce a framework for learning to reject
low-quality explanations (LtX) in which predictors are equipped with a rejector
that evaluates the quality of explanations. In this problem setting, the key
challenges are how to properly define and assess explanation quality and how to
design a suitable rejector. Focusing on popular attribution techniques, we
introduce ULER (User-centric Low-quality Explanation Rejector), which learns a
simple rejector from human ratings and per-feature relevance judgments to
mirror human judgments of explanation quality. Our experiments show that ULER
outperforms both state-of-the-art and explanation-aware learning to reject
strategies at LtX on eight classification and regression benchmarks and on a
new human-annotated dataset, which we will publicly release to support future
research.

</details>


### [56] [SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs](https://arxiv.org/abs/2507.13001)
*Kossi Amouzouvi,Bowen Song,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.LG

TL;DR: The paper proposes a framework to evaluate and assign the best geometric transformations for each relation in knowledge graphs, improving representation learning.


<details>
  <summary>Details</summary>
Motivation: Current KGE models use generic geometric transformations for all relations, lacking relation-specific adaptability.

Method: The framework ranks relations by their fit with different geometric transformations, assigns the best match, or uses majority voting. It employs an attention mechanism for learning relation-specific transformations.

Result: The model shows competitive performance on benchmark and real-world financial KGs.

Conclusion: Relation-specific geometric transformations enhance KGE model effectiveness, as demonstrated by evaluations.

Abstract: Knowledge graph representation learning approaches provide a mapping between
symbolic knowledge in the form of triples in a knowledge graph (KG) and their
feature vectors. Knowledge graph embedding (KGE) models often represent
relations in a KG as geometric transformations. Most state-of-the-art (SOTA)
KGE models are derived from elementary geometric transformations (EGTs), such
as translation, scaling, rotation, and reflection, or their combinations. These
geometric transformations enable the models to effectively preserve specific
structural and relational patterns of the KG. However, the current use of EGTs
by KGEs remains insufficient without considering relation-specific
transformations. Although recent models attempted to address this problem by
ensembling SOTA baseline models in different ways, only a single or composite
version of geometric transformations are used by such baselines to represent
all the relations. In this paper, we propose a framework that evaluates how
well each relation fits with different geometric transformations. Based on this
ranking, the model can: (1) assign the best-matching transformation to each
relation, or (2) use majority voting to choose one transformation type to apply
across all relations. That is, the model learns a single relation-specific EGT
in low dimensional vector space through an attention mechanism. Furthermore, we
use the correlation between relations and EGTs, which are learned in a low
dimension, for relation embeddings in a high dimensional vector space. The
effectiveness of our models is demonstrated through comprehensive evaluations
on three benchmark KGs as well as a real-world financial KG, witnessing a
performance comparable to leading models

</details>


### [57] [Fremer: Lightweight and Effective Frequency Transformer for Workload Forecasting in Cloud Services](https://arxiv.org/abs/2507.12908)
*Jiadong Chen,Hengyu Ye,Fuxin Jiang,Xiao He,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: Fremer is an efficient deep forecasting model for workload prediction in cloud services, outperforming Transformer-based models in accuracy and efficiency while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models lack computational efficiency for large-scale cloud environments, and workload series often have complex periodic patterns better addressed in the frequency domain.

Method: Fremer is proposed as a deep forecasting model that operates in the frequency domain to handle periodic patterns efficiently.

Result: Fremer surpasses SOTA models in accuracy (5.5% MSE, 4.7% MAE, 8.6% SMAPE improvements) and reduces computational costs. It also improves auto-scaling performance (18.78% latency reduction, 2.35% resource savings).

Conclusion: Fremer is a highly efficient and accurate solution for workload forecasting in cloud services, validated by extensive experiments and real-world applications.

Abstract: Workload forecasting is pivotal in cloud service applications, such as
auto-scaling and scheduling, with profound implications for operational
efficiency. Although Transformer-based forecasting models have demonstrated
remarkable success in general tasks, their computational efficiency often falls
short of the stringent requirements in large-scale cloud environments. Given
that most workload series exhibit complicated periodic patterns, addressing
these challenges in the frequency domain offers substantial advantages. To this
end, we propose Fremer, an efficient and effective deep forecasting model.
Fremer fulfills three critical requirements: it demonstrates superior
efficiency, outperforming most Transformer-based forecasting models; it
achieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in
workload forecasting; and it exhibits robust performance for multi-period
series. Furthermore, we collect and open-source four high-quality, open-source
workload datasets derived from ByteDance's cloud services, encompassing
workload data from thousands of computing instances. Extensive experiments on
both our proprietary datasets and public benchmarks demonstrate that Fremer
consistently outperforms baseline models, achieving average improvements of
5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while
simultaneously reducing parameter scale and computational costs. Additionally,
in a proactive auto-scaling test based on Kubernetes, Fremer improves average
latency by 18.78% and reduces resource consumption by 2.35%, underscoring its
practical efficacy in real-world applications.

</details>


### [58] [MUPAX: Multidimensional Problem Agnostic eXplainable AI](https://arxiv.org/abs/2507.13090)
*Vincenzo Dentamaro,Felice Franchini,Giuseppe Pirlo,Irina Voiculescu*

Main category: cs.LG

TL;DR: MUPAX is a deterministic, model-agnostic XAI technique with guaranteed convergence, offering principled feature importance attribution and enhancing model accuracy by focusing on key data patterns.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust XAI techniques that are deterministic, model-agnostic, and converge reliably, while improving model performance.

Method: MUPAX uses measure-theoretic formulation and structured perturbation analysis to identify input patterns and eliminate spurious relationships, applicable across various data modalities (1D, 2D, 3D).

Result: MUPAX outperforms other XAI methods by preserving or enhancing model accuracy and generating precise, consistent explanations.

Conclusion: MUPAX advances explainable and trustworthy AI by providing dimension-agnostic, rigorous, and understandable explanations.

Abstract: Robust XAI techniques should ideally be simultaneously deterministic, model
agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM
AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability
technique, with guaranteed convergency. MUPAX measure theoretic formulation
gives principled feature importance attribution through structured perturbation
analysis that discovers inherent input patterns and eliminates spurious
relationships. We evaluate MUPAX on an extensive range of data modalities and
tasks: audio classification (1D), image classification (2D), volumetric medical
image analysis (3D), and anatomical landmark detection, demonstrating dimension
agnostic effectiveness. The rigorous convergence guarantees extend to any loss
function and arbitrary dimensions, making MUPAX applicable to virtually any
problem context for AI. By contrast with other XAI methods that typically
decrease performance when masking, MUPAX not only preserves but actually
enhances model accuracy by capturing only the most important patterns of the
original data. Extensive benchmarking against the state of the XAI art
demonstrates MUPAX ability to generate precise, consistent and understandable
explanations, a crucial step towards explainable and trustworthy AI systems.
The source code will be released upon publication.

</details>


### [59] [Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI](https://arxiv.org/abs/2507.12913)
*Chenrui Zhu,Louenas Bounia,Vu Linh Nguyen,Sébastien Destercke,Arthur Hoarau*

Main category: cs.LG

TL;DR: The paper proposes using prediction uncertainty (aleatoric and epistemic) to enhance model interpretability by guiding explanation selection and rejecting unreliable ones.


<details>
  <summary>Details</summary>
Motivation: The need for transparency in complex machine learning models, as interpretability decreases with model complexity.

Method: Leveraging aleatoric and epistemic uncertainty to select appropriate explanations (feature-importance or counterfactual) and reject unreliable ones.

Result: Demonstrates improved robustness and attainability of explanations in traditional and deep learning scenarios.

Conclusion: Uncertainty-aware approaches enhance interpretability by providing reliable and context-specific explanations.

Abstract: Recent advancements in machine learning have emphasized the need for
transparency in model predictions, particularly as interpretability diminishes
when using increasingly complex architectures. In this paper, we propose
leveraging prediction uncertainty as a complementary approach to classical
explainability methods. Specifically, we distinguish between aleatoric
(data-related) and epistemic (model-related) uncertainty to guide the selection
of appropriate explanations. Epistemic uncertainty serves as a rejection
criterion for unreliable explanations and, in itself, provides insight into
insufficient training (a new form of explanation). Aleatoric uncertainty
informs the choice between feature-importance explanations and counterfactual
explanations. This leverages a framework of explainability methods driven by
uncertainty quantification and disentanglement. Our experiments demonstrate the
impact of this uncertainty-aware approach on the robustness and attainability
of explanations in both traditional machine learning and deep learning
scenarios.

</details>


### [60] [Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities](https://arxiv.org/abs/2507.13158)
*Hao Sun,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: A review of LLM alignment using inverse reinforcement learning (IRL), highlighting distinctions from conventional RL, neural reward models, and open challenges.


<details>
  <summary>Details</summary>
Motivation: Alignment is critical for reliable and controllable LLMs, with RL playing a key role. This paper explores IRL's potential for LLM alignment.

Method: Reviews recent advances in IRL for LLM alignment, covering neural reward models, datasets, benchmarks, and training techniques.

Result: Identifies key challenges and opportunities, emphasizing the need for human-data-driven reward models and efficient training methods.

Conclusion: Synthesizes findings to outline unresolved challenges and future directions for improving LLM alignment via RL and IRL.

Abstract: In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.

</details>


### [61] [Trace Reconstruction with Language Models](https://arxiv.org/abs/2507.12927)
*Franziska Weindel,Michael Girsch,Reinhard Heckel*

Main category: cs.LG

TL;DR: TReconLM uses language models for trace reconstruction, outperforming existing methods by adapting to error patterns in DNA data storage.


<details>
  <summary>Details</summary>
Motivation: The problem arises in DNA data storage due to errors in synthesis, storage, and sequencing, requiring robust reconstruction algorithms.

Method: TReconLM pretrains language models on synthetic data and fine-tunes on real-world data to handle technology-specific errors.

Result: TReconLM recovers more sequences without error than state-of-the-art methods, including deep learning approaches.

Conclusion: TReconLM demonstrates superior performance in trace reconstruction for DNA data storage applications.

Abstract: The general trace reconstruction problem seeks to recover an original
sequence from its noisy copies independently corrupted by deletions,
insertions, and substitutions. This problem arises in applications such as DNA
data storage, a promising storage medium due to its high information density
and longevity. However, errors introduced during DNA synthesis, storage, and
sequencing require correction through algorithms and codes, with trace
reconstruction often used as part of the data retrieval process. In this work,
we propose TReconLM, which leverages language models trained on next-token
prediction for trace reconstruction. We pretrain language models on synthetic
data and fine-tune on real-world data to adapt to technology-specific error
patterns. TReconLM outperforms state-of-the-art trace reconstruction
algorithms, including prior deep learning approaches, recovering a
substantially higher fraction of sequences without error.

</details>


### [62] [Merge Kernel for Bayesian Optimization on Permutation Space](https://arxiv.org/abs/2507.13263)
*Zikai Xie,Linjiang Chen*

Main category: cs.LG

TL;DR: A novel framework for generating kernel functions on permutation space is introduced, replacing the quadratic complexity of the Mallows kernel with a linearithmic Merge Kernel derived from merge sort, achieving better performance in Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: The current state-of-the-art BO approach for permutation spaces relies on the Mallows kernel, which has quadratic complexity and explicitly enumerates every pairwise comparison. The goal is to reduce this complexity while maintaining effectiveness.

Method: A framework for generating kernel functions based on sorting algorithms is proposed. The Merge Kernel, derived from merge sort, reduces complexity to Θ(n log n). Additional lightweight descriptors (shift histogram, split-pair line, sliding-window motifs) are incorporated for robustness and invariance.

Result: Empirical evaluation shows the Merge Kernel outperforms the Mallows kernel across permutation optimization benchmarks, providing a more compact and effective solution.

Conclusion: The Merge Kernel offers a significant improvement over the Mallows kernel in terms of complexity and performance, making it a superior choice for Bayesian optimization in permutation spaces.

Abstract: Bayesian Optimization (BO) algorithm is a standard tool for black-box
optimization problems. The current state-of-the-art BO approach for permutation
spaces relies on the Mallows kernel-an $\Omega(n^2)$ representation that
explicitly enumerates every pairwise comparison. Inspired by the close
relationship between the Mallows kernel and pairwise comparison, we propose a
novel framework for generating kernel functions on permutation space based on
sorting algorithms. Within this framework, the Mallows kernel can be viewed as
a special instance derived from bubble sort. Further, we introduce the
\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic
complexity with $\Theta(n\log n)$ to achieve the lowest possible complexity.
The resulting feature vector is significantly shorter, can be computed in
linearithmic time, yet still efficiently captures meaningful permutation
distances. To boost robustness and right-invariance without sacrificing
compactness, we further incorporate three lightweight, task-agnostic
descriptors: (1) a shift histogram, which aggregates absolute element
displacements and supplies a global misplacement signal; (2) a split-pair line,
which encodes selected long-range comparisons by aligning elements across the
two halves of the whole permutation; and (3) sliding-window motifs, which
summarize local order patterns that influence near-neighbor objectives. Our
empirical evaluation demonstrates that the proposed kernel consistently
outperforms the state-of-the-art Mallows kernel across various permutation
optimization benchmarks. Results confirm that the Merge Kernel provides a more
compact yet more effective solution for Bayesian optimization in permutation
space.

</details>


### [63] [From a Mixed-Policy Perspective: Improving Differentiable Automatic Post-editing Optimization](https://arxiv.org/abs/2507.12931)
*Hongze Tan*

Main category: cs.LG

TL;DR: Two modifications to DAPO improve stability and sample efficiency: using a pre-trained guiding policy and re-utilizing zero-reward samples.


<details>
  <summary>Details</summary>
Motivation: Standard policy gradient methods face instability and inefficiency in sparse reward settings.

Method: 1. Incorporate a pre-trained guiding policy for off-policy experience. 2. Re-use zero-reward samples guided by an expert policy.

Result: Improved training stability, faster convergence, and enhanced sample efficiency. Theoretical analysis confirms convergence to optimal solutions.

Conclusion: The mixed-policy framework balances exploration and exploitation, enabling more stable and efficient policy optimization.

Abstract: This paper introduces two novel modifications to the Differentiable Automatic
Post-editing Optimization (DAPO) algorithm, approached from a mixed-policy
perspective. Standard policy gradient methods can suffer from instability and
sample inefficiency, particularly in sparse reward settings. To address this,
we first propose a method that incorporates a pre-trained, stable guiding
policy ($\piphi$) to provide off-policy experience, thereby regularizing the
training of the target policy ($\pion$). This approach improves training
stability and convergence speed by adaptively adjusting the learning step size.
Secondly, we extend this idea to re-utilize zero-reward samples, which are
often discarded by dynamic sampling strategies like DAPO's. By treating these
samples as a distinct batch guided by the expert policy, we further enhance
sample efficiency. We provide a theoretical analysis for both methods,
demonstrating that their objective functions converge to the optimal solution
within the established theoretical framework of reinforcement learning. The
proposed mixed-policy framework effectively balances exploration and
exploitation, promising more stable and efficient policy optimization.

</details>


### [64] [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
*Weiqiu You,Anton Xue,Shreya Havaldar,Delip Rao,Helen Jin,Chris Callison-Burch,Eric Wong*

Main category: cs.LG

TL;DR: ARES is a probabilistic framework for detecting propagated errors in LLM reasoning chains, outperforming existing methods with certified guarantees.


<details>
  <summary>Details</summary>
Motivation: Initial errors in LLM reasoning chains propagate, undermining reliability. Current methods fail to detect these due to improper accounting for error corruption in downstream reasoning.

Method: Introduces ARES, an autoregressive framework that judges claims based on sound premises, providing nuanced scores and statistical guarantees.

Result: ARES achieves state-of-the-art performance (72.1% Macro-F1, +8.2 points) and excels in robustness on long reasoning chains (90.3% F1, +27.6 points).

Conclusion: ARES effectively detects propagated errors, offering certified soundness and superior performance over existing methods.

Abstract: In reasoning chains generated by large language models (LLMs), initial errors
often propagate and undermine the reliability of the final conclusion. Current
LLM-based error detection methods often fail to detect propagated errors
because they do not properly account for how earlier errors might corrupt
judgments of downstream reasoning. To better detect such propagated errors, we
introduce Autoregressive Reasoning Entailment Stability (ARES), a novel
probabilistic framework that prevents error propagation by judging each claim
based only on previously-assessed sound premises. This inductive method yields
a nuanced score for each step and provides certified statistical guarantees of
its soundness, rather than a brittle binary label. ARES achieves
state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2
points) and demonstrates superior robustness on very long synthetic reasoning
chains, where it excels at detecting propagated errors (90.3% F1, +27.6
points).

</details>


### [65] [Insights into a radiology-specialised multimodal large language model with sparse autoencoders](https://arxiv.org/abs/2507.12950)
*Kenza Bouzid,Shruthi Bannur,Daniel Coelho de Castro,Anton Schwaighofer,Javier Alvarez-Valle,Stephanie L. Hyland*

Main category: cs.LG

TL;DR: The study uses Matryoshka-SAE to interpret MAIRA-2, a radiology-specialized multimodal LLM, identifying clinically relevant features and demonstrating steering control, despite challenges.


<details>
  <summary>Details</summary>
Motivation: Improving interpretability of AI models in healthcare to enhance safety, transparency, and trust.

Method: Applied Matryoshka-SAE to MAIRA-2 for automated interpretability of features, including medical devices, pathologies, and textual features.

Result: Identified clinically relevant concepts and demonstrated directional control over model behavior, though with mixed success.

Conclusion: The study advances mechanistic understanding of MAIRA-2, highlighting challenges but paving the way for improved transparency in radiology-adapted LLMs.

Abstract: Interpretability can improve the safety, transparency and trust of AI models,
which is especially important in healthcare applications where decisions often
carry significant consequences. Mechanistic interpretability, particularly
through the use of sparse autoencoders (SAEs), offers a promising approach for
uncovering human-interpretable features within large transformer-based models.
In this study, we apply Matryoshka-SAE to the radiology-specialised multimodal
large language model, MAIRA-2, to interpret its internal representations. Using
large-scale automated interpretability of the SAE features, we identify a range
of clinically relevant concepts - including medical devices (e.g., line and
tube placements, pacemaker presence), pathologies such as pleural effusion and
cardiomegaly, longitudinal changes and textual features. We further examine the
influence of these features on model behaviour through steering, demonstrating
directional control over generations with mixed success. Our results reveal
practical and methodological challenges, yet they offer initial insights into
the internal concepts learned by MAIRA-2 - marking a step toward deeper
mechanistic understanding and interpretability of a radiology-adapted
multimodal large language model, and paving the way for improved model
transparency. We release the trained SAEs and interpretations:
https://huggingface.co/microsoft/maira-2-sae.

</details>


### [66] [A Spectral Interpretation of Redundancy in a Graph Reservoir](https://arxiv.org/abs/2507.12963)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: The paper proposes a variant of the Multiresolution Reservoir Graph Neural Network (MRGNN) using a Fairing algorithm to address over-smoothing in GNNs, connecting it to spectral filtering and random walks theory.


<details>
  <summary>Details</summary>
Motivation: Over-smoothing in GNNs due to repeated layer operations is a common issue. The work aims to improve spectral reservoir models by introducing a controlled smoothing method.

Method: The paper revisits the MRGNN reservoir definition and introduces a Fairing algorithm-based variant, adapting it to graphs via the Laplacian operator for spectral filtering.

Result: Theoretical analysis links spectral coefficient tuning to modulating redundant random walks. Experiments with MRGNN show promising results for graph classification.

Conclusion: The proposed method offers controlled smoothing for GNNs, with potential applications in tasks like graph classification, supported by theoretical insights and experimental validation.

Abstract: Reservoir computing has been successfully applied to graphs as a
preprocessing method to improve the training efficiency of Graph Neural
Networks (GNNs). However, a common issue that arises when repeatedly applying
layer operators on graphs is over-smoothing, which consists in the convergence
of graph signals toward low-frequency components of the graph Laplacian. This
work revisits the definition of the reservoir in the Multiresolution Reservoir
Graph Neural Network (MRGNN), a spectral reservoir model, and proposes a
variant based on a Fairing algorithm originally introduced in the field of
surface design in computer graphics. This algorithm provides a pass-band
spectral filter that allows smoothing without shrinkage, and it can be adapted
to the graph setting through the Laplacian operator. Given its spectral
formulation, this method naturally connects to GNN architectures for tasks
where smoothing, when properly controlled, can be beneficial,such as graph
classification. The core contribution of the paper lies in the theoretical
analysis of the algorithm from a random walks perspective. In particular, it
shows how tuning the spectral coefficients can be interpreted as modulating the
contribution of redundant random walks. Exploratory experiments based on the
MRGNN architecture illustrate the potential of this approach and suggest
promising directions for future research.

</details>


### [67] [WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring](https://arxiv.org/abs/2507.12969)
*Reza Riahi Samani,Alfredo Nunez,Bart De Schutter*

Main category: cs.LG

TL;DR: A deep learning framework, WaveletInception-BiLSTM, is proposed for infrastructure health monitoring using drive-by vibration signals, outperforming existing methods in railway track stiffness estimation.


<details>
  <summary>Details</summary>
Motivation: The need for accurate, localized, and automated infrastructure health monitoring drives the development of a novel deep learning approach leveraging spectral and temporal information from vibration signals.

Method: The framework combines a Learnable Wavelet Packet Transform (LWPT) for spectral feature extraction, 1D Inception networks for multi-scale features, and BiLSTM for temporal dependency capture, enabling high-resolution health assessment without preprocessing.

Result: The model significantly outperforms state-of-the-art methods in estimating railway ballast and railpad stiffness, demonstrating its effectiveness for automated monitoring.

Conclusion: The WaveletInception-BiLSTM framework offers a robust solution for accurate and automated infrastructure health monitoring, validated by superior performance in railway track stiffness estimation.

Abstract: This paper presents a novel deep learning-based framework for infrastructure
health monitoring using drive-by vibration response signals. Recognizing the
importance of spectral and temporal information, we introduce the
WaveletInception-BiLSTM network. The WaveletInception feature extractor
utilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting
vibration signal features, incorporating spectral information in the early
network layers. This is followed by 1D Inception networks that extract
multi-scale, high-level features at deeper layers. The extracted vibration
signal features are then integrated with operational conditions via a Long
Short-term Memory (LSTM) layer. The resulting feature extraction network
effectively analyzes drive-by vibration signals across various measurement
speeds without preprocessing and uses LSTM to capture interrelated temporal
dependencies among different modes of information and to create feature vectors
for health condition estimation. The estimator head is designed with a
sequential modeling architecture using bidirectional LSTM (BiLSTM) networks,
capturing bi-directional temporal relationships from drive-by measurements.
This architecture allows for a high-resolution, beam-level assessment of
infrastructure health conditions. A case study focusing on railway track
stiffness estimation with simulated drive-by vibration signals shows that the
model significantly outperforms state-of-the-art methods in estimating railway
ballast and railpad stiffness parameters. Results underscore the potential of
this approach for accurate, localized, and fully automated drive-by
infrastructure health monitoring.

</details>


### [68] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: FedGA is a fairness-aware federated learning algorithm that addresses performance disparities in heterogeneous data settings by dynamically adjusting aggregation weights based on real-time fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning leads to unfair performance disparities across clients, raising concerns about equitable model behavior.

Method: Uses the Gini coefficient to measure disparity, establishes a relationship between the Gini coefficient and model updates, and dynamically adjusts aggregation weights for fairness.

Result: Experiments on Office-Caltech-10, CIFAR-10, and Synthetic datasets show improved fairness metrics (variance, Gini coefficient) without compromising overall performance.

Conclusion: FedGA effectively balances fairness and performance in federated learning, demonstrating its practical utility.

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [69] [Fault detection and diagnosis for the engine electrical system of a space launcher based on a temporal convolutional autoencoder and calibrated classifiers](https://arxiv.org/abs/2507.13022)
*Luis Basora,Louison Bocquet-Nouaille,Elinirina Robinson,Serge Le Gonidec*

Main category: cs.LG

TL;DR: A fault detection and diagnostic framework for reusable space launchers' electrical systems, using a temporal convolutional autoencoder and calibrated classifiers, with techniques for confidence estimation, OOD detection, and false alarm control.


<details>
  <summary>Details</summary>
Motivation: To address the need for onboard fault detection in next-gen space launchers, meeting broader requirements like confidence estimation, OOD detection, and false alarm control.

Method: Uses a temporal convolutional autoencoder for feature extraction, binary/multiclass classifiers for fault detection/diagnosis, and techniques like inductive conformal anomaly detection, CUSUM, and threshold moving.

Result: Evaluated on simulated data, showing promise but requires real-data testing for operational maturity.

Conclusion: The framework is a promising initial step, needing further validation with real-world data.

Abstract: In the context of the health monitoring for the next generation of reusable
space launchers, we outline a first step toward developing an onboard fault
detection and diagnostic capability for the electrical system that controls the
engine valves. Unlike existing approaches in the literature, our solution is
designed to meet a broader range of key requirements. This includes estimating
confidence levels for predictions, detecting out-of-distribution (OOD) cases,
and controlling false alarms. The proposed solution is based on a temporal
convolutional autoencoder to automatically extract low-dimensional features
from raw sensor data. Fault detection and diagnosis are respectively carried
out using a binary and a multiclass classifier trained on the autoencoder
latent and residual spaces. The classifiers are histogram-based gradient
boosting models calibrated to output probabilities that can be interpreted as
confidence levels. A relatively simple technique, based on inductive conformal
anomaly detection, is used to identify OOD data. We leverage other simple yet
effective techniques, such as cumulative sum control chart (CUSUM) to limit the
false alarms, and threshold moving to address class imbalance in fault
detection. The proposed framework is highly configurable and has been evaluated
on simulated data, covering both nominal and anomalous operational scenarios.
The results indicate that our solution is a promising first step, though
testing with real data will be necessary to ensure that it achieves the
required maturity level for operational use.

</details>


### [70] [Confidence-Filtered Relevance (CFR): An Interpretable and Uncertainty-Aware Machine Learning Framework for Naturalness Assessment in Satellite Imagery](https://arxiv.org/abs/2507.13034)
*Ahmed Emam,Ribana Roscher*

Main category: cs.LG

TL;DR: The paper introduces Confidence-Filtered Relevance (CFR), a framework combining LRP Attention Rollout and Deep Deterministic Uncertainty to analyze how model uncertainty affects interpretability in naturalness assessments of satellite imagery.


<details>
  <summary>Details</summary>
Motivation: Current methods for monitoring protected natural areas lack interpretability and uncertainty-awareness, particularly in assessing naturalness.

Method: CFR integrates LRP Attention Rollout with Deep Deterministic Uncertainty (DDU) to partition datasets by uncertainty thresholds, analyzing how uncertainty impacts relevance heatmaps.

Result: CFR identified shrublands, forests, and wetlands as highly relevant for naturalness, but interpretability declined as uncertainty increased, showing more ambiguous attributions.

Conclusion: CFR offers a data-centric approach to assess naturalness in satellite imagery by linking relevance to certainty, improving interpretability and uncertainty-awareness.

Abstract: Protected natural areas play a vital role in ecological balance and ecosystem
services. Monitoring these regions at scale using satellite imagery and machine
learning is promising, but current methods often lack interpretability and
uncertainty-awareness, and do not address how uncertainty affects naturalness
assessment. In contrast, we propose Confidence-Filtered Relevance (CFR), a
data-centric framework that combines LRP Attention Rollout with Deep
Deterministic Uncertainty (DDU) estimation to analyze how model uncertainty
influences the interpretability of relevance heatmaps. CFR partitions the
dataset into subsets based on uncertainty thresholds, enabling systematic
analysis of how uncertainty shapes the explanations of naturalness in satellite
imagery. Applied to the AnthroProtect dataset, CFR assigned higher relevance to
shrublands, forests, and wetlands, aligning with other research on naturalness
assessment. Moreover, our analysis shows that as uncertainty increases, the
interpretability of these relevance heatmaps declines and their entropy grows,
indicating less selective and more ambiguous attributions. CFR provides a
data-centric approach to assess the relevance of patterns to naturalness in
satellite imagery based on their associated certainty.

</details>


### [71] [The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting](https://arxiv.org/abs/2507.13043)
*Lefei Shen,Mouxiang Chen,Han Fu,Xiaoxue Ren,Xiaoyun Joy Wang,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.LG

TL;DR: The paper analyzes Transformer architectures for Long-term Time Series Forecasting (LTSF), proposing a taxonomy to disentangle design variations and identify optimal choices.


<details>
  <summary>Details</summary>
Motivation: To determine the best Transformer architecture for LTSF, as existing models are often conflated with time-series-specific designs, obscuring architectural impact.

Method: Introduces a taxonomy to isolate architectural components (attention mechanisms, forecasting aggregations, paradigms, normalization) and conducts experiments to compare them.

Result: Bi-directional attention with joint-attention is most effective; complete forecasting aggregation improves performance; direct-mapping outperforms autoregressive approaches.

Conclusion: The combined model with optimal architectural choices outperforms existing models, providing clear guidance for future Transformer designs in LTSF.

Abstract: Transformer-based models have recently become dominant in Long-term Time
Series Forecasting (LTSF), yet the variations in their architecture, such as
encoder-only, encoder-decoder, and decoder-only designs, raise a crucial
question: What Transformer architecture works best for LTSF tasks? However,
existing models are often tightly coupled with various time-series-specific
designs, making it difficult to isolate the impact of the architecture itself.
To address this, we propose a novel taxonomy that disentangles these designs,
enabling clearer and more unified comparisons of Transformer architectures. Our
taxonomy considers key aspects such as attention mechanisms, forecasting
aggregations, forecasting paradigms, and normalization layers. Through
extensive experiments, we uncover several key insights: bi-directional
attention with joint-attention is most effective; more complete forecasting
aggregation improves performance; and the direct-mapping paradigm outperforms
autoregressive approaches. Furthermore, our combined model, utilizing optimal
architectural choices, consistently outperforms several existing models,
reinforcing the validity of our conclusions. We hope these findings offer
valuable guidance for future research on Transformer architectural designs in
LTSF. Our code is available at https://github.com/HALF111/TSF_architecture.

</details>


### [72] [On statistical learning of graphs](https://arxiv.org/abs/2507.13054)
*Vittorio Cipriani,Valentino Delle Rose,Luca San Mauro,Giovanni Solda*

Main category: cs.LG

TL;DR: The paper explores PAC and online learnability of hypothesis classes derived from countably infinite graphs, focusing on finite-support permutations. It links PAC learnability to online learnability via automorphic triviality and characterizes non-learnable graphs using relaxed extension properties. It also simplifies learnability conditions for k-vertex permutations.


<details>
  <summary>Details</summary>
Motivation: To understand the learnability of graph labelings under structural constraints, particularly for infinite graphs with finite-support permutations, and to bridge PAC and online learnability.

Method: Analyzes hypothesis classes formed by permuting vertices of a countably infinite graph, focusing on finite-support permutations. Uses automorphic triviality and relaxed extension properties for characterizations.

Result: PAC learnability of finite-support copies implies online learnability of the full isomorphism type, equivalent to automorphic triviality. Non-learnable graphs are characterized, and learnability for k-vertex permutations simplifies to 2-vertex cases.

Conclusion: The study provides a framework for understanding learnability in infinite graphs, linking PAC and online learnability, and simplifies conditions for permutation-based learnability.

Abstract: We study PAC and online learnability of hypothesis classes formed by copies
of a countably infinite graph G, where each copy is induced by permuting G's
vertices. This corresponds to learning a graph's labeling, knowing its
structure and label set. We consider classes where permutations move only
finitely many vertices. Our main result shows that PAC learnability of all such
finite-support copies implies online learnability of the full isomorphism type
of G, and is equivalent to the condition of automorphic triviality. We also
characterize graphs where copies induced by swapping two vertices are not
learnable, using a relaxation of the extension property of the infinite random
graph. Finally, we show that, for all G and k>2, learnability for k-vertex
permutations is equivalent to that for 2-vertex permutations, yielding a
four-class partition of infinite graphs, whose complexity we also determine
using tools coming from both descriptive set theory and computability theory.

</details>


### [73] [DASViT: Differentiable Architecture Search for Vision Transformer](https://arxiv.org/abs/2507.13079)
*Pengjin Wu,Ferrante Neri,Zhenhua Feng*

Main category: cs.LG

TL;DR: DASViT introduces a differentiable search method for Vision Transformers, outperforming ViT-B/16 with fewer parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Existing NAS methods for ViTs are computationally expensive and struggle with innovation, prompting the need for a differentiable approach.

Method: DASViT applies Differentiable Architecture Search (DARTS) to Vision Transformers, enabling efficient and novel architectural designs.

Result: DASViT discovers non-traditional Transformer encoder designs, outperforms ViT-B/16 on multiple datasets, and achieves better efficiency.

Conclusion: DASViT successfully bridges the gap in differentiable search for ViTs, offering innovative and efficient architectures.

Abstract: Designing effective neural networks is a cornerstone of deep learning, and
Neural Architecture Search (NAS) has emerged as a powerful tool for automating
this process. Among the existing NAS approaches, Differentiable Architecture
Search (DARTS) has gained prominence for its efficiency and ease of use,
inspiring numerous advancements. Since the rise of Vision Transformers (ViT),
researchers have applied NAS to explore ViT architectures, often focusing on
macro-level search spaces and relying on discrete methods like evolutionary
algorithms. While these methods ensure reliability, they face challenges in
discovering innovative architectural designs, demand extensive computational
resources, and are time-intensive. To address these limitations, we introduce
Differentiable Architecture Search for Vision Transformer (DASViT), which
bridges the gap in differentiable search for ViTs and uncovers novel designs.
Experiments show that DASViT delivers architectures that break traditional
Transformer encoder designs, outperform ViT-B/16 on multiple datasets, and
achieve superior efficiency with fewer parameters and FLOPs.

</details>


### [74] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: The paper proposes a cross-modal knowledge distillation framework to address EEG learning challenges like modality gap and label misalignment, improving performance in emotion tasks.


<details>
  <summary>Details</summary>
Motivation: EEG signals in BCIs are prone to errors, degrading model performance. Multimodal knowledge distillation (KD) is explored but faces modality gaps and label misalignment.

Method: A novel framework aligns feature semantics via prototype-based similarity and uses a task-specific distillation head to resolve label inconsistencies.

Result: The approach outperforms unimodal and multimodal baselines in EEG-based emotion regression and classification on a public dataset.

Conclusion: The framework shows promise for enhancing EEG learning in BCI applications by mitigating modality and label inconsistencies.

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


### [75] [NGTM: Substructure-based Neural Graph Topic Model for Interpretable Graph Generation](https://arxiv.org/abs/2507.13133)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: NGTM is a novel graph generation framework enhancing interpretability by modeling graphs as mixtures of latent topics, enabling semantic tracing and control.


<details>
  <summary>Details</summary>
Motivation: Existing graph generation methods lack interpretability, obscuring structural decisions.

Method: NGTM uses latent topics to represent graphs, integrating topic distributions with a global structural variable for transparent generation.

Result: NGTM achieves competitive generation quality with fine-grained control and interpretability, allowing topic-level adjustments.

Conclusion: NGTM advances graph generation by combining interpretability and control, useful for domains like molecular design.

Abstract: Graph generation plays a pivotal role across numerous domains, including
molecular design and knowledge graph construction. Although existing methods
achieve considerable success in generating realistic graphs, their
interpretability remains limited, often obscuring the rationale behind
structural decisions. To address this challenge, we propose the Neural Graph
Topic Model (NGTM), a novel generative framework inspired by topic modeling in
natural language processing. NGTM represents graphs as mixtures of latent
topics, each defining a distribution over semantically meaningful
substructures, which facilitates explicit interpretability at both local and
global scales. The generation process transparently integrates these topic
distributions with a global structural variable, enabling clear semantic
tracing of each generated graph. Experiments demonstrate that NGTM achieves
competitive generation quality while uniquely enabling fine-grained control and
interpretability, allowing users to tune structural features or induce
biological properties through topic-level adjustments.

</details>


### [76] [NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech](https://arxiv.org/abs/2507.13155)
*Maksim Borisov,Egor Spirin,Daria Diatlova*

Main category: cs.LG

TL;DR: NonverbalTTS (NVTTS) is a 17-hour open-access dataset for expressive speech synthesis, annotated with 10 NV types and 8 emotions. It enhances TTS models to match closed-source systems.


<details>
  <summary>Details</summary>
Motivation: Limited availability of diverse nonverbal vocalization datasets restricts expressive TTS research.

Method: Dataset derived from VoxCeleb and Expresso, using ASR, NV tagging, emotion classification, and human validation. Fine-tuned TTS models on NVTTS.

Result: Achieved parity with closed-source systems (e.g., CosyVoice2) in human evaluation and automatic metrics.

Conclusion: NVTTS addresses a key bottleneck in expressive TTS research; dataset and guidelines are publicly available.

Abstract: Current expressive speech synthesis models are constrained by the limited
availability of open-source datasets containing diverse nonverbal vocalizations
(NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access
dataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional
categories. The dataset is derived from popular sources, VoxCeleb and Expresso,
using automated detection followed by human validation. We propose a
comprehensive pipeline that integrates automatic speech recognition (ASR), NV
tagging, emotion classification, and a fusion algorithm to merge transcriptions
from multiple annotators. Fine-tuning open-source text-to-speech (TTS) models
on the NVTTS dataset achieves parity with closed-source systems such as
CosyVoice2, as measured by both human evaluation and automatic metrics,
including speaker similarity and NV fidelity. By releasing NVTTS and its
accompanying annotation guidelines, we address a key bottleneck in expressive
TTS research. The dataset is available at
https://huggingface.co/datasets/deepvk/NonverbalTTS.

</details>


### [77] [Spectral Bellman Method: Unifying Representation and Exploration in RL](https://arxiv.org/abs/2507.13181)
*Ofir Nabati,Bo Dai,Shie Mannor,Guy Tennenholtz*

Main category: cs.LG

TL;DR: Spectral Bellman Representation aligns feature learning with Bellman updates for value-based RL, improving exploration and performance in hard tasks.


<details>
  <summary>Details</summary>
Motivation: Existing representation learning in RL misaligns with tasks; this work aims to align it with Bellman updates for better value-based RL.

Method: Introduces Spectral Bellman Representation, leveraging the Inherent Bellman Error (IBE) condition and spectral relationships to learn Bellman-aligned features.

Result: Improves structured exploration and performance, especially in hard-exploration and long-horizon tasks.

Conclusion: Provides a principled, effective framework for learning powerful representations in value-based RL.

Abstract: The effect of representation has been demonstrated in reinforcement learning,
from both theoretical and empirical successes. However, the existing
representation learning mainly induced from model learning aspects, misaligning
with our RL tasks. This work introduces Spectral Bellman Representation, a
novel framework derived from the Inherent Bellman Error (IBE) condition, which
aligns with the fundamental structure of Bellman updates across a space of
possible value functions, therefore, directly towards value-based RL. Our key
insight is the discovery of a fundamental spectral relationship: under the
zero-IBE condition, the transformation of a distribution of value functions by
the Bellman operator is intrinsically linked to the feature covariance
structure. This spectral connection yields a new, theoretically-grounded
objective for learning state-action features that inherently capture this
Bellman-aligned covariance. Our method requires a simple modification to
existing algorithms. We demonstrate that our learned representations enable
structured exploration, by aligning feature covariance with Bellman dynamics,
and improve overall performance, particularly in challenging hard-exploration
and long-horizon credit assignment tasks. Our framework naturally extends to
powerful multi-step Bellman operators, further broadening its impact. Spectral
Bellman Representation offers a principled and effective path toward learning
more powerful and structurally sound representations for value-based
reinforcement learning.

</details>


### [78] [GradNetOT: Learning Optimal Transport Maps with GradNets](https://arxiv.org/abs/2507.13191)
*Shreyas Chaudhari,Srinivasa Pranav,José M. F. Moura*

Main category: cs.LG

TL;DR: The paper introduces Monotone Gradient Networks (mGradNets) to learn optimal transport maps by minimizing a loss based on the Monge-Ampère equation, demonstrating effectiveness in robot swarm control.


<details>
  <summary>Details</summary>
Motivation: Optimal transport problems, crucial in fields like fluid dynamics and robot swarm control, require solving the Monge formulation. Brenier's theorem links these problems to monotone gradient maps, motivating the development of mGradNets.

Method: The authors propose mGradNets, neural networks parameterizing monotone gradient maps, and train them by minimizing a loss derived from the Monge-Ampère equation.

Result: Empirical results show mGradNets effectively learn optimal transport maps, with applications in robot swarm control.

Conclusion: mGradNets provide a structured approach to learning optimal transport maps, validated by practical applications.

Abstract: Monotone gradient functions play a central role in solving the Monge
formulation of the optimal transport problem, which arises in modern
applications ranging from fluid dynamics to robot swarm control. When the
transport cost is the squared Euclidean distance, Brenier's theorem guarantees
that the unique optimal map is the gradient of a convex function, namely a
monotone gradient map, and it satisfies a Monge-Amp\`ere equation. In
[arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks
(mGradNets), neural networks that directly parameterize the space of monotone
gradient maps. In this work, we leverage mGradNets to directly learn the
optimal transport mapping by minimizing a training loss function defined using
the Monge-Amp\`ere equation. We empirically show that the structural bias of
mGradNets facilitates the learning of optimal transport maps and employ our
method for a robot swarm control problem.

</details>


### [79] [MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling](https://arxiv.org/abs/2507.13207)
*Etienne Le Naour,Tahar Nabil,Ghislain Agoua*

Main category: cs.LG

TL;DR: The paper introduces MoTM, a foundation model for time series imputation using implicit neural representations (INRs) to handle missing data across domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored task of out-of-domain imputation of missing values in time series, leveraging INRs for continuous modeling.

Method: Proposes MoTM, a mixture of INRs trained on distinct time series families, combined with a ridge regressor for context adaptation.

Result: Demonstrates robust in-domain and out-of-domain generalization for various missing data scenarios.

Conclusion: MoTM paves the way for adaptable foundation models in time series imputation.

Abstract: Recent years have witnessed a growing interest for time series foundation
models, with a strong emphasis on the forecasting task. Yet, the crucial task
of out-of-domain imputation of missing values remains largely underexplored. We
propose a first step to fill this gap by leveraging implicit neural
representations (INRs). INRs model time series as continuous functions and
naturally handle various missing data scenarios and sampling rates. While they
have shown strong performance within specific distributions, they struggle
under distribution shifts. To address this, we introduce MoTM (Mixture of
Timeflow Models), a step toward a foundation model for time series imputation.
Building on the idea that a new time series is a mixture of previously seen
patterns, MoTM combines a basis of INRs, each trained independently on a
distinct family of time series, with a ridge regressor that adapts to the
observed context at inference. We demonstrate robust in-domain and
out-of-domain generalization across diverse imputation scenarios (e.g., block
and pointwise missingness, variable sampling rates), paving the way for
adaptable foundation imputation models.

</details>


### [80] [Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead Electricity Price Forecasting in European Markets](https://arxiv.org/abs/2507.13250)
*Maria Margarida Mascarenhas,Jilles De Blauwe,Mikael Amelin,Hussain Kazmi*

Main category: cs.LG

TL;DR: The paper explores using asynchronously published electricity prices from markets with earlier gate closure times (GCTs) to improve forecasting accuracy in other markets. Results show significant accuracy improvements in Belgian and Swedish markets, with insights on model recalibration and data usage.


<details>
  <summary>Details</summary>
Motivation: Accurate short-term electricity price forecasting is vital for strategic bidding in day-ahead markets. The study investigates leveraging price data from interconnected markets with earlier GCTs to enhance forecasting.

Method: The research employs a state-of-the-art ensemble of models, incorporating price data from markets with earlier GCTs (Germany-Luxembourg, Austria, Switzerland) to forecast prices in Belgian and Swedish markets.

Result: Forecast accuracy improved by 22% in Belgium (BE) and 9% in Sweden (SE3). The study highlights the need for frequent model recalibration and notes that more data doesn't always improve performance.

Conclusion: The findings offer practical insights for optimizing bidding strategies in interconnected European energy markets, emphasizing the value of asynchronous price data and the trade-offs in model recalibration.

Abstract: Accurate short-term electricity price forecasting is crucial for
strategically scheduling demand and generation bids in day-ahead markets. While
data-driven techniques have shown considerable prowess in achieving high
forecast accuracy in recent years, they rely heavily on the quality of input
covariates. In this paper, we investigate whether asynchronously published
prices as a result of differing gate closure times (GCTs) in some bidding zones
can improve forecasting accuracy in other markets with later GCTs. Using a
state-of-the-art ensemble of models, we show significant improvements of 22%
and 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3)
respectively, when including price data from interconnected markets with
earlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement
holds for both general as well as extreme market conditions. Our analysis also
yields further important insights: frequent model recalibration is necessary
for maximum accuracy but comes at substantial additional computational costs,
and using data from more markets does not always lead to better performance - a
fact we delve deeper into with interpretability analysis of the forecast
models. Overall, these findings provide valuable guidance for market
participants and decision-makers aiming to optimize bidding strategies within
increasingly interconnected and volatile European energy markets.

</details>


### [81] [Boosting Team Modeling through Tempo-Relational Representation Learning](https://arxiv.org/abs/2507.13305)
*Vincenzo Marco De Luca,Giovanna Varni,Andrea Passerini*

Main category: cs.LG

TL;DR: TRENN is a novel tempo-relational architecture for team modeling, integrating temporal and relational dynamics, with explainability modules for actionable insights. MT-TRENN extends it for multi-task learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the gap in jointly modeling team dynamics and relations for interpretable, actionable insights in team performance enhancement.

Method: TRENN combines an automatic temporal graph extractor, tempo-relational encoder, decoder, and explainability modules. MT-TRENN replaces the decoder with a multi-task head for shared embeddings and multi-construct prediction.

Result: TRENN and MT-TRENN outperform temporal or relational-only methods, providing interpretable insights and actionable recommendations.

Conclusion: The approach is well-suited for Human-Centered AI applications, like decision-support systems in collaborative environments.

Abstract: Team modeling remains a fundamental challenge at the intersection of
Artificial Intelligence and the Social Sciences. Social Science research
emphasizes the need to jointly model dynamics and relations, while practical
applications demand unified models capable of inferring multiple team
constructs simultaneously, providing interpretable insights and actionable
recommendations to enhance team performance. However, existing works do not
meet these practical demands. To bridge this gap, we present TRENN, a novel
tempo-relational architecture that integrates: (i) an automatic temporal graph
extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct
prediction, and (iv) two complementary explainability modules. TRENN jointly
captures relational and temporal team dynamics, providing a solid foundation
for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task
head, enabling the model to learn shared Social Embeddings and simultaneously
predict multiple team constructs, including Emergent Leadership, Leadership
Style, and Teamwork components. Experimental results demonstrate that our
approach significantly outperforms approaches that rely exclusively on temporal
or relational information. Additionally, experimental evaluation has shown that
the explainability modules integrated in MT-TRENN yield interpretable insights
and actionable suggestions to support team improvement. These capabilities make
our approach particularly well-suited for Human-Centered AI applications, such
as intelligent decision-support systems in high-stakes collaborative
environments.

</details>


### [82] [GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM](https://arxiv.org/abs/2507.13323)
*Kyeongjin Ahn,Sungwon Han,Seungeon Lee,Donghyun Ahn,Hyoshin Kim,Jungwon Kim,Jihee Kim,Sangyoon Park,Meeyoung Cha*

Main category: cs.LG

TL;DR: GeoReg integrates satellite and geospatial data with LLM-derived features to estimate socio-economic indicators, outperforming baselines in data-scarce regions.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in developing regions for accurate socio-economic indicator estimation.

Method: Combines LLM-extracted features, linear estimator with tailored weights, and nonlinear transformations.

Result: Outperforms baselines in estimating indicators across three countries, including low-income regions.

Conclusion: GeoReg is effective for socio-economic estimation in data-scarce areas, leveraging diverse data and LLM capabilities.

Abstract: Socio-economic indicators like regional GDP, population, and education
levels, are crucial to shaping policy decisions and fostering sustainable
development. This research introduces GeoReg a regression model that integrates
diverse data sources, including satellite imagery and web-based geospatial
information, to estimate these indicators even for data-scarce regions such as
developing countries. Our approach leverages the prior knowledge of large
language model (LLM) to address the scarcity of labeled data, with the LLM
functioning as a data engineer by extracting informative features to enable
effective estimation in few-shot settings. Specifically, our model obtains
contextual relationships between data features and the target indicator,
categorizing their correlations as positive, negative, mixed, or irrelevant.
These features are then fed into the linear estimator with tailored weight
constraints for each category. To capture nonlinear patterns, the model also
identifies meaningful feature interactions and integrates them, along with
nonlinear transformations. Experiments across three countries at different
stages of development demonstrate that our model outperforms baselines in
estimating socio-economic indicators, even for low-income countries with
limited data availability.

</details>


### [83] [Training Transformers with Enforced Lipschitz Constants](https://arxiv.org/abs/2507.13338)
*Laker Newhouse,R. Preston Hess,Franz Cesista,Andrii Zahorodnii,Jeremy Bernstein,Phillip Isola*

Main category: cs.LG

TL;DR: The paper explores training transformers with Lipschitz bounds, improving stability and performance using novel tools and optimizer dynamics.


<details>
  <summary>Details</summary>
Motivation: Neural networks' sensitivity to perturbations leads to issues like adversarial vulnerability and overfitting. The study aims to enforce Lipschitz bounds in modern architectures like transformers.

Method: Developed efficient tools for norm-constrained weight matrices, trained transformers with Lipschitz bounds, and experimented with optimizers (AdamW vs. Muon) and weight constraints.

Result: Achieved 60% validation accuracy with a 2-Lipschitz transformer on Shakespeare text and 21% accuracy with a 10-Lipschitz transformer on internet text. However, matching baseline performance required a high Lipschitz bound (10^264).

Conclusion: Lipschitz transformers can train without stability measures, but balancing performance and Lipschitz bounds remains challenging.

Abstract: Neural networks are often highly sensitive to input and weight perturbations.
This sensitivity has been linked to pathologies such as vulnerability to
adversarial examples, divergent training, and overfitting. To combat these
problems, past research has looked at building neural networks entirely from
Lipschitz components. However, these techniques have not matured to the point
where researchers have trained a modern architecture such as a transformer with
a Lipschitz certificate enforced beyond initialization. To explore this gap, we
begin by developing and benchmarking novel, computationally-efficient tools for
maintaining norm-constrained weight matrices. Applying these tools, we are able
to train transformer models with Lipschitz bounds enforced throughout training.
We find that optimizer dynamics matter: switching from AdamW to Muon improves
standard methods -- weight decay and spectral normalization -- allowing models
to reach equal performance with a lower Lipschitz bound. Inspired by Muon's
update having a fixed spectral norm, we co-design a weight constraint method
that improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter
transformers. Our 2-Lipschitz transformer on Shakespeare text reaches
validation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz
transformer reaches 21% accuracy on internet text. However, to match the
NanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound
increases to 10^264. Nonetheless, our Lipschitz transformers train without
stability measures such as layer norm, QK norm, and logit tanh softcapping.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [84] [On multiagent online problems with predictions](https://arxiv.org/abs/2507.12486)
*Gabriel Istrate,Cosmin Bonchis,Victor Bogdan*

Main category: cs.MA

TL;DR: The paper explores competitive algorithms with predictions in multiagent settings, focusing on a two-predictor framework for self and other agents' behavior. It analyzes achievable competitive ratios under varying predictor qualities, illustrated via a multiagent ski-rental problem.


<details>
  <summary>Details</summary>
Motivation: To understand the best competitive ratios achievable in multiagent settings using predictors for self and others' behavior, addressing robustness and optimality trade-offs.

Method: Introduces a two-predictor framework and applies it to a multiagent ski-rental problem, where agents collaborate for group licenses. Analyzes algorithms under perfect and imperfect predictions.

Result: With perfect predictions, following the self-predictor is optimal but not robust. An alternative algorithm with better robustness is proposed and benchmarked.

Conclusion: The framework provides insights into competitive algorithms with predictions in multiagent systems, highlighting trade-offs between optimality and robustness.

Abstract: We study the power of (competitive) algorithms with predictions in a
multiagent setting. We introduce a two predictor framework, that assumes that
agents use one predictor for their future (self) behavior, and one for the
behavior of the other players. The main problem we are concerned with is
understanding what are the best competitive ratios that can be achieved by
employing such predictors, under various assumptions on predictor quality.
  As an illustration of our framework, we introduce and analyze a multiagent
version of the ski-rental problem. In this problem agents can collaborate by
pooling resources to get a group license for some asset. If the license price
is not met then agents have to rent the asset individually for the day at a
unit price. Otherwise the license becomes available forever to everyone at no
extra cost.
  In the particular case of perfect other predictions the algorithm that
follows the self predictor is optimal but not robust to mispredictions of
agent's future behavior; we give an algorithm with better robustness properties
and benchmark it.

</details>
