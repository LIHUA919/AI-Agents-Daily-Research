<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: This paper presents a monad-based clause framework built on the Artificial Age Score (AAS) to impose law-like constraints on LLM memory and control, demonstrated through six minimal Python implementations that show bounded, interpretable behavior.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but opaque systems lacking principled governance of their internal memory and self-like behavior, requiring an auditable framework.

Method: Twenty monads from Leibniz's Monadology are grouped into six bundles and realized as executable specifications on the AAS kernel, implemented in Python with experiments on recall scores, redundancy, and weights.

Result: The clause system exhibits continuous, rate-limited AAS trajectories; penalties for contradictions; hierarchical refinement; aligned dual views; and separation of improvement from degradation via windowed drift.

Conclusion: The framework provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents, making it both philosophically motivated and directly implementable.

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [2] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: Novel approach for real-life parallel machine scheduling with complex constraints using constraint modeling for small instances and metaheuristics for large-scale deployment.


<details>
  <summary>Details</summary>
Motivation: Automated scheduling techniques have significant potential to reduce production costs in modern manufacturing. However, existing approaches cannot effectively handle the complex precedence constraints and calendar-based cumulative resource constraints present in real-life production environments.

Method: The study introduced a constraint modeling approach as an exact method for small scenarios using state-of-the-art constraint-solving technology. For large-scale instances, a construction heuristic and a tailored metaheuristic based on local search were proposed.

Result: The study successfully introduced a constraint modeling approach for small scenarios and an effective metaheuristic for large-scale instances, with the latter being deployed and used in an industrial setting.

Conclusion: The study successfully introduced a constraint modeling approach for small scenarios and an effective metaheuristic for large-scale instances, with the latter being deployed and used in an industrial setting. The results highlight the practical viability and advantage of the proposed methods over existing techniques in addressing complex real-world scheduling constraints.

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [3] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [4] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: Introduces Mirror Mode where AI mimics player strategies to enhance challenge in turn-based games. Tests in Fire Emblem Heroes show good defensive imitation but limited offensive imitation, with higher player satisfaction.


<details>
  <summary>Details</summary>
Motivation: To make enemy AI more surprising and unpredictable by having it imitate player strategies, forcing players to adapt their gameplay.

Method: Built a simplified Fire Emblem Heroes game in Unity with Standard and Mirror Modes. Used Reinforcement Learning and Imitation Learning (GAIL, Behavioral Cloning, PPO) to train AI on player demonstrations.

Result: Player tests showed effective imitation of defensive behavior but not offensive strategies. Surveys indicated recognition of personal tactics and higher satisfaction with Mirror Mode.

Conclusion: Mirror Mode increases player satisfaction by mimicking strategies, though imitation quality needs refinement, especially for offensive tactics.

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [5] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: Finch is a finance benchmark using real enterprise data from Enron and financial institutions to evaluate AI agents on complex professional workflows involving data tasks, calculations, and reporting.


<details>
  <summary>Details</summary>
Motivation: Existing AI benchmarks don't capture the messy, long-horizon, and collaborative nature of real enterprise finance workflows, which involve multimodal artifacts and complex interleaving of tasks.

Method: LLM-assisted discovery combined with expert annotation to derive workflows from real email threads and spreadsheet version histories, resulting in 172 composite workflows with 384 tasks across 1,710 spreadsheets.

Result: Top AI systems perform poorly: GPT 5.1 Pro passes only 38.4% of workflows after 48 hours, Claude Sonnet 4.5 passes 25.0%, revealing significant challenges in handling real enterprise workflows.

Conclusion: Real-world enterprise workflows pose substantial challenges for current AI systems, highlighting the need for benchmarks that capture professional complexity and the gap between AI capabilities and practical business requirements.

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [6] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: A method for structuring LLM personalization with complex constraints by modeling them as submodular optimization under laminar matroid constraints.


<details>
  <summary>Details</summary>
Motivation: Personalizing LLMs requires balancing utility and privacy, but real-world constraints (dependencies, quotas, hierarchies) break standard selection algorithms.

Method: Transform user knowledge graphs into macro-facets, prove constraints form a laminar matroid, and apply greedy algorithms with theoretical guarantees.

Result: Enables efficient, near-optimal personalization under realistic constraints with constant-factor approximation guarantees.

Conclusion: The approach extends submodular optimization to practical personalization scenarios, offering rigorous foundations for constrained LLM customization.

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [7] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

Main category: cs.AI

TL;DR: LLMs and humans reason differently on causal tasks: humans are more consistent and use causal structure; LLMs are variable and not aligned.


<details>
  <summary>Details</summary>
Motivation: Causal reasoning is considered key to intelligence. Evaluating LLMs and humans on identical causal reasoning tasks allows direct comparison of their reasoning patterns, strengths, and weaknesses.

Method: Evaluated 20+ LLMs on 11 causal reasoning tasks based on a collider graph (C1→E←C2). Used two prompting methods: Direct (one-shot number response) and Chain-of-Thought (CoT, think first then answer). Modeled judgments with a leaky noisy-OR causal Bayes net (CBN) with parameters θ=(b,m1,m2,p(C)). Selected between symmetric (m1=m2) and asymmetric (m1≠m2) model variants via AIC.

Result: (Q1) LLM-human alignment: LLMs fail to match human-like causal reasoning patterns. (Q2) Consistency: Humans are more consistent across tasks; LLMs show more variability. (Q3) Reasoning signatures: Humans generally rely on causal structure; LLMs exhibit distinct, often inconsistent signatures.

Conclusion: LLMs exhibit different reasoning signatures compared to humans, with greater variability in their responses. While both show some use of causal structure, humans are more consistent and nuanced. The findings suggest LLMs are not yet fully aligned with human-like causal reasoning.

Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [8] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: Study compares robustness of different probabilistic models to data corruption, finding language models most resilient while diffusion models severely degrade.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate how different modern probabilistic models respond to low-quality/corrupted data across various tasks.

Method: Conducted comparative experiments with data corruption on autoregressive language models, class-conditional diffusion models, and classifiers. Used multi-perspective analysis combining information theory, PAC learning, and gradient dynamics.

Result: Language models show remarkable resilience (GPT-2 NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). Diffusion models degrade catastrophically (image-label consistency drops 56.81%). Classifiers show moderate impact that diminishes with scale.

Conclusion: Robustness is determined by two key principles: richness of conditioning information that constrains learning, and absolute information content allowing correct signal to dominate noise.

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [9] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: CXL-SpecKV is a novel disaggregated KV-cache architecture using CXL interconnects and FPGAs that achieves 3.2× higher throughput and 2.8× memory reduction for LLMs through speculative execution and memory optimization.


<details>
  <summary>Details</summary>
Motivation: The massive memory requirements of key-value (KV) caches in autoregressive decoding limit batch sizes and throughput in LLM deployment, creating a memory wall challenge.

Method: Implemented a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory, a speculative KV-cache prefetching mechanism, and an FPGA-accelerated KV-cache compression/decompression engine.

Result: Achieved up to 3.2× higher throughput compared to GPU-only baselines, reduced memory costs by 2.8× while maintaining accuracy, and demonstrated 4× reduction in memory bandwidth requirements.

Conclusion: CXL-SpecKV successfully addresses the memory wall challenge in LLM serving by combining CXL-based memory disaggregation with speculative execution, demonstrating that intelligent memory management can significantly improve efficiency without sacrificing accuracy.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [10] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: AGAPI is an open-access AI platform that integrates multiple LLMs with materials science tools to autonomously execute complex workflows for reproducible materials discovery, addressing fragmentation in computational ecosystems.


<details>
  <summary>Details</summary>
Motivation: The use of AI in materials research is limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). There is a need for an open-access platform that integrates diverse computational tools to enable reproducible, autonomous materials discovery workflows.

Method: AGAPI uses an Agent-Planner-Executor-Summarizer architecture to autonomously construct and execute multi-step workflows that span materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design.

Result: The authors demonstrate AGAPI through end-to-end workflows including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. They evaluate AGAPI using 30+ example prompts, compare predictions with experimental data, and report more than 1,000 active users, showing the platform's scalability and practical utility.

Conclusion: AGAPI successfully addresses the fragmented computational ecosystems and reproducibility challenges in AI-driven materials research by providing an open-access agentic AI platform that integrates multiple open-source LLMs with materials science tools, enabling reproducible and automated multi-step workflows for materials discovery.

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [11] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: Introduces a logic-based language and answer-set programming pipeline for hypergames, addressing representation and computation gaps to connect hypergame theory with multi-agent systems and strategic AI.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of unifying formal representation and scalable algorithms for hypergames, which hinders their practical adoption in multi-agent systems despite their ability to model heterogeneous, subjective game perceptions.

Method: A declarative, logic-based domain-specific language for encoding hypergame structures and solution concepts, combined with answer-set programming for automated hypergame structure instantiation and a novel hypergame rationalisation procedure.

Result: A practical framework consisting of a representation language and automated pipeline for hypergame analysis, enabling belief structure discovery to justify seemingly irrational outcomes and providing verifiable logical guarantees.

Conclusion: The work establishes a connection between hypergame theory and multi-agent systems by providing a formal, computational framework with logical guarantees. It offers a foundation for developing heterogeneous AI reasoners that can handle strategic interactions under misaligned mental models.

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: The paper introduces value profiles - reusable bundles of value-related parameters that enable adaptive behavior in volatile environments without requiring separate parameters for each context. This approach outperforms simpler models in probabilistic reversal learning tasks.


<details>
  <summary>Details</summary>
Motivation: Maintaining separate preferences, policy biases, and action-confidence parameters for every possible situation in volatile environments is computationally intractable, yet adaptive behavior requires switching among value-control regimes across latent contexts.

Method: Developed value profiles as small sets of reusable parameter bundles (outcome preferences, policy priors, policy precision) assigned to hidden states in a generative model. Effective control parameters emerge through belief-weighted mixing based on evolving posterior beliefs.

Result: Model comparison in probabilistic reversal learning favored the profile-based model over simpler alternatives (about 100-point AIC differences). Parameter-recovery analyses supported structural identifiability even with noisy context inference.

Conclusion: Value profiles provide a tractable computational framework for belief-conditioned value control in volatile environments, with evidence suggesting adaptive control is driven primarily by policy prior modulation and belief-dependent profile recruitment.

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [13] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: CR3G improves chest X-ray report generation using causal reasoning to explain AI findings, showing enhanced capability for 2 out of 5 abnormalities analyzed.


<details>
  <summary>Details</summary>
Motivation: Current AI models for chest X-ray analysis primarily identify patterns and correlations but lack understanding of deeper cause-and-effect relationships between radiological findings and patient conditions. This limitation reduces the clinical usefulness and trustworthiness of AI-generated reports. The research aims to enhance AI diagnostics by developing a framework that provides patient-centric explanations through causal reasoning.

Method: The paper introduces CR3G, a prompt-driven framework for chest X-ray report generation that incorporates causal reasoning mechanisms. This framework is applied to chest X-ray analysis to move beyond pattern recognition and uncover deeper causal relationships between radiological findings and patient conditions.

Result: CR3G has demonstrated improved causal relationship capability and explanation capability, showing better performance specifically for 2 out of 5 abnormalities analyzed. The framework shows promising results in enhancing understanding of AI-generated reports by focusing on cause-and-effect relationships.

Conclusion: While Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) shows promise in enhancing AI-driven chest X-ray diagnostics by focusing on cause-and-effect relationships, it currently demonstrates limited effectiveness with improvements observed for only 2 out of 5 abnormalities tested.

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [14] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for analyzing and improving shortcut diffusion models, achieving state-of-the-art performance on ImageNet-256x256 with FID50k of 2.85 without requiring pre-training or distillation.


<details>
  <summary>Details</summary>
Motivation: Current few-step diffusion models have theoretical derivation and implementation closely coupled, which obscures the design space and limits systematic improvement.

Method: Developed a common design framework for shortcut diffusion models that provides theoretical justification and disentangles component-level choices to enable systematic optimization.

Result: Achieved new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under classifier-free guidance setting with simplified training (no pre-training, distillation, or curriculum learning required).

Conclusion: The proposed framework lowers barriers to component-level innovation in shortcut models and enables principled exploration of diffusion model design space.

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [15] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

TL;DR: Simple IDW outperforms both ordinary kriging and neural representation for sparse climate data reconstruction in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of different reconstruction methods for sparse climate data, particularly investigating whether more sophisticated methods provide better performance than simpler approaches.

Method: The study evaluated three reconstruction methods (IDW, OK, and MMGN) through hyper-parameter optimization using validation splits, conducted extensive experiments on 100 randomly sampled sparse datasets from the ECA&D database, and performed comprehensive statistical analysis including post-hoc tests.

Result: IDW achieved superior performance across all evaluation metrics with the lowest RMSE (3.00 ± 1.93), MAE (1.32 ± 0.77), Δ_MAX (24.06 ± 17.15) and the highest R² (0.68 ± 0.16), with statistically significant differences showing moderate to large effect sizes.

Conclusion: The simple inverse distance weighting (IDW) method outperforms both ordinary kriging (OK) and the advanced implicit neural representation (MMGN) model for sparse climate data reconstruction, demonstrating that sophisticated methods don't always provide better results and that computational efficiency should be considered alongside accuracy.

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [16] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: Implementation of Soft Decision Tree (SDT) and Short-term Memory SDT (SM-SDT) showing comparable performance to XGBoost and superiority over other classifiers on medical datasets.


<details>
  <summary>Details</summary>
Motivation: To develop explainable machine learning models for medical applications while maintaining competitive performance with black-box models.

Method: Implemented SDT and SM-SDT using PyTorch and evaluated them on simulated and clinical datasets, comparing against XGBoost, Random Forest, Logistic Regression, and Decision Tree.

Result: SDT, SM-SDT, and XGBoost showed similar AUC values, all outperforming Random Forest, Logistic Regression, and Decision Tree. All classification methods except basic Decision Tree yielded comparable results on clinical data.

Conclusion: Soft Decision Trees offer a promising balance between explainability and performance for medical classification tasks, achieving results comparable to state-of-the-art methods while providing better interpretability.

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [17] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

TL;DR: Hybrid physics-data approach combining PBDW framework with DeepONet for enhanced state estimation and prediction while preserving physical model interpretability.


<details>
  <summary>Details</summary>
Motivation: Need to reconcile imperfect theoretical models with noisy experimental data for accurate state estimation of complex uncertain physical systems.

Method: Augments PBDW framework with orthogonal DeepONet to learn model discrepancies, plus optimal sensor placement strategy.

Result: Validated on Helmholtz equation problems with various modeling errors, showing effective correction of model bias.

Conclusion: Proposal provides an effective hybrid approach that maintains model fidelity while improving estimation/prediction accuracy.

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [18] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: ML pipeline predicts Food Compass Score from text descriptions using neural networks with hybrid features, achieving strong predictive accuracy for nutritional assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional nutritional assessment systems require detailed data that is often unavailable from colloquial text descriptions of food, creating a barrier for public health applications.

Method: The pipeline uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, domain heuristics, and USDA FNDDS data to estimate nutrient and food components needed for the Food Compass Score 2.0 algorithm.

Result: The system demonstrated strong predictive power: median R² of 0.81 for individual nutrients, Pearson's r = 0.77 correlation between predicted and published FCS values, with mean absolute difference of 14.0 points. Performance was reduced for ambiguous or processed foods.

Conclusion: The developed machine learning pipeline effectively translates text descriptions into actionable nutritional insights, enabling scalable dietary assessment for both consumer applications and research despite challenges with ambiguous or processed foods.

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [19] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

TL;DR: DPO doesn't fundamentally change LLM beliefs but acts as a low-rank steering mechanism that nudges activations along preference directions, creating a behavioral illusion of alignment.


<details>
  <summary>Details</summary>
Motivation: To understand what internal changes DPO actually induces in large language models and whether it rewrites internal beliefs or just steers behavior.

Method: Used mathematical derivation of DPO gradients, extracted empirical steering vectors from DPO-tuned models, and performed spectral analyses of activation spaces.

Result: DPO gradient depends only on logit embedding differences, creating first-order shifts in hidden representations; steering vector addition reproduces alignment while subtraction restores original behavior; spectral analysis shows rank-one dominance and entropy collapse.

Conclusion: DPO teaches models how to act aligned rather than fundamentally changing what they believe, supporting a behavioral illusion view of alignment.

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [20] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

TL;DR: Trailblazer is a framework using LLMs as generalist network policies that outperform specialist approaches in cross-task and cross-environment generalization.


<details>
  <summary>Details</summary>
Motivation: Specialist network policies based on handcrafted rules or deep learning models generalize poorly across diverse tasks and environments.

Method: Trailblazer incorporates network alignment to ground LLMs in networking tasks and adaptive policy collaboration to offload simple cases to lightweight policies for efficiency.

Result: Extensive simulations and real-world evaluation on Douyin show Trailblazer achieves stronger generalization than conventional specialist policies.

Conclusion: LLMs provide a foundation for generalist network policies, with Trailblazer representing a shift toward generalization with minimal policy design effort.

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [21] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

TL;DR: LLM-based agents' decision-making in network-effect games depends critically on historical data patterns—ordered sequences enable convergence while randomization disrupts it, revealing AI optimism bias and different reasoning from humans.


<details>
  <summary>Details</summary>
Motivation: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts, which are underexplored in multi-agent systems despite real-world prevalence.

Method: Using LLM-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength in network-effect games.

Result: 1. Without historical data, agents fail to infer equilibrium. 2. Ordered historical sequences enable partial convergence under weak network effects, but strong effects cause "AI optimism"—agents overestimate participation despite contradictory evidence. 3. Randomized history disrupts convergence entirely, demonstrating temporal coherence shapes LLMs' reasoning differently from humans.

Conclusion: AI agents' equilibrium outcomes in network-effect games are critically shaped by the historical data sequence they observe. This introduces a new paradigm where game outcomes depend on how history is presented, contrasting with traditional game theory models that only consider incentives.

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [22] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: Proposes an Agent Assessment Framework for evaluating multi-agent AI systems, addressing behavioral uncertainty overlooked by conventional metrics.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for AI agent systems fail to capture non-deterministic behavior and runtime uncertainties, relying on inadequate binary task completion metrics.

Method: Develops an end-to-end framework with four evaluation pillars: LLMs, Memory, Tools, and Environment, validated through Autonomous CloudOps experiments.

Result: Framework successfully identifies behavioral deviations missed by traditional metrics, demonstrating effectiveness in capturing runtime uncertainties.

Conclusion: The proposed framework provides comprehensive assessment capabilities crucial for evaluating modern agentic AI systems and their complex interactions.

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [23] [Quantigence: A Multi-Agent AI Framework for Quantum Security Research](https://arxiv.org/abs/2512.12989)
*Abdulmalik Alquwayfili*

Main category: cs.MA

TL;DR: Quantigence is a multi-agent AI framework that accelerates quantum-security research by decomposing analysis into specialized roles, achieving 67% faster turnaround than manual methods while maintaining comprehensive coverage.


<details>
  <summary>Details</summary>
Motivation: CRQCs threaten global digital security via SNDL attacks, requiring urgent PQC migration, but current transition is slowed by fast-evolving research, changing standards, and deployment complexity.

Method: Multi-agent framework with specialized roles (Cryptographic Analyst, Threat Modeler, Standards Specialist, Risk Assessor) coordinated by supervisor; uses cognitive parallelism for independent reasoning with serialized execution on constrained hardware, integrated with Model Context Protocol.

Result: 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, enabling accessible quantum risk assessment.

Conclusion: Quantigence successfully accelerates quantum-security analysis through structured multi-agent collaboration, making high-fidelity risk assessment more accessible despite hardware constraints.

Abstract: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.

</details>


### [24] [The Optimal Control Algorithm of Connected and Automated Vehicles at Roundabouts with Communication Delay](https://arxiv.org/abs/2512.13056)
*Chen Huang,Ronghui Hou*

Main category: cs.MA

TL;DR: Proposes a roundabout control algorithm for connected automated vehicles that addresses communication delays and optimizes vehicle sequencing through distributed model predictive control.


<details>
  <summary>Details</summary>
Motivation: Communication delays in CAVs can degrade control performance, especially in high-speed scenarios like roundabout intersections where precise coordination is critical.

Method: Develops a vehicle motion model incorporating time delays, uses time-to-collision for conflict zone identification, implements distributed MPC for motion control, and creates multiscale optimization integrating vehicle motion and system indicators.

Result: The proposed algorithm effectively handles communication delays and optimizes vehicle sequencing, verified through simulation experiments showing superior performance compared to other control algorithms.

Conclusion: The roundabout control algorithm successfully addresses communication delay challenges and demonstrates robust performance across different penetration rates and traffic conditions.

Abstract: Connected and automated vehicles (CAVs) rely on wireless communication to exchange state information for distributed control, making communication delays a critical factor that can affect vehicle motion and degrade control performance, particularly in high-speed scenarios. To address these challenges in the complex environment of roundabout intersections, this paper proposes a roundabout control algorithm, which takes into account the uncertainty of interactive information caused by time delays. First, to maintain the required distance between the current vehicle and its preceding and following vehicles, conflicting vehicles are identified based on the time-to-collision (TTC) in the conflict zone. To fully consider communication performance, a vehicle motion model incorporating time delays is established. According to the distributed model predictive control (DMPC) mechanism, the vehicle motion control that satisfies the roundabout constraints is determined. Second, by scheduling the sequence of vehicles entering the roundabout, a multiscale optimization objective is developed by integrating vehicle motion indicators and roundabout system indicators. Traffic density and travel time are embedded into the optimization problem to guide vehicles to enter the roundabout safely and stably. Through a variety of simulation experiments, the effectiveness of the proposed control algorithm is verified by comparing its performance with that of multiple control algorithms under different autonomous vehicle penetration rates and heavy traffic load scenarios.

</details>
