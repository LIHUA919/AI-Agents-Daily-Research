<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI](https://arxiv.org/abs/2511.15717)
*Bo Wen,Chen Wang,Erhan Bilal*

Main category: cs.AI

TL;DR: The paper investigates how different input modalities (text vs. images) affect perception and execution on ARC-AGI tasks, finding that structured text excels at precise coordinates while images capture 2D shapes, with multimodal approaches yielding the best performance.


<details>
  <summary>Details</summary>
Motivation: Current ARC-AGI systems use generate-execute-select loops but lack understanding of how input encodings affect perception and how to separate instruction errors from execution errors. The authors hypothesize that different modalities create perceptual bottlenecks that shape which grid features are reliably perceived.

Method: Used a two-stage reasoning pipeline with weighted set-disagreement metric to isolate perception from reasoning across nine text and image modalities, testing how modality affects feature perception.

Result: Structured text provides precise coordinates for sparse features, images capture 2D shapes but are resolution-sensitive, and combining modalities improves execution (about 8 perception points; about 0.20 median similarity).

Conclusion: Aligning representations with transformer inductive biases and enabling cross-validation between text and image modalities yields more accurate instructions and reliable execution without changing the underlying model architecture.

Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems](https://arxiv.org/abs/2511.15862)
*Devang Kulshreshtha,Wanyu Du,Raghav Jain,Srikanth Doss,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.MA

TL;DR: Novel framework for simulating uncooperative behaviors in LLM-based multi-agent systems, featuring game theory taxonomy and multi-stage simulation pipeline. Demonstrates 96.7% accuracy in behavior generation and shows uncooperative agents cause rapid system collapse within 1-7 rounds.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap in understanding how uncooperative behaviors can destabilize LLM-based multi-agent systems, highlighting the need for more resilient system design.

Method: Game theory-based taxonomy of uncooperative behaviors and structured multi-stage simulation pipeline that dynamically generates and refines behaviors as agent states evolve.

Result: 96.7% accuracy in generating realistic uncooperative behaviors; cooperative agents maintain perfect stability (100% survival, 0% resource overuse) while uncooperative agents cause collapse within 1-7 rounds.

Conclusion: Uncooperative agents significantly degrade collective outcomes, emphasizing the critical need for designing more resilient multi-agent systems that can withstand adversarial behaviors.

Abstract: This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. Our framework includes two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, addressing a notable gap in the existing literature; and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines uncooperative behaviors as agents' states evolve. We evaluate the framework via a collaborative resource management setting, measuring system stability using metrics such as survival time and resource overuse rate. Empirically, our framework achieves 96.7% accuracy in generating realistic uncooperative behaviors, validated by human evaluations. Our results reveal a striking contrast: cooperative agents maintain perfect system stability (100% survival over 12 rounds with 0% resource overuse), while any uncooperative behavior can trigger rapid system collapse within 1 to 7 rounds. These findings demonstrate that uncooperative agents can significantly degrade collective outcomes, highlighting the need for designing more resilient multi-agent systems.

</details>
