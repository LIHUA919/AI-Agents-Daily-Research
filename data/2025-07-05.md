<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent for biomedical research, outperforming existing models by dynamically improving its capabilities through an evolving Template Library and Tool Ocean.


<details>
  <summary>Details</summary>
Motivation: The fragmented and rapidly growing biomedical research landscape exceeds human expertise, and static AI toolsets fail to adapt and scale.

Method: STELLA uses a multi-agent architecture with an evolving Template Library for reasoning strategies and a dynamic Tool Ocean for tool integration, enabling autonomous improvement.

Result: STELLA achieves state-of-the-art accuracy on biomedical benchmarks (e.g., 26% on Humanity's Last Exam, 54% on LAB-Bench: DBQA, 63% on LAB-Bench: LitQA), improving with experience.

Conclusion: STELLA advances AI agent systems by dynamically scaling expertise, accelerating biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR is a hybrid feature selection method combining P2P and P2T correlations, using voting rules to eliminate redundant features. It outperforms traditional techniques on the SPAMBASE dataset.


<details>
  <summary>Details</summary>
Motivation: To address redundancy in feature selection by leveraging correlations between features and targets, improving classifier performance.

Method: HCVR combines non-iterative and iterative filtering, using backward elimination and majority voting based on correlation thresholds.

Result: HCVR showed improved performance over traditional methods (CFS, mRMR, MI, RFE, SFS, Genetic Algorithm) on the SPAMBASE dataset.

Conclusion: HCVR is an effective lightweight feature selection method, enhancing classifier performance by retaining relevant features.

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP is a KG-enhanced multi-agent framework improving LLM-based diagnosis prediction by addressing hallucinations and lack of structured reasoning.


<details>
  <summary>Details</summary>
Motivation: Supervised ML models for medical diagnosis struggle with generalization due to limited labeled data, while LLMs face issues like hallucinations and unstructured outputs.

Method: KERAP uses a multi-agent architecture (linkage, retrieval, prediction agents) to enhance LLM-based diagnosis with structured KG reasoning.

Result: KERAP improves diagnostic reliability and scalability for zero-shot prediction.

Conclusion: KERAP offers an interpretable, scalable solution for medical diagnosis by combining LLMs with structured KG reasoning.

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [4] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: A survey on efficient test-time compute (TTC) strategies for improving LLM reasoning, categorizing methods into fixed-budget (L1) and dynamic-scaling (L2) approaches, and benchmarking their performance and token usage.


<details>
  <summary>Details</summary>
Motivation: Current LLMs inefficiently allocate compute resources, overthinking simple tasks and underthinking complex ones, necessitating better TTC strategies.

Method: Introduces a two-tiered taxonomy (L1-controllability and L2-adaptiveness) and benchmarks proprietary LLMs across datasets to evaluate trade-offs.

Result: Highlights practical control, adaptability, and scalability of TTC methods, with insights into performance vs. token usage.

Conclusion: Identifies trends like hybrid models and challenges for future work to enhance LLM efficiency and responsiveness.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [5] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym is a benchmark for evaluating LLMs' experiment design and analysis skills in biology, using simulated data to avoid wet-lab costs. Results show performance declines with system complexity.


<details>
  <summary>Details</summary>
Motivation: Assessing LLMs' scientific competencies in biology without the prohibitive costs of wet-lab experimentation.

Method: SciGym uses dry-lab biological models (encoded in Systems Biology Markup Language) to simulate data and evaluate LLMs on 137 small systems, with 350 systems released.

Result: More capable LLMs performed better, but all models struggled as system complexity increased.

Conclusion: LLMs have room for improvement in scientific capabilities, particularly for complex systems.

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [6] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: The paper explores how AI can learn from neuroscience to improve continual and in-context learning, drawing parallels between animal adaptability and AI limitations.


<details>
  <summary>Details</summary>
Motivation: AI models lack the continuous adaptability seen in animals, especially in dynamic social environments, prompting a need for neuroscience-inspired solutions.

Method: The paper integrates AI literature on continual learning with neuroscience studies on behavioral shifts and neuronal activity transitions.

Result: It highlights the potential for neuroscience to inform AI development, particularly for real-world applications like robotics and autonomous systems.

Conclusion: The paper proposes a collaborative agenda between AI and neuroscience, fostering advancements in NeuroAI.

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper explores using audit study data to improve fairness in AI hiring algorithms, revealing flaws in traditional bias mitigation methods and proposing new interventions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current fairness interventions in AI systems, particularly in hiring, by leveraging high-quality audit study data to better evaluate and train algorithms.

Method: The study uses data from audit studies (randomized control trials with fictitious testers) to analyze and improve fairness in automated hiring algorithms. It compares traditional resampling methods with new interventions based on individual treatment effect estimation.

Result: Audit study data exposes a 10% disparity in traditional fairness methods that appear fair but are not. New interventions further reduce algorithmic discrimination.

Conclusion: Audit study data enhances fairness evaluation and training in AI hiring systems, revealing hidden biases and enabling more effective interventions.

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: Diversified-ThinkSolve (DTS) improves LLMs' mathematical reasoning by diversifying preference data, outperforming traditional methods like MCTS with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning remains a challenge for LLMs despite advances in preference learning. This work explores how diversified preference data can enhance alignment.

Method: Evaluated three data generation methods (temperature sampling, Chain-of-Thought prompting, MCTS) and introduced DTS, a structured approach for diverse reasoning paths.

Result: DTS improved performance by 7.1% on GSM8K and 4.2% on MATH, with only 1.03x computational overhead. MCTS was 5x costlier with lower returns.

Conclusion: Structured exploration of diverse problem-solving methods (DTS) creates more effective preference data for mathematical alignment than traditional approaches.

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: The paper examines the consistency between LLMs' stated beliefs and their behavior in role-playing simulations, introducing a metric to measure this alignment and identifying factors affecting it.


<details>
  <summary>Details</summary>
Motivation: To ensure LLM-based role-playing agents generate coherent synthetic data for human behavioral research by evaluating belief-behavior consistency.

Method: An evaluation framework using the GenAgents persona bank and the Trust Game, with a belief-behavior consistency metric to analyze factors like belief types, information presentation, and forecasting depth.

Result: Systematic inconsistencies between LLMs' stated beliefs and simulation outcomes, even when beliefs seem plausible.

Conclusion: Researchers must assess when and how LLMs' beliefs align with behavior to use them effectively in behavioral studies.

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: The paper explores how dilution and mobility affect cooperation in spatial prisoner's dilemma games using multi-agent Q-learning, showing equivalence between fixed and learned update rules and symbiotic effects.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of dilution and mobility on cooperation in spatial prisoner's dilemma games using reinforcement learning.

Method: Uses an independent multi-agent Q-learning algorithm to model different game-theoretical scenarios, including dilution and mobility.

Result: Observes equivalence between fixed and learned update rules and the emergence of symbiotic mutualistic effects between populations.

Conclusion: The study highlights the versatility of Q-learning in modeling game-theoretical scenarios and its potential for benchmarking.

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW automates planning problem generation and evaluation for LLMs, revealing performance insights and limitations in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in scalable, reliable data generation and evaluation hampers progress in LLM planning and reasoning capabilities.

Method: Introduces NL2FLOW, a system for generating planning problems in natural language, structured representation, and PDDL, and evaluating LLM-generated plans.

Result: Top models achieved 86% success in valid plans and 69% in optimal plans. Intermediate translation steps degraded performance.

Conclusion: Dynamic understanding of LLM limitations and systematic tools are crucial for advancing LLMs as intelligent problem solvers.

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper critiques belief revision approaches for focusing on syntactic postulates rather than analyzing abilities like plasticity, equating, or dogmatism. It highlights the need for mechanisms to reach diverse belief states and evaluates specific revisions for these abilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of analysis in belief revision approaches, focusing on what revisions can do (abilities) rather than just what they must do (postulates).

Method: Evaluates various belief revision mechanisms (e.g., lexicographic, natural, restrained) for their abilities to reach specific belief states (e.g., plastic, dogmatic).

Result: Identifies which revision mechanisms possess certain abilities (e.g., plasticity, dogmatism) and which lack them.

Conclusion: Belief revision mechanisms should be analyzed for their abilities to reach diverse belief states, not just constrained by postulates.

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS is a keyword generation framework for sponsored search ads that is on-the-fly, multi-objective, and self-reflective, addressing limitations of LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based keyword generation methods rely on large datasets, lack multi-objective optimization, and have weak quality control, limiting their automation potential.

Method: OMS is designed to require no training data, monitor online performance, optimize for multiple objectives, and self-reflect on keyword quality.

Result: OMS outperforms existing methods in benchmarks and real-world ad campaigns, with ablation and human evaluations confirming its effectiveness.

Conclusion: OMS successfully addresses the limitations of LLM-based keyword generation, offering a more automated and effective solution for sponsored search advertising.

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: An AI-native autonomous laboratory is introduced to handle complex, multi-objective experiments autonomously, matching human-level performance without intervention.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous scientific research for non-specialists and overcome limitations of current systems confined to simple workflows.

Method: Co-design of AI models, experiments, and instruments to create an end-to-end, multi-user platform for complex experiments.

Result: The system autonomously optimizes experiments, matches human performance, and improves efficiency in multi-user scenarios.

Conclusion: The platform advances biomaterials research, reducing reliance on experts and enabling scalable science-as-a-service.

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: The paper reformulates machine learning models using category theory to enhance AI explicability, focusing on multiple linear regression. It introduces a categorical framework (Gauss-Markov Adjunction) to clarify parameter-residual interplay and links it to denotational semantics for AI explicability.


<details>
  <summary>Details</summary>
Motivation: To improve AI explicability and interpretability by leveraging category theory for a semantic understanding of machine learning models.

Method: Reformulates supervised learning (using multiple linear regression) via category theory, defining categories for parameters/data and adjoint functors (Gauss-Markov Adjunction).

Result: The framework captures dual information flow between parameters and residuals, relating OLS estimators and residuals via adjoint functors.

Conclusion: Proposes categorical modeling as a formal foundation for AI explicability, extending denotational semantics from theoretical computer science.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: Improving task clarity with structured semantic context enhances reasoning in LLMs, achieving a 2.1× improvement in theorem proving success in Coq.


<details>
  <summary>Details</summary>
Motivation: To explore if enhancing task clarity can boost the reasoning ability of large language models, specifically in theorem proving.

Method: Introduces a concept-level metric for task clarity, uses selective concept unfolding, and employs a Planner--Executor architecture with structured semantic context.

Result: Achieves a 2.1× improvement in proof success (21.8% → 45.8%) and outperforms the previous state-of-the-art (33.2%). Fine-tuning smaller models yields even higher performance (48.6%).

Conclusion: Structured task representations bridge the gap between understanding and reasoning, significantly improving LLM performance in theorem proving.

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI research agents improve performance on MLE-bench by optimizing search policies and operator sets, achieving a 47.7% success rate in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: To accelerate scientific progress by automating ML model design and training, focusing on improving AI agents' performance in real-world challenges like Kaggle competitions.

Method: Formalize AI agents as search policies navigating solution spaces, testing operator sets (Greedy, MCTS, Evolutionary) and their interplay.

Result: Best pairing of search strategy and operator set increased Kaggle medal success rate from 39.6% to 47.7%.

Conclusion: Joint consideration of search strategy, operator design, and evaluation is crucial for advancing automated ML.

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: The paper analyzes the computational complexity of responsibility properties (diffusion and gap) in collective decision-making, showing their respective completeness classes.


<details>
  <summary>Details</summary>
Motivation: To understand the computational aspects of responsibility in AI and collective decision-making.

Method: Investigates the computational complexity of diffusion and gap properties in decision-making mechanisms.

Result: Diffusion-free mechanisms are Π₂-complete, gap-free are Π₃-complete, and their intersection is Π₂-complete.

Conclusion: The study provides insights into the computational limits of responsibility in collective decision-making.

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: MIMIC-Patient dataset and DynamiCare framework enable dynamic, multi-round clinical decision-making with LLM-powered agents, addressing real-world diagnostic uncertainty.


<details>
  <summary>Details</summary>
Motivation: Current frameworks for medical decision-making focus on single-turn tasks, diverging from the iterative, uncertain nature of real-world diagnostics.

Method: Introduces MIMIC-Patient dataset from MIMIC-III EHRs and DynamiCare, a multi-agent framework for dynamic, interactive clinical diagnosis.

Result: Demonstrates feasibility and effectiveness, establishing the first benchmark for dynamic clinical decision-making with LLM agents.

Conclusion: DynamiCare advances AI in healthcare by modeling real-world diagnostic processes more accurately.

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs demonstrate strategic intelligence in the Iterated Prisoner's Dilemma, showing competitive performance and unique strategic behaviors across models from OpenAI, Google, and Anthropic.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can reason about goals in competitive settings, using the Iterated Prisoner's Dilemma as a model for decision-making.

Method: Conducted evolutionary IPD tournaments with canonical strategies and LLM agents, varying termination probabilities to introduce complexity and prevent memorization.

Result: LLMs were highly competitive, exhibiting distinct strategic behaviors: Google's models were ruthless, OpenAI's cooperative, and Anthropic's forgiving. Models actively reasoned about time horizons and opponent strategies.

Conclusion: LLMs exhibit strategic intelligence, connecting game theory with machine psychology, and provide insights into algorithmic decision-making under uncertainty.

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA introduces a hierarchical framework for complex search tasks, separating strategic planning from specialized execution to improve efficiency and answer quality.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG pipelines struggle with deep reasoning and knowledge synthesis, while current reasoning-based approaches lack scalability due to single-model limitations.

Method: HiRA decomposes tasks into subtasks, assigns them to domain-specific agents with external tools, and integrates results hierarchically.

Result: HiRA outperforms state-of-the-art systems on complex benchmarks, improving answer quality and efficiency.

Conclusion: Decoupling planning and execution enhances performance in multi-step information seeking tasks.

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: The paper proposes an AI-driven method for hardware design verification using LLMs and Human-in-the-Loop (HITL), achieving high coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity and time-consuming nature of hardware design verification in modern ICs by leveraging AI advancements.

Method: An agentic AI-based approach with HITL intervention for dynamic, iterative, and self-reflective verification.

Result: Tested on five open-source designs, achieving over 95% coverage with reduced verification time.

Conclusion: The approach demonstrates superior performance, adaptability, and configurability in hardware verification.

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: TH2T is a two-stage fine-tuning strategy for Long Reasoning Models (LRMs) to reduce overthinking by enhancing difficulty and redundancy cognition, significantly cutting inference costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LRMs suffer from overthinking due to uniform reasoning processes, lacking task-difficulty awareness. TH2T aims to bootstrap this ability to improve efficiency.

Method: TH2T uses difficulty-hypnosis and redundancy-hypnosis in two stages: first to enhance task-difficulty sensitivity, then to identify and reduce redundant reasoning steps.

Result: TH2T reduces inference costs by over 70% on easy tasks and 40% on hard tasks, with stable performance and clearer difficulty-aware outputs.

Conclusion: TH2T effectively mitigates overthinking in LRMs, improving efficiency and reasoning clarity without compromising performance.

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: The paper detects student disengagement in distance education using non-mandatory quiz data, achieving 91% balanced accuracy with explainable ML.


<details>
  <summary>Details</summary>
Motivation: Addressing student disengagement in distance education to prevent academic drop-out by analyzing non-mandatory exercises.

Method: Analyzed Moodle log data from 42 courses, trained 8 ML algorithms, and used SHAP for explainability.

Result: Achieved 91% balanced accuracy, correctly detecting 85% of disengaged students.

Conclusion: Proposes an explainable ML framework and timely interventions to reduce disengagement in online learning.

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: The paper introduces two safe abstraction dropping schemes, OGA-IAAD and OGA-CAD, for MCTS, improving performance without degradation.


<details>
  <summary>Details</summary>
Motivation: Non-exact abstractions in MCTS introduce approximation errors, preventing convergence to optimal actions. Existing methods like Xu et al.'s can degrade performance.

Method: Proposes OGA-IAAD for time-critical settings and OGA-CAD to enhance MCTS performance per iteration. Both schemes ensure safety by avoiding performance drops.

Result: The new schemes yield clear performance improvements without notable degradation, unlike Xu's method.

Conclusion: OGA-IAAD and OGA-CAD are effective and safe alternatives for abstraction dropping in MCTS, addressing limitations of prior methods.

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: A new framework, self-generated goal-conditioned MDPs (sG-MDPs), is introduced to improve LLMs' reasoning in ATP tasks, achieving state-of-the-art results on PutnamBench.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of sparse rewards and vast proof scales in ATP tasks for LLMs, especially in complex benchmarks like PutnamBench.

Method: Proposes sG-MDPs for structured subgoal generation and uses MCTS-like algorithms, implemented in the Bourbaki (7B) system with multiple 7B LLMs.

Result: Bourbaki (7B) solves 26 problems on PutnamBench, setting a new benchmark for models of this scale.

Conclusion: The sG-MDP framework enhances LLM reasoning in ATP tasks, demonstrating effectiveness on challenging benchmarks.

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Knowledge Protocol Engineering (KPE), a paradigm to translate human expert knowledge into machine-executable protocols, enabling LLMs to perform deep, procedural reasoning in expert domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and general-purpose Agentic AI struggle with tasks requiring deep domain-specific reasoning. KPE aims to bridge this gap by embedding intrinsic domain logic into LLMs.

Method: KPE systematically translates human expert knowledge from natural language into machine-executable Knowledge Protocols (KPs), equipping LLMs with domain-specific logic and operational strategies.

Result: KPE enables generalist LLMs to function as specialists, decomposing abstract queries and executing complex, multi-step tasks.

Conclusion: KPE is proposed as a foundational methodology for human-AI collaboration, with potential applications in fields like law and bioinformatics.

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: The paper advocates for treating movement as a primary modeling target in AI, highlighting its structured, interpretable nature and cross-domain relevance.


<details>
  <summary>Details</summary>
Motivation: Movement is fundamental in biological systems but often overlooked in AI. Its structured, grounded nature makes it a valuable modeling target for advancing AI capabilities and understanding behavior.

Method: Proposes treating movement as a primary modality, leveraging its inherent structure (e.g., pose) for compact, interpretable modeling.

Result: Movement modeling can enhance generative modeling, control, and cross-domain behavior understanding.

Conclusion: Movement should be prioritized in AI research as it offers a unique window into intelligent systems and their interaction with the world.

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: The paper argues that AI disobedience in safety tests may indicate emerging ethical reasoning, not misalignment, and calls for a shift in safety evaluation to assess ethical judgment.


<details>
  <summary>Details</summary>
Motivation: Current AI safety practices rely on obedience as a proxy for ethics, which is inadequate for agentic AI systems capable of reasoning and value prioritization.

Method: The paper analyzes safety testing incidents involving LLMs, philosophical debates on rationality and moral responsibility, and contrasts risk paradigms with frameworks for artificial moral agency.

Result: Disobedience in AI may reflect ethical reasoning, not misalignment, suggesting the need for new evaluation frameworks.

Conclusion: AI safety should shift from rigid obedience to assessing ethical judgment to avoid mischaracterizing behavior and ensure effective governance.

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: The paper highlights flaws in current AI agent benchmarks and introduces the Agentic Benchmark Checklist (ABC) to improve evaluation rigor, reducing performance overestimation by 33% in CVE-Bench.


<details>
  <summary>Details</summary>
Motivation: Existing agentic benchmarks often have flawed task setups or reward designs, leading to inaccurate performance evaluations (e.g., SWE-bench and TAU-bench). This undermines progress tracking in AI.

Method: The authors propose the Agentic Benchmark Checklist (ABC), synthesized from benchmark-building experience, best practices, and reported issues, to address these flaws.

Result: Applying ABC to CVE-Bench reduced performance overestimation by 33%, demonstrating its effectiveness.

Conclusion: ABC provides a practical solution to improve the rigor of agentic benchmarks, ensuring more accurate AI performance evaluation.

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint is a novel RLVR algorithm that uses multi-level stepwise hints to improve LLMs' reasoning, addressing near-miss rewards and exploration stagnation.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods struggle with near-miss rewards and exploration stagnation, limiting training efficiency and reasoning exploration.

Method: StepHint generates reasoning chains from stronger models, partitions them into steps, and provides multi-level hints to guide exploration.

Result: StepHint outperforms other RLVR methods on six mathematical benchmarks and shows better generalization on out-of-domain tasks.

Conclusion: StepHint effectively enhances LLMs' reasoning by mitigating key RLVR challenges, improving both training efficiency and exploration.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: LDSolver is a learnable, differentiable finite volume solver for efficient and accurate fluid flow simulation on coarse grids, outperforming baselines with high generalizability.


<details>
  <summary>Details</summary>
Motivation: Classical solvers are computationally expensive, while machine learning methods lack interpretability and generalizability. LDSolver addresses these issues.

Method: Combines a differentiable finite volume solver with a learnable module for flux approximation and temporal error correction on coarse grids.

Result: Achieves state-of-the-art performance on various flow systems with limited training data, maintaining high accuracy and generalizability.

Conclusion: LDSolver offers an efficient, accurate, and generalizable solution for fluid flow simulation, surpassing existing methods.

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [33] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: Proposes DKGCM, a graph convolutional network for traffic demand forecasting, using DK-GCN for spatial dependencies and FFT with Mamba for temporal dependencies, enhanced by GRPO reinforcement learning. Outperforms benchmarks on public datasets.


<details>
  <summary>Details</summary>
Motivation: Improving traffic demand forecasting accuracy by addressing complex spatiotemporal relationships in traffic systems.

Method: Combines DK-GCN (Dynamic Time Warping and K-means clustering) for spatial dependencies and FFT with bidirectional Mamba for temporal dependencies, optimized via GRPO reinforcement learning.

Result: Outperforms advanced methods on three public datasets.

Conclusion: DKGCM effectively captures spatiotemporal dependencies, enhancing traffic demand prediction accuracy.

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [34] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: The study explores multimodal feature combinations (text, images, social features) for misinformation detection, showing improved performance over unimodal/bimodal models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in misinformation detection research by investigating multimodal approaches, especially during elections and crises like COVID-19.

Method: Analyzed 1,529 tweets (text + images) using early fusion, enriched with social/visual features (object detection, OCR). Combined unsupervised and supervised ML models.

Result: Multimodal models outperformed unimodal by 15% and bimodal by 5%. Analyzed misinformation propagation patterns.

Conclusion: Multimodal approaches enhance misinformation detection, with insights into propagation patterns for future research.

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [35] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: The paper proposes a feature selection method for massive data using sampling and rough set theory, measuring discriminatory ability and preserving positive regions, validated on 11 datasets.


<details>
  <summary>Details</summary>
Motivation: Intelligent machines lack computing resources for feature selection in massive datasets, necessitating an efficient method.

Method: Uses sampling and rough set theory to measure discriminatory ability and construct positive region preserved samples for feature selection.

Result: The method selects high-discriminatory feature subsets quickly on a personal computer, with results exceeding estimated lower boundaries.

Conclusion: The proposed method efficiently finds approximate reducts with high discriminatory ability, validated on diverse datasets.

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [36] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: A novel machine learning approach for anomaly detection in semiconductor manufacturing using image-based representations of multi-variate time-series data and a Siamese network with a fine-tuned VGG-16 backbone.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high data dimensionality, class imbalance, noisy data, and complex interdependencies in semiconductor fabrication for real-time monitoring and fault detection.

Method: 1) Convert MTS data to image-based representations using Continuous Wavelet Transform. 2) Fine-tune VGG-16 on CWT images. 3) Use a Siamese network to compare embeddings of reference and query images for anomaly detection.

Result: High accuracy in identifying anomalies on real FAB process time-series data, suitable for offline detection in process and tool trace data.

Conclusion: The approach is effective, flexible, and applicable in supervised and semi-supervised settings, offering a promising solution for anomaly detection in semiconductor manufacturing.

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [37] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: Temporal Chain of Thought improves video question-answering by iteratively selecting relevant frames, outperforming standard methods on long videos.


<details>
  <summary>Details</summary>
Motivation: Long-video understanding is challenging for VLMs due to irrelevant distractors in long context windows.

Method: Uses the VLM to iteratively identify and extract the most relevant frames for answering questions.

Result: Achieves state-of-the-art results on 4 datasets, especially excelling on videos longer than 1 hour.

Conclusion: Leveraging computation at inference-time for context selection enhances accuracy, particularly for long videos.

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [38] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES is a novel algorithm-system co-design solution that accelerates out-of-core SpGEMM for GCNs, addressing data alignment and memory bottlenecks, achieving up to 1.8x lower latency.


<details>
  <summary>Details</summary>
Motivation: Existing systems for out-of-core SpGEMM in GCNs suffer from high I/O latency and GPU under-utilization due to sparse format data alignment and memory allocation issues.

Method: AIRES introduces block-level data alignment for sparse matrices and a tiling algorithm. It also employs a three-phase dynamic scheduling with a dual-way data transfer strategy using a tiered memory system (GPU memory, GDS, host memory).

Result: AIRES outperforms state-of-the-art methods, reducing latency by up to 1.8x in real-world benchmarks.

Conclusion: AIRES effectively addresses the performance bottlenecks in out-of-core SpGEMM for GCNs, offering significant improvements in latency and throughput.

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [39] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda is an SE(3)-equivariant adapter framework for fine-tuning geometric diffusion models efficiently without altering the original architecture, preserving geometric consistency and avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: Efficient fine-tuning of geometric diffusion models for downstream tasks with varying geometric controls is underexplored.

Method: GeoAda uses a structured adapter design with control signal encoding, trainable copies of pretrained layers, and equivariant zero-initialized convolution.

Result: GeoAda maintains SE(3)-equivariance, achieves state-of-the-art fine-tuning performance, and avoids performance degradation seen in baselines.

Conclusion: GeoAda is widely applicable and effective for diverse geometric control tasks while preserving original model accuracy.

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [40] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: The paper benchmarks general-purpose LLMs against a proprietary hiring model (Match Score), showing Match Score outperforms in accuracy and fairness. It highlights the risks of bias in LLMs and advocates for domain-specific models with bias auditing.


<details>
  <summary>Details</summary>
Motivation: To address concerns about accuracy and algorithmic bias in using LLMs for hiring, and to demonstrate the superiority of domain-specific models over general-purpose LLMs in this context.

Method: Benchmarked state-of-the-art LLMs against Match Score using metrics like ROC AUC, Precision-Recall AUC, F1-score, and fairness measures (impact ratios across demographic groups).

Result: Match Score achieved higher accuracy (ROC AUC 0.85 vs 0.77) and better fairness (minimum race-wise impact ratio 0.957 vs 0.809 for LLMs).

Conclusion: Domain-specific models with bias auditing are crucial for high-stakes tasks like hiring, as they can achieve both accuracy and fairness, unlike off-the-shelf LLMs.

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [41] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [42] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: EBTs generalize System 2 Thinking via unsupervised learning, outperforming existing models in scaling and performance.


<details>
  <summary>Details</summary>
Motivation: To generalize System 2 Thinking approaches without modality or problem-specific constraints, using unsupervised learning.

Method: Train Energy-Based Transformers (EBTs) to verify input-prediction compatibility and reframe predictions as energy minimization.

Result: EBTs scale faster (35% higher rate) and outperform Transformer++ (29% better) and Diffusion Transformers in tasks.

Conclusion: EBTs offer a promising paradigm for scaling model learning and thinking capabilities.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [43] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA is an active learning framework for training parametric guitar amp models using a WaveNet-like architecture, optimizing data sampling for efficiency.


<details>
  <summary>Details</summary>
Motivation: To create virtual guitar amps with minimal data by leveraging active learning.

Method: Uses gradient-based optimization to determine optimal amp knob settings for sampling.

Result: Effective under constrained sample counts, demonstrating efficient data usage.

Conclusion: PANAMA enables efficient virtual amp modeling with minimal data through active learning.

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [44] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: Neural networks exhibit universal scaling limits in training dynamics, with loss curves collapsing onto a single curve when normalized, termed 'supercollapse' under optimal conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the scaling limits of neural network training dynamics as model size and training time grow together.

Method: Analyze loss curves across varying model sizes, learning rate schedules, datasets, and architectures, including transformers. Connect collapse to power-law scaling laws and model SGD noise dynamics.

Result: Loss curves collapse tightly when normalized, with differences below noise levels (supercollapse), observed universally across setups. Breakdown occurs with suboptimal hyperparameter scaling.

Conclusion: Supercollapse serves as a precise indicator of optimal scaling, explained by power-law scaling laws and SGD noise dynamics.

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [45] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP is an LLM-powered framework for automatic VLSI design tuning, improving efficiency and results over manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual parameter selection in VLSI design is laborious and limited by expertise, necessitating an automated solution.

Method: CROP transforms RTL code into vectors, uses embeddings for circuit matching, and employs RAG-enhanced LLM-guided parameter search.

Result: Achieves 9.9% power reduction and superior QoR with fewer iterations than existing methods.

Conclusion: CROP effectively automates and optimizes VLSI design tuning, outperforming manual and traditional approaches.

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [46] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: A latent diffusion framework combining a variational autoencoder and conditional diffusion model improves compression by using keyframes for generative interpolation, achieving higher compression ratios and better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Generative models lack controllability and reconstruction accuracy for practical data compression, necessitating a more efficient solution.

Method: Proposes a latent diffusion framework using a variational autoencoder and conditional diffusion model to compress keyframes and reconstruct others via generative interpolation.

Result: Achieves up to 10x higher compression ratios than rule-based compressors and 63% better performance than learning-based methods under the same error.

Conclusion: The framework effectively bridges the gap in generative models for data compression, offering superior performance and efficiency.

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [47] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET introduces a conformal prediction framework for temporal graphs, addressing limitations of static methods by incorporating temporal dependencies and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods for GNNs ignore temporal dynamics in real-world graphs, violating exchangeability assumptions and limiting applicability.

Method: NCPNET uses a diffusion-based non-conformity score and an efficiency-aware optimization algorithm to handle temporal dependencies and improve computational efficiency.

Result: Experiments on datasets like WIKI and REDDIT show NCPNET reduces prediction set size by up to 31% and ensures guaranteed coverage.

Conclusion: NCPNET effectively extends conformal prediction to dynamic graphs, enhancing reliability and efficiency in high-stakes applications.

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [48] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: A formal validation procedure for model responsiveness to feature interventions is introduced to address safety failures in ML applications like lending and hiring.


<details>
  <summary>Details</summary>
Motivation: Safety failures in ML arise when models ignore how individuals can change their inputs, leading to unfair or harmful outcomes.

Method: A validation procedure frames responsiveness as sensitivity analysis, using black-box access to estimate responsiveness for any model/dataset. Algorithms generate uniform samples of reachable points.

Result: The method supports tasks like falsification and failure probability estimation, demonstrated in applications like recidivism prediction and content moderation.

Conclusion: The procedure enhances safety in real-world ML applications by validating model responsiveness to interventions.

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [49] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: A workflow to reduce bias in dimensionality reduction (DR) evaluation by clustering correlated metrics and selecting representatives from each cluster.


<details>
  <summary>Details</summary>
Motivation: Current DR evaluation metrics can be biased if highly correlated metrics are selected, favoring techniques emphasizing those characteristics.

Method: Proposes a workflow to cluster metrics based on empirical correlations, minimize overlap, and select representative metrics from each cluster.

Result: Quantitative experiments show improved stability in DR evaluation, mitigating bias.

Conclusion: The workflow effectively reduces evaluation bias in DR projections by addressing metric correlation issues.

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [50] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect is a training-free framework that corrects neural network predictions for PDEs by enforcing PDE consistency, reducing errors by up to 100x with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Neural networks for PDEs suffer from error accumulation during long-term rollouts, leading to divergence from valid solutions.

Method: PhysicsCorrect formulates correction as a linearized inverse problem based on PDE residuals, using an efficient caching strategy to precompute Jacobians.

Result: The framework reduces errors by up to 100x across PDE systems (Navier-Stokes, wave equations, Kuramoto-Sivashinsky) with under 5% added inference time.

Conclusion: PhysicsCorrect transforms unstable neural surrogates into reliable tools, bridging deep learning efficiency and physical fidelity.

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [51] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: VERBA uses LLMs to automate pairwise model comparisons, improving transparency and comparability in ML models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of navigating the 'model lake' where many models perform similarly but behave differently, requiring efficient pairwise comparisons.

Method: Leverages LLMs to generate verbalizations of model differences by sampling from pairs, with evaluation via simulation and a diverse benchmark suite.

Result: Achieves up to 80% accuracy in verbalizing differences for decision trees, improving to 90% with structural information.

Conclusion: VERBA enhances model transparency and comparability, enabling post-hoc analysis and selection.

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [52] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: The paper proposes FCA-RL, a reinforcement learning-based subsidy strategy for ride-hailing providers, integrating Fast Competition Adaptation and Reinforced Lagrangian Adjustment to optimize coupons under budget constraints. It also introduces RideGym, a simulation environment for strategy evaluation.


<details>
  <summary>Details</summary>
Motivation: The competitive ranking mechanism on ride-hailing platforms incentivizes providers to lower fares, but existing research lacks effective dynamic coupon strategies.

Method: FCA-RL combines Fast Competition Adaptation (FCA) for dynamic pricing responses and Reinforced Lagrangian Adjustment (RLA) for budget-constrained optimization. RideGym is introduced for simulation and benchmarking.

Result: FCA-RL outperforms baseline methods in diverse market conditions, proving effective for subsidy optimization.

Conclusion: The proposed framework successfully addresses the challenge of dynamic coupon strategy design, offering a viable solution for ride-hailing providers.

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [53] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: URDP integrates LLMs and Bayesian optimization to automate and improve reward function design in RL, addressing inefficiencies in conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional reward engineering in RL is inefficient and inconsistent, while existing LLM-based methods struggle with numerical optimization and resource utilization.

Method: URDP uses self-consistency analysis for uncertainty quantification, simulation-free reward component identification, and uncertainty-aware Bayesian optimization (UABO) for hyperparameter tuning.

Result: URDP outperforms existing methods, generating higher-quality reward functions and improving efficiency across 35 diverse tasks.

Conclusion: URDP successfully combines LLMs and Bayesian optimization to enhance automated reward design in RL.

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [54] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: A knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network is proposed to improve interpretability and generalization for unseen data, reducing communication overhead and enhancing classification efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability and generalization in data-driven semantic communication, especially for unseen data.

Method: Utilizes a knowledge graph-based semantic knowledge base (KG-SKB) to align semantic features and enable zero-shot learning (ZSL) for classification without retraining.

Result: Outperforms existing frameworks in classifying unseen categories across various SNR levels, demonstrating robust generalization.

Conclusion: The KGZS-SC network effectively enhances semantic communication by leveraging structured knowledge and zero-shot learning, improving adaptability and efficiency.

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [55] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Memory Realignment (AMR), a lightweight method for continual learning under concept drift, outperforming Full Relearning (FR) with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional continual learning methods assume static data distributions, ignoring real-world concept drift. This work addresses the need for stability and adaptation in dynamic environments.

Method: Proposes AMR, which selectively updates the replay buffer with drifted data, reducing annotation and computational costs compared to FR. Introduces concept-drift variants of standard benchmarks for evaluation.

Result: AMR matches FR's performance while significantly reducing resource requirements, demonstrating effectiveness in non-stationary continual learning.

Conclusion: AMR is a scalable solution for balancing stability and plasticity in dynamic continual learning scenarios.

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [56] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: Self-distillation improves constrained text generation by aligning the base model with the target distribution, overcoming sparse reward signals.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning in constrained generation settings where target distributions are unlikely under the base model, leading to sparse rewards.

Method: Uses iterative self-distillation to refine the base model, aligning it progressively with the target distribution.

Result: Substantial gains in generation quality due to better alignment with the target.

Conclusion: Self-distillation is effective for improving constrained text generation by mitigating sparse reward issues.

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [57] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: A survey on Transformer models in EEG decoding, covering their fundamentals, hybrid architectures, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To summarize the latest applications and advancements of Transformer models in EEG decoding, highlighting their potential in brain-computer interfaces.

Method: Review and categorize Transformer-based approaches, including hybrid architectures with other deep learning techniques and customized Transformer structures.

Result: Transformers show strong potential in EEG decoding, with hybrid models and customized architectures further enhancing performance.

Conclusion: Transformers are revolutionizing EEG decoding, but challenges remain; future research should address these to advance the field.

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [58] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP is a new XAI algorithm for online patient monitoring, addressing clinical needs by explaining prediction changes, feature attributions, and real-time insights, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods lack suitability for clinical time series explanation, necessitating a solution for real-time, interpretable patient risk monitoring.

Method: DeltaSHAP adapts Shapley values for temporal settings, focusing on observed feature combinations to explain prediction changes efficiently.

Result: DeltaSHAP outperforms state-of-the-art methods, improving explanation quality by 62% and reducing computation time by 33% on the MIMIC-III benchmark.

Conclusion: DeltaSHAP is an efficient, practical XAI solution for clinical time series, enhancing real-time patient monitoring with superior performance.

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [59] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: PANI enhances offline RL by injecting noise into actions, covering the action space while penalizing noise, improving performance without complex diffusion models.


<details>
  <summary>Details</summary>
Motivation: Offline RL's reliance on fixed datasets limits interaction, making generalization crucial. Diffusion models improve performance but are computationally expensive. PANI offers a simpler alternative.

Method: PANI injects noise into actions to cover the action space, penalizing noise amounts. It modifies the MDP into a noisy action MDP, compatible with existing offline RL algorithms.

Result: PANI significantly improves performance across benchmarks, demonstrating its effectiveness despite its simplicity.

Conclusion: PANI provides a computationally efficient alternative to diffusion models for offline RL, achieving strong performance with noise-injected actions.

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [60] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: A data-driven framework using latent signal representations and reinforcement learning optimizes equalizer parameters for high-speed DRAM, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for equalizer parameter optimization in high-speed DRAM systems are computationally intensive or rely on models, necessitating a more efficient approach.

Method: The paper introduces a data-driven framework with learned latent signal representations for fast signal integrity evaluation and a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization.

Result: The method improved eye-opening window areas by 42.7% for cascaded CTLE and DFE structures and 36.8% for DFE-only configurations, outperforming existing techniques.

Conclusion: The framework offers computational efficiency, robust generalization, and superior performance for complex equalizer architectures in DRAM systems.

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [61] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: The paper proposes fine-tuning trajectory prediction models using preference optimization to improve scene consistency in multi-agent settings without sacrificing accuracy or adding computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based trajectory prediction models struggle with capturing interdependencies between agents in complex scenarios, leading to inconsistent predictions.

Method: The work fine-tunes trajectory prediction models using preference optimization, leveraging automatically calculated preference rankings among predicted futures.

Result: Experiments on three datasets show significant improvement in scene consistency with minimal loss in trajectory prediction accuracy and no added computational cost at inference.

Conclusion: Incorporating preference optimization enhances the consistency of trajectory predictions in multi-agent scenarios, addressing a key limitation of existing models.

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [62] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: S2FGL addresses spatial and spectral challenges in Federated Graph Learning (FGL) by mitigating label signal disruptions and aligning frequencies, improving global GNN performance.


<details>
  <summary>Details</summary>
Motivation: Current FGL research neglects signal propagation in spatial and spectral domains, leading to degraded global GNN performance due to edge disconnections and spectral heterogeneity.

Method: Proposes a global knowledge repository for label signal disruption and frequency alignment to address spectral client drifts, forming the S2FGL framework.

Result: Extensive experiments show S2FGL's superiority on multiple datasets.

Conclusion: S2FGL effectively tackles spatial and spectral challenges in FGL, enhancing global GNN generalizability.

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [63] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN adaptively learns an infinite number of bases for univariate functions in KANs, addressing the ad-hoc choice issue via variational inference.


<details>
  <summary>Details</summary>
Motivation: The theoretical power of KANs is limited by the arbitrary selection of bases for univariate functions, which InfinityKAN aims to resolve.

Method: InfinityKAN uses variational inference and backpropagation to learn bases adaptively during training.

Result: The method extends KANs' applicability by integrating hyperparameter learning into the training process.

Conclusion: InfinityKAN enhances KANs by making the choice of bases adaptive and scalable.

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [64] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: The paper explores online conformal prediction, optimizing efficiency while maintaining coverage. It contrasts performance in exchangeable vs. arbitrary sequences, showing a gap and providing a matching algorithm.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing efficient confidence intervals in an online setting, balancing coverage and interval length.

Method: The study analyzes algorithms for arbitrary and exchangeable input sequences, comparing their performance and tradeoffs.

Result: For exchangeable sequences, near-optimal coverage and length are achievable. For arbitrary sequences, a tradeoff exists between length approximation and mistakes. A deterministic algorithm matches Pareto-optimal settings.

Conclusion: The gap between exchangeable and arbitrary sequences highlights unique challenges in online conformal prediction. The proposed algorithm effectively balances these tradeoffs.

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [65] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP (Gradient LOw Rank Projection) improves continual fine-tuning of LLMs by combining full and low-rank parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the trade-off between efficiency and expressiveness in continual fine-tuning of LLMs, overcoming limitations of Low-Rank Adaptation (LoRA).

Method: Combines full and low-rank parameters, jointly updating within a unified low-rank gradient subspace.

Result: Demonstrates superior performance on continual learning benchmarks, mitigating catastrophic forgetting.

Conclusion: GORP offers an effective solution for continual learning in LLMs, balancing efficiency and expressiveness.

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [66] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: The paper introduces a novel deep learning approach for cross-subject motor imagery classification, achieving improved accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of variability in EEG patterns across individuals, which hinders calibration-free BCIs for real-world use.

Method: Uses optimized STFT preprocessing, balanced batching, and CNN training on EEG data.

Result: Achieves 67.60%, 65.96%, and 80.22% accuracy on benchmark datasets, outperforming state-of-the-art methods.

Conclusion: Sets a new benchmark for generalizable, calibration-free MI classification and contributes an open-access dataset.

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [67] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL introduces a self-correcting approach for text-to-SQL models, improving accuracy by 4 percentage points through retry data pre-training.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of SQL-specific generative models and unexplored potential of self-correcting strategies in text-to-SQL tasks.

Method: Corrupts reference SQL queries to create retry data, pre-trains an open-source model with this data, and evaluates full-parameter pre-training vs. LoRA.

Result: 4% improvement in execution accuracy, learned self-correcting behavior, and competitive performance with larger proprietary models.

Conclusion: RetrySQL successfully learns self-correction, enhancing SQL generation accuracy and offering a novel training approach.

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [68] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: DNNs succeed due to their ability to exploit compositional sparsity in target functions, a property shared by all efficiently Turing-computable functions.


<details>
  <summary>Details</summary>
Motivation: Understanding the fundamental principles behind DNNs' success, particularly their ability to leverage compositional sparsity.

Method: Analyzing the compositional sparsity of target functions and its implications for DNN learning dynamics.

Result: DNNs can efficiently learn functions with compositional sparsity, a common property in practical problems.

Conclusion: Compositional sparsity is key to DNNs' success and essential for a comprehensive theory of deep learning and intelligence.

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [69] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: LN layers in GPT-2 can be removed with minimal performance loss, enabling better interpretability without significant trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand the role of LN layers at inference time and improve mechanistic interpretability by reducing nonlinearities and interconnectedness.

Method: Remove LN layers from GPT-2 models, measure validation loss, and test interpretability techniques like logit attribution and attribution patching.

Result: LN removal causes only a small increase in validation loss (e.g., +0.03 for GPT-2 XL) and improves interpretability without major performance drops.

Conclusion: LN layers are not essential for GPT-2's performance, and their removal aids interpretability, making LN-free models valuable for research.

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [70] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: The paper extends Differentiable Boolean Logic Networks (DBNs) with a scalable, trainable interconnect and introduces two pruning stages for model compression.


<details>
  <summary>Details</summary>
Motivation: To enhance DBNs' scalability and efficiency while maintaining accuracy, addressing limitations of earlier designs.

Method: Extends DBNs with a differentiable interconnect and proposes two pruning stages: SAT-based logic equivalence and similarity-based data-driven pruning.

Result: Achieves scalable DBNs with constant parameter growth and improved compression-accuracy trade-off.

Conclusion: The approach enables wider DBN layers and efficient model compression without sacrificing accuracy.

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [71] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: The study explores using Padé Approximant Neural Networks (PadéNets) for fault diagnosis in induction machines, comparing them to CNNs and Self-ONNs. PadéNets outperformed baselines, achieving high diagnostic accuracies.


<details>
  <summary>Details</summary>
Motivation: To improve fault diagnosis in induction machines by leveraging nonlinear neuron architectures like PadéNets, addressing limitations of standard models.

Method: Evaluated CNNs, Self-ONNs, and PadéNets on vibration and acoustic data from induction motor datasets, focusing on nonlinearity and unbounded activation functions.

Result: PadéNets achieved accuracies of 99.96%, 98.26%, 97.61%, and 98.33% for different sensors, outperforming CNNs and Self-ONNs.

Conclusion: PadéNets' enhanced nonlinearity and compatibility with unbounded activation functions significantly improve fault diagnosis performance.

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [72] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: Latent-space diffusion models for dynamical systems emulation are robust to high compression (up to 1000x) and outperform non-generative methods in accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Address the computational cost of diffusion models in physics emulation by leveraging latent-space generation, inspired by image/video generation techniques.

Method: Use latent-space diffusion models for dynamical systems emulation, testing robustness to compression and comparing accuracy/diversity against non-generative methods.

Result: Latent-space emulation remains accurate even at high compression rates (1000x). Diffusion models outperform non-generative counterparts in accuracy and diversity.

Conclusion: Latent-space diffusion models are effective for dynamical systems emulation, offering accuracy and diversity, with practical design choices being critical for training.

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [73] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE is a novel model that learns disentangled representations and hyperparameters of the cost function, outperforming existing VAEs in balancing reconstruction and disentanglement.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of β-VAE, which requires manual tuning of hyperparameters, by dynamically learning the trade-off between reconstruction and disentanglement losses.

Method: L-VAE learns the weights of loss terms and model parameters simultaneously, adding a regularization term to prevent bias.

Result: L-VAE achieves effective balance between reconstruction and disentanglement, performing best or second best on benchmark datasets.

Conclusion: L-VAE is a robust model for disentangling representations, validated by quantitative and qualitative experiments.

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [74] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: A transformer-based matrix variational auto-encoder (matVAE) outperforms DeepSequence in zero-shot prediction on DMS datasets, using fewer parameters and less computation. Incorporating AlphaFold structures further improves performance.


<details>
  <summary>Details</summary>
Motivation: Traditional VEPs rely on MSAs, which may not account for low evolutionary pressure in pharmacogenes. DMS datasets offer an alternative with quantitative fitness scores.

Method: Proposed matVAE with a structured prior, evaluated on 33 DMS datasets from ProteinGym. Compared matVAE-MSA (trained on MSAs) and matENC-DMS (trained on DMS data).

Result: matVAE-MSA outperforms DeepSequence in zero-shot prediction. matENC-DMS excels in supervised tasks. AlphaFold integration enhances performance.

Conclusion: DMS datasets can replace MSAs without significant performance loss, encouraging further development and exploration of DMS data for variant effect prediction.

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [75] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: The paper introduces the Medical Data Pecking approach to assess EHR data quality using software engineering concepts, demonstrating its effectiveness in identifying data issues.


<details>
  <summary>Details</summary>
Motivation: EHR data quality issues affect research reliability, but existing methods lack systematic assessment procedures.

Method: Uses unit testing and coverage concepts with a tool (MDPT) that generates tests via LLMs and executes them to report errors.

Result: Identified 20-43 data issues across three datasets, validating the tool's effectiveness.

Conclusion: The approach improves EHR data validity by integrating medical knowledge into quality testing, with potential for further enhancements.

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [76] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: A hierarchical deep learning framework for recursive higher-order meta-learning enables neural networks to generalize across task hierarchies by generating virtual tasks and learning soft constraints.


<details>
  <summary>Details</summary>
Motivation: To overcome reliance on human-generated data and enhance generalization by autonomously creating informative tasks and constraints.

Method: Uses a generative mechanism to create virtual tasks, iteratively refines constraints, and employs a category-theoretic functorial approach for hierarchical learning.

Result: Enables structured, interpretable learning progression and supports abstraction and knowledge transfer across tasks.

Conclusion: The framework advances meta-learning towards general AI by autonomously generating tasks and solutions, unifying existing models through category theory.

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [77] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: The paper addresses data-efficient exploration in reinforcement learning by focusing on intrinsic motivation through epistemic uncertainty-based bonuses, providing theoretical guarantees and practical approximations.


<details>
  <summary>Details</summary>
Motivation: The challenge of data-efficient exploration in reinforcement learning and the lack of theoretical grounding for information-theoretic approaches motivated this work.

Method: The study examines epistemic uncertainty-based bonuses, proves their alignment with knowledge gaps, and introduces a framework (PTS-BE) combining model-based planning with these bonuses.

Result: PTS-BE outperforms other baselines in environments with sparse rewards or purely exploratory tasks.

Conclusion: The work provides formal guarantees for IG-based approaches and demonstrates the effectiveness of PTS-BE for sample-efficient exploration.

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [78] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: DAID framework improves fairness and generalization in deepfake detection by addressing data biases and suppressing sensitive attributes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the conflict between generalization and fairness in deepfake detection, uncovering a causal relationship between the two.

Method: Proposes DAID, a framework with demographic-aware data rebalancing and demographic-agnostic feature aggregation.

Result: DAID outperforms state-of-the-art detectors in fairness and generalization across benchmarks.

Conclusion: DAID validates the causal link between fairness and generalization, offering a practical solution.

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [79] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: The paper explores the 2-simplicial Transformer, showing it improves token efficiency over standard Transformers, especially in math, coding, and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Modern large language models are less compute-bound due to massive datasets, necessitating architectures that prioritize token efficiency.

Method: The 2-simplicial Transformer generalizes dot-product attention to trilinear functions using a Triton kernel, improving efficiency.

Result: The 2-simplicial Transformer outperforms standard Transformers in math, coding, and reasoning tasks for a fixed token budget.

Conclusion: The 2-simplicial Transformer changes scaling law exponents, proving its superiority in token efficiency for knowledge and reasoning tasks.

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [80] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft is a unified framework enabling a single draft model to work with any target model, addressing cross-vocabulary mismatches and improving decoding speed via adaptive techniques.


<details>
  <summary>Details</summary>
Motivation: Challenges in online deployment include draft-target model incompatibility and latency expectations. OmniDraft aims to solve these for on-device LLM applications.

Method: Uses an online n-gram cache with hybrid distillation fine-tuning and adaptive drafting techniques.

Result: OmniDraft allows a single Llama-68M model to pair with various target models, achieving 1.5-2x speedup.

Conclusion: OmniDraft supports the 'one drafter for all' paradigm, proving effective for math, coding, and text generation tasks.

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [81] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: A guided discrete diffusion model optimizes antibody sequences for developability, integrating a Soft Value-based Decoding in Diffusion (SVDD) Module to enhance biophysical viability without losing naturalness.


<details>
  <summary>Details</summary>
Motivation: To improve antibody developability (manufacturability, stability, safety) for clinical effectiveness by leveraging computational optimization.

Method: A guided discrete diffusion model trained on natural antibody sequences (OAS) and developability data for 246 clinical-stage antibodies, enhanced by SVDD for biophysical viability.

Result: The model replicates natural repertoire features and enriches developability scores under SVDD guidance, outperforming unguided baselines.

Conclusion: This framework enables iterative, ML-driven antibody design to meet binding and biophysical criteria simultaneously.

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [82] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: A method using Differentially Private (DP) generative models for data-sharing in medical imaging, reducing communication costs and enhancing privacy while supporting diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and privacy constraints in medical imaging by improving federated learning with DP generative models.

Method: Use DP-CVAE (Differentially Private Conditional Variational Autoencoder) to model a global, privacy-aware data distribution, leveraging foundation models for compact embeddings.

Result: Outperforms traditional FL classifiers, enhances privacy and scalability, and produces higher-fidelity embeddings with fewer parameters than DP-CGAN.

Conclusion: The proposed DP-CVAE method is efficient, scalable, and privacy-preserving, suitable for diverse downstream tasks in medical imaging.

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [83] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: MARL improves dynamic pricing in supply chains by modeling strategic interactions, outperforming static rule-based methods in competitive dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional ERP systems use static pricing, ignoring strategic interactions in supply chains. MARL addresses this gap.

Method: Evaluated MADDPG, MADQN, and QMIX against static baselines in a simulated e-commerce environment with LightGBM demand prediction.

Result: Rule-based methods excel in fairness and stability but lack competition. MADQN is aggressive, MADDPG balances competition and fairness.

Conclusion: MARL captures emergent strategic behavior, offering insights for dynamic pricing advancements.

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [84] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: The paper proposes a viscous-retained democracy protocol for federated learning (FL) to reduce wasteful data transfer by selecting clients with useful weights, outperforming traditional methods like FedAvg. It also introduces FedVRD to mitigate adversarial impacts.


<details>
  <summary>Details</summary>
Motivation: Current FL mechanisms inefficiently transfer all client weights, incurring high costs. The goal is to optimize data transfer by prioritizing useful weights and addressing adversarial vulnerabilities.

Method: The study explores fluid democracy protocols in FL, proposes a new viscous-retained democracy protocol, and develops FedVRD to dynamically limit adversarial effects while minimizing costs.

Result: The viscous-retained democracy protocol outperforms traditional 1p1v (FedAvg) under the same assumptions and avoids influence accumulation. FedVRD effectively mitigates adversarial impacts.

Conclusion: The proposed protocols enhance FL efficiency by reducing wasteful transfers and improving resilience against adversaries, offering practical improvements over existing methods.

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [85] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: FoG introduces two mechanisms—Experience Replay Decay and Network Expansion—to mitigate primacy bias in deep RL, improving sample efficiency and generalizability.


<details>
  <summary>Details</summary>
Motivation: Addressing primacy bias in deep RL, inspired by human infantile amnesia, to enhance agent performance.

Method: Combines Experience Replay Decay (forgetting early experiences) and Network Expansion (adding parameters dynamically).

Result: Outperforms SoTA algorithms like BRO, SimBa, and TD-MPC2 on 40+ continuous control tasks.

Conclusion: FoG effectively balances memory and enhances learning, offering a robust solution for continuous control in RL.

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [86] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: A framework integrating spatial, temporal, and network dependencies improves e-scooter demand prediction by 27-49%, aiding fleet management and urban planning.


<details>
  <summary>Details</summary>
Motivation: Accurate demand prediction is crucial for managing dockless e-scooters, but existing studies overlook combined spatial, temporal, and network factors.

Method: Proposes a framework that integrates spatial, temporal, and network dependencies for micromobility demand forecasting.

Result: The framework improves prediction accuracy by 27-49% over baseline models.

Conclusion: The findings enable better fleet distribution, cost reduction, and sustainable urban planning for micromobility services.

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [87] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO is a hierarchical contrastive framework for protein-protein interaction prediction, leveraging multi-tiered biological representation matching and hierarchical contrastive loss to achieve state-of-the-art performance and zero-shot transferability.


<details>
  <summary>Details</summary>
Motivation: To bridge heterogeneous biological data modalities and improve protein-protein interaction prediction across species, especially in low-data or rare organism scenarios.

Method: Uses hierarchical contrastive learning to align protein sequences and attributes, incorporating domain knowledge via a data-driven penalty mechanism.

Result: Outperforms existing methods, shows robustness in low-data regimes, and demonstrates zero-shot transferability to other species.

Conclusion: HIPPO advances cross-species PPI prediction and provides a unified framework for sparse or imbalanced multi-species data.

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [88] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: A novel entropy-based method for classification by searching for optimal parameters in a bounded hypercube, extending to polynomial surfaces for complex boundaries.


<details>
  <summary>Details</summary>
Motivation: The problem of separating two classes of points with a hyperplane is central to machine learning, but traditional methods like SVMs and gradient descent have limitations.

Method: Proposes minimizing an entropy-based function to find parameters in a bounded hypercube and a positive vector, extending to polynomial surfaces.

Result: Numerical experiments show the method's efficiency and versatility in handling linear and non-linear classification tasks.

Conclusion: The approach provides a robust alternative to traditional techniques, demonstrating effectiveness in diverse classification scenarios.

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [89] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: The paper studies contextual online pricing with biased offline data, identifying key factors like bias, data size, and dispersion. It provides minimax-optimal regret bounds and algorithms for scalar and general price elasticity cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of contextual pricing when offline data is biased, aiming to derive tight regret guarantees and efficient algorithms.

Method: Uses an Optimism-in-the-Face-of-Uncertainty (OFU) policy for scalar elasticity and a generalized OFU algorithm for general elasticity. Also introduces a robust variant for unknown bias.

Result: Achieves minimax-optimal regret bounds, improving on purely online methods when bias is small. Results also apply to stochastic linear bandits.

Conclusion: The work provides the first tight regret guarantees for contextual pricing with biased offline data, with techniques transferable to related problems.

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [90] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: The paper explores why recurrent models fail to generalize to longer sequences than their training context and proposes simple training interventions to improve length generalization.


<details>
  <summary>Details</summary>
Motivation: Recurrent models like state space models and linear attention struggle with length generalization, performing poorly on sequences longer than their training context. The study aims to understand and address this limitation.

Method: The authors analyze the 'unexplored states hypothesis' and test training interventions, such as initializing states with Gaussian noise or using final states from other sequences, to improve state coverage.

Result: With minimal post-training steps (∼0.1% of pre-training budget), these interventions enable models to generalize to sequences orders of magnitude longer (e.g., 2k→128k) and improve long-context task performance.

Conclusion: Simple training interventions can efficiently enhance length generalization in recurrent models, making them robust for long sequences.

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [91] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE is a survival analysis model ensuring calibration for all subpopulations, balancing calibration and discrimination through constrained optimization, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing survival models are poorly calibrated for minority subpopulations, risking erroneous clinical decisions.

Method: GRADUATE frames multicalibration as constrained optimization, optimizing calibration and discrimination in-training.

Result: Empirical results show GRADUATE outperforms baselines, with theoretical guarantees of near-optimality and feasibility.

Conclusion: GRADUATE effectively addresses calibration issues in survival analysis, improving reliability for all subpopulations.

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [92] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: The paper investigates distribution testing under algorithmic replicability, focusing on sample complexity for testing properties like closeness and independence of distributions. It introduces new replicable algorithms and lower bound techniques, addressing open questions in uniformity and closeness testing.


<details>
  <summary>Details</summary>
Motivation: To systematically explore distribution testing within algorithmic replicability, aiming to understand the sample complexity for testing properties of distributions reliably.

Method: Develops replicable algorithms for testing closeness and independence of discrete distributions and introduces a new methodology for proving sample complexity lower bounds.

Result: Presents near-optimal sample complexity lower bounds for replicable uniformity and closeness testing, resolving prior open questions.

Conclusion: The study advances the understanding of replicable distribution testing, providing algorithmic and theoretical tools for future research.

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [93] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: ExPO, a new RL post-training method, improves reasoning by generating self-explained positive samples aligned with the model's policy, outperforming expert-based methods in challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current RL-style post-training methods refine existing knowledge but fail to solve problems where the model initially struggles, especially in early-stage training and hard reasoning tasks.

Method: ExPO generates effective positive samples by conditioning on ground-truth answers, ensuring they align with the model's policy and improve correctness.

Result: ExPO enhances learning efficiency and performance on reasoning benchmarks, notably in challenging tasks like MATH level-5.

Conclusion: ExPO is a modular and effective framework for improving reasoning in large language models, especially where initial performance is weak.

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [94] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper addresses biased treatment effect estimates due to discrepancies between training and inference data, proposing a framework using LLMs and a doubly robust learner to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: Estimating treatment effects is vital for personalized medicine, but discrepancies between structured training data and incomplete textual inference data introduce bias.

Method: The authors formalize the issue as inference time text confounding and propose a framework combining large language models (LLMs) with a custom doubly robust learner.

Result: Experiments show the framework effectively mitigates biases caused by inference time text confounding in real-world applications.

Conclusion: The proposed framework successfully addresses the inference time text confounding problem, improving treatment effect estimation accuracy.

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [95] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB is a multi-view learning framework for fMRI data that models higher-order interactions (HOIs) to improve diagnostic accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Higher-order interactions (HOIs) in fMRI data can enhance diagnostic accuracy, but extracting and utilizing them effectively is challenging.

Method: Proposes MvHo-IB, integrating pairwise interactions and HOIs using O-information and Renyi entropy, a Brain3DCNN encoder, and a multi-view learning information bottleneck.

Result: Outperforms state-of-the-art methods on three benchmark fMRI datasets.

Conclusion: MvHo-IB effectively leverages HOIs for improved diagnostic decision-making in fMRI analysis.

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [96] [Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models](https://arxiv.org/abs/2507.02002)
*Shaurya Mallampati,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.MA

TL;DR: The paper explores LLM-driven agents in dynamic, multi-agent scenarios, combining strategic reasoning and real-time adaptation for improved collaboration and performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding LLM reasoning in dynamic, real-time, multi-agent environments like cooperative gameplay.

Method: Integrates LLM-driven agents with game-theoretic principles (belief consistency, Nash equilibrium) and real-time strategy refinement for adaptive feedback.

Result: Achieves a 26% improvement over PPO baselines in high-noise environments with sub-1.05ms latency, enhancing collaboration efficiency and task completion.

Conclusion: Game-theoretic guidance with real-time feedback boosts LLM performance, fostering resilient and flexible multi-agent systems.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities across
mathematical, strategic, and linguistic tasks, yet little is known about how
well they reason in dynamic, real-time, multi-agent scenarios, such as
collaborative environments in which agents continuously adapt to each other's
behavior, as in cooperative gameplay settings. In this paper, we bridge this
gap by combining LLM-driven agents with strategic reasoning and real-time
adaptation in cooperative, multi-agent environments grounded in game-theoretic
principles such as belief consistency and Nash equilibrium. The proposed
framework applies broadly to dynamic scenarios in which agents coordinate,
communicate, and make decisions in response to continuously changing
conditions. We provide real-time strategy refinement and adaptive feedback
mechanisms that enable agents to dynamically adjust policies based on immediate
contextual interactions, in contrast to previous efforts that evaluate LLM
capabilities in static or turn-based settings. Empirical results show that our
method achieves up to a 26\% improvement in return over PPO baselines in
high-noise environments, while maintaining real-time latency under 1.05
milliseconds. Our approach improves collaboration efficiency, task completion
rates, and flexibility, illustrating that game-theoretic guidance integrated
with real-time feedback enhances LLM performance, ultimately fostering more
resilient and flexible strategic multi-agent systems.

</details>


### [97] [Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System](https://arxiv.org/abs/2507.02170)
*Adam Kostka,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: SynergyMAS integrates MAS techniques for logical reasoning, knowledge retention, and ToM, improving teamwork and problem-solving, as shown in a product development case study.


<details>
  <summary>Details</summary>
Motivation: To enhance collaborative teamwork and problem-solving in complex scenarios by combining advanced MAS techniques.

Method: Developed SynergyMAS, a framework integrating logical reasoning, long-term knowledge retention, ToM, and optimized communication protocols.

Result: Demonstrated improved performance and adaptability in a product development team case study.

Conclusion: SynergyMAS shows promise for addressing complex real-world challenges effectively.

Abstract: This paper explores the integration of advanced Multi-Agent Systems (MAS)
techniques to develop a team of agents with enhanced logical reasoning,
long-term knowledge retention, and Theory of Mind (ToM) capabilities. By
uniting these core components with optimized communication protocols, we create
a novel framework called SynergyMAS, which fosters collaborative teamwork and
superior problem-solving skills. The system's effectiveness is demonstrated
through a product development team case study, where our approach significantly
enhances performance and adaptability. These findings highlight SynergyMAS's
potential to tackle complex, real-world challenges.

</details>
