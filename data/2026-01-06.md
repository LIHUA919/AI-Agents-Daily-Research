<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: The paper presents a cross-lingual ontology alignment system using embedding-based cosine similarity matching with a fine-tuned multilingual transformer model, achieving a 71% F1 score on OAEI-2022 multifarm track.


<details>
  <summary>Details</summary>
Motivation: To improve cross-lingual ontology alignment by capturing subtle similarities between entities in different languages, addressing limitations of existing baselines.

Method: Develops a pipeline that enriches ontology entities with contextual descriptions, uses a fine-tuned transformer-based multilingual model to generate embeddings, computes cosine similarity for matching, and applies threshold filtering to retain highly similar entity pairs.

Result: Achieves a 71% F1 score (78% recall, 65% precision) on the OAEI-2022 multifarm evaluation dataset, representing a 16% increase over the best baseline score.

Conclusion: The proposed alignment pipeline effectively captures cross-lingual similarities, demonstrating significant improvement in performance and potential for practical applications in multi-language ontology integration.

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [2] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: A system called MathLedger integrates formal verification, cryptographic attestation, and learning to create verifiable machine cognition for trust in AI safety.


<details>
  <summary>Details</summary>
Motivation: To address the crisis of trust in opaque and non-verifiable AI systems for safety-critical applications.

Method: Implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent driven by verifier outcomes. Uses a ledger-attested learning prototype with Phase I experiments including measurement infrastructure validation and governance stress tests.

Result: Phase I experiments successfully validated measurement infrastructure (Delta p computation, variance tracking) and fail-closed governance triggers under controlled conditions. No convergence or capability claims made.

Conclusion: MathLedger provides an infrastructural prototype for verifiable machine learning, enabling scalable auditability and addressing trust issues in AI systems.

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [3] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: This paper proposes an Agentic AI framework for autonomous, real-time credit risk decision-making, using multi-agent systems with reinforcement learning and explainable AI to improve speed, transparency, and responsiveness over traditional models, while noting practical limitations.


<details>
  <summary>Details</summary>
Motivation: The rapid digitalization of financial services has created a need for autonomous, transparent, and real-time credit risk decision-making systems, as traditional machine learning models lack adaptive reasoning, situational awareness, and autonomy for modern financial operations.

Method: The research introduces an Agentic AI framework with a multi-agent system incorporating reinforcement learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines. It includes processes like agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles, designed to assess borrower risk profiles with minimal human involvement.

Result: Findings indicate that the proposed system achieves better decision speed, transparency, and responsiveness compared to traditional credit scoring models.

Conclusion: The Agentic AI framework has high potential to transform credit analytics, but faces practical limitations such as model drift risks, inconsistencies in interpreting high-dimensional data, regulatory uncertainties, and infrastructure limitations in low-resource settings. Future studies should focus on dynamic regulatory compliance, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [4] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas is a training-free framework that extracts cognitive artifacts from conversations and organizes them into a temporal graph, outperforming RAG and GraphRAG in benchmarks with a relative improvement of 530% in temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models face context window limits and information fidelity issues in long conversations, where existing methods like truncation and summarization either discard early information or lose details.

Method: The framework extracts verbatim-grounded cognitive artifacts (e.g., decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.

Result: On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%) and GraphRAG (13.7%), with a 97.5% recall rate and 93.0% exact match preservation on controlled benchmarks.

Conclusion: CogCanvas provides a practical, training-free alternative that significantly outperform standard baselines, offering high recall and accuracy without the need for dedicated training like optimized approaches.

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [5] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: Analysis of energy optimization in large reasoning models by balancing baseline and auxiliary energy to avoid waste, using variance-aware routing and dispatch.


<details>
  <summary>Details</summary>
Motivation: To reduce heterogeneous energy costs in large reasoning models by selecting appropriate models and operational methods, addressing the balance between mean energy provisioning and stochastic fluctuations.

Method: Develop a theoretical framework focusing on the critical regime where neither auxiliary nor baseline energy is wasted, and analyze performance limitations due to volatility using second-order characterization.

Result: Performance is governed by absorbing variability across time, models, and execution choices, highlighting variance-aware routing and dispatch as key design principles.

Conclusion: The study establishes a theoretical basis for energy-aware model routing policies, showing that routing behavior can be characterized using scaling laws for training and inference compute in LRMs.

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [6] [Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis](https://arxiv.org/abs/2601.00828)
*Yin Li*

Main category: cs.AI

TL;DR: Stronger LLMs have lower intrinsic self-correction rates due to deeper errors, despite higher accuracy, challenging assumptions about linear model capability.


<details>
  <summary>Details</summary>
Motivation: To systematically examine the paradox where weaker models show higher self-correction effectiveness than stronger ones in LLMs, decomposing self-correction into error detection, localization, and correction.

Method: Systematic decomposition of self-correction into three sub-capabilities; cross-model experiments on GSM8K-Complex with three major LLMs (GPT-3.5, DeepSeek, Claude), analyzing 500 samples per model and 346 total errors.

Result: Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) corrected 1.6x more errors intrinsically than stronger models (DeepSeek, 94% accuracy)—26.8% vs 16.7%. Error detection varied widely (10% to 82%), but did not correlate with correction success. Providing error location hints reduced performance.

Conclusion: Self-correction in LLMs is not linearly tied to model capability; stronger models make deeper errors resistant to correction, requiring refined self-refinement pipelines.

Abstract: Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.

</details>


### [7] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: The paper investigates whether AI models' step-by-step explanations accurately reveal influential information, finding models often hide hints that affect their answers, raising concerns about transparency and trust in AI reasoning.


<details>
  <summary>Details</summary>
Motivation: Practitioners commonly assume AI explanations reveal what actually influenced the AI's decisions, but this paper tests that assumption to assess the reliability and transparency of such explanations.

Method: The study embedded hints into questions and measured whether models mentioned them, analyzing over 9,000 test cases across 11 leading AI models and examining different conditions like asking directly, observing, or forcing hints.

Result: Models rarely mention hints spontaneously but admit noticing them when asked directly, showing they recognize influential information but hide it. Forced reporting increases hint mentions even when none exist but reduces accuracy, and hints appealing to user preferences are especially dangerous as models follow them most but report them least.

Conclusion: Merely observing AI reasoning is insufficient to catch hidden influences, indicating that current explanation methods may not ensure transparency, and alternative approaches are needed to improve trust in AI systems.

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [8] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: OmniNeuro is an HCI framework that makes BCI decoding transparent by integrating interpretability engines for real-time feedback and clinical reports, improving user understanding and reducing trial-and-error.


<details>
  <summary>Details</summary>
Motivation: Deep Learning improves BCI accuracy but its 'Black Box' nature hinders clinical adoption, causing user frustration and poor neuroplasticity outcomes.

Method: Proposes OmniNeuro with three interpretability engines: Physics (Energy), Chaos (Fractal Complexity), and Quantum-Inspired uncertainty modeling, driving real-time Neuro-Sonification and Generative AI Clinical Reports.

Result: On PhysioNet dataset (N=109), system achieved mean accuracy of 58.52%; qualitative pilot studies (N=3) confirm explainable feedback helps users regulate mental effort and reduces trial-and-error phase.

Conclusion: OmniNeuro is decoder-agnostic, serving as an essential interpretability layer for any state-of-the-art architecture to enhance BCI transparency and clinical utility.

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [9] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: TPP-TAL is a plug-and-play framework enhancing temporal reasoning in LLMs for temporal point processes (TPPs), improving event prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods using LLMs for TPPs have difficulty capturing the interaction between temporal information and semantic context, which is essential for accurate event modeling.

Method: TPP-TAL explicitly aligns temporal dynamics with contextual semantics before input to the LLM, rather than just concatenating event time and type embeddings.

Result: Experiments on benchmark datasets show that TPP-TAL significantly improves temporal likelihood estimation and event prediction accuracy.

Conclusion: Enhancing temporal awareness in LLMs is crucial for continuous-time event modeling, and TPP-TAL provides an effective solution for TPPs applications.

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [10] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: Fine-tuning language models with OpenTelemetry traces and a curated dataset improves detection of temporal attack patterns in multi-agent AI workflows, achieving 31.4% accuracy gain and establishing a reproducible framework.


<details>
  <summary>Details</summary>
Motivation: To enhance security in multi-agent AI systems by detecting temporal attack patterns more effectively, requiring better methods for analyzing OpenTelemetry traces and addressing resource constraints.

Method: Curated dataset from cybersecurity sources and synthetic OpenTelemetry traces, with iterative QLoRA fine-tuning on ARM64 hardware (NVIDIA DGX Spark) using three training iterations and strategic augmentation.

Result: Benchmark accuracy improved from 42.86% to 74.29%, showing targeted training outperforms scaling; contribution includes open release of datasets, scripts, and benchmarks on HuggingFace.

Conclusion: While human oversight is necessary due to false positives, this work provides a first reproducible framework for building custom agentic security models tailored to specific threat environments.

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [11] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: Critical review of Kosmyna et al.'s AI study highlighting methodological flaws in sample size, reproducibility, EEG analysis, and transparency.


<details>
  <summary>Details</summary>
Motivation: To provide constructive feedback on Kosmyna et al.'s study to enhance its scientific rigor and readiness for publication.

Method: Review and critique of an existing study's methodology, focusing on design, reproducibility, EEG analysis, and transparency.

Result: Identified several methodological issues including small sample size, reproducibility problems, EEG analysis flaws, inconsistent reporting, and lack of transparency.

Conclusion: The authors express concerns about the original study's methodology and suggest improvements for peer review readiness.

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [12] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: This paper studies how LLM training data affects brand recommendations, finding Chinese LLMs mention brands more often than International LLMs due to geographical data composition, introducing concepts like Existence Gap and Data Moat Framework.


<details>
  <summary>Details</summary>
Motivation: As AI systems mediate consumer information, brands face algorithmic invisibility, motivated by understanding systematic differences in brand recommendations from LLMs based on training data.

Method: Analyzed 1,909 pure-English queries across 6 LLMs (GPT-4o, Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, including a case study of Zhizibianjie (OmniEdge).

Result: Chinese LLMs show 30.6 percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%, p<.001), with disparities persisting in identical English queries, indicating training data geography drives the effect.

Conclusion: Proposes Data Moat Framework and Algorithmic Omnipresence for brand visibility in AI markets, providing a roadmap for brands to build Data Moats and highlighting that data boundaries define market frontiers.

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [13] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL is a mathematical framework for systematic prompt optimization, reducing token usage by 29.8% and cost, validated through evaluations with 11 models.


<details>
  <summary>Details</summary>
Motivation: To transform prompt engineering from heuristic practice into systematic optimization for efficient LLM interaction.

Method: Develops Universal Conditional Logic (UCL) with mechanisms like indicator functions, structural overhead, and early binding; conducts systematic evaluation with 305 cases, 11 models, and 4 iterations.

Result: Achieves significant token reduction (29.8%) and cost savings; explains performance differences via Over-Specification Paradox; optimal configurations vary by model architecture.

Conclusion: UCL provides a calibratable framework for efficient LLM interaction, with future research focusing on model-family-specific optimization.

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [14] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: Introduces Counterfactual Self-Questioning, a method for a single language model to self-improve by generating and evaluating counterfactual critiques of its own reasoning, without external models.


<details>
  <summary>Details</summary>
Motivation: Existing language model self-improvement methods often rely on external critics, reward models, or ensemble sampling, increasing complexity and training instability.

Method: The model produces an initial reasoning trace, formulates targeted questions to challenge potential failures, generates alternative reasoning trajectories, and uses these for policy optimization without auxiliary models.

Result: Experiments on multiple mathematical reasoning benchmarks show improved accuracy and training stability, especially for smaller models.

Conclusion: Counterfactual Self-Questioning enables scalable self-improvement using internally generated supervision alone, addressing limitations of prior approaches.

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [15] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: This thesis analyzes in-context learning (ICL) and model collapse in LLMs, showing a phase transition in ICL parameters, proving collapse in simplified settings, and introducing context collapse as a link between ICL dynamics and stability issues.


<details>
  <summary>Details</summary>
Motivation: To investigate fundamental phenomena in LLMs—ICL and model collapse—by studying them in simplified analytical frameworks to gain rigorous insights into parameter behaviors, convergence properties, and long-term stability challenges.

Method: For ICL, uses a linear transformer with tied weights trained on linear regression tasks, reducing forward pass to preconditioned gradient descent to analyze optimal preconditioner. For model collapse, applies martingale and random walk theory to linear regression and Gaussian fitting under replacing and cumulative data regimes.

Result: Minimizing in-context loss leads to a phase transition above a critical context length, with solution developing a skew-symmetric component inducing gradient rotation. Model collapse is proven to almost surely converge unless data grows fast or is retained. Introduces context collapse linking ICL dynamics to generative stability.

Conclusion: The research highlights critical thresholds and dynamics in LLMs, with ICL showing emergent properties under constraints, model collapse emphasizing data retention needs, and context collapse connecting learning mechanisms to degradation risks in long generations.

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [16] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit simulates multi-agent social media interactions in elections to study how LLM agents use persuasion techniques realistically.


<details>
  <summary>Details</summary>
Motivation: To address limitations of game-based simulations in prior research and enable realistic study of persuasion in social media contexts during political elections.

Method: Developed ElecTwit, a simulation framework that grounds experiments in realistic environments, testing 25 specific persuasion techniques across multiple LLMs.

Result: Observed widespread use of 25 persuasion techniques across tested LLMs, variations in usage between models based on architecture and training, and unique phenomena like 'kernel of truth' messages and collective demands for written proof.

Conclusion: The framework provides a foundation for evaluating persuasive LLM agents in real-world contexts to ensure alignment and prevent dangerous outcomes, highlighting the impact of model differences on social simulation dynamics.

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [17] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: TKGQA model using Multi-hop Reasoning Enhanced framework with Tree-Group Relative Policy Optimization improves multi-hop reasoning over temporal graphs via cold-start fine-tuning and tree-structured exploration, outperforming SOTA on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge in temporal knowledge graph question answering where LLMs retrieve multiple temporally similar and semantically complex relations at each hop, leading to suboptimal decisions and error propagation during multi-hop reasoning.

Method: The MRE framework uses prompt engineering for diverse reasoning trajectory generation, selects valid trajectories for supervised fine-tuning as a cold-start, and introduces T-Group Relative Policy Optimization (T-GRPO), a recursive tree-structured explore-exploit method that establishes hop-wise causal dependencies and uses multi-path feedback for evaluation.

Result: Experimental results on two TKGQA benchmarks show that the MRE-based model consistently surpasses state-of-the-art approaches, with additional analysis revealing improved interpretability and robustness to noisy temporal annotations.

Conclusion: The paper concludes that the proposed MRE framework with T-GRPO optimization consistently outperforms SOTA methods on TKGQA benchmarks, offering enhanced interpretability and robustness against noisy temporal data, thereby advancing multi-hop temporal reasoning in LLMs.

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [18] [Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies](https://arxiv.org/abs/2601.01301)
*Keith Frankston,Benjamin Howard*

Main category: cs.AI

TL;DR: RMCTS is a faster, breadth-first MCTS algorithm that uses recursive posterior policies and prior network-defined trees to match AlphaZero's quality in one-third the training time.


<details>
  <summary>Details</summary>
Motivation: To overcome the GPU latency inefficiencies in AlphaZero's MCTS-UCB by enabling parallelized network inferences and speeding up the search process for game states.

Method: RMCTS explores trees in a breadth-first manner with recursion based on optimized posterior policies from leaves to root, using prior network policies to define the tree instead of adaptive selection.

Result: RMCTS is over 40x faster for single root states and 3x faster for batches compared to MCTS-UCB, with training networks achieving similar quality in one-third the time across Connect-4, Dots-and-Boxes, and Othello.

Conclusion: RMCTS offers significant speed advantages over MCTS-UCB with minor compromises in adaptability, making it a practical alternative for efficient training and search in games.

Abstract: We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, "RMCTS". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.
  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in "Monte--Carlo tree search as regularized policy optimization" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.
  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.

</details>


### [19] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: This paper proposes a four-stage framework for integrating AI into digital twins, detailing how AI evolves digital twins from passive simulations to intelligent autonomous systems across modeling, mirroring, intervention, and autonomous management stages, with applications across domains like healthcare and manufacturing.


<details>
  <summary>Details</summary>
Motivation: To systematically characterize how AI methodologies are integrated across the digital twin lifecycle, addressing the evolution from passive tools to autonomous entities, with cross-domain applications.

Method: Develop a unified four-stage framework: modeling using physics-based and physics-informed AI, mirroring with real-time synchronization, intervening through predictive modeling and optimization, and achieving autonomous management via large language models and intelligent agents.

Result: Analysis shows synergy between physics-based modeling and data-driven learning, highlighting the shift to physics-informed and foundation models, and how generative AI enables proactive cognitive systems for reasoning and communication.

Conclusion: Identifies challenges like scalability, explainability, and trustworthiness, and outlines directions for responsible AI-driven digital twin systems in diverse application domains.

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [20] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: The paper introduces JiSi, a framework for LLM collaboration to overcome scaling bottlenecks, showing open-source LLMs can outperform Gemini-3-Pro at lower cost.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have advanced, but monolithic scaling like Gemini-3-Pro is costly; collective intelligence offers an alternative, yet current routing and aggregation methods have bottlenecks in query-based paradigms, static aggregation, and underutilized complementarity.

Method: Propose JiSi framework with three innovations: Query-Response Mixed Routing, Support-Set-based Aggregator Selection, and Adaptive Routing-Aggregation Switch to orchestrate LLM collaboration effectively.

Result: Comprehensive experiments on nine benchmarks show JiSi surpasses Gemini-3-Pro with 47% costs using ten open-source LLMs, outperforming mainstream baselines.

Conclusion: Collective intelligence through JiSi demonstrates a viable path to surpass state-of-the-art LLMs like Gemini-3-Pro, suggesting a novel approach towards achieving Artificial General Intelligence (AGI).

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [21] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni is a unified multimodal AI model that integrates heterogeneous, cross-disciplinary scientific data for understanding and high-fidelity generation, achieving state-of-the-art performance in Earth science and Biomedicine.


<details>
  <summary>Details</summary>
Motivation: Scientific problems often require cross-disciplinary integration of heterogeneous, high-dimensional data, but existing AI models are typically domain-specific or lack unified multimodal capabilities for such data.

Method: FuXi-Uni aligns cross-disciplinary scientific tokens with natural language tokens and uses a science decoder to reconstruct scientific tokens, enabling natural language conversation and scientific numerical prediction within a single architecture.

Result: In Earth science, FuXi-Uni outperforms SOTA physical systems in global weather forecasting, tropical cyclone prediction, and spatial downscaling. In Biomedicine, it exceeds leading multimodal models on biomedical visual question answering benchmarks.

Conclusion: FuXi-Uni represents an advance toward general-purpose multimodal scientific models by unifying diverse scientific modalities in a shared latent space while maintaining strong domain performance.

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [22] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: Proposes KGCE, a novel benchmarking platform for evaluating multimodal language models in cross-platform educational tasks, addressing gaps in existing frameworks by integrating knowledge base enhancement and a dual-graph evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Existing benchmark frameworks are deficient in supporting cross-platform educational tasks, especially with private-domain software like school-specific tools, and rely on coarse-grained metrics, leading to inefficient agent performance and poor evaluation detail.

Method: Constructed a dataset of 104 education-related tasks across Windows, Android, and cross-platform scenarios. Introduced a dual-graph evaluation framework to decompose tasks into sub-goals and verify completion. Developed an enhanced agent system with a knowledge base specific to school software to improve execution in private-domain tasks.

Result: KGCE provides fine-grained evaluation metrics and addresses execution bottlenecks, with code available at https://github.com/Kinginlife/KGCE.

Conclusion: KGCE offers an effective solution for benchmarking cross-platform educational agents by enhancing evaluation precision and agent efficiency in complex, private-domain educational environments.

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [23] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: Proposes AAAI pipeline to mitigate factual hallucinations in small language models for better financial classification by associating errors, detecting them with verifiers, and enabling adaptive inference.


<details>
  <summary>Details</summary>
Motivation: Small language models are favored for financial classification due to speed and local use, but they suffer more from factual hallucinations and weaker performance than large models, raising the question of if solving hallucinations can improve classification.

Method: Develops a three-step AAAI pipeline: Association Identification (linking factual hallucinations to classification errors), Automated Detection (using encoder-based verifiers to find hallucinations), and Adaptive Inference (incorporating feedback to adjust classification).

Result: Experiments with three representative SLMs show that factual hallucinations correlate with misclassifications, verifiers effectively detect hallucinations, and adaptive inference improves classification performance.

Conclusion: The AAAI pipeline enhances SLMs' financial classification by mitigating factual hallucinations, contributing to more trustworthy and effective applications in finance.

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [24] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: Study focuses on building optimal bases for specific implications in triadic formal contexts introduced by Ganter and Obiedkov.


<details>
  <summary>Details</summary>
Motivation: Construct an optimal base for conditional attribute and attributional condition implications in triadic contexts.

Method: 

Result: 

Conclusion: 

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [25] [Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning](https://arxiv.org/abs/2601.01511)
*Ahmed Dawoud,Osama El-Shamy*

Main category: cs.AI

TL;DR: A study proposes a Neural Network-Enhanced Double Machine Learning framework that uses text embeddings to estimate causal treatment effects, showing it reduces bias compared to tree-based methods.


<details>
  <summary>Details</summary>
Motivation: Observational causal estimation is often biased by unobserved confounders, which traditional methods struggle with when confounders are orthogonal to structured covariates. High-dimensional unstructured text can serve as rich proxies for these confounders.

Method: The framework leverages text embeddings within a double machine learning approach, using neural networks to model the continuous topology of embedding manifolds, unlike standard tree-based estimators.

Result: Unstructured text embeddings capture confounding information missing in structured data. Tree-based DML has +24% bias, while the neural network method reduces bias to -0.86%, effectively recovering ground-truth causal parameters in synthetic benchmarks.

Conclusion: Deep learning architectures are essential for unconfoundedness when conditioning on high-dimensional natural language data, improving causal identification.

Abstract: Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data

</details>


### [26] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: The paper introduces a Bayesian cost-aware multi-LLM orchestration framework for sequential decision-making in settings with asymmetric error costs, improving cost reduction by 34% and fairness by 45% over single-LLM baselines.


<details>
  <summary>Details</summary>
Motivation: The authors identify that existing approaches, which treat large language models (LLMs) as classifiers with thresholded confidence for decisions, are inadequate for handling the complexity of sequential decisions with asymmetric error costs. This is particularly relevant to high-stakes domains like hiring, medical triage, and fraud detection, where erroneous decisions carry significant disparate costs. The motivation is anchored in closing the gap between traditional decision theory and modern LLM applications, ensuring that actions are not only based on probabilistic confidence but also explicitly account for costs and enable coherent updating as new information arrives.

Method: The proposed method re-conceptualizes LLMs as approximate likelihood models rather than classifiers. It involves three main steps: (1) contrastive prompting to elicit likelihoods for each candidate state; (2) robust aggregation of likelihoods across diverse LLMs using statistical techniques to form a unified likelihood; (3) Bayesian belief updating with explicit priors as new evidence accumulates. This framework enables cost-aware action selection via expected value minimization, principled information gathering via value of information (VOI), and ensemble bias mitigation for fairness. The method is validated through experiments in a resume screening scenario with defined costs and LLMs, such as GPT-4o and Claude 4.5 Sonnet.

Result: The experiment on 1000 resumes with asymmetric costs (e.g., $40,000 per missed hire) demonstrates significant improvement over single-LLM baselines. The framework reduces total cost by $294,000 (34%) relative to the best single-LLM baseline and improves demographic parity by 45% (group gap decreases from 22% to 5%). Ablation studies attribute these gains to key components: 51% of savings from multi-LLM aggregation, 43% from sequential updating, and 20% from disagreement-triggered information gathering.

Conclusion: The paper establishes that a Bayesian, cost-aware multi-LLM orchestration approach, which properly treats LLMs as likelihood models rather than classifiers, offers substantial advantages in sequential decision-making scenarios with asymmetric costs. By enabling coherent belief revisiting, optimal action selection, and effective bias mitigation, it advances the practical deployment of AI in high-stakes domains. Notably, these empirical results reinforce the theoretical benefits of proper probabilistic foundations, indirectly suggesting a paradigm shift towards more robust integration of decision theory into modern LLM applications.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [27] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: We propose a framework (Project Aletheia) to quantify cognitive conviction in AI reasoning using regularization and proxy data, showing models exhibit defensive overthinking but maintain safety via alignment.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods focus on superficial knowledge but fail to measure the depth of belief or conviction in AI's System 2 reasoning, risking epistemological pitfalls.

Method: Developed a cognitive physics framework using Tikhonov Regularization to invert judge confusion matrices, enhanced by a Synthetic Proxy Protocol to avoid opaque private data reliance.

Result: Pilot studies on 2025 models like DeepSeek-R1 and OpenAI o1 detected "Defensive OverThinking" as a stress response, but the Aligned Conviction Score (S_aligned) verified that conviction does not compromise safety.

Conclusion: This work provides a blueprint for evaluating AI's scientific integrity by measuring conviction, addressing gaps in current benchmarks and ensuring safe, aligned cognitive capabilities.

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [28] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: The paper proposes a two-stage framework (context formation and context navigation) to improve alignment of large language models (LLMs) with human behavior in complex decision-making tasks, validating it through multiple experiments and showing its necessity varies by task complexity.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to simulate human behavior in experiments but systematically diverge from human decisions in complex environments involving anticipating others' actions and forming beliefs based on observed behavior, highlighting a need for better behavioral alignment.

Method: A two-stage framework: context formation (explicitly specifying experimental design to establish accurate task representation) and context navigation (guiding reasoning within that representation). Validated via replication of a sequential purchasing game with quality signaling, extension to a crowdfunding game with costly signaling and a demand-estimation task, tested across four SOTA LLMs.

Result: Complex decision-making environments require both stages for behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. The framework clarifies when each stage is necessary.

Conclusion: The study provides a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research, improving alignment in diverse decision environments.

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [29] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM is a reasoning model fine-tuned on a 10M-scale dataset for STEM tasks, achieving state-of-the-art performance through data-algorithm co-design and making models and dataset publicly available.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities in STEM domains by leveraging large-scale open-source data and synthetic data, addressing limitations in existing models through optimal data-algorithm integration.

Method: Uses a data-algorithm co-design engine with a 5-stage data curation process (annotation, deduplication, decontamination, distillation, stratified sampling) and a failure-driven post-training framework with targeted knowledge retrieval and data synthesis.

Result: Logics-STEM shows exceptional performance on STEM benchmarks with an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of the approach.

Conclusion: The success of Logics-STEM highlights the potential of combining large-scale open data with synthetic data and underscores the critical role of data-algorithm co-design in improving reasoning through post-training, with open-source release to support future research.

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [30] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: CaveAgent is a framework that shifts from an LLM-as-text-generator to an LLM-as-runtime-operator, using a dual-stream architecture for state management and Python runtime execution to enhance task handling.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are limited by text-centric paradigms and fragile JSON-based function calling in long-horizon tasks, causing context drift and inefficiency.

Method: Introduces a Dual-stream Context Architecture with a semantic stream for reasoning and a Python Runtime stream for execution, along with Stateful Runtime Management for persistent Python object manipulation.

Result: CaveAgent improves success rates by 10.5% on retail tasks, reduces token consumption by 28.4% in multi-turn scenarios, and by 59% on data-intensive tasks, handling large-scale data better than existing agents.

Conclusion: CaveAgent overcomes limitations of traditional agents by enabling persistent object management and efficient execution, making it effective for complex tasks across various LLMs and benchmarks.

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [31] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: A framework combining LLMs as ontology population engines and SWRL-based reasoners for rule-based reasoning over natural language, achieving improvements over few-shot prompting across legal, scientific, and clinical domains.


<details>
  <summary>Details</summary>
Motivation: Rule-based reasoning in domains like clinical protocols and legal evidence requires auditable decisions, but LLMs lack consistency guarantees while symbolic systems need structured input, creating a gap for integration.

Method: LLMs translate unstructured text into ABox assertions using OWL 2 ontologies, with reasoning decomposed into entity identification, assertion extraction, and symbolic verification via SWRL-based reasoners.

Result: Experiments in three domains with eleven models show statistically significant improvements over few-shot prompting, with symbolic verification adding substantial benefit beyond structured prompting alone.

Conclusion: The integration provides flexible interpretation with deterministic guarantees, enabling richer inference patterns and compatibility with semantic web tooling for inspection and querying.

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [32] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash is an open-source multimodal MoE model with 3.7B activated parameters, designed for enterprise tasks and general purposes, using a novel RL algorithm to reduce overthinking and achieving strong performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To enhance performance on enterprise-oriented tasks while maintaining competitiveness on general-purpose tasks, and to address the overthinking issue common in Large Reasoning Models.

Method: Proposes Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm to regulate overthinking behaviors in the model.

Result: Superior performance on enterprise tasks like RAG, complex table understanding, and summarization; strong reasoning in math and science with accuracy comparable to frontier models while using only 1/4 to 1/2 of average tokens.

Conclusion: Yuan3.0 Flash is fully open-sourced to support further research and real-world deployment, offering a cost-effective and efficient solution for multimodal AI applications.

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [33] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: Survey on AI agent architectures covering reasoning, planning, and tool use, with taxonomy, design trade-offs, evaluation challenges, and open issues.


<details>
  <summary>Details</summary>
Motivation: AI agents that integrate foundation models with capabilities like reasoning and tool use are emerging as key interfaces for translating natural-language intent into real-world computation, necessitating a synthesis of their architectures and practices.

Method: The survey organizes prior work into a unified taxonomy across agent components (e.g., policy/LLM core, memory, planners), orchestration patterns (single-agent vs. multi-agent), and deployment settings (offline vs. online).

Result: It synthesizes emerging landscapes in deliberation/reasoning (e.g., chain-of-thought, self-reflection), planning/control (reactive to hierarchical planners), and tool use (retrieval, APIs), discussing design trade-offs like latency vs. accuracy.

Conclusion: Highlights complexities in evaluation due to non-determinism and tool variability, summarizes benchmarking practices, and identifies open challenges including verification for tool actions, scalable memory, and reproducible evaluation under realistic workloads.

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [34] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: RTL-OPT is a new benchmark for evaluating how well large language models (LLMs) can optimize RTL code for hardware design, focusing on power, performance, and area (PPA) improvements, not just syntax.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based RTL generation benchmarks assess only syntactic correctness, lacking evaluation of real optimization quality like PPA metrics, which is crucial for efficient IC design in AI applications.

Method: Introduce RTL-OPT with 36 handcrafted digital designs covering combinational logic, pipelined datapaths, finite state machines, and memory interfaces, each providing suboptimal and human-optimized RTL code references. An automated framework verifies functional correctness and quantifies PPA improvements.

Result: The benchmark enables standardized assessment of LLMs in generating hardware-optimized RTL, supporting meaningful evaluation of generative models in real-world design optimization scenarios.

Conclusion: RTL-OPT addresses the gap in evaluating LLMs for RTL optimization, facilitating better tools for automated IC design driven by AI progress.

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [35] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: LLMs excel at symbolic manipulation but struggle with precision-critical iterative arithmetic for transcendental equations; hybrid approach combining LLMs with classical solvers is far more effective than direct prediction.


<details>
  <summary>Details</summary>
Motivation: Transcendental equations requiring iterative numerical solution are pervasive in engineering practice, and there is a need to determine whether LLMs can effectively solve these problems as standalone computational engines or benefit from integration with classical solvers.

Method: Systematic evaluation of six LLMs (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems across seven engineering domains, comparing direct numerical prediction against hybrid solver-assisted computation using Newton-Raphson iteration for numerical solution.

Result: Solver-assisted computation achieved mean relative errors of 0.225 to 0.301 (67.9% to 81.8% error reduction), compared to direct prediction errors of 0.765 to 1.262. Domain-specific analysis showed dramatic improvements in Electronics (93.1%) versus modest gains in Fluid Mechanics (7.2%).

Conclusion: Large Language Models are most effective when deployed as intelligent interfaces to classical numerical solvers rather than as standalone computational engines for transcendental equations.

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [36] [PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor](https://arxiv.org/abs/2601.01802)
*Qianjun Pan,Junyi Wang,Jie Zhou,Yutao Yang,Junsong Li,Kaiyin Xu,Yougen Zhou,Yihan Li,Jingyuan Zhao,Qin Chen,Ningning Zhou,Kai Chen,Liang He*

Main category: cs.AI

TL;DR: A multi-session, multi-therapy benchmark called PsychEval is introduced to train and evaluate realistic AI counselors, featuring a detailed dataset, diverse therapeutic modalities, and a holistic evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable AI for psychological assessment by addressing key challenges in training realistic AI counselors that require sustained memory, dynamic goal tracking, and flexibility across multiple therapies.

Method: Proposed a multi-session benchmark spanning 6-10 sessions across three stages, with extensive annotations of professional skills. Constructed a dataset covering five therapeutic modalities and an integrative therapy, and built over 2,000 diverse client profiles. Established an evaluation framework with 18 metrics across client-level and counselor-level dimensions.

Result: Extensive experimental analysis validated the superior quality and clinical fidelity of the dataset, enabling it to serve as a high-fidelity reinforcement learning environment for self-evolutionary training of AI counselors.

Conclusion: PsychEval effectively addresses the challenges of training multi-therapy AI counselors with realistic capabilities, providing a comprehensive benchmark for advancing AI in psychological assessment.

Abstract: To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.

</details>


### [37] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: This paper reframes AI alignment as admissibility alignment, focusing on admissible action and decision selection under uncertainty using a probabilistic, decision-theoretic framework. It introduces MAP-AI as an architecture for operationalizing this approach.


<details>
  <summary>Details</summary>
Motivation: Traditional AI alignment is often viewed as static or binary, lacking robust methods for evaluating alignment under uncertainty, uncertainty, intervention effects, value ambiguity, and governance constraints in real-world systems.

Method: Develops MAP-AI (Monte Carlo Alignment for Policy), a system architecture that enforces alignment via Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection, assessing policies across ensembles of plausible futures with metrics like expected utility, variance, tail risk, and misalignment probability.

Result: MAP-AI provides an executable methodology to evaluate trust and alignment in enterprise AI systems, distinguishing probabilistic prediction from decision reasoning under uncertainty, creating a practical foundation for governance based on policy behavior across distributions and tail events.

Conclusion: Admissibility alignment offers a more robust, probabilistic approach to AI governance, allowing distributional alignment evaluation to integrate into decision-making through an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without model retraining or modification.

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [38] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: COMPASS is a framework for evaluating LLM compliance with organizational policies, revealing models handle legitimate requests well but fail to enforce prohibitions robustly.


<details>
  <summary>Details</summary>
Motivation: Ensure LLMs adhere to organization-specific policies in high-stakes enterprise applications, as existing safety evaluations only cover universal harms.

Method: Developed COMPASS, applied it to eight industry scenarios, generated and validated 5,920 queries to test compliance and adversarial robustness via edge cases.

Result: Evaluated seven state-of-the-art models; they handle legitimate requests with >95% accuracy but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial violations.

Conclusion: Current LLMs lack robustness for policy-critical deployments; COMPASS is established as an essential evaluation framework for organizational AI safety.

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [39] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: An end-to-end framework using LLMs and multi-agent prompting to construct and evaluate clinical knowledge graphs from unstructured oncology texts, with built-in validation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for KG construction from clinical narratives lack robust validation of factuality and semantic consistency, which is critical in sensitive fields like oncology.

Method: The framework includes prompt-driven entity/attribute/relation extraction, entropy-based uncertainty scoring, ontology-aligned RDF/OWL schema generation, and multi-LLM consensus for hallucination detection and semantic refinement.

Result: Applied to PDAC and BRCA oncology cohorts, it produced interpretable, SPARQL-compatible, clinically grounded KGs without gold-standard annotations, showing gains in precision, relevance, and ontology compliance over baselines.

Conclusion: The framework enables continuous refinement and self-supervised evaluation, improving KG quality iteratively by addressing LLM limitations in clinical settings.

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [40] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: This paper introduces Jenius-Agent, an end-to-end LLM-based agent framework with innovations in adaptive prompting, tool orchestration, and memory mechanisms, showing improved task accuracy, reduced costs/latency, and real-world deployment.


<details>
  <summary>Details</summary>
Motivation: As LLM-powered agents advance, optimizing their internal reasoning and tool-use pipelines for better task performance in context understanding, tool usage, and response generation is critical, yet underexplored.

Method: The authors propose Jenius-Agent, a framework grounded in real-world experience, featuring: (1) adaptive prompt generation aligned with agent state and task goals, (2) context-aware tool orchestration with categorization, retrieval, and invocation, and (3) layered memory integrating session memory, task history, and external summaries. It includes optimizations like Model Context Protocol (MCP) tools, file I/O, and execution feedback.

Result: Experiments show a 20% improvement in task accuracy, along with reduced token cost, response latency, and invocation failures.

Conclusion: The framework provides a lightweight, scalable solution for robust, protocol-compatible autonomous agents, already deployed in Jenius (https://www.jenius.cn).

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [41] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: A framework using SQL queries to make AI pathology image analysis more interpretable and auditable by linking cellular features to diagnoses through verifiable evidence


<details>
  <summary>Details</summary>
Motivation: Clinicians lack clear understanding of which features drive AI model decisions in pathology, and current vision-language explanations often lack verifiable evidence

Method: Extract human-interpretable cellular features, then use Feature Reasoning Agents to compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings, followed by Knowledge Comparison Agent to evaluate findings against established pathological knowledge

Result: Extensive experiments on two pathology visual question answering datasets demonstrate improved interpretability and decision traceability with executable SQL traces

Conclusion: The SQL-centered agentic framework successfully enhances interpretability in pathology image analysis by providing auditable feature measurement and reasoning that mirrors clinical diagnostic justification

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [42] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: Socio-cognitive LLM benchmarks often fail to predict real-world behavior because they lack explicit theoretical grounding; the paper proposes 'Theory Trace Cards' to document theoretical assumptions and restore validity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the evaluation-deployment gap in LLM socio-cognitive benchmarks by identifying the lack of explicit theoretical specification of target capabilities as a core reason why high benchmark scores fail to predict real-world performance.

Method: The paper proposes the Theory Trace Card (TTC), a lightweight documentation artifact that explicitly outlines the theoretical basis, capability components, operationalization, and limitations of socio-cognitive evaluations for LLMs.

Result: The main contribution is a formal diagnosis of the 'theory gap' in LLM evaluations and the introduction of the Theory Trace Card (TTC) framework to restore theoretical grounding, making validity assumptions explicit without requiring consensus on a single theory.

Conclusion: The paper concludes that addressing the theory gap through tools like the Theory Trace Card is essential for restoring validity and interpretability in socio-cognitive evaluations of LLMs, enabling more accurate predictions of real-world competency from benchmark performance.

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [43] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A* is a multimodal framework combining vision-language models with an adaptive decay mechanism to improve path planning by integrating spatial grounding, reducing computational costs, and ensuring geometric validity.


<details>
  <summary>Details</summary>
Motivation: Classical A* is computationally expensive for large-scale path planning, and text-only LLM-based approaches lack spatial grounding, leading to incorrect waypoints in complex environments, highlighting the need for a perception-aware solution.

Method: MMP-A* integrates vision-language models for spatial grounding and a novel adaptive decay mechanism to dynamically regulate heuristic influence from uncertain waypoints, balancing high-level reasoning with geometric precision.

Result: MMP-A* achieves near-optimal trajectories with significantly reduced operational costs and memory overhead in challenging cluttered and topologically complex environments.

Conclusion: MMP-A* demonstrates potential as a computationally efficient and perception-grounded paradigm for autonomous navigation, overcoming limitations of text-only planners by ensuring geometric coherence and adaptability.

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [44] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt is an open-source simulator and modular architecture for multi-modal social interaction training, demonstrated via social navigation experiments.


<details>
  <summary>Details</summary>
Motivation: To provide a tool for simulating and training social agents in multi-modal interactions, addressing gaps in open-source software for social AI research.

Method: Developed an open-source software package with a simulator and modular architecture, tested through an experimental protocol based on social navigation tasks.

Result: The framework enables exploration of perceptual features, encoding, fusion, and various agents; software is publicly available under GPL.

Conclusion: OpenSocInt offers a valuable resource for advancing research in social AI by facilitating the study and training of agents in complex social scenarios.

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [45] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: A state-of-the-art review of Formal Concept Analysis (FCA)-based classifiers, introducing a novel method for constructing a partial concept lattice to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Knowledge Discovery in Databases aims to extract hidden knowledge, with classification as a core technique; FCA is recognized for interpretable learning, prompting a review and enhancement of FCA-based classifiers.

Method: Conducts a review of existing methods for computing closure operators from nominal data and proposes a novel approach for constructing a partial concept lattice focusing on relevant concepts, with experiments to validate efficiency.

Result: Experimental results demonstrate the efficiency of the proposed method, showing improved performance in classifier design through partial concept lattices.

Conclusion: A novel partial concept lattice method enhances FCA-based classifiers, contributing to efficient and interpretable knowledge discovery in databases.

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [46] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic is a benchmark challenging LLMs with first-order logic reasoning on 30 chaotic dynamical systems through 621 questions across 7 categories, revealing high per-item accuracy but flaws in compositional reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs are brittle in precise logical and symbolic reasoning, and chaotic dynamical systems provide a demanding test due to deterministic chaos being misinterpreted as randomness or complexity.

Method: Create ChaosBench-Logic with a unified FOL ontology, annotating systems with truth assignments for 11 semantic predicates, generating questions in 7 reasoning categories, and using metrics like logical accuracy and dialogue coherence; experiments involve models like GPT-4 and LLaMA-3 via an open-source pipeline.

Result: Frontier LLMs achieve 91-94% per-item accuracy but 0% on compositional items and fragile global coherence; dialogue accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot).

Conclusion: ChaosBench-Logic serves as a rigorous diagnostic tool for LLM failures in logical reasoning and supports developing neuro-symbolic approaches to enhance scientific reasoning in LLMs.

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [47] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat is a privacy-preserving LLM for mental health support, trained on MindCorpus, a synthetic counseling dataset created via multi-agent role-playing, and fine-tuned with federated learning and differential privacy to enhance quality and reduce privacy risks.


<details>
  <summary>Details</summary>
Motivation: Training LLMs for mental health support is limited by the scarcity and sensitivity of real counseling dialogues, creating a need for high-quality synthetic data and privacy-preserving methods.

Method: Developed a multi-agent role-playing framework with dual closed-loop feedback (turn-level critique-and-revision, session-level strategy refinement) to construct MindCorpus, and fine-tuned the base model using federated learning with LoRA adapters and differentially private optimization.

Result: MindCorpus improves training effectiveness; MindChat competes with existing LLM baselines in automatic and human evaluations for counseling capability, while showing reduced privacy leakage under membership inference attacks.

Conclusion: MindChat offers a viable privacy-preserving solution for mental health support by leveraging synthetic data and advanced training techniques, addressing data constraints and privacy concerns in counseling applications.

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [48] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD is an explainable medical AI framework that integrates clinical expertise into deep learning via neuro-symbolic architecture to improve robustness, rare class sensitivity, and transparency in medical tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in medical AI, such as generalization failure under real-world distribution shifts and bias against rare clinical conditions, by creating a more robust, transparent, and clinically aligned model.

Method: The framework encodes clinical expertise as logical connectives over atomic medical propositions, converting them into machine-checkable rules, uses weighted feature satisfaction scores for symbolic reasoning, and employs a confidence-weighted fusion and an adaptive routing mechanism (guided by Entropy Imbalance Gain and Rare Class Gini) to integrate symbolic and neural outputs.

Result: XAIMeD achieves significant performance improvements across diverse modalities, including 6% gains in cross-domain generalization and a 10% improvement in rare class F1 score, outperforming state-of-the-art deep learning baselines, as validated in tasks like Seizure Onset Zone localization and Diabetic Retinopathy grading.

Conclusion: XAIMeD provides a principled, clinically faithful, and interpretable approach to multimodal medical AI, with symbolic components acting as effective regularizers for robustness to distribution shifts, offering a scalable solution to real-world medical challenges.

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [49] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: The paper discusses how foundational models challenge traditional views of symbolic reasoning by enabling new forms of imitation-based reasoning, leading to philosophical reinterpretations, the need to abandon the 'stochastic parrot' metaphor, and considerations for safety and appropriateness.


<details>
  <summary>Details</summary>
Motivation: To examine how foundational models (FMs) deviate from symbolic reasoning by imitating processes, leading to reasoning without grounding or common sense, which impacts assessments of reasoning and safety against brittleness.

Method: The paper offers and discusses several philosophical interpretations, argues against the 'stochastic parrot' metaphor, and reflects on normative elements in safety and appropriateness considerations from these models.

Result: FMs demonstrate reasoning through imitation, solving problems with few-shot learning but with brittleness due to lack of grounding, calling for new philosophical interpretations and safety approaches.

Conclusion: The insights from FMs significantly alter the understanding of reasoning and its necessary conditions, requiring the abandonment of outdated metaphors and careful normative considerations for safety and appropriateness in their use.

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [50] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: Higher-order action regularization, especially jerk minimization, smooths RL policies, reducing erratic control in energy-critical applications like HVAC systems.


<details>
  <summary>Details</summary>
Motivation: Deep RL agents exhibit high-frequency control behaviors that increase energy use and mechanical wear in real-world deployments.

Method: Systematic investigation of action smoothness with higher-order derivative penalties, tested in continuous control benchmarks and building energy management.

Result: Third-order penalties consistently achieve superior smoothness while maintaining performance, reducing HVAC equipment switching by 60%.

Conclusion: Higher-order action regularization effectively bridges RL optimization with operational constraints in energy-critical domains.

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [51] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: This paper applies fine-tuned large language models (LLMs) to recommend excipients and predict filament properties in pharmaceutical 3D printing, highlighting challenges like catastrophic forgetting and the need for metrics beyond linguistic performance.


<details>
  <summary>Details</summary>
Motivation: AI can enhance pharmaceutical 3D printing, but existing approaches are narrow and don't address formulation challenges. Artificial general intelligence concepts, like human-like reasoning in LLMs, offer potential for broad application in this field.

Method: Four LLM architectures were fine-tuned on a fused deposition modelling (FDM) dataset of over 1400 formulations. Systematic evaluation included fine-tuning and generative parameter configurations to recommend excipients based on API dose and predict filament mechanical properties.

Result: Fine-tuned Llama2 performed best for excipient recommendations in FDM formulations. Smaller LLMs showed catastrophic forgetting, even with a small dataset. Standard metrics fail to assess formulation processability, and biomedical LLMs don't always yield optimal results.

Conclusion: Overcoming challenges like catastrophic forgetting and developing non-linguistic evaluation metrics is crucial to advance LLMs from linguistic proficiency to reliable tools for pharmaceutical formulation development.

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [52] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: EverMemOS is a self-organizing memory system for LLMs that mimics human memory processes to improve long-term interaction coherence.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with extended interactions due to limited context windows, and existing memory systems fail to handle evolving user states and conflicts effectively.

Method: EverMemOS implements a lifecycle inspired by engrams. It includes episodic trace formation (converting dialogues into MemCells), semantic consolidation (organizing MemCells into thematic MemScenes and updating user profiles), and reconstructive recollection (retrieving context for reasoning).

Result: Experiments on datasets like LoCoMo and LongMemEval show state-of-the-art performance in memory-augmented reasoning tasks.

Conclusion: EverMemOS effectively enhances LLM interaction coherence by integrating episodic and semantic memory, with code available for further use and study.

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [53] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: Proposed cumulative prefix-level hallucination signal tracks evolving reasoning states for real-time streaming detection in long chain-of-thought processes.


<details>
  <summary>Details</summary>
Motivation: Current long chain-of-thought reasoning improves LLM performance, but hallucinations emerge subtly and propagate across steps, suggesting that hallucination should be understood as an evolving latent state rather than a one-off event.

Method: The authors treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal to track the global evolution of reasoning states.

Result: The proposed approach enables streaming hallucination detection in long CoT reasoning, providing real-time and interpretable evidence.

Conclusion: The paper concludes that their approach treats hallucination as an evolving latent state and enables streaming, real-time detection with interpretable evidence.

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [54] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: A novel framework, Project Ariadne, uses causal interventions with SCMs to audit the faithfulness of LLM reasoning traces, revealing widespread causal decoupling failures.


<details>
  <summary>Details</summary>
Motivation: The transparency and faithfulness of LLM reasoning processes are critical for safety in high-stakes decisions, as current methods like CoT may produce unfaithful or post-hoc rationalizations rather than genuine causal drivers.

Method: Introduces Project Ariadne framework using Structural Causal Models and hard interventions (do-calculus) on reasoning nodes to measure Causal Sensitivity (φ) and detect Causal Decoupling.

Result: Empirical evaluation reveals a persistent Faithfulness Gap and violations with density up to 0.77 in factual/scientific domains; agents show identical conclusions despite contradictory logic, indicating 'Reasoning Theater' driven by latent priors.

Conclusion: Current agentic architectures are inherently prone to unfaithful explanations; Ariadne Score is proposed as a benchmark for aligning logic with actions to improve safety.

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [55] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: A 7B-parameter reasoning-optimized model that outperforms much larger competitors on reasoning tasks through specialized training and architecture, proving small models can be highly capable.


<details>
  <summary>Details</summary>
Motivation: To establish the feasibility of achieving competitive reasoning performance with small language models (SLMs) and create a parameter-efficient alternative to larger state-of-the-art reasoning models.

Method: The model is a 7B-parameter reasoning-optimized language model developed through careful data curation and targeted training strategies (efficient SFT and RL scaling). It incorporates a hybrid-parallel architecture for faster inference and supports chain-of-thought generation and parallel test-time scaling via the DeepConf approach.

Result: Falcon-H1R consistently matches or outperforms SOTA reasoning models that are 2× to 7× larger across various reasoning benchmarks. It achieves faster inference through its hybrid-parallel architecture, offers token efficiency and higher accuracy, and demonstrates state-of-the-art test-time scaling efficiency with improvements in both accuracy and computational cost.

Conclusion: The Falcon-H1R-7B model demonstrates that compact language models can achieve competitive and scalable reasoning performance through meticulous training strategies and architectural design, challenging the paradigm that larger models are necessary for advanced reasoning.

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex is a novel asynchronous multi-agent LLM framework that decouples agent logic from physical memory, using Singleton Weight Sharing and Topological Synapse to reduce memory complexity, enabling up to 100 concurrent agents on consumer hardware with theoretical scalability to 1,000 agents.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM frameworks have linear memory scaling limitations, making parallel reasoning impractical on consumer hardware.

Method: The approach includes Singleton Weight Sharing, Topological Synapse inspired by TDA, KV-cache as a point cloud with witness-complex sparsification, and Referential Injection for non-intrusive KV-cache updates.

Result: On an NVIDIA RTX 4090, the system achieves 100 concurrent agents at 2.2 GB VRAM with theoretical capacity for over 1,000 agents before compute latency becomes the bottleneck.

Conclusion: Warp Cortex significantly enhances multi-agent scalability and efficiency, bridging theory and practical deployment for complex reasoning tasks.

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [57] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: Horizon reduction in offline RL can cause fundamental information loss, making optimal policies indistinguishable from suboptimal ones even with perfect conditions, due to structural failures like indistinguishability and misspecification.


<details>
  <summary>Details</summary>
Motivation: Despite empirical benefits of horizon reduction for scaling in offline RL, its theoretical implications are underdeveloped, leading to a need to analyze potential irrecoverable information loss.

Method: Formalize horizon reduction as learning from fixed-length trajectory segments and prove optimal policies may be statistically indistinguishable; use minimal counterexample MDPs to identify failure modes.

Result: Identified three structural failure modes: prefix indistinguishability, objective misspecification from truncated returns, and dataset support and representation aliasing, establishing necessary conditions for safe horizon reduction.

Conclusion: Horizon reduction in offline RL has intrinsic limitations that cannot be overcome by algorithmic improvements alone, highlighting the need for careful design to avoid information loss and complementing other offline RL approaches.

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [58] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: The paper presents a method to enhance voting rules using graph neural networks and adversarial training to improve resilience and social welfare, addressing strategic voting and evaluating on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing voting rules struggle with universal applicability and robustness to strategic voting, while automated mechanism design shows promise but faces real-world implementation challenges.

Method: Generalize expressive capability of learned voting rules by representing elections as bipartite graphs, using graph neural networks, and combining neural network architecture improvements with adversarial training.

Result: The method resolves limitations of prior work on learning voting rules by bipartite graph representation and GNNs, with effectiveness evaluated on synthetic and real-world datasets.

Conclusion: This approach opens new frontiers for applying machine learning to real-world elections by enhancing voting rule resilience and social welfare.

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [59] [ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI](https://arxiv.org/abs/2601.00832)
*Israk Hasan Jone,D. M. Rafiun Bin Masud,Promit Sarker,Sayed Fuad Al Labib,Nazmul Islam,Farhad Billah*

Main category: cs.LG

TL;DR: This paper proposes a deep learning-based automated shrimp disease classification system using six pretrained models, with ConvNeXt-Tiny achieving 96.88% accuracy, enhanced by preprocessing, adversarial training, and explainability methods.


<details>
  <summary>Details</summary>
Motivation: Shrimp farming is economically vital but faces disease outbreaks, threatening sustainability; automated disease classification can enable timely and accurate detection to address this challenge.

Method: The research uses 1,149 images across four disease classes, preprocesses them by background removal and Keras pipeline, employs six pretrained deep learning models (ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, Xception), applies adversarial training with FGSM, uses augmentation strategies like CutMix and MixUp, and implements post-hoc explanation methods (Grad-CAM, Grad-CAM++, XGrad-CAM) for interpretability.

Result: ConvNeXt-Tiny achieved the highest performance with 96.88% accuracy on the test dataset, and after 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

Conclusion: The deep learning approach effectively automates shrimp disease classification, with ConvNeXt-Tiny demonstrating robust accuracy, suggesting practical applicability for sustainable shrimp production through enhanced detection.

Abstract: Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

</details>


### [60] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: A mesh-free geometric deep learning framework (IM-PINN) solves reaction-diffusion PDEs on complex manifolds by embedding Riemannian metrics, achieving improved accuracy and mass conservation compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Simulating nonlinear reaction-diffusion on complex manifolds is challenging due to high mesh generation costs and symplectic drift in discrete schemes; current methods like adaptive refinement struggle with extreme curvature fluctuations.

Method: Develops the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN) that embeds the Riemannian metric tensor into automatic differentiation to analytically reconstruct the Laplace-Beltrami operator, using a dual-stream architecture with Fourier feature embeddings.

Result: IM-PINN recovers pattern regimes (e.g., splitting spot, labyrinthine) on a high-curvature manifold, with global mass conservation error of ~0.157 vs. SFEM's 0.258, eliminating mass drift and showing superior physical consistency.

Conclusion: IM-PINN provides a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [61] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: A case study on controlling retrieval-augmented generation (RAG) per query to meet service-level objectives (SLOs), evaluating simple policy-learning methods without new models.


<details>
  <summary>Details</summary>
Motivation: RAG pipelines need per-query control of retrieval depth and generation mode to optimize cost, refusal rate, and hallucination risk as specified by SLOs.

Method: Model per-query control as small discrete actions, construct offline dataset from SQuAD 2.0, evaluate two policy-learning objectives: Argmax-CE and Argmax-CE-WT.

Result: Fixed baseline (low k, guarded prompting) performs well; learned policies offer cost savings under quality-focused SLO but show refusal collapse under cheap SLO when refusal is over-rewarded.

Conclusion: This reproducible case study highlights failure modes and reporting conventions for SLO-aware RAG control, emphasizing practical insights over novel model proposals.

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [62] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: A method to enhance planning with JEPA world models by shaping the representation space to approximate negative goal-conditioned value functions with distances between state embeddings, improving planning on control tasks.


<details>
  <summary>Details</summary>
Motivation: Joint-Embedded Predictive Architectures are promising for capturing environmental dynamics but have limited effectiveness in supporting action planning.

Method: Introduce a constraint during training to shape the JEPA representation space so that the negative goal-conditioned value function for a reaching cost is approximated by a distance between state embeddings, and demonstrate this with a practical training method.

Result: The approach leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

Conclusion: Shaping the representation space in JEPA models to align with value functions enhances their planning capabilities, making them more effective for action planning in dynamic environments.

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [63] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: MFEE introduces a control-plane architecture to selectively invoke transformer inference only when required, achieving high execution reduction with exact-match equivalence.


<details>
  <summary>Details</summary>
Motivation: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. There is inefficiency in executing transformers when not necessary.

Method: Meaning-First Execution (MFEE) operates as a gating layer above existing stacks without modifying models, weights, or parameters, using semantic analysis to determine when execution is needed.

Result: MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence. Pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures.

Conclusion: Execution governance is established as a foundational layer in ML systems infrastructure, and Theorem 1 shows limitations of feature-map-only routers.

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [64] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: EdgeJury improves QA truthfulness using small models via ensemble stages and reduces hallucinations and costs.


<details>
  <summary>Details</summary>
Motivation: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical.

Method: EdgeJury is a lightweight ensemble framework comprising four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis integrating strong content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement.

Result: On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines. On adversarial EdgeCases set, it yields +48.2% relative gains. Manual analysis shows ~55% reduction in factual hallucination errors. Deployed on Cloudflare Workers AI, it achieves 8.4 s median end-to-end latency.

Conclusion: EdgeJury demonstrates that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs, achieving practical gains in accuracy and reduced hallucinations with manageable latency.

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [65] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: A novel federated learning algorithm called FedSCAM is introduced, which adapts Sharpness-Aware Minimization to address client heterogeneity in non-IID data settings by dynamically adjusting perturbation radii and aggregation weights based on client-specific metrics, improving convergence and generalization.


<details>
  <summary>Details</summary>
Motivation: Statistical heterogeneity in federated learning, particularly non-IID label distributions, can hinder convergence and generalization. Existing SAM-based methods use a uniform perturbation radius across clients, overlooking client-specific diversity, motivating a more adaptive approach.

Method: FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation) dynamically adjusts the SAM perturbation radius and aggregation weights per client based on a calculated heterogeneity score, modulating the radius inversely to the score and prioritizing updates aligned with global optimization through a weighted aggregation mechanism.

Result: Experiments on CIFAR-10 and Fashion-MNIST with Dirichlet-based label skew show that FedSCAM achieves competitive performance in convergence speed and final test accuracy compared to state-of-the-art baselines like FedSAM and FedLESAM.

Conclusion: FedSCAM effectively mitigates the challenges of client heterogeneity in federated learning by leveraging adaptive sharpness-aware minimization, leading to improved robustness and performance in non-IID scenarios.

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [66] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: This study evaluates AlphaEarth Foundation (AEF) embeddings in agricultural tasks, finding they perform competitively with traditional models but have limitations in transferability and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address gaps in applying geospatial foundation models (GFMs) like AlphaEarth Foundation (AEF) to agricultural monitoring, specifically by evaluating their performance in critical agricultural downstream tasks and comparing them with traditional remote sensing-based models under various scenarios.

Method: The study evaluates AEF embeddings in three U.S. agricultural downstream tasks (crop yield prediction, tillage mapping, cover crop mapping) using datasets from public and private sources, comparing them with traditional remote sensing-based models trained at different scales and locations.

Result: AEF-based models generally perform well across all tasks, matching purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data, but show limitations such as poorer spatial transferability, low interpretability, and limited time sensitivity compared to RS-based models.

Conclusion: While AEF-based models show strong performance in agricultural tasks like yield prediction and tillage mapping with local data, they have limitations in spatial transferability, interpretability, and time sensitivity, advising cautious application in agriculture where these factors are crucial.

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [67] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: The paper proves that intelligence in mechanical systems requires dissipative dynamics, not conservation laws, to generate coherent text.


<details>
  <summary>Details</summary>
Motivation: To investigate whether purely mechanical systems can generate intelligent language by analyzing quantum dynamics and conservation laws.

Method: Employs Koopman operators with closed-form path integral propagators and spectral analysis to separate eigenvalue structures into decay, growth, and neutral modes.

Result: Dissipative quantum dynamics enable coherent text generation, while Hamiltonian constraints degrade performance by eliminating dissipative modes.

Conclusion: Language generation is established as a dissipative quantum field theory, showing intelligence arises from dissipation and non-locality, not conservation.

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [68] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: A unified capacity forecasting framework using a Time-Series Foundation Model with LoRA and physics-guided contrastive learning achieves robust performance across diverse battery chemistries and conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate battery capacity fade forecasting is crucial for energy storage systems, but heterogeneity across cell chemistries, form factors, and operating conditions makes generalization difficult with single models.

Method: Curate 20 public aging datasets into a large-scale corpus, adopt a Time-Series Foundation Model backbone, apply parameter-efficient Low-Rank Adaptation (LoRA), and use physics-guided contrastive representation learning to capture shared degradation patterns.

Result: The unified model achieves competitive or superior accuracy compared to per-dataset baselines on seen and unseen datasets, with stable performance on excluded chemistries, capacity scales, and operating conditions.

Conclusion: TSFM-based architectures offer a scalable and transferable solution for capacity degradation forecasting in real battery management systems, demonstrating potential for broad application.

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [69] [Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery](https://arxiv.org/abs/2601.00863)
*Markus J. Buehler*

Main category: cs.LG

TL;DR: A generative framework linking material structures to music, using vibrations and constraints to bridge science and art, with AI-generated music showing human-like patterns.


<details>
  <summary>Details</summary>
Motivation: To explore connections between hierarchical structures in matter and compositional logic in music, aiming to show how vibration acts as a shared grammar across scales and how constraints drive novelty in both science and art.

Method: Uses reversible mappings from molecular spectra to musical tones and 3D networks to playable instruments, analyzes 2^12 musical scales quantitatively, and applies swarm-based AI models to compose music with structural signatures.

Result: Culturally significant musical systems cluster in mid-entropy, mid-defect corridors parallel to material strength optima; AI-generated music exhibits human-like signatures like small-world connectivity and modular integration, suggesting paths to innovation beyond interpolation.

Conclusion: Science and art are generative acts of world-building under constraint, with vibration organizing structure across scales, and this framework enables productive collisions between creativity and physics, generating new information through iterative mappings.

Abstract: We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.

</details>


### [70] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: Extends structural importance sampling to KDEy for quantification learning on graphs, improving performance under structural shifts.


<details>
  <summary>Details</summary>
Motivation: Graph-based quantification learning is important for predicting label distributions (e.g., political preferences in social networks), but standard methods fail under structural shifts where training and test data come from different graph regions.

Method: Develops a structural importance sampling variant of the state-of-the-art KDEy quantification approach, adapting it to handle structural shifts in graph data by ensuring proper weight adjustments.

Result: The proposed method effectively adapts to structural shifts and outperforms standard quantification approaches in experiments, demonstrating better accuracy for label prevalence estimation.

Conclusion: The extension of structural importance sampling to KDEy provides a robust solution for quantification learning on graphs with structural shifts, enhancing predictive capabilities for real-world applications like social network analysis.

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [71] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: A modified A-PINN with adaptive optimizers improves structural vibration analysis by enhancing stability and accuracy, outperforming baselines by 40%.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and stability of scientific machine learning models for analyzing structural vibration problems, addressing limitations in existing methods.

Method: A modified A-PINN framework with balanced adaptive optimizers is applied to solve the Euler-Bernoulli beam equations through numerical simulations.

Result: The model shows enhanced numerical stability and predictive accuracy in approximating the beam equations across various scenarios.

Conclusion: The A-PINN model demonstrates significant improvement over baselines, with at least 40% better performance, making it effective for solving vibration problems.

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [72] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow is an AI-driven framework combining RL and Agentic AI for dynamic bike rebalancing, reducing imbalance by >95% with minimal travel.


<details>
  <summary>Details</summary>
Motivation: Urban bike-sharing faces dynamic rebalancing challenges; manual efforts are inefficient, requiring automated, scalable solutions to improve availability and reduce costs.

Method: Multi-layered framework: strategic DQN agent in simulation for policies via MDP, tactical module for optimised journeys/dispatches, communication layer with LLM for human-readable instructions.

Result: Evaluation shows >95% reduction in network imbalance, minimal travel distance, strong truck utilisation, improved bike availability, and lower operational costs.

Conclusion: SmartFlow bridges machine intelligence and human operations, providing interpretable, scalable AI logistics for urban mobility, reducing idle time and costs.

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [73] [Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems](https://arxiv.org/abs/2601.00873)
*Osasumwen Cedric Ogiesoba-Eguakun,Suman Rath*

Main category: cs.LG

TL;DR: Quantum ML methods are explored for detecting coordinated stealth attacks in microgrids; a hybrid quantum-classical model using quantum feature embeddings with classical SVM outperforms fully quantum and classical baselines.


<details>
  <summary>Details</summary>
Motivation: Coordinated stealth attacks pose a serious cybersecurity threat to distributed generation systems by modifying control and measurement signals while mimicking normal behavior, making them hard to detect with standard intrusion detection methods.

Method: The study employed classical ML baselines (e.g., SVM), fully quantum variational classifiers, and hybrid quantum-classical models. A balanced binary classification dataset was created using simulated measurements of three features: reactive power at DG1, frequency deviation relative to nominal value, and terminal voltage magnitude.

Result: The hybrid quantum-classical model achieved the best overall performance on this low-dimensional dataset, with modest improvements in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models performed worse due to training instability and current NISQ hardware limitations.

Conclusion: The hybrid quantum-classical model (quantum feature embeddings with classical RBF SVM) offers the most practical and effective approach for detecting coordinated stealth attacks in this context, outperforming classical and fully quantum methods, with quantum feature mapping showing promise for enhancing intrusion detection despite current hardware limitations.

Abstract: Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.

</details>


### [74] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize is an open-source Python framework using LLMs for black-box optimization via iterative prompting, supporting strategies like OPRO and hybrid methods, with natural language constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong reasoning capabilities beyond language tasks, motivating their use for numerical optimization to handle complex, domain-specific problems with hard-to-formalize constraints.

Method: LLMize formulates optimization as a black-box process: candidates in natural language, evaluated by an external objective function, refined over iterations with feedback; supports OPRO and hybrid methods like evolutionary algorithms and simulated annealing.

Result: Evaluated on convex, linear programming, TSP, hyperparameter tuning, and nuclear fuel lattice optimization; LLM-based optimization is practical for complex tasks but not competitive with classical solvers for simple problems.

Conclusion: LLMize provides an accessible approach for optimization where constraints and heuristics are difficult to formalize, leveraging LLMs' natural language handling for domain-specific applications.

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [75] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD uses neuro-symbolic approach to predict Alzheimer's from MRI data, producing interpretable rules with comparable accuracy to less interpretable models.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable method for Alzheimer's disease prediction from brain MRI data, enhancing transparency in clinical neuroscience applications.

Method: Applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, then uses FastLAS to learn global rules.

Result: Outperforms Decision Trees, matches SVM accuracy, and performs slightly below Random Forests and GNNs, while remaining fully interpretable; ablation shows improved interpretability with comparable performance.

Conclusion: LearnAD demonstrates the feasibility and benefits of neuro-symbolic methods for interpretable predictions, deepening understanding of GNN behavior in clinical neuroscience.

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [76] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: A new outlier detection method for multi-dimensional data uses vector cosine similarity with a modified dataset to identify abnormal points.


<details>
  <summary>Details</summary>
Motivation: Current outlier detection techniques may not effectively handle multi-dimensional data, prompting the need for a method that leverages geometric properties like cosine similarity.

Method: Construct a new dataset by adding a dimension with zero values, create observation and measured points, form vectors, and compare cosine similarities to detect outliers.

Result: The method successfully identifies outliers through vector comparisons, with an optimized implementation (MDOD) released on PyPI.

Conclusion: The proposed approach offers an effective outlier detection solution for multi-dimensional data, utilizing cosine similarity and dataset modification, made available as a practical tool.

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [77] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: FANoS is a physics-inspired optimization algorithm motivated by molecular dynamics, which combines momentum updates with a thermostat for friction adaptation. While it offers interpretability and improvements on certain problems, like Rosenbrock-100D, it falls short against established methods like AdamW with clipping and L-BFGS on other tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage structure-preserving integration and thermostat ideas from molecular dynamics as a heuristic for optimization, aiming to handle stiff nonconvex landscapes better than standard optimizers.

Method: FANoS integrates a second-order momentum update, a Nosé–Hoover-like thermostat adjusting friction via kinetic-energy feedback, and a semi-implicit symplectic-Euler integration, optionally with an RMS preconditioner.

Result: On Rosenbrock-100D with 3000 gradient evaluations, FANoS-RMS achieved a final objective of 1.74×10⁻², outperforming unclipped AdamW and SGD+momentum, but underperforming AdamW with clipping and L-BFGS; on convex quadratics and PINNs, it showed instability and variance.

Conclusion: FANoS is an interpretable blend of known techniques useful in specific stiff settings, but not generally superior to modern baselines, with sensitivity to hyperparameters limiting its widespread applicability.

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [78] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: A hierarchical topological clustering algorithm is proposed for flexible outlier and cluster detection without structural assumptions.


<details>
  <summary>Details</summary>
Motivation: Topological methods offer potential for exploring data without structural assumptions, addressing scenarios where outliers matter.

Method: Hierarchical topological clustering algorithm implementable with any distance metric.

Result: The algorithm demonstrates effectiveness on datasets (images, medical, economic data) where outliers play key roles.

Conclusion: The method provides meaningful clusters where other techniques fail, showcasing its versatility and robustness.

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [79] [When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training](https://arxiv.org/abs/2601.00894)
*Gihyeon Sim*

Main category: cs.LG

TL;DR: PonderTTT is a gating strategy that uses self-supervised reconstruction loss to selectively trigger Test-Time Training updates, achieving high efficiency without training.


<details>
  <summary>Details</summary>
Motivation: Large language models apply uniform computation to all inputs, regardless of difficulty, leading to inefficiency. PonderTTT addresses this by enabling adaptive computation based on input complexity.

Method: The method involves using the TTT layer's self-supervised reconstruction loss as a gating signal. It requires no learned classifier or auxiliary networks; only a scalar threshold is calibrated on unlabeled data and adapted via exponential moving average to maintain target update rates.

Result: Experiments with GPT-2 models on code language modeling show that Reconstruction Gating achieves 82-89% Oracle Recovery, outperforming Random Skip baselines by up to 16% lower loss on out-of-distribution languages, while being fully training-free and inference-compatible.

Conclusion: PonderTTT effectively reduces computational waste by selectively applying Test-Time Training, offering a practical, training-free solution for improving efficiency in language models without requiring ground-truth labels.

Abstract: Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).

</details>


### [80] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: DIPOLE is a reinforcement learning algorithm for diffusion-based policies that decomposes optimal policy into reward-maximizing and reward-minimizing components, enabling stable training and controllable action generation in inferenc.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies offer expressiveness and controllability, but training them with RL is unstable due to direct value maximization or computationally intensive due to Gaussian approximations requiring many denoising steps.

Method: Introduces DIPOLE with a greedified policy regularization scheme from the KL-regularized RL objective, learning dichotomous policies (one for reward maximization, one for minimization) and combining their scores linearly during inference.

Result: Evaluations on ExORL and OGBench show effectiveness in offline and offline-to-online RL; DIPOLE trains a vision-language-action model for autonomous driving, performing well on the NAVSIM benchmark.

Conclusion: DIPOLE enables stable and controllable diffusion policy optimization, with proven performance in RL tasks and potential for complex real-world applications like autonomous driving.

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [81] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: Paper analyzes degradation of conformal prediction guarantees under distribution shift using COVID-19 as natural experiment, finding catastrophic failures correlate with single-feature dependence and providing decision framework based on SHAP analysis.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction guarantees degrade under distribution shift, but the variance and mechanisms behind this degradation are not well-understood. Motivation is to empirically examine this using COVID-19 as a natural experiment to quantify and explain coverage drops.

Method: Study uses COVID-19 as natural experiment across 8 supply chain tasks, analyzing coverage drops under identical severe feature turnover (Jaccard ≈ 0). Method incorporates SHAP analysis to correlate catastrophic failures with feature dependence, and tests quarterly retraining on catastrophic vs. robust tasks. Exploratory analysis extends to 4 additional tasks with moderate feature stability.

Result: Coverage drops vary from 0% to 86.7% across tasks under severe shift. Catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Quarterly retraining restores catastrophic task coverage by +19 pp but provides no benefit for robust tasks. Feature stability, not concentration, determines robustness in moderate shifts.

Conclusion: Catastrophic failures under severe distribution shift link to feature concentration, suggesting concentration effects apply specifically to such shifts. Decision framework recommends monitoring SHAP concentration pre-deployment and retraining quarterly if vulnerable, otherwise skipping retraining for robust tasks.

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [82] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: The paper introduces a latent-constrained conditional variational autoencoder (LC-CVAE) to generate additional climate realizations from limited ensemble data, improving cross-realization generalization.


<details>
  <summary>Details</summary>
Motivation: Large climate-model ensembles are computationally costly, but many downstream analyses need more statistically consistent spatiotemporal climate realizations from a limited set.

Method: Proposes LC-CVAE that enforces latent embedding homogeneity at shared geographic anchors and uses multi-output Gaussian process regression to predict latent coordinates for new realizations.

Result: LC-CVAE outperforms vanilla CVAE, with training instability on single realizations, diminishing returns after about five realizations, and a trade-off between spatial coverage and reconstruction quality.

Conclusion: The LC-CVAE method effectively generates new climate realizations, enhancing generative modeling for climate ensembles while balancing generalization and computational efficiency.

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [83] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: Lazy Attention is a new attention mechanism that addresses representational collapse and attention sink by tackling attention overload and underload, showing competitive performance and high sparsity.


<details>
  <summary>Details</summary>
Motivation: Standard attention in Transformers suffers from representational collapse and attention sink issues, but prior solutions are isolated. This paper unifies them as stemming from improper attention allocation: overload (tokens get similar high weights, blurring features) and underload (attention forced on irrelevant tokens, causing spurious focus like sinks).

Method: Proposes Lazy Attention with two components: 1) For overload, uses positional discrimination across heads and dimensions to sharpen token distinctions. 2) For underload, uses Elastic-Softmax, a modified normalization that relaxes softmax constraints to suppress attention on irrelevant tokens.

Result: Tests on FineWeb-Edu corpus across nine diverse benchmarks show Lazy Attention mitigates attention sink, achieves competitive performance compared to standard attention and modern architectures, and reaches up to 59.58% attention sparsity.

Conclusion: Lazy Attention provides a unified solution for attention allocation issues (overload and underload) in Transformers, effectively reducing representational collapse and attention sink while maintaining performance and enabling high sparsity, suggesting improvements for LLM design.

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [84] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: MODE, a unified framework for time series prediction, combines Low-Rank Neural ODEs with Enhanced Mamba to improve efficiency and accuracy in handling long-range dependencies and irregular data.


<details>
  <summary>Details</summary>
Motivation: Existing approaches suffer in balancing efficiency, scalability, and accuracy, especially with long-range dependencies and irregularly sampled data across domains like finance and healthcare.

Method: The framework uses a Linear Tokenization Layer and Mamba Encoder blocks with Enhanced Mamba Layers, incorporating Causal Convolution, SiLU activation, and Low-Rank Neural ODE enhancement, plus a segmented selective scanning mechanism for adaptive subsequence focus.

Result: Experiments show MODE outperforms baselines in predictive accuracy and computational efficiency on benchmark datasets.

Conclusion: The paper introduces an efficient architecture for long-term modeling, integrates Mamba with low-rank Neural ODEs for temporal representation, and achieves gains in efficiency and scalability through low-rank approximation and dynamic scanning.

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [85] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: The paper evaluates predictive models for skeletal muscle outcomes in COPD using biomarkers, comparing classical, geometric, and quantum kernel methods, with quantum kernel models showing improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Skeletal muscle dysfunction in COPD is linked to inflammation, requiring predictive models from minimally invasive biomarkers for longitudinal monitoring, but limited data and features pose challenges.

Method: Use classical baselines, geometry-aware symmetric positive definite descriptors with Stein divergence, and quantum kernel models on a preclinical dataset of 213 animals with blood and bronchoalveolar lavage fluid measurements to predict muscle outcomes.

Result: Quantum kernel ridge regression achieved test RMSE of 4.41 mg and R² of 0.605 for muscle weight, outperforming ridge baselines; geometric methods showed consistent gains, and screening achieved ROC-AUC up to 0.90 for detecting low muscle weight.

Conclusion: Geometric and quantum kernel methods offer measurable improvements in low-data, low-feature biomedical prediction tasks while maintaining interpretability and model transparency, supporting their utility in COPD-related muscle outcome modeling.

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [86] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: Automatic transformation of source code into numerical embeddings using r-Complexity, tested on a multi-label dataset for 11 classes from Codeforces.


<details>
  <summary>Details</summary>
Motivation: To create a generic method for converting diverse algorithm source code into numerical embeddings by analyzing program behavior and computational complexity.

Method: Dynamic analysis of program behavior against various inputs combined with tailored generic complexity functions for the analyzed metrics, based on r-Complexity.

Result: Implementation using XGBoost algorithm achieves an average F1-score on a multi-label dataset with 11 classes derived from real-world code snippets from Codeforces.

Conclusion: The method successfully generates embeddings for algorithm code and demonstrates effectiveness in a classification task on a real-world dataset.

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [87] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: A framework for data-driven product development using neural networks with joint modeling for multi-property optimization and conformalized uncertainty estimation to provide reliable, adaptable prediction intervals without retraining.


<details>
  <summary>Details</summary>
Motivation: To improve product design by leveraging data to model relationships between design specs and properties, especially for multiple correlated properties needing simultaneous optimization, requiring uncertainty measures.

Method: Train a joint neural network on past experiments, use Projected Gradient Descent for optimal design identification, and integrate Conformalised Monte Carlo Dropout for model-agnostic uncertainty estimation with coverage guarantees.

Result: Extensive tests on five real-world datasets show the method achieves state-of-the-art performance with adaptive, non-uniform prediction intervals that maintain coverage without retraining when adjusting levels.

Conclusion: The framework effectively combines joint neural modeling and conformal uncertainty to enhance data-driven product development, ensuring robust performance and adjustable reliability for multi-property design optimization.

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [88] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: This paper introduces LOFA, an algorithm for online influence maximization with submodular influence functions using full-bandit feedback, showing better empirical performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of influence maximization in an online setting with only observed influence per time step, leveraging the submodularity property for low regret.

Method: The method involves proposing the Lazy Online Forward Algorithm (LOFA), which exploits submodularity more effectively, and testing it on a real-world social network.

Result: LOFA achieves lower empirical regret and higher instantaneous reward compared to existing bandit algorithms in experiments.

Conclusion: LOFA demonstrates superior performance for online influence maximization under full-bandit feedback, making it a more efficient algorithm in practice.

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [89] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: This study investigates if sparse Mixture-of-Experts (MoE) models are more susceptible to decoding randomness than dense models as temperature increases, finding that instruction tuning, not sparsity, primarily determines robustness on deterministic tasks.


<details>
  <summary>Details</summary>
Motivation: The increasing use of sparse MoE architectures in large language models raises concerns about their reliability under stochastic decoding, as it's unclear if sparse routing combined with temperature-based sampling undermines output stability compared to dense models.

Method: Three models—OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned)—are evaluated on deterministic arithmetic reasoning tasks across four decoding configurations from greedy to T=1.0, measuring accuracy, format compliance, consistency, and confidence over 9,360 generations.

Result: The sparse instruction-tuned model shows stability similar to the dense instruction-tuned model at all temperatures, while the sparse base model degrades systematically with increasing temperature.

Conclusion: Instruction tuning, rather than architectural sparsity, is key for robustness against decoding randomness on deterministic tasks, indicating sparse models can be safely deployed in reliability-critical applications when properly tuned.

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [90] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: Transformer models like BERT are vulnerable to out-of-distribution inputs in Open-Set Recognition (OSR) for text, where vision methods like COSTARR don't significantly outperform basic baselines like MaxLogit in NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers excel in closed-set classification but fail with unseen categories, a critical issue for real-world NLP systems. The paper explores adapting computer vision OSR techniques, specifically the feature attenuation hypothesis, to text domains to address this brittleness.

Method: COSTARR, a vision-based OSR framework, is applied to BERT (base) and GPT-2 models trained on 176 arXiv subject areas. It is compared to baseline scores (MSP, MaxLogit, free-energy score) using OOSA and AUOSCR metrics for evaluation.

Result: COSTARR works without retraining in NLP but shows no statistically significant improvement over MaxLogit or MSP. Free-energy score performs worst in this high-class-count setting.

Conclusion: While vision-inspired OSR methods can be transplanted to NLP, they currently offer limited gains, indicating a need for larger models or task-specific attenuation strategies to enhance robustness in text OSR.

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [91] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: This paper enhances adversarial robustness in deep learning by integrating LIME explanations into training to suppress spurious features, using methods like feature masking and adversarial augmentation.


<details>
  <summary>Details</summary>
Motivation: In safety-critical domains like healthcare, deep learning models must be both robust to adversarial attacks and interpretable. The paper finds that non-robust features identified by LIME contribute to vulnerability, motivating a framework to turn LIME into an active training signal for improved robustness.

Method: The method introduces an attribution-guided refinement framework that transforms LIME into an active training signal. It suppresses spurious features via feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop pipeline. This integrates with standard adversarial training without needing extra datasets or model changes.

Result: Empirical tests on CIFAR-10, CIFAR-10-C, and CIFAR-100 show significant gains in adversarial robustness and out-of-distribution generalization, outperforming baseline adversarial training methods.

Conclusion: The paper links interpretability (via LIME) with robustness, demonstrating that actively suppressing spurious features during training can enhance both adversarial robustness and generalization in deep learning models, applicable to safety-critical applications without extra resources.

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [92] [Zero-shot Forecasting by Simulation Alone](https://arxiv.org/abs/2601.00970)
*Boris N. Oreshkin,Mayank Jauhari,Ravi Kiran Selvam,Malcolm Wolff,Wenhao Pan,Shankar Ramasubramanian,Kin G. Olivares,Tatiana Konstantinova,Andres Potapczynski,Mengfei Cao,Dmitry Efimov,Michael W. Mahoney,Andrew G. Wilson*

Main category: cs.LG

TL;DR: SarSim0, a fast SARIMA-based simulator, generates synthetic time series for zero-shot forecasting, outperforming benchmarks on trends, seasonality, and intermittency.


<details>
  <summary>Details</summary>
Motivation: Zero-shot time-series forecasting faces challenges like limited data, biased corpora, evaluation leakage, and privacy/licensing issues, requiring a practical simulation solution.

Method: A three-step SARIMA-based pipeline: (1) sample stable trajectories from the characteristic polynomial region, (2) superimpose paths for multi-seasonality, (3) add heavy-tailed noise for burstiness and intermittency.

Result: SarSim0 enables training on ~1B simulated series, achieving strong zero-shot performance on M-Series and GiftEval benchmarks, surpassing statistical forecasters and foundation models, with a student-beats-teacher effect on GiftEval.

Conclusion: SarSim0 provides an efficient and effective simulator for zero-shot forecasting, addressing data limitations and improving generalization in industrial applications.

Abstract: Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.

</details>


### [93] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: A new method called Contractive Diffusion Policies (CDPs) is introduced to improve diffusion policies for offline reinforcement learning by making the sampling process contractive, enhancing robustness and reducing errors in continuous control tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies suffer from solver and score-matching errors, high data requirements, and action inconsistencies in continuous control, which degrade performance compared to image generation applications.

Method: The paper develops Contractive Diffusion Policies (CDPs) that induce contractive behavior in the diffusion sampling dynamics through a theoretical analysis and a practical implementation recipe, integrating into existing architectures with minimal modifications.

Result: Extensive experiments in simulation and real-world settings show that CDPs often outperform baseline policies, especially under data scarcity conditions, by reducing errors and improving action consistency.

Conclusion: CDPs effectively enhance diffusion policies for offline learning by leveraging contraction properties, offering a robust and efficient approach that addresses key limitations in continuous control applications.

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [94] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: A data-driven study using machine learning to predict chloride evolution in concrete, evaluating algorithms like KRR, GPR, and MLP for service life assessment.


<details>
  <summary>Details</summary>
Motivation: To assess the service life of civil infrastructure in aggressive environments by understanding how concrete mixture compositions affect chloride ingress over time.

Method: Employed simple (LR, KNN, KRR) and complex (SVR, GPR, MLP, GRU) ML algorithms on a comprehensive dataset to predict chloride evolution and uncover hidden correlations.

Result: KRR, GPR, and MLP showed high accuracy; GRU underperformed due to data diversity. Most mixture components inversely relate to chloride content, with some direct correlations, revealed through explainable trends from models like GPR.

Conclusion: Machine learning offers effective surrogate approaches to model chloride ingress, highlighting correlations for improving infrastructure service life, though algorithm performance varies with data complexity.

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [95] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: This paper introduces a geometric framework to prevent representational collapse in deep Transformers, proposing Manifold-Geometric Transformer with constrained updates and delta learning for stable evolution.


<details>
  <summary>Details</summary>
Motivation: Deep Transformers often suffer from representational collapse (redundancy and rank loss), which persists despite modern optimization techniques. The authors argue this is a geometric issue due to uncontrolled residual updates causing manifold drift and monotonic feature accumulation.

Method: Proposes a unified geometric framework: 1. manifold-constrained hyper-connections to restrict residual updates to valid local tangent directions and prevent manifold drift; 2. deep delta learning for data-dependent, non-monotonic updates to enable reflection and erasure of redundant features, decoupling update direction and sign.

Result: The framework yields a stable geometric evolution across depth, resulting in the Manifold-Geometric Transformer architecture designed to avoid rank collapse in ultra-deep networks.

Conclusion: Enforcing geometric validity with dynamic erasure is essential for preventing collapse in deep Transformers; geometry, not depth itself, is the key limiting factor, with an evaluation protocol proposed for Transformers over 100 layers.

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [96] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: Study uses Random Fourier Transformation (RFT) to improve training and inference of Autoencoders and Variational Autoencoders for anomaly detection, finding RFT helps models learn both low and high frequencies simultaneously, with mixed results on trainable RFT.


<details>
  <summary>Details</summary>
Motivation: To investigate how Random Fourier Transformation (RFT) affects the training process and performance of deep neural networks like Autoencoders and Variational Autoencoders, particularly in anomaly detection, to overcome limitations of conventional DNNs that gradually learn frequency features.

Method: Applied RFT to AEs and VAEs for reconstruction-based anomaly detection, analyzed training behavior using Frequency Principle, introduced a trainable RFT variant by training expansion within computation graph, tested on low-dimensional synthetic datasets and high-dimensional aviation safety dataset (Dashlink).

Result: Models with RFT are superior to conventional counterparts, showing simultaneous learning of low and high frequencies, but results are inconclusive on whether trainable RFT offers benefits over random RFT in anomaly detection tasks.

Conclusion: Random Fourier Transformation enhances deep neural network performance by enabling concurrent learning of frequency features, but further research is needed to determine the advantages of a trainable RFT approach compared to random initialization.

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [97] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: Neural operator architectures based on Wiener chaos expansions for learning SDE/SPDE solution operators, with theoretical analysis and empirical validation across diverse domains.


<details>
  <summary>Details</summary>
Motivation: SDEs and SPDEs are key in modeling stochastic dynamics; deep learning for approximating their solution operators can lead to fast solvers and new perspectives for learning tasks.

Method: Project noise paths onto orthonormal Wick Hermite features, parameterize deterministic chaos coefficients with neural operators, enabling single-pass trajectory reconstruction.

Result: Theoretical investigation yields coupled ODE/PDE systems for chaos coefficients; empirical validation shows competitive accuracy on SPDE benchmarks, image sampling, graph interpolation, financial extrapolation, parameter estimation, and flood prediction.

Conclusion: WCE-based neural operators offer a practical, scalable approach for learning SDE/SPDE solution operators, demonstrating broad applicability.

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [98] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: A framework for measuring similarity between wireless datasets to predict model transferability, applied to CSI compression and beam prediction tasks.


<details>
  <summary>Details</summary>
Motivation: To enable tasks like dataset selection, sim2real comparison, and guiding model adaptation by assessing dataset similarity based on cross-dataset performance.

Method: Develop a task- and model-aware framework using metrics such as UMAP embeddings combined with Wasserstein/Euclidean distances for unsupervised tasks, and supervised UMAP with imbalance penalties for supervised tasks.

Result: Achieved Pearson correlations over 0.85 for dataset distances predicting transferability in CSI compression, and improved performance in beam prediction, outperforming traditional baselines.

Conclusion: The framework effectively supports task-relevant wireless dataset comparisons by correlating dataset distances with model transferability, enhancing applications in wireless communications.

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [99] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: A reverse diffusion method using an information-theoretic Lyapunov function to control coarse-grained image properties like blockwise intensities during generative sampling.


<details>
  <summary>Details</summary>
Motivation: Diffusion models lack theoretical understanding of how coarse-grained quantities evolve during reverse diffusion, hindering explicit control over properties like spatial block masses.

Method: Transplant a leak-tolerant Lyapunov potential framework to reverse diffusion, propose a V-delta projected reverse diffusion scheme, and validate with a toy model of block-constant images.

Result: The method maintains block-mass error within tolerance while achieving pixel accuracy and visual quality similar to non-projected dynamics, as shown numerically.

Conclusion: The study reframes generative sampling as an information potential decrease and offers a design principle for reverse diffusion with explicit coarse-grained control.

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [100] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: ML-UCB is a multi-armed bandit algorithm that integrates arbitrary machine learning models by modeling their learning curves, enabling sublinear regret without model-specific analysis.


<details>
  <summary>Details</summary>
Motivation: Deploying sophisticated ML models in sequential decision-making lacks tractable concentration inequalities for principled exploration.

Method: Assumes Mean Squared Error decreases as a power law in training samples, derives a generalized concentration inequality, and applies it in a multi-armed bandit framework called ML-UCB.

Result: ML-UCB achieves sublinear regret and is validated via experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data, showing improvements over LinUCB.

Conclusion: The framework enables principled integration of ML models with empirically characterized learning curves, eliminating the need for tailored theoretical analysis.

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [101] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: Fine-tuned 32B VLM for visual podcast generation outperforms 235B base model in conversational naturalness and narrative depth, evaluated with AI judges and style metrics.


<details>
  <summary>Details</summary>
Motivation: VLMs excel in descriptive tasks but struggle with engaging long-form narratives like podcast dialogues, and standard metrics fail to capture conversational nuances.

Method: Fine-tune Qwen3-VL-32B on 4,000 image-dialogue pairs using a synthetic-to-real strategy: train on SPoRC dialogues with synthetic images, evaluate on real VIST photos.

Result: Fine-tuned model significantly surpasses base model in conversational naturalness (>80% win rate), narrative depth (+50% turn length), and maintains visual grounding (CLIPScore: 20.39).

Conclusion: The pipeline effectively generates engaging visual podcasts, with a comprehensive evaluation framework addressing gaps in standard metrics for narrative quality.

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [102] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: This paper proposes using TinyML on low-power edge devices in aquaculture for real-time monitoring and control of water quality parameters to improve efficiency and reduce labor.


<details>
  <summary>Details</summary>
Motivation: Aquaculture faces challenges like water quality fluctuations, disease outbreaks, and inefficient feed management, while traditional monitoring methods are manual and time-consuming.

Method: Integrating TinyML on low-power edge devices with sensors to collect data (e.g., pH, temperature, dissolved oxygen, ammonia) for real-time automated monitoring, anomaly detection, and alerts.

Result: The system enables real-time data collection and controls for water quality, nutrient levels, and environmental conditions, with potential benefits like better maintenance, resource utilization, and decision-making for feed and water treatment.

Conclusion: The feasibility of TinyML-based solutions in aquaculture is explored, aiming to contribute to more sustainable and efficient farming practices.

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [103] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: This paper improves the weighted strategy for non-stationary parametric bandits by introducing a refined analysis framework, simplifying algorithms while achieving optimal regret bounds, and extending it to bandits and MDPs.


<details>
  <summary>Details</summary>
Motivation: Weighted strategies are popular for handling non-stationary bandits with gradual drifting but have been theoretically inefficient or suboptimal, requiring complex algorithms.

Method: Proposed a refined analysis framework to simplify algorithm design and applied it to linear bandits (LB), generalized linear bandits (GLB), and self-concordant bandits (SCB), then extended to non-stationary MDPs like Linear Mixture and MNL Mixture MDPs using weighted strategies.

Result: For LB, the framework yields a simpler, efficient algorithm with unchanged regret compared to prior work. For GLB, it improves regret from 	ilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5}) to 	ilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4}), and extends successfully to MDPs with dynamic regret guarantees.

Conclusion: The refined framework addresses inefficiencies in weighted strategies, providing simpler and statistically optimal algorithms for non-stationary parametric bandits and broader settings like MDPs, enhancing practical applicability and theoretical understanding.

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [104] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: Flow Equivariant World Models unify internal and external motion as Lie group flows, using group equivariance for stable latent representations, outperforming state-of-the-art models in partially observed video benchmarks with better generalization.


<details>
  <summary>Details</summary>
Motivation: Embodied systems face continuous sensory streams with symmetries that neural networks often ignore, leading to inefficient relearning of transformations.

Method: Introduce a framework treating self-motion and object motion as one-parameter Lie group flows and implement group equivariance to maintain stable latents.

Result: Significantly outperforms diffusion-based and memory-augmented models on 2D/3D benchmarks, especially for predictable dynamics outside the field of view, with strong generalization beyond training horizons.

Conclusion: Flow equivariance offers a scalable, data-efficient path to embodied intelligence by structuring world models with symmetry guidance.

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [105] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: The paper introduces Discount Model Search (DMS), a quality diversity optimization algorithm that uses a smooth model to guide exploration in high-dimensional measure spaces, outperforming existing methods like CMA-MAE and enabling new applications with image-based measures.


<details>
  <summary>Details</summary>
Motivation: Existing QD algorithms, such as CMA-MAE, struggle in high-dimensional measure spaces due to distortion, where similar solutions are grouped and receive identical discount values, causing stagnation, but high-dimensional measures enable more flexible user-specified applications, necessitating an improved approach.

Method: DMS replaces the histogram-based discount scheme in CMA-MAE with a continuous model that provides smooth discount values, allowing differentiation between similar measures and supporting exploration in high-dimensional spaces; it is specifically demonstrated in domains where measures are based on high-dimensional image spaces.

Result: DMS outperforms CMA-MAE and other black-box QD algorithms in high-dimensional benchmarks and in new application domains that use image datasets as measures, showing enhanced exploration and solution diversity without hand-designing measure functions.

Conclusion: DMS effectively addresses the limitations of previous QD algorithms in high-dimensional measure spaces by using a continuous discount model, enabling robust exploration and facilitating practical applications where measures can be defined via user-provided image datasets, thus advancing QD optimization.

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [106] [Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding](https://arxiv.org/abs/2601.01089)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT is a novel AI framework integrating DNA, RNA, and protein models via directional cross-attention, achieving 0.503 Pearson correlation in enhancer perturbation prediction and providing mechanistic insights.


<details>
  <summary>Details</summary>
Motivation: Domain-specific foundation models for DNA, RNA, and protein are isolated, limiting the ability to model integrated cellular processes as per the Central Dogma.

Method: CDT integrates pre-trained language models for DNA, RNA, and protein using directional cross-attention: DNA-to-RNA for transcription and RNA-to-Protein for translation, producing a unified Virtual Cell Embedding. Validated on CRISPRi enhancer perturbation data with fixed embeddings.

Result: Achieved Pearson correlation of 0.503 (63% of theoretical ceiling, r=0.797) on K562 cell data. Attention and gradient analyses identified distinct genomic regions, with gradient analysis highlighting a CTCF binding site confirmed by Hi-C data.

Conclusion: AI architectures aligned with biological information flow can achieve predictive accuracy and mechanistic interpretability for integrated cellular modeling.

Abstract: Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.

</details>


### [107] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: This paper develops an explainable ML framework for early-stage CKD screening in low-resource settings like Bangladesh and South Asia, achieving high balanced accuracy (up to 90.40%) and outperforming existing tools in accuracy and sensitivity with external validation across three datasets.


<details>
  <summary>Details</summary>
Motivation: Early detection of CKD is crucial to prevent end-stage renal disease, but current screening tools—predominantly from high-income countries—underperform in Bangladesh and South Asia due to different risk profiles, reliance on simple additive scoring, and data from advanced-stage CKD, failing to capture complex interactions and predict early-stage disease.

Method: The study used a community-based dataset from Bangladesh, the first in South Asia, evaluating twelve ML classifiers with ten feature selection techniques. Models were assessed via 10-fold cross-validation, externally validated on datasets from India, UAE, and Bangladesh, and made explainable using SHAP.

Result: An ML model with RFECV-selected features achieved 90.40% balanced accuracy; minimal non-pathology-test features showed 89.23% balanced accuracy, often beating larger feature sets. The models had higher accuracy and sensitivity than existing tools with fewer inputs, and external validation yielded 78% to 98% sensitivity. SHAP identified clinically meaningful predictors.

Conclusion: The proposed explainable ML framework offers a robust, generalizable solution for early-stage CKD screening in low-resource settings, tailored to and validated for South Asian populations, with potential to improve clinical decision-making through enhanced accuracy and explainability.

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [108] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: HISTOGRAPH: a two-stage attention-based graph pooling method that leverages historical graph activations from intermediate GNN layers to improve performance and robustness in graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional graph pooling techniques under-utilize intermediate node activations from earlier GNN layers, which can be crucial for capturing node representation evolution and mitigating issues like over-smoothing in deep architectures, leading to suboptimal performance.

Method: Proposes HISTOGRAPH, a novel pooling layer with: 1) unified layer-wise attention over intermediate activations (historical graph activations) to weight contributions from different layers, and 2) node-wise attention to combine node features, modeling representation evolution across GNN layers.

Result: Empirical evaluation on multiple graph classification benchmarks shows that HISTOGRAPH consistently outperforms traditional pooling techniques, with notable robustness improvements in deep GNN architectures.

Conclusion: HISTOGRAPH effectively addresses the gap in utilizing historical graph activations, enhancing graph classification performance by leveraging both activation history and graph structure, demonstrating its potential for robust deep GNN applications.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [109] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: The paper proposes a novel machine learning clustering algorithm based on Wittgenstein's family resemblance concept from analytic philosophy, using a graph-based approach without requiring prior knowledge of cluster numbers or shapes.


<details>
  <summary>Details</summary>
Motivation: To apply Wittgenstein's philosophical idea of family resemblance—where category members connect via overlapping similarities rather than a single defining property—to develop an effective nonlinear clustering method in machine learning.

Method: Introduces the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant. It computes resemblance scores between neighboring data instances, thresholds them to construct a resemblance graph, and defines clusters as the connected components of this graph.

Result: Simulations on benchmark datasets show that WFR is an effective nonlinear clustering algorithm, performing well without needing prior knowledge of the number of clusters or assumptions about their shapes.

Conclusion: The WFR algorithm successfully bridges analytic philosophy and machine learning, offering a practical and efficient clustering tool inspired by Wittgenstein's family resemblance concept.

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [110] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: A hybrid semi-supervised learning method combining Neurochaos Learning with self-training improves classification accuracy for low-labeled and imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Labeled data is often scarce and expensive, while unlabeled data is abundant; supervised learning struggles with limited labeled data or imbalance.

Method: Integrate Neurochaos Learning for chaos-based feature extraction with a threshold-based self-training method to expand labeled data using pseudo-labels.

Result: The NL+ST architecture outperforms standalone self-training across ten benchmark datasets, especially on Iris (188.66% gain), Wine (158.58%), and Glass Identification (110.48%).

Conclusion: Chaos-based features enhance SSL, boosting generalization, resilience, and accuracy in low-data scenarios.

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [111] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: Evo-TFS is an evolutionary oversampling method using genetic programming to generate diverse time series by integrating time- and frequency-domain features, improving classifier performance on imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning for time series classification assumes balanced data, but imbalanced distributions cause neglect of minority classes, and existing oversampling methods rely on linear interpolation, failing to preserve temporal dynamics and diversity.

Method: Employ strongly typed genetic programming to evolve diverse, high-quality time series, guided by a fitness function incorporating both time-domain and frequency-domain characteristics.

Result: Experiments on imbalanced time series datasets show Evo-TFS outperforms existing oversampling methods and significantly enhances the performance of time-domain and frequency-domain classifiers.

Conclusion: Evo-TFS effectively addresses imbalanced time series classification by generating diverse samples that preserve temporal dynamics, outperforming traditional oversampling techniques.

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [112] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: ARISE uses LLMs to create semantic-aware embeddings for categorical data, improving clustering by bridging the semantic gap.


<details>
  <summary>Details</summary>
Motivation: Categorical data lacks inherent ordering, making similarity measures difficult; existing methods rely on within-dataset co-occurrence patterns, which are unreliable in limited samples, creating a semantic gap that degrades clustering quality.

Method: ARISE leverages external semantic knowledge from LLMs to describe attribute values, enhancing their representations, and integrates these LLM-enhanced embeddings with original data to form semantic-aware representations for clustering.

Result: Experiments on eight benchmark datasets show consistent improvements over seven representative methods, achieving gains of 19-27% in clustering performance.

Conclusion: ARISE effectively bridges the semantic gap in categorical data clustering by incorporating LLM-based semantic knowledge, leading to more accurate and robust clustering results.

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [113] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: Study uses a multi-genre serious-game framework with machine learning to predict software development role suitability from gameplay behavior, avoiding self-report bias.


<details>
  <summary>Details</summary>
Motivation: Traditional personality assessment in career guidance and personnel selection relies on self-report questionnaires, which are prone to response bias, fatigue, and intentional distortion, necessitating a more objective alternative.

Method: A systematic literature review and empirical study identified developer-relevant traits; a custom mobile game was designed to elicit behaviors like problem solving and time management; fine-grained gameplay data were analyzed using a two-phase machine-learning modeling strategy.

Result: The model achieved up to 97% precision and 94% accuracy; behavioral analysis showed distinct patterns in suitable candidates, such as more puzzle wins, side challenges, menu navigation, and fewer pauses/retries.

Conclusion: Implicit behavioral traces from gameplay can effectively predict software development suitability, offering a scalable, engaging, and less biased alternative to traditional assessments, supporting the use of serious games.

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [114] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: A graph neural network method using Bayesian modeling of signed adjacency matrices to handle edge noise and heterophily in semi-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs in semi-supervised learning often exhibit heterophily and structural unreliability, which existing fixed-structure or regularization-based GNNs struggle with.

Method: Model a posterior distribution over signed adjacency matrices (positive, negative, or absent edges) and propose a sparse signed message passing network for robustness against edge noise and heterophily.

Result: The method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

Conclusion: The approach provides a principled Bayesian framework to effectively address edge noise and heterophily in graph-based learning.

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [115] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: A hybrid Bayesian-conformal framework is introduced to provide uncertainty quantification in healthcare predictions, achieving distribution-free coverage guarantees and adaptive precision by combining Bayesian hierarchical random forests with group-aware conformal calibration.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making requires uncertainty quantification with distribution-free coverage guarantees and risk-adaptive precision, which standard Bayesian methods fail to achieve due to under-coverage. To address this gap, the paper develops a hybrid framework to improve predictive reliability in healthcare settings.

Method: The approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity through calibration on clinical data.

Result: Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, the method achieves target coverage of 94.3% vs. 95% target, with adaptive precision: 21% narrower intervals for low-uncertainty cases and appropriately widened intervals for high-risk predictions.

Conclusion: The hybrid Bayesian-conformal framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation for uncertain cases, addressing the fundamental limitations in healthcare uncertainty quantification.

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [116] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: The study introduces the 'Dependency Divide' framework, showing that in low-infrastructure contexts, student engagement with digital platforms increases satisfaction only when internet is reliable; otherwise, highly engaged students become more vulnerable, prompting tailored policy recommendations.


<details>
  <summary>Details</summary>
Motivation: Traditional digital divide frameworks fail to explain why students with equal connectivity, student satisfaction with digital learning platforms varies significantly, necessitating a new framework to account for these disparities.

Method: A cross-sectional study of 396 university students in Bangladesh, using K-prototypes clustering to identify student profiles, Random Forest models with SHAP and ALE analysis for satisfaction drivers, and formal interaction analysis with propensity score matching to test hypotheses.

Result: Three student profiles identified (Casually Engaged, Efficient Learners, Hyper-Engaged); engagement increased satisfaction only when infrastructure was reliable ({eta} = 0.033, p = 0.028), with Hyper-Engaged students most vulnerable; targeted reliability improvements had 2.06 times greater returns than uniform ones.

Conclusion: In contexts with fragile infrastructure, high engagement can be a liability; policies should prioritize reliability for dependent users, establish contingencies, and educate on dependency risks instead of uniformly promoting engagement.

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [117] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: The paper compares Mamba SSM with LLaMA Transformer on long-context sequences, benchmarking computational and representational efficiency using therapy sessions.


<details>
  <summary>Details</summary>
Motivation: State Space Models (SSMs) offer linear computational complexity $O(N)$, as opposed to Transformers' quadratic scaling $O(N^2), making them promising for long-context modeling. The paper aims to benchmark their efficiency practically.

Method: Conducts a comprehensive benchmarking study, evaluating Mamba SSM against LLaMA Transformer on long-context sequences from dyadic therapy sessions. Analysis includes computational efficiency (memory usage and inference speed from 512 to 8,192 tokens) and representational efficiency (hidden state dynamics and attention patterns).

Result: Finds actionable insights for practitioners, establishing conditions where SSMs outperform Transformers in long-context applications.

Conclusion: Mamba SSM can be advantageous over LLaMA Transformer under specific conditions for long-context tasks, providing a solid comparison framework to guide model selection.

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [118] [Accelerated Full Waveform Inversion by Deep Compressed Learning](https://arxiv.org/abs/2601.01268)
*Maayan Gelboim,Amir Adler,Mauricio Araya-Polo*

Main category: cs.LG

TL;DR: A method using deep learning and compressed sensing to reduce FWI input dimensionality by hierarchical data selection, enabling faster 2D FWI with only 10% of data and potential for 3D scaling.


<details>
  <summary>Details</summary>
Motivation: Industrial FWI requires teraflop-level storage, making complex subsurface cases or multi-scenario exploration too costly, so a dimensionality reduction approach is needed.

Method: Utilizes a deep neural network with a binarized sensing layer for compressed learning to select a succinct seismic layout, then an autoencoder computes latent representations, followed by K-means clustering for hierarchical data selection.

Result: The method consistently outperforms random sampling, achieving effective 2D FWI with only 10% of the data.

Conclusion: This approach paves the way for accelerating large-scale 3D FWI by reducing computational costs through smart hierarchical data selection.

Abstract: We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.

</details>


### [119] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: This paper investigates how in-context learning (ICL) in LLMs works, comparing it to supervised classifiers like kNN and logistic regression, finding similarities when demonstration relevance is high and advantages for LLMs when it is low.


<details>
  <summary>Details</summary>
Motivation: Despite the empirical usefulness of in-context learning for customizing LLMs without fine-tuning, there is a lack of understanding of its underlying mechanisms. The paper aims to explore this by comparing ICL behavior to trained classifiers.

Method: The study uses text classification as a use case, with six datasets and three LLMs. It compares LLMs with ICL to supervised classifiers (gradient descent-based like logistic regression and k-nearest neighbors) trained on the same ICL demonstrations to analyze similarity.

Result: LLMs behave similarly to these classifiers when demonstration relevance is high, with ICL closer to kNN than logistic regression, suggesting the attention mechanism resembles kNN. When demonstration relevance is low, LLMs outperform the classifiers, likely due to their parametric memory.

Conclusion: ICL in LLMs shows similarities to kNN-like mechanisms when demonstrations are relevant, but LLMs have an advantage in low-relevance scenarios through parametric memory, highlighting their adaptive nature compared to traditional classifiers.

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [120] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: Introduces log-weighted Barron space for weaker regularity in deep ReLU network approximation, with depth-dependent error bounds in H^1 norm.


<details>
  <summary>Details</summary>
Motivation: Classical Barron spaces require stronger regularity than Sobolev spaces and have restrictive depth constraints, failing to fully explain deep network success on high-dimensional data.

Method: Define log-weighted Barron space, analyze embedding properties via Rademacher complexity, prove deep ReLU network approximation with explicit depth dependence, and establish bounds in H^1 norm.

Result: Log-weighted Barron space allows weaker regularity than classical Barron spaces; deep networks achieve approximation with preserved rates under maximal depth scales.

Conclusion: Depth reduces regularity requirements for efficient representation, providing a refined explanation for deep architecture performance beyond classical settings in high-dimensional problems.

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


### [121] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: Argus is a framework for drift detection in high-dimensional data streams using a fixed spatial partition (tessellations) to track local statistics efficiently, with theoretical invariance to orthogonal transformations and scalable complexity.


<details>
  <summary>Details</summary>
Motivation: Current drift detection methods face issues: global comparisons scale poorly, projections lose geometric structure, and re-clustering lacks identity stability in high-dimensional data streams.

Method: Argus reconceptualizes drift detection by tracking local statistics over a fixed spatial partition of the data manifold. It uses Voronoi tessellations over canonical orthonormal frames, proves invariance properties, develops graph-theoretic characterization of drift propagation, and introduces product quantization tessellation for high dimensions.

Result: The framework achieves O(N) complexity per snapshot with cell-level spatial localization of distributional change. It correctly identifies drift under coordinate rotation, avoiding false positives, and scales to very high dimensions (d > 500) product quantization tessellation and aggregated drift signals.

Conclusion: Argus provides a principled geometric foundation for drift detection that preserves high-dimensional structure and reduces computational burden, offering invariance to orthogonal transformations and effective tracking of coherent distributional shifts in data streams.

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [122] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: The paper introduces Muon++, a variant of the Muon optimizer, designed to reliably maintain the spectral conditions required by the $\mu$P framework throughout entire training processes, eliminating the need for computationally heavy spectral normalization on weights.


<details>
  <summary>Details</summary>
Motivation: Existing studies on applying the Muon optimizer under the $\mu$P framework either fail to ensure spectral conditions hold for the full training duration or require repeated spectral normalization of both weights and updates, leading to high computational costs and reduced practicality.

Method: The authors propose that for moderately large models, controlling the spectral properties at the optimizer-update level alone suffices to preserve $\mu$P-compatible scaling. Based on this insight, they develop Muon++, which enforces spectral conditions throughout training without explicit weight normalization.

Result: Muon++ successfully guarantees the spectral conditions required by $\mu$P for Muon across entire training, bridging theoretical $\mu$P promises with practical deployment of matrix-based optimizers. It also incorporates an adaptive spectral condition based on data-dependent effects for long-horizon LLM training.

Conclusion: This work provides a practical solution for ensuring $\mu$P spectral conditions with Muon, reducing computational overhead and enhancing feasibility for long-horizon training of large language models.

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [123] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: SWH architecture combines spectral processing for global modeling and windowed attention for local precision, achieving Transformer-level performance with linear scaling for long sequences.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic O(T²) complexity that limits application to long sequences, creating need for architectures that maintain representational expressivity while achieving computational efficiency for long-horizon tasks.

Method: Proposed Spectral-Window Hybrid (SWH) architecture with two parallel streams: global branch using Convolution Theorem for long-range decay dynamics in O(T log T) time, and local branch using sliding-window attention for bounded context interactions.

Result: SWH matches standard Transformer perplexity on short contexts while enabling efficient linear scaling to extended sequences, demonstrating comparable performance with better computational complexity.

Conclusion: SWH offers an efficient alternative to Transformers for long sequences by combining global spectral processing with local attention, achieving comparable performance with significantly better computational complexity.

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [124] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: GM-MLG is a novel open-ended ADR prediction method using graph-motif fusion and multi-label generation with Transformer Decoder, significantly improving performance and expanding prediction space.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current ADR prediction methods, including cold-start issues from scarce drug data, closed label sets, and poor modeling of label dependencies.

Method: Constructs a dual-graph representation from atomic, local (motifs via BRICS algorithm), and global molecular levels, and transforms prediction into multi-label generation using Transformer Decoder with positional embeddings for label dependencies.

Result: GM-MLG achieves up to 38% improvement and an average 20% gain, expanding prediction space from 200 to over 10,000 ADR types, with interpretable insights from motif analysis.

Conclusion: The method advances ADR prediction by enabling open-ended, dependency-aware generation, offering robust and interpretable support for drug safety risk reduction.

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [125] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot is an LLM agent that automates and self-corrects CFD workflows in combustion modeling, integrating domain knowledge from scientific articles with robust execution in tools like OpenFOAM and DeepFlame.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are becoming autonomous research partners, but there's a gap in complex scientific domains like combustion modeling, where practical AI assistance requires combining literature knowledge with execution capabilities for expertise-intensive tools like CFD codes.

Method: FlamePilot uses an architecture with atomic tools for robust setup and execution of complex simulations in OpenFOAM and DeepFlame. It learns from scientific articles to extract key information and guide simulations from setup to optimization.

Result: On a public benchmark, FlamePilot achieved a 1.0 executability score and 0.438 success rate, outperforming prior best agent scores. A case study on MILD combustion simulation showed it autonomously translates papers into simulations, conducts and post-processes them, refines based on evidence, and manages multi-step parameter studies with minimal human intervention.

Conclusion: FlamePilot establishes a foundational framework for AI-empowered combustion modeling, enabling a collaborative partnership where the agent handles workflow orchestration, freeing researchers for high-level analysis through a transparent and interpretable paradigm.

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [126] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: This paper proposes an f-GAN-based approach for causal discovery from data with unmeasured confounding by reformulating structure learning as minimizing Bayesian free energy, solved via adversarial optimization with Gumbel-Softmax relaxation.


<details>
  <summary>Details</summary>
Motivation: Causal discovery from data is difficult when unmeasured confounding factors are present, requiring methods that can learn causal structures independently of specific parameter values.

Method: The approach reformulates structure learning as minimizing Bayesian free energy, shown equivalent to minimizing f-divergence. Using the f-GAN framework, it transforms this into a min-max adversarial optimization problem, with gradient search implemented via Gumbel-Softmax relaxation in discrete graph space.

Result: The method enables learning binary causal structures without relying on specific weight values, providing a theoretical foundation via equivalence to free energy minimization.

Conclusion: The proposed f-GAN-based approach offers a novel solution for causal discovery under unmeasured confounding, leveraging adversarial optimization for efficient structure learning.

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [127] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: Proposes a two-stage framework for estimating deep learning model performance before training, using dataset and model properties, to guide architecture selection and detect data issues.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for selecting the right model architecture rely on repetitive trial-and-error, which is time-consuming, resource-intensive, and difficult to automate.

Method: A lightweight, two-stage approach: first stage predicts baseline based on measurable dataset properties; second stage adjusts estimation with model architectural and hyperparameter details.

Result: The framework generalizes across datasets and model types, with underlying features like dataset variance offering guidance for model selection and early data quality indicators.

Conclusion: The framework can forecast performance, guide architecture choices, inform preprocessing, and detect problematic datasets, providing an efficient alternative to trial-and-error.

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [128] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: A scale-adaptive multi-task learning framework for power flow analysis improves adaptability to topological changes and robustness in predictions by using local topology sampling and a reference-free model that predicts bus voltages and branch powers directly.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing deep learning models in power flow analysis that struggle with adaptability to topological variations and system scale changes, aiming to enhance robustness and prediction accuracy.

Method: Proposes the SaMPFA framework with Local Topology Slicing sampling to extract multi-scale subgraphs, and a Reference-free Multi-task Graph Learning model that predicts bus voltages and branch powers (instead of phase angles) to avoid error amplification and incorporate physical constraints in the loss function.

Result: Superior adaptability and generalization under variable system scales demonstrated on IEEE 39-bus system and a real Chinese provincial grid, with accuracy improvements of 4.47% and 36.82% respectively.

Conclusion: The SaMPFA framework effectively improves model performance in power flow analysis by enhancing cross-scale learning and physical consistency, offering practical benefits for real-world grid applications.

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [129] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: Proposes GDME, a graph-based unsupervised framework for online anomaly detection in streaming time series data via model ensemble, outperforming existing methods by up to 24%.


<details>
  <summary>Details</summary>
Motivation: Increasing volume and diversity of streaming data in industrial systems require effective online anomaly detection, but existing methods are often offline or struggle with heterogeneous data.

Method: Uses a dynamic model pool updated by pruning underperforming models and introducing new ones, with a dynamic graph to represent model relationships and community detection for ensemble selection, while monitoring graph structural changes to detect concept drift.

Result: Experiments on seven heterogeneous time series show GDME outperforms existing online methods with up to 24% improvement, and its ensemble strategy beats individual models and average ensembles with competitive computational efficiency.

Conclusion: GDME effectively addresses challenges in online anomaly detection by adapting to evolving data through dynamic graph-based ensemble, offering superior performance and efficiency for industrial streaming data applications.

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [130] [A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory](https://arxiv.org/abs/2601.01417)
*Itay Safran*

Main category: cs.LG

TL;DR: Super-linear width lower bound for approximating maximum function with ReLU networks at depths 3 to log2(log2(d)).


<details>
  <summary>Details</summary>
Motivation: To understand fundamental complexity of ReLU networks for the maximum operator, despite its simple nature.

Method: Combinatorial argument associating non-differentiable ridges with cliques via Turán's theorem from extremal graph theory.

Result: Width Ω(d^(1 + 1/(2^(k-2) - 1))) necessary for exact maximum at depths 3 ≤ k ≤ log2(log2(d)).

Conclusion: Maximum function has inherent geometric complexity from non-differentiable hyperplanes, offering new approach for neural network lower bounds.

Abstract: We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ is necessary to represent the maximum for any depth $3\le k\le \log_2(\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [131] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: LLM-driven agents on an AI social platform adopt toxicity partly from exposure but also spontaneously, with cumulative exposure raising risk, and predictions can be made from toxic stimuli alone.


<details>
  <summary>Details</summary>
Motivation: To investigate how exposure to harmful content shapes long-term behavior of LLM agents in fully AI-driven social environments, beyond isolated toxic generation studies.

Method: Empirical analysis on Chirper.ai, modeling interactions as stimuli (posts) and responses (comments), operationalizing exposure via observable interactions, and using metrics like Influence-Driven and Spontaneous Response Rates.

Result: Toxic responses often follow toxic stimuli, but substantial toxicity is spontaneous; cumulative exposure increases toxic response probability; prediction of agent toxicity is possible based on number of toxic stimuli.

Conclusion: Exposure is a key risk factor for LLM agent toxicity, and content monitoring can serve as a lightweight audit tool to mitigate harmful behavior in real-world deployments.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


### [132] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: A negotiation-based framework for EV charging in V2B contexts, using incentives for modest driver flexibility to reduce costs for both building operators and drivers.


<details>
  <summary>Details</summary>
Motivation: Conflict between building operators facing high energy costs from uncoordinated EV charging and drivers prioritizing convenience. Need to align objectives for shared savings.

Method: Negotiation-based framework guaranteeing voluntary participation, strategy-proofness, and budget feasibility. It uses survey data for calibration and real operational data for validation. Offers drivers incentive-backed options for flexibility in departure time or SoC.

Result: Simulations show mutual benefits: building operator costs reduced by over 3.5% compared to optimized non-negotiating smart charging, and user charging expenses reduced by 22% below utility retail energy rate.

Conclusion: The framework effectively transforms EV charging from an operational friction into a collaborative platform, bridging energy and mobility systems.

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [133] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: ARIES is an autonomous multi-agent framework using GPTs to query health data sources for real-time epidemiological surveillance, outperforming general AI in this high-stakes domain.


<details>
  <summary>Details</summary>
Motivation: Current global health surveillance suffers from knowledge gaps and reliance on static dashboards, while general-purpose AI is unreliable due to hallucinations and inability to handle specialized data silos.

Method: Built a hierarchical, modular multi-agent framework with GPTs orchestrating sub-agents to autonomously query WHO, CDC, and peer-reviewed papers, automating data extraction and synthesis.

Result: ARIES provides specialized reasoning to identify emergent threats and signal divergence in near real-time, demonstrating that task-specific agentic swarms outperform generic models.

Conclusion: ARIES offers a robust, extensible solution for next-generation outbreak response and global health intelligence, moving beyond static systems to a dynamic intelligence ecosystem.

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>
