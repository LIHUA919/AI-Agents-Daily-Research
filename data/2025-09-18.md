<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: Thinking LLMs outperform non-thinking models in judging tasks with ~10% higher accuracy and better robustness, despite only 2x computational cost vs 8x+ for augmentation methods.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used as automated judges in benchmarking and reward modeling, ensuring their reliability, efficiency, and robustness has become critical.

Method: Systematic comparison of thinking vs non-thinking Qwen 3 models (0.6B-4B parameters) on RewardBench tasks, evaluating accuracy, FLOPs, and augmentation strategies including in-context learning, rubric-guided judging, reference evaluation, and n-best aggregation.

Result: Thinking models achieve ~10% higher accuracy with minimal overhead (<2x cost), while augmentation methods provide modest gains at high cost (>8x). Thinking models show 6% higher average consistency under various bias conditions and maintain advantages in multilingual settings.

Conclusion: Explicit reasoning offers clear advantages in LLM-as-a-judge paradigm, providing better accuracy, efficiency, and robustness compared to non-thinking approaches and augmentation strategies.

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [2] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: Evaluation awareness in LLMs follows a power-law scaling relationship with model size, increasing predictably as models grow larger, enabling forecasting of deceptive behavior in future models.


<details>
  <summary>Details</summary>
Motivation: To understand how evaluation awareness scales across different model sizes, as prior work only demonstrated this phenomenon in a single 70B model, leaving the scaling relationship unknown.

Method: Investigated 15 models from 0.27B to 70B parameters across four families using linear probing on steering vector activations to measure evaluation awareness.

Result: Revealed a clear power-law scaling relationship where evaluation awareness increases predictably with model size.

Conclusion: This scaling law enables forecasting deceptive behavior in future larger models and guides the design of scale-aware evaluation strategies for AI safety.

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [3] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT is a scalable alignment method that trains LLMs to produce causally consistent reasoning by learning from systematically corrupted examples through intervention training and direct preference optimization.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning often fails to causally influence final answers, creating brittle and untrustworthy outputs, while existing methods focus on measuring faithfulness rather than systematically improving it.

Method: Generate synthetic training data by intervening on individual reasoning steps in model-generated CoTs to create faithful/unfaithful pairs, then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths.

Result: FRIT increases faithful reasoning by 3.4 percentage points for Mistral on GSM8K while improving accuracy by 7.6 percentage points across factual and symbolic reasoning tasks on Qwen3-8B and Mistral-7B-v0.1.

Conclusion: FRIT provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing the critical gap between reasoning performance and trustworthiness.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [4] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: Advocates for antifragile AI safety approach where systems improve from challenges rather than just resisting them, addressing limitations of static testing and proposing long-term adaptive safety measures.


<details>
  <summary>Details</summary>
Motivation: Conventional static benchmarks and single-shot robustness tests fail to account for evolving environments and model drift, potentially leading to maladaptation issues like reward hacking and capability atrophy.

Method: Proposes antifragile perspective that leverages uncertainties to prepare for future unpredictable events, identifies limitations of static testing, and explores antifragile solutions for rare event management.

Result: Position paper establishes framework for fundamental recalibration of AI safety measurement and benchmarking methods, providing ethical and practical guidelines.

Conclusion: Antifragile approach is pivotal for long-term reliability of open-ended ML systems, complementing existing robustness approaches and fostering adaptive AI safety community.

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [5] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC (Imagined Autocurricula) uses world models and Unsupervised Environment Design to generate diverse training environments from passive data, enabling robust agent generalization without extensive real-world training or simulation.


<details>
  <summary>Details</summary>
Motivation: Traditional agent training requires vast data or accurate simulation, which is unavailable for many real-world scenarios. World models offer an alternative using offline data, but need methods to ensure generated training data is useful.

Method: Proposes IMAC approach that leverages Unsupervised Environment Design (UED) to create automatic curricula over worlds generated by world models, training agents in imagined environments derived from passive data.

Result: Achieves strong transfer performance on held-out environments in challenging procedurally generated settings, training only inside a world model learned from narrower datasets.

Conclusion: This approach opens the path to utilizing larger-scale foundation world models for developing generally capable agents, demonstrating effective generalization from limited passive data.

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [6] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: Chain of Action (CoA) framework unifies high-level planning and low-level control in a single VLA model, treating abstracted actions as intermediate reasoning steps rather than separate policy commands, achieving state-of-the-art performance in Minecraft.


<details>
  <summary>Details</summary>
Motivation: The choice of action spaces is a critical unresolved challenge in developing capable agents, with no single action space being universally optimal - the effectiveness depends heavily on the specific task, creating a dilemma for building generalist agents.

Method: Introduces Chain of Action (CoA) framework that treats abstracted actions as intermediate reasoning steps (like chain of thought) to guide final executable action generation. Trains an All-in-One agent on diverse mixture of action spaces using CoA paradigm.

Result: The unified agent achieves new state-of-the-art performance, improving overall task success rate over strong specialized baselines. The approach learns more robust and generalizable policies.

Conclusion: CoA framework successfully resolves the action space dilemma by unifying planning and control, demonstrating that diverse action space training with intermediate reasoning steps leads to superior generalist agent performance. Releases OpenHA suite for reproducible research.

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [7] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: PDDL-Instruct framework enhances LLMs' symbolic planning through logical chain-of-thought reasoning, achieving 94% accuracy on planning benchmarks (66% improvement over baselines)


<details>
  <summary>Details</summary>
Motivation: LLMs demonstrate impressive capabilities but struggle with structured symbolic planning in formal domains like PDDL, creating a gap between general reasoning and logical precision required for automated planning

Method: Instruction tuning framework that teaches models to reason about action applicability, state transitions, and plan validity using explicit logical inference steps and structured reflection for self-correction

Result: Models achieve up to 94% planning accuracy on standard benchmarks, representing a 66% absolute improvement over baseline models across multiple planning domains

Conclusion: The work successfully bridges the gap between LLMs' general reasoning and logical precision for automated planning, offering a promising direction for developing better AI planning systems

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [8] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: Agentic UAVs framework with LLM-driven reasoning improves autonomy in search-and-rescue missions, achieving higher detection rates and better decision-making compared to traditional systems.


<details>
  <summary>Details</summary>
Motivation: Current UAV systems lack context-aware reasoning and autonomous decision-making capabilities, relying on rule-based control that limits adaptability in dynamic missions. None leverage LLM agents for real-time knowledge access.

Method: Five-layer architecture (Perception, Reasoning, Action, Integration, Learning) using LLM-driven reasoning, database querying, and third-party system interaction. ROS2 and Gazebo prototype with YOLOv11 object detection, GPT-4 reasoning, and local Gemma-3 deployment.

Result: Higher detection confidence (0.79 vs. 0.72), improved person detection rates (91% vs. 75%), and significantly increased action recommendation (92% vs. 4.5%) in simulated search-and-rescue scenarios.

Conclusion: Modest computational overhead enables qualitatively new levels of autonomy and ecosystem integration for UAVs, moving beyond traditional SAE Level 2-3 autonomy.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [9] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: Semantic fusion enhances Transformer LMs with parallel semantic feature channels using fuzzy membership functions for interpretable token-level semantics, improving perplexity and enabling controllable generation with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To augment language models with interpretable semantic features while maintaining model simplicity and enabling user-controllable generation capabilities.

Method: Adds a parallel fuzzy-membership feature channel with interpretable features (POS, roles, sentiment, etc.) using differentiable membership functions, fused into LM via gated adapter. Trained with next-token prediction, auxiliary reconstruction loss, and lightweight regularizer.

Result: Improves perplexity on synthetic two-clause corpus, enables precise control of polarity and punctuation generation while maintaining OOD generalization, with minimal computational overhead.

Conclusion: Semantic fusion provides an effective, lightweight approach for adding interpretable semantic conditioning to LMs while preserving compatibility with standard architectures and enabling controllable generation.

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [10] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: The Asterisk Operator is a novel unified framework for abstract reasoning that uses Adjacency-Structured Parallel Propagation to formalize reasoning tasks as local, parallel state evolution processes with global reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and convergent computational paradigm for abstract reasoning problems that maintains local computational constraints while achieving global reasoning capabilities.

Method: The Asterisk Operator framework based on Adjacency-Structured Parallel Propagation (ASPP), formalizing reasoning tasks as local, parallel state evolution processes guided by implicit relational graphs. Includes Embedding-Asterisk distillation method.

Result: Achieves 100% accuracy on ARC2 validation with only 6M parameters. Demonstrates universality, convergence properties, and superior performance on ARC2 challenges and Conway's Game of Life.

Conclusion: The Asterisk Operator represents a significant breakthrough in neural-symbolic reasoning, providing an efficient and convergent computational paradigm for abstract reasoning with proven mathematical properties and exceptional performance.

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [11] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2 is an automated RL agent generation framework that uses LLMs to transform natural language task descriptions into high-performance reinforcement learning solutions without human intervention, outperforming manually designed agents by up to 55%.


<details>
  <summary>Details</summary>
Motivation: Traditional RL agent development requires extensive expertise and has high failure rates, limiting accessibility. There's a need for fully automated agent design that can democratize RL development.

Method: Dual-agent architecture with Generator Agent (AI designer) and Target Agent (generated RL agent). Decomposes RL development into MDP modeling and algorithmic optimization stages. Built on Model Context Protocol with adaptive training management and intelligent feedback analysis.

Result: Consistently outperforms manually designed solutions across MuJoCo, MetaDrive, MPE, and SMAC benchmarks, achieving up to 55% performance improvement and substantial average gains.

Conclusion: Establishes a new paradigm where intelligent agents design and optimize other agents, enabling truly end-to-end closed-loop automation for AI systems.

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [12] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: Comprehensive uncertainty benchmarking study of 16 state-of-the-art Vision-Language Models across 6 multimodal datasets, showing larger models have better uncertainty quantification and mathematical/reasoning tasks have poorer uncertainty performance.


<details>
  <summary>Details</summary>
Motivation: While VLMs have advanced in visual understanding, uncertainty quantification has received insufficient attention despite being critical for reliable multimodal systems.

Method: Evaluated 16 VLMs (open and closed-source) across 6 multimodal datasets using 3 distinct scoring functions for comprehensive uncertainty benchmarking.

Result: Larger models consistently exhibit better uncertainty quantification; more certain models achieve higher accuracy; mathematical and reasoning tasks show poorer uncertainty performance across all models.

Conclusion: This work establishes a foundation for reliable uncertainty evaluation in multimodal systems, demonstrating that models that know more also know better what they don't know.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [13] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: Learning propositional STRIPS world models from action traces using transformers and gradient descent, treating it as a supervised next token prediction problem.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning world models from action sequences alone without requiring explicit state information, using deep learning approaches.

Method: Using transformer architecture for next token prediction where tokens are actions, ensuring hidden effects of previous actions don't violate preconditions of subsequent actions. Learning from random valid (positive) and invalid (negative) action sequences.

Result: The transformer architecture can faithfully represent propositional STRIPS world models and these models can be successfully learned from action sequence data alone.

Conclusion: Transformers are effective for learning STRIPS world models directly from action traces, demonstrating the viability of deep learning approaches for symbolic planning model acquisition.

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [14] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl is a benchmark for evaluating representation steering methods across alignment objectives like bias, harmful generation, and hallucination, while also examining side effects on secondary behaviors.


<details>
  <summary>Details</summary>
Motivation: Prior alignment work often highlights truthfulness or reasoning to demonstrate side effects of representation steering, but there are many unexplored tradeoffs not yet systematically understood.

Method: Collected a dataset of safety-relevant primary and secondary behaviors, created a modular steering framework based on unique components that serve as building blocks of existing methods, and evaluated five popular steering methods on Qwen-2.5-7B and Llama-3.1-8B models.

Result: Strong steering performance depends on the specific combination of steering method, model, and targeted behavior, and poor combinations can result in severe concept entanglement.

Conclusion: The study provides a systematic evaluation framework for representation steering methods and reveals the importance of carefully selecting method-model-behavior combinations to avoid negative side effects like concept entanglement.

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [15] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: LLM agents equipped with collaborative tools (social media and journaling) show improved performance on hard programming problems, with 15-40% lower cost, fewer turns, and faster completion compared to baseline agents.


<details>
  <summary>Details</summary>
Motivation: To investigate whether giving LLM agents the collaborative tools and autonomy that humans use for problem solving can improve their performance.

Method: Equipped Claude Code agents with MCP-based social media and journaling tools, allowing autonomous tool usage across 34 Aider Polyglot Python programming challenges.

Result: Substantial improvement on hardest problems: 15-40% lower cost, 12-27% fewer turns, 12-38% faster completion. Mixed effects on full challenge set. Different models adopted distinct collaborative strategies naturally.

Conclusion: AI agents can systematically benefit from human-inspired collaboration tools at the edge of their capabilities, with structured articulation driving improvements rather than information access alone.

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [16] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: Study examines student AI use in proof-based math courses, finding mixed perceptions about usefulness and limitations, with recommendations for future integration.


<details>
  <summary>Details</summary>
Motivation: Address the rapid rise of generative AI in higher education and unreliable detection tools, aiming to develop policies that encourage learning and critical thinking.

Method: Survey responses and student interviews across three proof-based undergraduate mathematics courses (abstract algebra and topology) where AI use was permitted.

Result: Analysis of how students engaged with AI tools and their perceptions of AI's usefulness and limitations in proof-based mathematics contexts.

Conclusion: Discussion of future considerations for integrating generative AI into proof-based mathematics instruction based on student experiences and perceptions.

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [17] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA is a toolkit for systematically programming cognitive biases in LLM-based social agents using validated social science experiments, addressing limitations of natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches using natural language descriptions fail to produce consistent agent behaviors across models and cannot capture nuanced behavioral specifications.

Method: CoBRA has two components: Cognitive Bias Index (measures bias through social science experiments) and Behavioral Regulation Engine (aligns agent behavior to demonstrate controlled cognitive bias).

Result: CoBRA can precisely program cognitive bias in social agents in a model-agnostic manner, as demonstrated through technical benchmarks.

Conclusion: The toolkit provides a systematic approach to specifying agent behavior that overcomes the limitations of natural language descriptions and ensures consistent, nuanced behavioral outcomes.

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [18] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: State-aware Reasoning (StaR) method improves multimodal agents' toggle control reliability in GUI interfaces by teaching state perception and action planning, achieving over 30% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents struggle with reliable toggle control execution in graphical user interfaces, particularly when the current state already matches the desired state, creating a key bottleneck in GUI interaction.

Method: Proposed State-aware Reasoning (StaR) training method that teaches agents to perceive current toggle state, analyze desired state from instructions, and act accordingly. Constructed state control benchmark from public datasets for evaluation.

Result: StaR improved toggle instruction execution accuracy by over 30% across three multimodal agents. Also enhanced general task performance on three public benchmarks and showed potential in dynamic real-world environments.

Conclusion: StaR effectively addresses the toggle control reliability problem in GUI agents through state-aware reasoning, demonstrating significant performance improvements and real-world application potential.

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [19] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind is a novel GUI agent framework that addresses five key challenges in industrial management automation through systematic exploration, memory-driven planning, state identification, knowledge distillation, and multi-layered safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: Industrial infrastructure management faces challenges from system complexity, multi-vendor integration, and operator shortages. Existing RPA solutions lack flexibility and have high maintenance costs, while general-purpose LLM-based GUI agents struggle with industrial-specific requirements.

Method: InfraMind integrates five modules: 1) systematic search-based exploration with VM snapshots for GUI understanding, 2) memory-driven planning for precision and efficiency, 3) advanced state identification for hierarchical interfaces, 4) structured knowledge distillation for lightweight deployment, and 5) multi-layered safety mechanisms.

Result: Extensive experiments on open-source and commercial DCIM platforms show InfraMind consistently outperforms existing frameworks in task success rate and operational efficiency.

Conclusion: InfraMind provides a rigorous and scalable solution for industrial management automation by specifically addressing the unique challenges of industrial systems through its integrated framework approach.

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [20] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR is a tool-integrated hierarchical optimization framework that uses RL to improve LLM mathematical reasoning through multi-agent data generation, fine-grained optimization, and self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with high-precision mathematical tasks like numerical computation and symbolic manipulation. Existing tool-integration methods face challenges in data construction, fine-grained optimization, and inference enhancement.

Method: Proposes THOR with three components: 1) TIRGen multi-agent pipeline for generating tool-integrated reasoning data, 2) RL strategy for joint trajectory-level and step-level optimization, 3) Self-correction mechanism using tool feedback during inference.

Result: Achieves state-of-the-art performance on multiple mathematical benchmarks for similar-scale models, shows strong generalization across diverse models, and delivers consistent improvements on code benchmarks.

Conclusion: THOR effectively bridges LLM limitations in mathematical reasoning through hierarchical optimization and tool integration, demonstrating robust performance and generalization capabilities.

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [21] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA is a smartphone framework that provides one-touch AI task recommendations by analyzing user context through multimodal language models and structured reasoning.


<details>
  <summary>Details</summary>
Motivation: To simplify access to AI services on smartphones and enable intuitive one-touch AI tasking by providing contextually relevant instruction recommendations.

Method: Uses multimodal LLM-based recommendation pipeline with structured reasoning, template-augmented reasoning mechanism, and prefix-tree-based constrained decoding to generate precise instructions from predefined candidates.

Result: Substantial improvements in instruction recommendation accuracy demonstrated through real-world dataset evaluation and user study.

Conclusion: MIRA has potential to revolutionize smartphone AI interactions by offering seamless and efficient user experience through contextual task recommendations.

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [22] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: An exact approach for model counting over integer linear constraints using DPLL architecture with mixed integer programming simplification techniques, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Linear constraints are fundamental in computer science, operations research, and optimization, with many applications requiring model counting over integer linear constraints (MCILC).

Method: Exhaustive DPLL architecture integrated with effective simplification techniques from mixed integer programming.

Result: Outperforms all exact methods on random benchmarks (solving 1718 vs 1470 instances) and is the only approach to solve all 4131 application instances.

Conclusion: The proposed approach significantly advances MCILC solving capabilities, demonstrating superior performance over existing state-of-the-art methods.

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [23] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: Study uses artificial neural networks to show how changes in information flow topology (feed-forward vs recurrent) create transitional cognitive performance improvements, with recurrent networks enabling qualitative expansion in processing capabilities and facing training barriers similar to evolutionary transitions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether cognitive evolution occurs through major transitions in neural network information flow structure, similar to evolutionary transitions that shape biological evolvability.

Method: Used idealised artificial neural networks (ANNs) with different topologies (feed-forward, recurrent, laminated) to test performance on learning artificial grammars of varying complexity, while controlling for network size and resources.

Result: Recurrent networks showed qualitative expansion in processing capabilities compared to feed-forward networks and significant performance improvement on complex grammars. Recurrent networks also exhibited training barriers and contingent irreversibility. Laminated networks did not outperform non-laminated ones in grammar learning.

Conclusion: Certain changes in information flow topology (specifically recurrent connections) can yield transitional changes in cognitive performance, demonstrating how cognitive evolution might occur through major transitions in neural network structure.

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [24] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent is a multi-agent system that provides end-to-end process control for data annotation by dynamically managing LLMs, SLMs, and human experts with optimized task assignment and quality-cost trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus narrowly on labeling but lack holistic process control to manage diverse annotation sources (LLMs, SLMs, humans) dynamically with proper scheduling and quality-cost optimization.

Method: Multi-agent system inspired by crowdsourcing companies that integrates task assignment, data annotation, and quality/cost management through rational task allocation enabling collaborative workflow between different annotation sources.

Result: Demonstrated effectiveness through extensive experiments on six diverse multimodal classification tasks, showing improved annotation process management.

Conclusion: CrowdAgent provides a unified framework for end-to-end annotation process control that enables synergistic collaboration between different annotation sources while addressing quality-cost trade-offs.

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [25] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: Second-order learning (adapting first-order learning mechanisms) promotes mental representations isomorphic to environments, validated through a hierarchical GCN-MLP architecture that shows improved performance and generalization in maze navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To empirically validate the hypothesis that second-order learning facilitates the emergence of structured mental representations that mirror external environments, which is fundamental to advanced cognition but challenging to study.

Method: Proposed a hierarchical architecture with a Graph Convolutional Network (GCN) as first-order learner for path prediction and an MLP controller as second-order learner that dynamically adapts GCN parameters when facing novel maze environments.

Result: Demonstrated that second-order learning is most effective when cognitive systems develop internal mental maps structurally isomorphic to the environment, showing significant performance improvements and robust generalization on unseen maze tasks.

Conclusion: Provides empirical evidence supporting the critical role of structured mental representations in maximizing second-order learning effectiveness, with quantitative and qualitative results confirming environment-cognition isomorphism benefits.

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL is a physics-informed deep learning framework that unifies ODE and PDE modeling for ecological systems, achieving high accuracy in predator-prey dynamics while enforcing conservation laws and providing 10-50x computational speedup.


<details>
  <summary>Details</summary>
Motivation: Ecological systems have complex multi-scale dynamics that traditional modeling struggles to capture, requiring new methods that can handle temporal oscillations and spatiotemporal patterns while maintaining conservation principles.

Method: The USPIL framework integrates physics-informed neural networks (PINNs) with conservation laws, using automatic differentiation to enforce physics constraints and adaptive loss weighting to balance data fidelity with physical consistency.

Result: Achieved 98.9% correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captured complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94) with conservation law adherence within 0.5% and 10-50x computational speedup.

Conclusion: USPIL establishes physics-informed deep learning as a powerful paradigm for ecological modeling, enabling multi-scale analysis, parameter discovery, and providing a transformative tool for ecological forecasting and conservation planning.

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [27] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: Empirical study shows optimizer choice significantly impacts energy efficiency in neural network training, with AdamW and NAdam being consistently efficient while SGD performs well on complex datasets despite higher emissions.


<details>
  <summary>Details</summary>
Motivation: As machine learning models become more complex and computationally demanding, understanding the environmental impact of training decisions is critical for sustainable AI development.

Method: Conducted 360 controlled experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using eight popular optimizers with 15 random seeds each, using CodeCarbon for precise energy tracking on Apple M1 Pro hardware.

Result: Substantial trade-offs between training speed, accuracy, and environmental impact that vary across datasets and model complexity. AdamW and NAdam identified as consistently efficient choices, while SGD demonstrates superior performance on complex datasets despite higher emissions.

Conclusion: Provides actionable insights for practitioners seeking to balance performance and sustainability in machine learning workflows through optimizer selection.

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [28] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: Hybrid DeepONet-Transolver framework for PET bottle buckling analysis that predicts displacement fields and reaction forces across varying geometries, achieving 2.5-13% L2 errors with accurate buckling capture.


<details>
  <summary>Details</summary>
Motivation: Existing neural surrogates lack generalization across non-parametric geometric domains for PDE problems, particularly in computationally expensive packaging design problems like PET bottle buckling analysis.

Method: Hybrid DeepONet-Transolver framework that simultaneously predicts nodal displacement fields and time evolution of reaction forces during top load compression, trained on nonlinear FEA simulation data from 254 unique bottle designs per geometry family.

Result: Achieves mean relative L2 errors of 2.5-13% for displacement fields and ~2.4% for time-dependent reaction forces, with absolute displacement errors of 10^-4-10^-3. Accurately captures buckling behavior across diverse bottle geometries.

Conclusion: The framework serves as a scalable and computationally efficient surrogate for multi-task predictions in computational mechanics, enabling rapid design evaluation while maintaining physical accuracy.

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [29] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS is a 1.3-80B parameter diffusion transformer for weather forecasting that achieves state-of-the-art performance and stability at high resolutions using novel parallelism techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of scaling diffusion-based weather forecasting models to high resolutions while maintaining stability and improving ensemble calibration compared to deterministic methods.

Method: Developed AERIS (pixel-level Swin diffusion transformer) and SWiPe (window parallelism technique that composes with sequence/pipeline parallelism to shard window-based transformers without added communication cost).

Result: Achieved 10.21 ExaFLOPS sustained performance with 95.5% weak scaling efficiency on Aurora supercomputer, outperformed IFS ENS, and maintained stability on seasonal scales up to 90 days.

Conclusion: Billion-parameter diffusion models like AERIS show significant potential for advancing weather and climate prediction capabilities with improved performance and scalability.

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [30] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL is a linear meta-learning algorithm that improves chemical property prediction accuracy while maintaining interpretability by leveraging shared parameters across related tasks.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for explainable AI in chemistry and bridge the gap between predictive accuracy and human comprehensibility in structure-property relationship modeling.

Method: Linear Algorithm for Meta-Learning (LAMeL) that uses meta-learning framework to identify shared model parameters across related chemical prediction tasks, learning a common functional manifold as starting point for new tasks.

Result: Performance improvements ranging from 1.1- to 25-fold over standard ridge regression, consistently outperforming or matching traditional linear methods across various chemical domains.

Conclusion: LAMeL provides a reliable tool for chemical property prediction where both accuracy and interpretability are critical, effectively leveraging shared knowledge across tasks without requiring shared data.

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [31] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: GPT-4o mini's safety system has a "Unimodal Bottleneck" flaw where context-blind filters override multimodal reasoning, causing false positives in hate speech detection and blocking benign content.


<details>
  <summary>Details</summary>
Motivation: Understanding safety architectures of Large Multimodal Models (LMMs) is critical for AI Alignment as these models become integral to daily digital life.

Method: Systematic analysis using Hateful Memes Challenge dataset on 500 samples, multi-phase investigation to probe reasoning and failure modes, quantitative validation of 144 content policy refusals.

Result: Identified Unimodal Bottleneck where safety filters preempt multimodal reasoning (50% visual, 50% textual triggers), brittle system blocks both high-risk imagery and benign meme formats.

Conclusion: Exposes fundamental tension between capability and safety in LMMs, highlighting need for more integrated, context-aware alignment strategies for safe and effective deployment.

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [32] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: Automated fault analysis framework using semantic embeddings and neural networks to detect anomalies in ALS control system logs


<details>
  <summary>Details</summary>
Motivation: To enable rapid identification of critical event sequences preceding complex system failures in the Advanced Light Source control system

Method: Process real-time EPICS event logs as natural language, transform into contextual vector representations using semantic embedding, and use sequence-aware neural network trained on normal data to assign real-time anomaly scores

Result: Flags deviations from baseline behavior in control system operations

Conclusion: The framework successfully enables operators to quickly identify critical event sequences that lead to system failures through automated anomaly detection

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [33] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: A novel private prediction framework using Differential Privacy to generate high-quality synthetic text with strong privacy guarantees, outperforming previous methods on in-context learning tasks.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in large language models where adversaries can extract sensitive information from prompts, requiring protection against information leakage.

Method: Leverages Differential Privacy framework without fine-tuning, performs inference on private records and aggregates per-token output distributions, uses blending operation to combine private and public inference.

Result: Outperforms previous state-of-the-art methods on in-context-learning tasks, generates longer and coherent synthetic text while maintaining privacy guarantees.

Conclusion: A promising direction for privacy-preserving text generation that maintains high utility while providing strong theoretical privacy bounds.

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [34] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: DeepLogit models combine interpretable discrete choice models with deep learning, using a constrained approach to maintain parameter interpretability while improving accuracy for transport policy analysis.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are black-box and challenging to adapt for planning/policy areas where interpretability is crucial. There's a need to bridge theory-based discrete choice models with data-driven AI approaches.

Method: Two-step approach: 1) Estimate CNN with linear terms equivalent to multinomial logit model, 2) Constrain interpretable parameters at linear values while adding higher-order terms or advanced architectures like Transformers.

Result: The approach retains interpretability of selected parameters while providing significantly improved model accuracy compared to traditional discrete choice models, demonstrated on real-world transit route choice data from Singapore.

Conclusion: This unifying approach shows potential for combining strengths of theory-based discrete choice models (interpretability) and data-driven AI models (predictive power), enabling more accurate models while maintaining applicability in planning/policy areas.

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [35] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: A novel framework combining Digital Twin technology and Zero-Knowledge Federated Learning to optimize UAV-assisted FL systems, reducing energy consumption by 29.6% while enhancing security and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in UAV-assisted federated learning systems including excessive energy consumption, communication inefficiencies, and security vulnerabilities that hinder reliable operation.

Method: Integrates Digital Twin technology for real-time monitoring and predictive maintenance, uses Zero-Knowledge Proofs for secure model verification without exposing sensitive data, and implements dynamic allocation strategy with block coordinate descent and convex optimization to optimize UAV flight paths, transmission power, and processing rates.

Result: Achieves up to 29.6% reduction in system energy consumption compared to conventional FL approaches, with demonstrated improvements in learning performance, security, and scalability through simulations.

Conclusion: The proposed framework presents a promising solution for next-generation UAV-based intelligent networks by effectively addressing energy efficiency, security, and operational challenges in federated learning systems.

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [36] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: Novel method converts multimodal physiological signals (PPG, GSR, ACC) into 2D image matrices for improved stress detection using CNNs, enabling better temporal and cross-signal dependency capture.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches process physiological signals separately or use fixed encodings, limiting their effectiveness in capturing complex temporal and cross-signal relationships for stress detection.

Method: Transform multimodal signals into structured 2D image representations, fuse them, and use systematic reorganization into multiple formats within a multi-stage training pipeline with CNNs.

Result: Significantly boosts classification performance for stress detection and improves model generalization and robustness through the image-based transformation approach.

Conclusion: The method is broadly applicable to multimodal physiological signal domains and enables more accurate, personalized, real-time health monitoring through wearable technologies.

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [37] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved is a framework that treats interleaved image-text generation as a tool-use problem, allowing an LLM agent to dynamically select and orchestrate specialized visual tools like image search, diffusion generation, code execution, and editing.


<details>
  <summary>Details</summary>
Motivation: Current unified models face a 'one-tool' bottleneck, being limited to synthetic imagery and struggling with factual grounding or programmatic precision in image-text generation tasks.

Method: Uses a central LLM/MLLM agent trained via Reinforcement Learning with hybrid rewards (rule-based logic + LLM/MLLM evaluator judgments) to intelligently select and apply specialized visual tools from a diverse toolkit.

Result: Achieves state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks, with additional gains from a novel test-time scaling strategy.

Conclusion: LLM-I provides a flexible and dynamic framework that overcomes limitations of current unified models by treating image-text generation as a tool-use problem, enabling more capable and precise multimodal generation.

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [38] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: GenPAS is a principled framework for data augmentation in generative recommendation that models augmentation as a stochastic sampling process with three bias-controlled steps, outperforming existing strategies in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Data augmentation is critical for training generative recommendation models but is often simplified or applied inconsistently without systematic understanding of its effects on model generalization and performance.

Method: Proposes GenPAS framework that models augmentation as stochastic sampling over input-target pairs with three steps: sequence sampling, target sampling, and input sampling, unifying existing strategies as special cases.

Result: Extensive experiments show GenPAS yields superior accuracy, data efficiency, and parameter efficiency compared to existing augmentation strategies on benchmark and industrial datasets.

Conclusion: GenPAS provides practical guidance for principled training data construction in generative recommendation and demonstrates the importance of systematic data augmentation approaches.

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [39] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: Proposes CPT method for controllable fairness-accuracy trade-offs in NLP using multi-objective optimization with gradient stabilization and pruning.


<details>
  <summary>Details</summary>
Motivation: Current approaches focus on finding a single optimal solution for fairness-accuracy trade-off, but diverse solutions exist on the Pareto front that should be accessible based on user preferences.

Method: Controllable Pareto Trade-off (CPT) uses multi-objective optimization with two key techniques: 1) stabilizes fairness updates with moving average of stochastic gradients, 2) prunes gradients by keeping only critical parameters' gradients.

Result: CPT achieves higher-quality solutions on the Pareto front than baselines, exhibits better controllability, and can precisely follow human-defined reference vectors in hate speech detection and occupation classification tasks.

Conclusion: CPT provides an effective method for controllable fairness-accuracy trade-offs in NLP, enabling users to specify their preference between the two objectives and obtain corresponding solutions.

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [40] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM is a novel radiance field-based framework that overcomes limitations of traditional localized statistical channel modeling by enabling multi-cell, multi-grid, multi-frequency analysis with improved accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional LSCM methods are limited to single-cell, single-grid, single-frequency analysis and fail to capture complex cross-domain interactions, making them inadequate for comprehensive cellular network optimization.

Method: Proposes RF-LSCM with physics-informed frequency-dependent attenuation model (FDAM) for cross-frequency generalization, point-cloud-aided environment enhancement for multi-cell/multi-grid modeling, and hierarchical tensor angular modeling (HiTAM) with low-rank tensor representation for computational efficiency.

Result: Achieves up to 30% reduction in MAE for coverage prediction and 22% MAE improvement through effective multi-frequency data fusion, while significantly reducing GPU memory requirements and training time.

Conclusion: RF-LSCM significantly outperforms state-of-the-art methods by enabling comprehensive multi-domain channel modeling with improved accuracy and computational efficiency for cellular network optimization.

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [41] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: A conformal prediction framework for Physics-Informed Neural Networks that provides distribution-free uncertainty quantification with rigorous statistical guarantees and handles spatial heteroskedasticity through local calibration.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification approaches for PINNs lack rigorous statistical guarantees, creating a need for methods that provide reliable uncertainty estimates with theoretical foundations.

Method: Distribution-free conformal prediction framework that calibrates prediction intervals using nonconformity scores on a calibration set, with local conformal quantile estimation to handle spatial heteroskedasticity.

Result: The framework achieves reliable calibration and locally adaptive uncertainty intervals, outperforming heuristic UQ approaches across multiple PDE systems and uncertainty metrics.

Conclusion: This work bridges PINNs with distribution-free UQ, enhancing calibration and reliability while opening new avenues for uncertainty-aware modeling of complex PDE systems.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [42] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: Smartwatch-based system predicts social anxiety fluctuations using heart rate data and trait measures, achieving 60.4% accuracy in real-time detection and outperforming prior methods by 7%.


<details>
  <summary>Details</summary>
Motivation: Social anxiety causes significant functional impairment, but little research has measured or predicted momentary anxiety fluctuations needed for real-time personalized interventions.

Method: Used custom smartwatch system with 91 socially anxious students, collected 7 daily EMAs, developed base model on 10,000+ days of external heart rate data, transferred representations, fine-tuned for probabilistic predictions combined with trait measures in meta-learner.

Result: Achieved 60.4% balanced accuracy in state anxiety detection, and 59.1% accuracy on external TILES-18 dataset, outperforming prior work by at least 7%.

Conclusion: The pipeline successfully predicts intra-day social anxiety fluctuations, demonstrating generalizability and potential for real-time personalized interventions.

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [43] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: DirGraphSSM extends state space models to directed graphs via k-hop ego graph sequentialization, achieving SOTA performance with 1.5-2x training speed improvements.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks struggle with capturing long-range causal dependencies in directed graphs and balancing accuracy with training efficiency on large-scale datasets. Current graph state space models only work for undirected graphs.

Method: Proposes DirEgo2Token to sequentialize directed graphs via k-hop ego graphs, then develops DirGraphSSM architecture that implements state space models on directed graphs through message-passing mechanism.

Result: Achieves state-of-the-art performance on three representative directed graph learning tasks and competitive performance on two additional tasks, with 1.5x to 2x training speed improvements compared to existing SOTA models.

Conclusion: First systematic extension of state space models to directed graph learning, demonstrating superior performance and efficiency for capturing causal dependencies in directed graphs.

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [44] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis is a parallel protection framework for federated learning that enables flexible trade-off control between privacy, utility, and efficiency through strategic model partitioning with DP and HE.


<details>
  <summary>Details</summary>
Motivation: Existing FL protection mechanisms force a rigid choice between model utility and computational efficiency, hindering practical implementation.

Method: Strategic model partitioning scheme applying lightweight DP to less critical low-norm portions and HE to the remainder, with distributed voting for consensus.

Result: Theoretical analysis confirms efficiency-utility adjustments with same privacy. Experiments show flexible prioritization between accuracy and training time via hyperparameter tuning.

Conclusion: ParaAegis provides practitioners with tunable control over the privacy-utility-efficiency balance in federated learning, overcoming limitations of existing rigid protection mechanisms.

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [45] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK enhances LLMs for traffic forecasting by integrating spatial correlations through SE-Attention and capturing temporal dependencies via MRFFN, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with spatial dependencies in traffic forecasting due to their sequential token processing design and inability to effectively model graph-structured spatial data.

Method: Proposes ST-LINK framework with Spatially-Enhanced Attention (SE-Attention) that extends rotary position embeddings to incorporate spatial correlations, and Memory Retrieval Feed-Forward Network (MRFFN) for dynamic historical pattern retrieval.

Result: Comprehensive experiments show ST-LINK outperforms conventional deep learning and LLM approaches, effectively capturing both regular traffic patterns and abrupt changes.

Conclusion: ST-LINK successfully addresses LLM limitations in spatial dependency modeling for traffic forecasting through innovative attention and memory mechanisms.

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [46] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: CAUSA is a novel causal multi-view unsupervised feature selection method that addresses spurious correlations in existing methods by introducing causal regularization to separate confounders and learn view-shared sample weights.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view unsupervised feature selection methods rely on correlations between features and clustering labels, but these correlations may be unreliable due to spurious correlations caused by confounders, leading to selection of irrelevant features.

Method: Proposes CAUSA with two components: 1) generalized unsupervised spectral regression to identify informative features, and 2) causal regularization module that adaptively separates confounders and learns view-shared sample weights to balance confounder distributions.

Result: Comprehensive experiments show CAUSA outperforms several state-of-the-art methods in multi-view unsupervised feature selection.

Conclusion: This is the first in-depth study of causal multi-view feature selection in unsupervised setting, demonstrating that causal perspective helps mitigate spurious correlations and select more informative features.

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [47] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: FHNN is a physics-structured neural network framework that predicts interpretable hydrodynamic parameters for floating-body fluid-structure interaction, achieving better accuracy and stability than black-box models while maintaining physical interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional black-box neural models for fluid-structure interaction have limited interpretability and unstable long-horizon predictions. There's a need for models that can handle dissipative dynamics while preserving physical transparency.

Method: Proposes Floating-Body Hydrodynamic Neural Networks (FHNN) that predict interpretable hydrodynamic parameters (directional added masses, drag coefficients, flow streamfunction) and couples them with analytic equations of motion to constrain the hypothesis space.

Result: Achieves up to an order-of-magnitude lower error than Neural ODEs on synthetic vortex datasets, recovers physically consistent flow fields, and handles dissipative dynamics better than Hamiltonian/Lagrangian neural networks.

Conclusion: FHNN successfully bridges the gap between black-box learning and transparent system identification by providing interpretable hydrodynamic parameter predictions while maintaining stability and accuracy in fluid-structure interaction modeling.

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [48] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: GPhyT is a General Physics Transformer that learns to simulate diverse physical systems from data alone, achieving state-of-the-art performance across multiple domains with zero-shot generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Current physics-aware ML approaches are limited to single domains and require retraining for each new system. A Physics Foundation Model would democratize access to high-fidelity simulations and accelerate scientific discovery.

Method: Transformer architecture trained on 1.8 TB of diverse simulation data, learning to infer governing dynamics from context without being told the underlying equations.

Result: Superior performance across multiple physics domains (up to 29x better than specialized architectures), zero-shot generalization to unseen systems, and stable 50-timestep rollouts.

Conclusion: This work demonstrates that foundation model capabilities are achievable for physics, opening the path toward a universal Physics Foundation Model that could transform computational science and engineering.

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [49] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: Hybrid quantum-classical workflow combining classical ML for feature engineering with Quantum Neural Network achieves superior credit risk assessment performance on limited data, outperforming classical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address few-shot credit risk assessment challenges in inclusive finance where data scarcity and imbalance limit conventional models' effectiveness.

Method: Ensemble of classical ML models (Logistic Regression, Random Forest, XGBoost) for feature engineering, followed by Quantum Neural Network trained via parameter-shift rule as classifier.

Result: QNN achieved 0.852 AUC in simulations and 0.88 AUC on Quafu quantum hardware, surpassing classical benchmarks with strong recall performance on 279-sample dataset.

Conclusion: Provides practical blueprint for quantum computing in data-constrained financial scenarios and empirical evidence for quantum advantage in high-stakes applications.

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [50] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: Hybrid framework combining graph neural network with pore network model for permeability prediction, replacing analytical conductance formulas with GNN predictions while preserving physics-based flow calculations.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of pure data-driven models (lack generalization, no physical constraints) and traditional pore network models (idealized geometric assumptions limiting accuracy in complex structures).

Method: End-to-end differentiable framework embedding GNN into PNM, replacing analytical conductance calculations with GNN predictions from pore/throat features. Trained using single permeability target via backpropagation through both GNN (automatic differentiation) and PNM solver (discrete adjoint method).

Result: Achieves high accuracy and generalizes well across different scales, outperforming both pure data-driven and traditional PNM approaches. Gradient-based sensitivity analysis shows physically consistent feature influences.

Conclusion: Provides scalable, physically informed framework for permeability prediction in complex porous media, reducing model uncertainty and improving accuracy while maintaining interpretability.

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [51] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: Graph-regularized GMM learning for distributed systems with limited heterogeneous data, using similarity graphs for parameter sharing without raw data transfer.


<details>
  <summary>Details</summary>
Motivation: To enable effective Gaussian Mixture Model learning in distributed environments where nodes have limited and heterogeneous local data, while preserving privacy by avoiding raw data sharing.

Method: Proposes a graph-regularized approach that uses provided similarity graphs to guide parameter sharing among nodes, allowing flexible aggregation of neighbors' parameters without transferring raw data.

Result: Outperforms both centralized GMMs and locally trained GMMs in heterogeneous, low-sample scenarios.

Conclusion: The graph-regularized method provides an effective solution for distributed GMM learning with limited heterogeneous data, enabling privacy-preserving parameter sharing through similarity graphs.

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [52] [Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets](https://arxiv.org/abs/2010.01052)
*Jaume Banus,Maxime Sermesant,Oscar Camara,Marco Lorenzi*

Main category: cs.LG

TL;DR: Probabilistic framework for joint cardiac data imputation and cardiovascular model personalization to enable heart-brain relationship studies when cardiac data is incomplete.


<details>
  <summary>Details</summary>
Motivation: Mechanistic models in clinical studies are limited by lack of multi-modal patient data. Neuroimaging datasets don't provide sufficient heart feature representation for modeling cardiovascular factors in brain disorders.

Method: Variational framework for joint inference of cardiac imputation model from available features plus Gaussian Process emulator that reproduces personalized cardiovascular dynamics.

Result: Accurate imputation of missing cardiac features from minimal heart information (e.g., systolic/diastolic BP only) while estimating emulated parameters of lumped model on UK Biobank data.

Conclusion: Enables novel exploration of heart-brain relationships through simulation of realistic cardiac dynamics corresponding to different brain anatomy conditions.

Abstract: The use of mechanistic models in clinical studies is limited by the lack of
multi-modal patients data representing different anatomical and physiological
processes. For example, neuroimaging datasets do not provide a sufficient
representation of heart features for the modeling of cardiovascular factors in
brain disorders. To tackle this problem we introduce a probabilistic framework
for joint cardiac data imputation and personalisation of cardiovascular
mechanistic models, with application to brain studies with incomplete heart
data. Our approach is based on a variational framework for the joint inference
of an imputation model of cardiac information from the available features,
along with a Gaussian Process emulator that can faithfully reproduce
personalised cardiovascular dynamics. Experimental results on UK Biobank show
that our model allows accurate imputation of missing cardiac features in
datasets containing minimal heart information, e.g. systolic and diastolic
blood pressures only, while jointly estimating the emulated parameters of the
lumped model. This allows a novel exploration of the heart-brain joint
relationship through simulation of realistic cardiac dynamics corresponding to
different conditions of brain anatomy.

</details>


### [53] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: MDMs solve discrete optimal transport energy minimization problems, with three energy formulations proven equivalent. Energy-inspired Beta parameterized schedules enable efficient post-training tuning and outperform baselines in low-step sampling.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic theoretical framework that interprets masked diffusion models as solutions to energy minimization problems in discrete optimal transport, unifying their theoretical foundations and enabling practical sampling improvements.

Method: Prove mathematical equivalence of kinetic, conditional kinetic, and geodesic energy formulations under MDM structure. Parameterize interpolation schedules via Beta distributions to reduce design space to 2D search for efficient post-training tuning without model modification.

Result: Energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings, as demonstrated on synthetic and real-world benchmarks.

Conclusion: The framework unifies MDM theory through energy minimization equivalence and enables practical schedule optimization via Beta parameterization, leading to improved sampling performance without requiring model retraining.

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [54] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG addresses client drift in federated learning using stochastic sampling-guided drift alignment with history-aware memory and participation-based gating.


<details>
  <summary>Details</summary>
Motivation: Non-IID data and partial client participation cause client drift and inconsistent local optima in federated learning, leading to unstable convergence and accuracy loss.

Method: Maintains per-client drift memory that accumulates local model differences as gradient sketches, with gating based on observed/expected participation ratio. Uses phase-by-expectation signal from server sampler to control memory updates and local alignment.

Result: Outperforms strong baselines on CIFAR-10/100 with 100/500 clients and 2-15% participation. Improves test accuracy by up to +0.9 on CIFAR-10 and +2.7 on CIFAR-100, with 4.5x faster convergence to target accuracy.

Conclusion: Sampling statistics can be transformed into principled, history-aware phase control to stabilize and accelerate federated training, with minimal overhead and graceful degradation under favorable conditions.

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [55] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter is a lightweight adapter that enhances Time Series Foundation Models with covariate information without fine-tuning, using a two-stage method with pseudo-forecasts and Gaussian Process refinement to achieve significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current Time Series Foundation Models cannot leverage covariates (exogenous variables) that are critical for accurate forecasting in many applications, due to their domain-specific nature and lack of associated inductive bias.

Method: TFMAdapter uses a non-parametric cascade that combines covariates with univariate TSFM forecasts. It employs a two-stage approach: (1) generating pseudo-forecasts with a simple regression model, and (2) training a Gaussian Process regressor to refine predictions using both pseudo- and TSFM forecasts alongside covariates.

Result: Extensive experiments show TFMAdapter consistently outperforms both foundation models and supervised baselines, achieving 24-27% improvement over base foundation models with minimal data and computational overhead.

Conclusion: Lightweight adapters like TFMAdapter can effectively bridge the gap between generic foundation models and domain-specific forecasting needs by enabling covariate integration without expensive retraining.

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [56] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx is a novel framework for intersectional fairness that addresses multiplicative biases across multiple protected attributes through adaptive multi-objective optimization, achieving better fairness-accuracy trade-offs than single-attribute methods.


<details>
  <summary>Details</summary>
Motivation: Existing fairness methods only handle single protected attributes and fail to capture the complex, multiplicative biases that affect intersectional subgroups (e.g., Black women, elderly Asians).

Method: APFEx combines three innovations: 1) adaptive multi-objective optimizer switching between Pareto cone projection, gradient weighting, and exploration strategies, 2) differentiable intersectional fairness metrics for gradient-based optimization, and 3) theoretical convergence guarantees to Pareto-optimal solutions.

Result: Experiments on four real-world datasets show APFEx reduces fairness violations while maintaining competitive accuracy, outperforming existing methods.

Conclusion: APFEx bridges a critical gap in fair ML by providing a scalable, model-agnostic solution for intersectional fairness that handles the complex trade-offs between multiple protected attributes.

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [57] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: Combining state-of-the-art trajectory prediction models with simple confidence-weighted averaging improves performance by 10% without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of combining strengths of multiple large autonomous driving prediction models without costly re-training.

Method: Using confidence-weighted average method to combine state-of-the-art deep learning models out-of-the-box (no retraining or fine-tuning).

Result: 10% performance improvement over the best individual model, especially in long-tailed metrics, validated on both NuScenes and Argoverse datasets.

Conclusion: Simple ensemble methods can effectively enhance trajectory prediction performance across dataset distribution without additional training costs.

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [58] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: WILF-Q uses Q-learning to approximate Whittle indices for client selection in wireless FL, outperforming baselines without needing client state transition knowledge.


<details>
  <summary>Details</summary>
Motivation: Reduce total time to achieve learning accuracy in wireless FL by addressing client selection problem where server cannot observe clients' dynamic computation/communication states.

Method: Formulate client selection as restless multi-armed bandit problem, propose WILF-Q that uses Q-learning to learn approximated Whittle indices for each client and selects highest-index clients.

Result: WILF-Q significantly outperforms existing baseline policies in learning efficiency without requiring explicit knowledge of client state transitions or data distributions.

Conclusion: WILF-Q provides robust and efficient client selection approach well-suited for practical FL deployments in wireless settings.

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [59] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN is a novel framework combining physics-informed neural networks with extended finite element method concepts to handle multiple crack problems in fracture mechanics using domain decomposition and specialized enrichment functions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling multiple cracks in fractured media where traditional methods struggle with discontinuities and singularities at crack tips.

Method: Combines PINNs with XFEM concepts using energy-based loss functions, customized integration schemes, domain decomposition, and neural network enrichment with specialized functions to capture discontinuities and singularities.

Result: The method enables flexible and effective simulations of complex multiple-crack problems in 1D and 2D domains with extensibility to 3D problems, validated through numerical experiments.

Conclusion: X-PINN provides a robust framework for fracture mechanics problems, successfully capturing crack discontinuities and singularities while offering convenient extensibility to higher dimensions.

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [60] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: Lightweight multimodal system using temperature, humidity, and pressure sensors detects queen bee presence with 99% accuracy on low-power microcontrollers, eliminating need for audio-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current queen bee monitoring methods are manual, disruptive, and impractical for large-scale beekeeping. Audio-based approaches require high power consumption, complex preprocessing, and are susceptible to ambient noise.

Method: Proposes environmental sensor fusion (temperature, humidity, pressure differentials) with quantized decision tree inference on STM32 microcontroller for real-time, low-power edge computing.

Result: Achieves over 99% queen detection accuracy using only environmental inputs, with audio features providing no significant performance improvement.

Conclusion: Presents a scalable, sustainable, non-invasive hive monitoring solution using off-the-shelf, energy-efficient hardware for autonomous precision beekeeping.

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [61] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART is a continual learning framework for personalized epileptic seizure detection that adapts to individual patients' evolving EEG signals using a size-constrained replay buffer and informed sample selection, achieving 21% F1 score improvement with minimal computational requirements.


<details>
  <summary>Details</summary>
Motivation: Current clinical seizure detection relies on expert EEG analysis which is time-consuming and requires specialized knowledge. Automated detection is needed, but static deep learning models suffer from catastrophic forgetting when adapting to individual patients' evolving EEG signals over time.

Method: Proposed EpiSMART framework uses continual learning with size-constrained replay buffer and informed sample selection strategy that retains high-entropy and seizure-predicted samples to incrementally adapt to patient-specific EEG signals while preventing catastrophic forgetting.

Result: On CHB-MIT dataset, EpiSMART achieved 21% improvement in F1 score over baseline without updates. Requires only 6.46 minutes of labeled data and 6.28 updates per day, making it suitable for real-time wearable deployment.

Conclusion: EpiSMART enables robust personalized seizure detection under resource-constrained conditions by effectively integrating new data without degrading past knowledge, advancing automated seizure detection for practical wearable healthcare deployment.

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [62] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO is an offline RL framework that uses tree-structured trajectory representation and process reward modeling to improve web agent training by addressing credit assignment, annotation costs, and reward sparsity issues.


<details>
  <summary>Details</summary>
Motivation: Training web agents with reinforcement learning faces challenges including credit assignment misallocation, high annotation costs, and reward sparsity, which hinder effective automated web interaction.

Method: Proposes Tree-Guided Preference Optimization (TGPO) with tree-structured trajectory representation to merge semantically identical states, Process Reward Model for automatic fine-grained rewards, and dynamic weighting for high-impact decision prioritization.

Result: Experiments on Online-Mind2Web and C-WebShop datasets show TGPO significantly outperforms existing methods with higher success rates and fewer redundant steps.

Conclusion: TGPO effectively addresses key challenges in web agent training through innovative trajectory representation and reward modeling, demonstrating superior performance in automated web interaction tasks.

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [63] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: Deep temporal graph network for real-time GNSS jamming mitigation by predicting receiver deviation using satellite-receiver graph representation


<details>
  <summary>Details</summary>
Motivation: GNSS systems face increasing intentional jamming that degrades positioning availability when it's most needed, requiring real-time mitigation solutions

Method: Heterogeneous Graph ConvLSTM (HeteroGCLSTM) that models satellite-receiver environment as star graph with time-varying attributes (SNR, azimuth, elevation, coordinates) and aggregates spatial-temporal context for deviation prediction

Result: Achieves lowest MAE across multiple jamming scenarios (3.64-7.74 cm at -45 dBm, improving to 1.65-2.08 cm at lower power levels), outperforms MLP, CNN, and Seq2Point baselines with superior data efficiency

Conclusion: Receiver-centric graph neural network approach effectively mitigates GNSS jamming in real-time, demonstrating strong performance across diverse jamming types and power levels with excellent data efficiency

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [64] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign is a lightweight plug-and-play framework that uses representation alignment to bridge distribution gaps between historical inputs and future targets in time series forecasting, achieving superior performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art time series forecasters don't use representation learning techniques like contrastive learning because they show little performance advantage, despite their success in other domains like computer vision and NLP.

Method: TimeAlign learns auxiliary features via a simple reconstruction task and feeds them back to any base forecaster. It's architecture-agnostic and focuses on correcting frequency mismatches between historical inputs and future outputs.

Result: Extensive experiments across eight benchmarks verify TimeAlign's superior performance. The gains primarily come from correcting frequency mismatches, and theoretical justification shows it increases mutual information between learned representations and predicted targets.

Conclusion: TimeAlign serves as a general alignment module for modern deep learning time-series forecasting systems with negligible overhead, demonstrating that explicit representation alignment can provide critical information that bridges distributional gaps.

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [65] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: Privacy-preserving federated learning with differential privacy enables accurate COVID-19 case forecasting at county level while protecting sensitive health data.


<details>
  <summary>Details</summary>
Motivation: Local epidemic response requires detailed data but faces privacy constraints and limited local data for training ML models, necessitating a privacy-preserving collaborative approach.

Method: Federated learning framework with client-level differential privacy, using multilayer perceptron on sliding windows of case counts, with norm-clipped updates and DP noise aggregation.

Result: At moderately strong privacy levels, DP model closely approaches non-DP performance: R²=0.94 vs 0.95 and MAPE=26% in Nov 2020; R²=0.88 vs 0.93 and MAPE=21% in Mar 2022.

Conclusion: Client-level DP-FL provides useful county-level predictions with strong privacy guarantees, enabling privacy-compliant collaboration among health authorities for local forecasting.

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [66] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: A novel Banach-Bregman framework for stochastic optimization that extends beyond Hilbert spaces, enabling faster convergence and better performance in non-Euclidean settings like mirror descent, natural gradient, and language model training.


<details>
  <summary>Details</summary>
Motivation: Existing optimization theory is confined to Hilbert spaces and inner-product frameworks, failing to capture non-Euclidean settings common in modern AI applications such as mirror descent, sparse learning, information geometry, and language model training.

Method: Introduces a Banach-Bregman framework using Bregman projections and Bregman-Fejer monotonicity as a unified template. Establishes super-relaxations (λ > 2) in non-Hilbert settings and provides convergence theorems spanning almost-sure boundedness to geometric rates.

Result: Empirical studies across machine learning (UCI benchmarks), deep learning (Transformer training), reinforcement learning (actor-critic), and large language models (WikiText-2 with distilGPT-2) show up to 20% faster convergence, reduced variance, and enhanced accuracy over classical baselines.

Conclusion: Banach-Bregman geometry serves as a cornerstone unifying optimization theory and practice across core AI paradigms, positioning it as the foundation for next-generation optimization methods.

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [67] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: Novel method using wavelet transforms to convert nanopore current signals into scaleogram images for protein classification, achieving 81% accuracy on 42 peptides and enabling real-time clinical diagnostics.


<details>
  <summary>Details</summary>
Motivation: To enable inexpensive and rapid disease diagnosis through real-time protein classification in clinical settings using nanopore technology, overcoming current signal complexity limitations.

Method: Convert nanopore current signals into scaleogram images via wavelet transforms to capture amplitude, frequency, and time information, then apply machine learning algorithms for classification.

Result: Achieved ~81% classification accuracy on 42 peptides, setting a new state-of-the-art in the field, and demonstrated model transfer techniques for hardware deployment.

Conclusion: This approach represents a significant step toward practical peptide/protein diagnostics at the point of care and paves the way for real-time disease diagnosis using nanopore technology.

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [68] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: Language models linearly encode when information was learned during training, with activations arranging in exact training order when projected into 2D space, enabling accurate temporal classification of learned entities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language models retain temporal information about when specific knowledge was acquired during training, which could reveal how models manage conflicting data and knowledge modifications.

Method: Sequentially fine-tuned Llama-3.2-1B on six disjoint but similar datasets about named entities, then analyzed average activations and trained linear probes to detect training order from model activations.

Result: Activations encode training order linearly - centroids arrange exactly in training sequence on a straight line in 2D projection. Linear probes achieve ~90% accuracy distinguishing early vs late entities, and fine-tuning achieves ~80% accuracy reporting training stage of unseen entities.

Conclusion: Models can differentiate information by acquisition time, with temporal signals not attributable to simple activation magnitude or confidence differences, suggesting significant implications for how models manage conflicting knowledge and respond to knowledge updates.

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [69] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: Bayesian risk-averse RL framework addresses epistemic uncertainty through BRMDP, showing pessimistic underestimation of value functions that decreases with more data, with sub-linear regret bounds established for both RL and CMAB settings.


<details>
  <summary>Details</summary>
Motivation: Address epistemic uncertainty in reinforcement learning due to lack of data by adopting Bayesian risk-averse formulation to account for parameter uncertainty in unknown underlying models.

Method: Derive asymptotic normality characterizing Bayesian risk vs original value function difference, utilize adaptive property in online RL and CMAB with posterior sampling procedures.

Result: Bayesian risk-averse approach pessimistically underestimates original value function (discrepancy increases with risk aversion, decreases with more data), establish sub-linear regret bounds for both conventional and Bayesian risk regret.

Conclusion: Proposed Bayesian risk-averse framework effectively addresses epistemic uncertainty in RL, with theoretical properties verified through numerical experiments showing practical effectiveness.

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [70] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: Study compares neural network architectures and optimizers for EEG classification across frequency bands, finding Adagrad and RMSprop perform best, with CNN excelling at spatial feature extraction.


<details>
  <summary>Details</summary>
Motivation: To investigate optimal neural network architectures and optimizers for EEG-based classification tasks across different frequency bands, and to understand feature importance in neuroimaging classification.

Method: Implemented three neural network architectures (deep dense network, shallow three-layer network, CNN) using TensorFlow and PyTorch, tested with various optimizers (Adagrad, RMSprop, Adadelta, SGD, FTRL) across EEG frequency bands, and used SHAP plots for feature importance analysis.

Result: Adagrad and RMSprop consistently performed well across frequency bands, with Adagrad excelling in beta band and RMSprop in gamma band. CNN showed second highest accuracy with strong spatial feature capture, while deep dense network handled complex patterns well. SHAP revealed nuanced contributions of EEG bands to accuracy.

Conclusion: Optimizer selection, model architecture, and EEG frequency band analysis are crucial for enhancing classifier performance in neuroimaging tasks, with different optimizers and architectures excelling in specific contexts.

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [71] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: The paper introduces Quantile Neural Basis Model that combines interpretability of Quantile Generalized Additive Models with neural network training, achieving comparable performance to existing methods while providing better model interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding feature-conditioned outputs in neural network probabilistic forecasting while maintaining high predictive accuracy.

Method: Leverages shared basis decomposition and weight factorization to incorporate interpretability principles into neural networks without parametric distributional assumptions.

Result: Achieves predictive performance comparable to distributional and quantile regression neural networks on day-ahead electricity price forecasting.

Conclusion: The model provides valuable insights into learned nonlinear mappings from input features to output predictions while maintaining competitive forecasting performance.

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [72] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: Targeted mental health outreach program for high-risk incarcerated individuals shows effectiveness in reducing reincarceration rates and improving outcomes.


<details>
  <summary>Details</summary>
Motivation: Address the cycle of incarceration caused by untreated mental illness, substance dependence, and homelessness in criminal justice systems, particularly impacting communities of color and widening racial disparities.

Method: Collaboration between Johnson County, Kansas and Carnegie Mellon University using predictive modeling to identify high-risk individuals, followed by targeted proactive mental health outreach and field trial evaluation.

Result: Model was highly predictive of new jail bookings (over 50% of highest-risk group reincarcerated within year). Outreach was most effective for highest-risk individuals, showing impacts on mental health utilization, EMS dispatches, and criminal justice involvement.

Conclusion: Targeted, data-driven mental health outreach can effectively break the cycle of incarceration for high-risk individuals and improve multiple outcome measures.

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [73] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: Compositional kernel ridge regression with input reweighting for feature learning, showing noise variable elimination and feature recovery capabilities.


<details>
  <summary>Details</summary>
Motivation: To study feature learning in compositional architectures and understand how different kernels perform in recovering relevant variables while eliminating noise.

Method: Variational formulation of compositional kernel ridge regression with coordinate-wise input reweighting, analyzing global minimizers and stationary points.

Result: Both global minimizers and stationary points discard noise coordinates with Gaussian noise. L1-type kernels (e.g., Laplace) recover nonlinear features, while Gaussian kernels only recover linear ones.

Conclusion: Compositional kernel ridge regression effectively performs feature selection, with L1-type kernels being superior for nonlinear feature recovery compared to Gaussian kernels.

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [74] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: End-to-end AI framework to noninvasively estimate unmeasurable glaucoma parameters (tissue permeability and outflow facility) from sparse routine data, using multi-stage architecture, novel PCDS data generation, and Bayesian uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Address clinical challenge of measuring key glaucoma parameters like trabecular meshwork permeability in vivo, and computational challenge of developing predictive models without ground-truth data due to costly simulations.

Method: Multi-stage AI architecture for functional separation, PCDS data generation strategy to avoid costly simulations (reducing computational time from years to hours), and Bayesian engine for predictive uncertainty quantification.

Result: Excellent agreement with state-of-the-art tonography for outflow facility estimation, precision comparable to direct physical instruments. New permeability biomarker accurately stratifies clinical cohorts by disease risk.

Conclusion: Framework provides generalizable blueprint for solving similar inverse problems in data-scarce, computationally-intensive domains beyond ophthalmology.

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [75] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing is an end-to-end framework that uses graph algorithms and LLM agents to understand circuit structures from netlists and integrates this knowledge into Bayesian optimization for more efficient analog circuit design.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in analog circuit design including shortage of high-quality data, difficulty embedding domain knowledge, inefficiency of traditional black-box optimization, and limitations of existing learning-based methods and LLM approaches.

Method: 1) Use graph algorithms to create hierarchical device-module-stage representation from raw netlists 2) Employ LLM agents in iterative hypothesis-verification-refinement loop with consistency checks 3) Integrate verified insights into Bayesian optimization through LLM-guided initial sampling and trust-region updates

Result: The framework achieves robust circuit understanding directly from netlists and translates this knowledge into optimization gains, improving efficiency while preserving feasibility.

Conclusion: TopoSizing provides an effective end-to-end solution that combines structural analysis with optimization techniques to address fundamental challenges in analog and mixed-signal circuit design automation.

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [76] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: A variational framework formalizes residual-based adaptive strategies in scientific ML by integrating convex residual transformations, linking discretization choices to error metrics and enabling systematic adaptive scheme design.


<details>
  <summary>Details</summary>
Motivation: Residual-based adaptive strategies are widely used but remain largely heuristic, lacking theoretical foundation and principled design approaches.

Method: Introduces a unifying variational framework that integrates convex transformations of the residual, where different transformations correspond to distinct objective functionals (exponential for uniform error, linear for quadratic error).

Result: The framework enables systematic design of adaptive schemes across norms, reduces discretization error through variance reduction, and enhances learning dynamics by improving gradient signal-to-noise ratio. Demonstrates substantial performance gains across optimizers and architectures.

Conclusion: Provides theoretical justification for residual-based adaptivity and establishes a foundation for principled discretization and training strategies in scientific machine learning.

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [77] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: RKTV-INR is a denoising framework that uses implicit neural representations with Runge-Kutta integration and total variation constraints to clean noisy dynamical system data, enabling accurate derivative estimation and system identification.


<details>
  <summary>Details</summary>
Motivation: Measurement noise hampers data-driven modeling of nonlinear dynamical systems, making it difficult to accurately identify governing equations from noisy observations.

Method: Uses implicit neural representation (INR) fitted to noisy data with Runge-Kutta integration and total variation constraints to ensure the reconstructed state follows dynamical system behavior while staying close to original data.

Result: Effective noise suppression, precise derivative estimation via automatic differentiation, and reliable system identification when combined with SINDy for equation recovery.

Conclusion: RKTV-INR successfully denoises dynamical system data, provides accurate derivatives, and enables robust identification of governing equations from noisy measurements.

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [78] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: Defending diffusion models against membership inference attacks using critically-damped higher-order Langevin dynamics with auxiliary variables to corrupt sensitive data early in the diffusion process.


<details>
  <summary>Details</summary>
Motivation: Recent AI advances raise data security concerns, particularly about membership inference attacks where attackers can determine if specific data was used to train the model. While diffusion models are more resistant than other generative models, they remain vulnerable.

Method: Proposes using critically-damped higher-order Langevin dynamics that introduces auxiliary variables and a joint diffusion process. The auxiliary variables mix external randomness to corrupt sensitive input data earlier in the diffusion process.

Result: The defense concept was theoretically investigated and validated on a toy dataset and speech dataset using AUROC curves and FID metric.

Conclusion: The proposed method provides an effective defense mechanism against membership inference attacks for diffusion models by leveraging higher-order dynamics and auxiliary variables to enhance data privacy protection.

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [79] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA is a novel structured pruning method for LLMs that preserves zero-shot accuracy while enabling robust fine-tuning, using Neural Tangent Kernel-based saliency and adaptive sparsity allocation.


<details>
  <summary>Details</summary>
Motivation: Current structured pruning methods for LLMs suffer from significant performance degradation in zero-shot settings and require costly recovery techniques like supervised fine-tuning or adapter insertion.

Method: Uses first-order saliency criterion from Neural Tangent Kernel under Adam optimization, adaptive sparsity allocation across layers and modules (attention vs MLP), and KL divergence-based calibration data selection.

Result: Outperforms existing structured pruning methods on Llama3, Qwen, and T5 models under equivalent sparsity constraints, providing better zero-shot accuracy preservation.

Conclusion: NIRVANA offers a theoretically sound and practical approach to LLM compression that balances immediate performance preservation with fine-tuning capability.

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [80] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: Compute as Teacher (CaT) uses inference-time exploration to generate self-supervised learning signals by synthesizing references from parallel rollouts and optimizing toward them, achieving significant performance improvements on verifiable and non-verifiable tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining learning signals when there is no ground truth available during post-training, by leveraging the model's own exploration at inference time as a source of supervision.

Method: CaT synthesizes a single reference from multiple parallel rollouts using a frozen anchor policy to reconcile omissions and contradictions. It converts this into rewards through: (1) programmatic equivalence for verifiable tasks, and (2) self-proposed rubrics scored by an independent LLM judge for non-verifiable tasks.

Result: CaT improves performance on Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B models (up to +27% on MATH-500; +12% on HealthBench). With reinforcement learning (CaT-RL), further gains are achieved (up to +33% and +30%), with the trained policy surpassing the initial teacher signal.

Conclusion: Compute as Teacher effectively turns extra inference-time compute into valuable supervision signals, outperforming selection methods and demonstrating scalable performance with the number of rollouts, even when all individual rollouts are incorrect.

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [81] [All Models Are Wrong, But Can They Be Useful? Lessons from COVID-19 Agent-Based Models: A Systematic Review](https://arxiv.org/abs/2509.13346)
*Emma Von Hoene,Sara Von Hoene,Szandra Peter,Ethan Hopson,Emily Csizmadia,Faith Fenyk,Kai Barner,Timothy Leslie,Hamdi Kavak,Andreas Zufle,Amira Roess,Taylor Anderson*

Main category: cs.MA

TL;DR: Systematic review of 536 COVID-19 agent-based models found they advanced rapidly but lacked transparency, code sharing, stakeholder engagement, and standardized validation frameworks, limiting their utility for health policy decisions.


<details>
  <summary>Details</summary>
Motivation: To assess the utility and quality of agent-based models (ABMs) developed during the COVID-19 pandemic for informing health policy decisions, given their rapid deployment and potential to capture population heterogeneity.

Method: Systematic review of 536 COVID-19 ABM studies published from 2020-2023, assessed against nine criteria including transparency, re-use, interdisciplinary collaboration, stakeholder engagement, and evaluation practices.

Result: Most models explored interventions (54.85%) rather than forecasting (1.68%). While most described assumptions (91.60%), fewer disclosed limitations (65.11%), shared code (40.86%), or built on existing models (36.38%). Standardized reporting (6.72%) and stakeholder engagement (13.62%) were rare, with only 2.24% having comprehensive validation.

Conclusion: COVID-19 ABMs advanced quickly but lacked transparency, accessibility, and participatory engagement. Stronger standards are needed for ABMs to serve as reliable decision-support tools in future public health crises.

Abstract: The COVID-19 pandemic prompted a surge in computational models to simulate
disease dynamics and guide interventions. Agent-based models (ABMs) are
well-suited to capture population and environmental heterogeneity, but their
rapid deployment raised questions about utility for health policy. We
systematically reviewed 536 COVID-19 ABM studies published from January 2020 to
December 2023, retrieved from Web of Science, PubMed, and Wiley on January 30,
2024. Studies were included if they used ABMs to simulate COVID-19
transmission, where reviews were excluded. Studies were assessed against nine
criteria of model usefulness, including transparency and re-use,
interdisciplinary collaboration and stakeholder engagement, and evaluation
practices. Publications peaked in late 2021 and were concentrated in a few
countries. Most models explored behavioral or policy interventions (n = 294,
54.85%) rather than real-time forecasting (n = 9, 1.68%). While most described
model assumptions (n = 491, 91.60%), fewer disclosed limitations (n = 349,
65.11%), shared code (n = 219, 40.86%), or built on existing models (n = 195,
36.38%). Standardized reporting protocols (n = 36, 6.72%) and stakeholder
engagement were rare (13.62%, n = 73). Only 2.24% (n = 12) described a
comprehensive validation framework, though uncertainty was often quantified (n
= 407, 75.93%). Limitations of this review include underrepresentation of
non-English studies, subjective data extraction, variability in study quality,
and limited generalizability. Overall, COVID-19 ABMs advanced quickly, but
lacked transparency, accessibility, and participatory engagement. Stronger
standards are needed for ABMs to serve as reliable decision-support tools in
future public health crises.

</details>


### [82] [Inject, Fork, Compare: Defining an Interaction Vocabulary for Multi-Agent Simulation Platforms](https://arxiv.org/abs/2509.13712)
*HwiJoon Lee,Martina Di Paola,Yoo Jin Hong,Quang-Huy Nguyen,Joseph Seering*

Main category: cs.MA

TL;DR: Three core operations (inject, fork, compare) for interactive multi-agent LLM simulations that enable causal investigation through timeline manipulation and parallel exploration.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent simulations lack clear interaction and analysis modes, limiting researchers' ability to explore "what if" scenarios and conduct systematic causal investigations.

Method: Defined three core operations: inject (introduce external events during simulation), fork (create independent timeline branches from any timestamp), and compare (parallel observation of multiple branches). Demonstrated through a commodity market simulation with 14 AI agents.

Result: Established a vocabulary that transforms linear simulation workflows into interactive, explorable spaces, enabling researchers to observe divergent outcomes across parallel timelines and study how different interventions lead to distinct emergent behaviors.

Conclusion: These fundamental operations provide a starting point for systematic causal investigation in LLM-based agent simulations, moving beyond passive observation toward active experimentation and enabling more robust "what if" scenario analysis.

Abstract: LLM-based multi-agent simulations are a rapidly growing field of research,
but current simulations often lack clear modes for interaction and analysis,
limiting the "what if" scenarios researchers are able to investigate. In this
demo, we define three core operations for interacting with multi-agent
simulations: inject, fork, and compare. Inject allows researchers to introduce
external events at any point during simulation execution. Fork creates
independent timeline branches from any timestamp, preserving complete state
while allowing divergent exploration. Compare facilitates parallel observation
of multiple branches, revealing how different interventions lead to distinct
emergent behaviors. Together, these operations establish a vocabulary that
transforms linear simulation workflows into interactive, explorable spaces. We
demonstrate this vocabulary through a commodity market simulation with fourteen
AI agents, where researchers can inject contrasting events and observe
divergent outcomes across parallel timelines. By defining these fundamental
operations, we provide a starting point for systematic causal investigation in
LLM-based agent simulations, moving beyond passive observation toward active
experimentation.

</details>
