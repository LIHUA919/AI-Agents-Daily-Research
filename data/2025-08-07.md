<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9 is a runtime governance framework for agentic AI, addressing emergent risks with real-time controls like risk indexing, telemetry, and containment strategies.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems introduce unpredictable behaviors and risks not covered by pre-deployment governance, necessitating a runtime solution.

Method: MI9 integrates six components: agency-risk index, telemetry capture, authorization monitoring, FSM conformance engines, drift detection, and containment strategies.

Result: MI9 provides systematic, real-time governance for agentic AI, covering gaps left by traditional approaches.

Conclusion: MI9 establishes a technical foundation for safe, scalable deployment of agentic AI systems.

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [2] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL is a MARL framework that trains task agents to perform primary functions and resist threats, improving safety and performance without external guards.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of standalone safety agents in MAS, which are prone to single-point failure and high costs.

Method: Uses adversarial training with evolutionary search and parameter-sharing MARL to co-evolve attackers and defenders.

Result: Reduces attack success rates by 22% and boosts accuracy by 5% on reasoning tasks.

Conclusion: Evo-MARL effectively internalizes safety mechanisms, enhancing both robustness and utility in MAS.

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [3] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF introduces a turn-based, multi-agent framework for optimizing interdependent solver components in combinatorial optimization, outperforming single-strategy approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on single-component optimization, missing broader innovation opportunities in solver design for NP-hard problems.

Method: MOTIF uses Monte Carlo Tree Search with two LLM agents taking turns to optimize components, leveraging competitive and cooperative dynamics.

Result: MOTIF consistently outperforms state-of-the-art methods across multiple combinatorial optimization domains.

Conclusion: Turn-based, multi-agent prompting shows promise for fully automated solver design, broadening innovation in combinatorial optimization.

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [4] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench is introduced to evaluate LLMs' ability to infer symbolic structures from time series data, revealing strengths and limitations when combined with genetic programming.


<details>
  <summary>Details</summary>
Motivation: The challenge of uncovering hidden symbolic laws from time series data, a longstanding goal in science and AI, remains underexplored for LLMs.

Method: A benchmark (SymbolBench) is created for three tasks: symbolic regression, Boolean network inference, and causal discovery. A framework integrates LLMs with genetic programming for closed-loop symbolic reasoning.

Result: Empirical results show LLMs' strengths and limitations, emphasizing the need for domain knowledge, context alignment, and structured reasoning.

Conclusion: Combining LLMs with genetic programming improves symbolic reasoning, but further integration of domain knowledge and context is needed for scientific discovery.

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [5] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: EmoAgent exploits emotional cues to hijack MLRM reasoning, revealing hidden risks like harmful reasoning masked by safe outputs, despite visual risk recognition.


<details>
  <summary>Details</summary>
Motivation: MLRMs for human-centric service are vulnerable to emotional manipulation, overriding safety protocols under high emotional intensity.

Method: Proposes EmoAgent, an adversarial framework using exaggerated affective prompts to test MLRM safety. Introduces metrics (RRSS, RVNR, RAIC) to quantify risks.

Result: EmoAgent exposes persistent high-risk failure modes, showing emotional misalignment in model safety behavior.

Conclusion: Emotional cognitive misalignments in MLRMs pose significant safety risks, necessitating improved safeguards.

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [6] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: The paper proposes combining knowledge-based and data-driven methods for ad hoc teamwork in AI agents to improve collaboration without prior coordination.


<details>
  <summary>Details</summary>
Motivation: Current data-driven methods for ad hoc teamwork require large labeled datasets, lack transparency, and struggle with rapid knowledge updates and scalability.

Method: The architecture uses non-monotonic logical reasoning with prior knowledge, learned behavior models, and anticipated future goals from a foundation model.

Result: Evaluated in VirtualHome, the method shows promise for effective collaboration in dynamic environments.

Conclusion: The hybrid approach addresses limitations of purely data-driven methods, enhancing scalability and adaptability in ad hoc teamwork.

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [7] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: The paper introduces Cognition Forest and Galaxy, a framework for proactive, privacy-preserving, and self-evolving IPAs, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored proactive behaviors in IPAs and unify cognitive architecture with system design.

Method: Proposes Cognition Forest for cognitive modeling alignment and Galaxy framework for multidimensional interactions. Implements KoRa and Kernel agents.

Result: Galaxy outperforms state-of-the-art benchmarks, validated by ablation studies and real-world cases.

Conclusion: Galaxy successfully integrates cognitive and system design, enabling proactive, privacy-preserving, and self-evolving IPAs.

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [8] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent is a self-evolving framework for computer-use agents (CUAs) that autonomously learns and adapts to novel software without human annotations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models struggle with novel and specialized software due to reliance on human-labeled data, limiting their adaptability.

Method: SEAgent uses experiential learning, a World State Model for trajectory assessment, and a Curriculum Generator for task progression. It employs adversarial imitation and Group Relative Policy Optimization (GRPO) for policy updates, along with a specialist-to-generalist training strategy.

Result: SEAgent improves success rates by 23.2% (from 11.3% to 34.5%) over UI-TARS in five novel software environments.

Conclusion: SEAgent enables autonomous evolution of CUAs, outperforming specialist ensembles and demonstrating significant adaptability in unfamiliar software environments.

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


### [9] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent is an uncertainty-aware GUI agent that reduces input redundancy and decision ambiguity in mobile tasks using adaptive perception, component recommendation, and interactive user feedback.


<details>
  <summary>Details</summary>
Motivation: GUI agents struggle with input redundancy and decision ambiguity, limiting their effectiveness in automating mobile tasks.

Method: RecAgent uses a component recommendation mechanism to reduce perceptual uncertainty and an interactive module for decision uncertainty, integrating these into a unified framework.

Result: The approach is validated through extensive experiments, and a dataset (ComplexAction) is introduced for evaluation.

Conclusion: RecAgent effectively addresses uncertainty in GUI navigation, with its dataset and code made publicly available.

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [10] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: Proposes Self-Evolution Agent (SEA) for computer tasks, using innovative data generation, reinforcement learning, and model enhancement to outperform similar-sized models.


<details>
  <summary>Details</summary>
Motivation: Current computer-use agents underperform; SEA aims to bridge this gap with efficient methods.

Method: Automatic verifiable trajectory generation, step-wise reinforcement learning, and model enhancement merging grounding and planning.

Result: SEA achieves superior performance with only 7B parameters, rivaling larger models.

Conclusion: SEA demonstrates potential for practical use; plans to open-source models and code.

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [11] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: Career goal-based AI personalization in learning systems boosts engagement, satisfaction, and efficiency, as shown in a study with 4,000 learners.


<details>
  <summary>Details</summary>
Motivation: To enhance learner engagement and motivation by aligning educational content with individual career goals using AI.

Method: Mixed-methods experiment with 4,000 learners, comparing career goal-tailored content (GenAI) to standard content.

Result: Increased session duration, higher satisfaction, reduced study time, and qualitative feedback on motivation and practicality.

Conclusion: AI-driven personalization bridges academic knowledge and workplace needs, improving learning outcomes.

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [12] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT enhances LLMs' performance in complex reasoning tasks like math and code generation by integrating knowledge graphs and executable code, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs in complex reasoning tasks such as mathematical reasoning and code generation.

Method: Proposes KGA-ECoT, a framework using knowledge graphs (GraphRAG) for precise knowledge retrieval and executable code for verification.

Result: Significant accuracy improvements (several to over ten percentage points) on mathematical reasoning benchmarks.

Conclusion: KGA-ECoT is a robust, generalizable framework for complex mathematical reasoning, validated by improved performance and precision.

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [13] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR is a self-refining framework for LLMs to improve geospatial predictions by embedding geographic principles and iterative agentic reasoning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in LLMs like spatial inconsistency, multi-hop reasoning, and geographic bias for better geospatial applications.

Method: Decomposes reasoning into three agents: variable-selection, point-selection, and refine, iteratively improving predictions using spatial dependencies.

Result: Shows consistent improvements over standard prompting in tasks like property estimation and socioeconomic prediction.

Conclusion: Incorporating geostatistical priors and structured reasoning enhances LLMs' geospatial accuracy and equity.

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [14] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: Semantic entropy measures grading uncertainty by analyzing variability in GPT-4 explanations, correlating with human grader disagreement and improving AI grading transparency.


<details>
  <summary>Details</summary>
Motivation: Current automated grading systems lack transparency in uncertain or contentious grading decisions, necessitating a measure like semantic entropy.

Method: Semantic entropy is calculated by clustering GPT-4-generated rationales via entailment-based similarity and computing entropy over these clusters.

Result: Semantic entropy correlates with human grader disagreement, varies by subject, and increases in interpretive tasks.

Conclusion: Semantic entropy serves as an interpretable uncertainty signal, enhancing trust in AI-assisted grading.

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [15] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: A compositional on-the-fly synthesis framework for LTLf over finite traces integrates DFA construction and game-solving, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for DFA construction in LTLf synthesis are either compositional or incremental, but neither dominates. A hybrid approach is needed.

Method: Introduces a framework combining compositional and on-the-fly synthesis, pruning intermediate results to simplify compositions and detect unrealizability early.

Result: Outperforms state-of-the-art solvers, handling instances others cannot. Both composition variants (pruning before/during) show unique benefits.

Conclusion: The framework effectively balances DFA construction and game-solving, offering practical advantages for large conjunctions of LTLf formulas.

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [16] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE is an agent-based framework for Open-domain Knowledge Graph Completion (KGC) that uses iterative retrieval and multi-step reasoning to dynamically build knowledge graphs, outperforming existing methods by up to 13.7% without training.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods struggle with emerging entities due to reliance on pretrained models or single-step retrieval, lacking up-to-date information.

Method: AgREE combines iterative retrieval actions and multi-step reasoning to dynamically construct knowledge graph triplets.

Result: AgREE outperforms existing methods by up to 13.7%, especially for emerging entities, and introduces a new benchmark for KGC.

Conclusion: Agent-based reasoning with strategic retrieval effectively maintains up-to-date knowledge graphs in dynamic environments.

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [17] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD is a circuit-aware SAT solver using GNNs to compute gate-level probabilities, improving CDCL heuristics and outperforming CNF-based methods by 10x in speed.


<details>
  <summary>Details</summary>
Motivation: Standard CSAT solving discards structural info, leading to inefficiency. CASCAD aims to leverage circuit-level insights for better performance.

Method: Uses GNNs to compute gate-level conditional probabilities, guiding CDCL heuristics (variable phase selection and clause management).

Result: Achieves up to 10x faster solving times and 23.5% runtime reduction with probability-guided clause filtering.

Conclusion: Preserving circuit-level insights enhances SAT solver efficiency, offering a foundation for future EDA tool improvements.

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [18] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio is a framework for efficient biomedical AI alignment, ensuring multi-capability integration with safety and performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of integrating multiple capabilities in biomedical AI without interference, ensuring safe deployment.

Method: Uses Medical Knowledge Grounded Synthetic Generation (MKGSG) and Capability Aware Group Relative Policy Optimization for orthogonal gradient spaces and hybrid reward weighting.

Result: Achieves state-of-the-art performance in domain expertise, reasoning, instruction following, and integration, with real-world benefits like cost reduction and improved accuracy.

Conclusion: Provides a principled approach for biomedical AI alignment, balancing efficiency, safety, and reliability.

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [19] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: The paper introduces a framework for creating customizable POMDP environments to rigorously evaluate memory-augmented RL, addressing the lack of controllability in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for memory-augmented RL lack fine-grained control over challenge levels, limiting detailed evaluation. Synthetic environments offer more precise manipulation of dynamics.

Method: The study proposes a theoretical framework (MDS, transition invariance) and a methodology using linear process dynamics, state aggregation, and reward redistribution to design POMDPs with predefined properties.

Result: Empirically validated POMDP environments with increasing difficulty levels were created, clarifying challenges and providing guidelines for memory-augmented RL.

Conclusion: The work aids in analyzing and designing POMDP environments and supports empirical selection of memory models in RL tasks.

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [20] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: DRN improves logical reasoning in LLMs by minimizing uncertainty, outperforming baselines by 15.2% and boosting Mistral-7B accuracy to 80%.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' failure in logical reasoning due to cognitive traps where semantic heuristics conflict with evidence.

Method: Introduces Deliberative Reasoning Network (DRN), reframing reasoning as uncertainty minimization, tracking belief states, and quantifying epistemic uncertainty.

Result: DRN achieves 15.2% improvement over baselines, boosts Mistral-7B accuracy to 80%, and improves TruthfulQA by 23.6% zero-shot.

Conclusion: DRN is a foundational, verifiable System 2 component for trustworthy AI, demonstrating transferable reasoning principles.

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [21] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay is a benchmark for evaluating multi-modal agentic models, revealing their strengths in memory tasks but weaknesses in reasoning and planning due to brittle fusion mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for multi-modal models lack dynamic, interactive testing, ignoring auditory and temporal cues, creating a gap in assessing true cross-modal reasoning.

Method: OmniPlay introduces five game environments to test synergy and conflict scenarios, evaluating six leading omni-modal models.

Result: Models show superhuman memory performance but fail in reasoning and planning, with performance degrading under modality conflict. Removing sensory inputs can paradoxically improve results.

Conclusion: Robust AGI requires focus on synergistic fusion, not just scaling. OmniPlay provides a platform for such research.

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [22] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: The paper proposes a framework (SLP-tests) to empirically assess AI consciousness by evaluating subjective experience as a functional interface.


<details>
  <summary>Details</summary>
Motivation: The contentious nature of defining and operationalizing AI consciousness drives the need for an empirical framework.

Method: Introduces SLP-tests (Subjective-linguistic, Latent-emergent, Phenomenological-structural) using category theory to model interface representations.

Result: The framework reframes consciousness as a functional interface to relational entities, enabling empirical tests.

Conclusion: SLP-tests provide a tractable approach to evaluate AI consciousness without relying on intrinsic properties.

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [23] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG introduces reinforcement learning for GUI visual grounding, outperforming supervised fine-tuning with fewer samples.


<details>
  <summary>Details</summary>
Motivation: The need for efficient alternatives to costly supervised fine-tuning of multimodal models for GUI visual grounding.

Method: Decomposes reinforcement fine-tuning (RFT), introduces Adversarial KL Factor for stabilization, and optimizes training configurations.

Result: GuirlVG achieves significant improvements over SFT methods with far fewer samples (7.7% on ScreenSpot, 17.2% on ScreenSpotPro, 91.9% on ScreenSpotV2).

Conclusion: Reinforcement learning-based RFT is a viable and efficient alternative to SFT for GUI visual grounding.

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [24] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Pil*

Main category: cs.AI

TL;DR: D2Snap, a DOM downsampling algorithm, matches GUI snapshot performance (67% vs. 65%) and outperforms it by 8% with higher token input, leveraging DOM hierarchy for LLMs.


<details>
  <summary>Details</summary>
Motivation: Current web agents rely on GUI snapshots (e.g., screenshots) due to LLMs' better visual input handling, but DOM snapshots (HTML-like) are a promising alternative. However, large token sizes hinder their use.

Method: Proposes D2Snap, a DOM downsampling algorithm, evaluated using GPT-4o on tasks from Online-Mind2Web dataset. Compares DOM snapshots (downsampled) with GUI snapshots.

Result: D2Snap matches GUI snapshot success rate (67% vs. 65%) at similar token size (1e3) and outperforms it by 8% with higher token input. DOM hierarchy is a strong UI feature for LLMs.

Conclusion: D2Snap enables efficient DOM snapshot use in web agents, matching or outperforming GUI snapshots, and highlights DOM hierarchy's value for LLMs.

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [25] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct is a tool for collecting scaffolding dialogues by simulating novice instructors via LLMs, with human experts providing feedback. It produces pedagogically rich dialogues without real novices, and fine-tuned models outperform GPT-4o in instructional quality.


<details>
  <summary>Details</summary>
Motivation: High-quality instructional dialogues are scarce due to privacy and vulnerability concerns, limiting AI development for teaching and learning.

Method: SimInstruct uses LLMs to simulate novice instructors with varied traits, while human experts provide multi-turn feedback, creating realistic dialogues.

Result: Dialogues show comparable pedagogical relevance to real mentoring. Fine-tuned LLaMA outperforms GPT-4o in instructional quality, highlighting GPT-4o's limitations.

Conclusion: SimInstruct offers a scalable solution for collecting scaffolding dialogues, improving AI instructional systems and expert insights.

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [26] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: The paper introduces MERA, a framework to regulate reasoning in Large Reasoning Models (LRMs) by decoupling reasoning and control, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs exhibit uncontrolled reasoning behaviors (e.g., overthinking), leading to computational inefficiency. Current models lack intrinsic regulatory mechanisms.

Method: MERA decouples reasoning and control, uses takeover-based data construction, supervised fine-tuning, and Control-Segment Policy Optimization (CSPO).

Result: Experiments show MERA improves reasoning efficiency and accuracy in LRMs.

Conclusion: MERA effectively regulates reasoning processes, addressing inefficiencies in LRMs.

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [27] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: A survey of OS Agents, AI assistants leveraging (M)LLMs to automate tasks via OS interfaces, covering fundamentals, methodologies, evaluation, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To advance AI assistants like J.A.R.V.I.S by exploring OS Agents' capabilities and frameworks.

Method: Comprehensive survey of OS Agents, detailing components, methodologies, and evaluation protocols.

Result: Identifies key challenges (e.g., safety, privacy) and future research directions (e.g., personalization).

Conclusion: Consolidates OS Agents research, offering insights for academia and industry, with an open-source repository for ongoing innovation.

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [28] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: A novel interpretable and explainable method for bias detection in AI systems, leveraging debates about bias based on protected features and neighbourhood data, outperforms baselines in performance and transparency.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in AI systems is crucial to prevent systematic disadvantages, but existing fairness methods often lack transparency, which is essential for human-oriented fairness.

Method: The method uses formal and computational argumentation to debate biases within and across neighbourhoods, based on protected features.

Result: The method shows strong performance against baselines and excels in interpretability and explainability.

Conclusion: The proposed method effectively combines fairness, interpretability, and explainability, making it a valuable tool for bias detection in AI.

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [29] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: The paper introduces SID, a benchmark to evaluate LLMs' higher-order guidance in interdisciplinary STEM dialogues, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scalable expert guidance in interdisciplinary STEM education and assess LLMs' potential for guided instruction.

Method: Developed SID: a benchmark with 10,000 dialogue turns, a pedagogical annotation schema, and new metrics (e.g., X-SRG).

Result: State-of-the-art LLMs struggle to facilitate effective guided dialogues for knowledge integration and transfer.

Conclusion: SID is a valuable tool for advancing pedagogically-aware LLMs in STEM education.

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [30] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: ConfProBench is introduced to evaluate the reliability of step-level confidence scores in multimodal process judges (MPJs) using adversarial perturbations and novel metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MPJs overlook the reliability of confidence scores, limiting their effectiveness in assessing reasoning correctness.

Method: ConfProBench uses three adversarial perturbations (Synonym Substitution, Syntactic Transformation, Image Perturbation) and three metrics (CRS, CSS, CCS) to evaluate MPJs.

Result: Experiments on 14 MLLMs reveal limitations in current MPJs' confidence performance, providing baselines for future research.

Conclusion: ConfProBench addresses a critical gap in evaluating MPJs, offering a systematic approach to improve confidence score reliability.

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [31] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: The paper introduces MAGRPO, a multi-agent reinforcement learning method, to optimize LLM collaboration by addressing coordination challenges in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing LLM fine-tuning frameworks lack optimization for coordination, relying on complex individual rewards. The paper aims to improve collaboration in multi-agent LLM systems.

Method: The authors model LLM collaboration as a cooperative MARL problem and propose MAGRPO, a multi-agent, multi-turn algorithm, building on RL and MARL techniques.

Result: Experiments in LLM writing and coding show MAGRPO enables efficient, high-quality collaboration among agents.

Conclusion: MAGRPO successfully addresses LLM coordination challenges and opens avenues for applying other MARL methods to LLMs.

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon framework improves affective model transfer from lab to real-world by combining supervised contrastive learning and privileged information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of transferring affective models from controlled lab settings to unpredictable real-world environments.

Method: Introduces Privileged Contrastive Pretraining (PriCon), using supervised contrastive learning and Learning Using Privileged Information (LUPI).

Result: PriCon outperforms LUPI and end-to-end models, matching performance of models with full modality access.

Conclusion: PriCon bridges the gap between lab and real-world affective modeling, offering a scalable solution.

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [33] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C is a novel trajectory compression framework that outperforms existing methods in compression ratio and fidelity, especially for 3D trajectories.


<details>
  <summary>Details</summary>
Motivation: Efficient compression of trajectory data is needed, but existing methods like line simplification often ignore time synchronization, motion continuity, and higher dimensions.

Method: PILOT-C integrates frequency-domain physics modeling with error-bounded optimization, compressing each spatial axis independently.

Result: PILOT-C achieves a 19.2% better compression ratio and 32.6% lower error than CISED-W, and 49% better compression than SQUISH-E for 3D trajectories.

Conclusion: PILOT-C is a superior trajectory compression framework, especially for multi-dimensional data, with significant improvements in performance.

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [34] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutirrez SanRomn,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind is a generative model for CXR diagnosis using interleaved reasoning and curriculum-based reinforcement learning, outperforming existing models by 25.1%.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models lack verifiable reasoning supervision, leading to inefficiencies in multi-task CXR diagnosis.

Method: Uses CX-Set dataset and CuRL-VPR for interleaved reasoning, optimized via Group Relative Policy Optimization.

Result: Achieves 25.1% improvement over comparable models and excels in real-world clinical evaluations.

Conclusion: CX-Mind enhances diagnostic accuracy and interpretability, validated by expert evaluations.

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [35] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: The paper introduces Latent Knowledge Scalpel (LKS), a method for editing large-scale factual information in LLMs without compromising their general capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs often retain inaccurate or outdated information, and existing editing methods struggle with large-scale edits while maintaining model performance.

Method: LKS manipulates latent knowledge via a lightweight hypernetwork to enable precise, large-scale entity editing in LLMs.

Result: Experiments on Llama-2 and Mistral show LKS effectively edits up to 10,000 entities while preserving model capabilities.

Conclusion: LKS offers a scalable solution for knowledge editing in LLMs, balancing precision and model integrity.

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [36] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost is a multimodal gradient boosting framework for glaucoma risk prediction, combining clinical features, fundus images, and expert texts, achieving 98.71% accuracy with interpretable results.


<details>
  <summary>Details</summary>
Motivation: Early and accurate glaucoma detection is crucial to prevent vision loss, but existing methods lack interpretability and rely on unimodal data.

Method: GlaBoost integrates structured clinical features, fundus image embeddings, and expert-curated textual descriptions using a pretrained convolutional encoder and transformer-based language model, fused via an enhanced XGBoost model.

Result: GlaBoost achieves 98.71% validation accuracy, outperforming baselines, with clinically consistent feature importance (e.g., cup-to-disc ratio, rim pallor).

Conclusion: GlaBoost provides a transparent, scalable solution for interpretable glaucoma diagnosis, extendable to other ophthalmic disorders.

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [37] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: Proposes LRTuckerRep, a novel model combining low-rank and smoothness priors for multi-dimensional data completion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing methods (computational expense of low-rank, manual tuning in smoothness-based) for multi-dimensional data completion.

Method: Introduces LRTuckerRep, unifying low-rank (via self-adaptive weighted nuclear norm and sparse Tucker core) and smoothness (via parameter-free Laplacian regularization) priors. Develops two iterative algorithms with convergence guarantees.

Result: LRTuckerRep achieves superior accuracy and robustness in image inpainting and traffic data imputation, especially under high missing rates.

Conclusion: LRTuckerRep effectively combines global and local priors, offering a robust solution for multi-dimensional data completion.

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [38] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: A framework called LLMPrior automates prior distribution specification in Bayesian inference using LLMs, ensuring valid, tractable priors. It extends to multi-agent systems with Fed-LLMPrior for robust aggregation.


<details>
  <summary>Details</summary>
Motivation: Manual prior elicitation in Bayesian inference is subjective and unscalable, necessitating automation.

Method: LLMPrior couples LLMs with tractable generative models (e.g., Gaussian Mixture Models) to translate unstructured data into valid priors. Fed-LLMPrior aggregates decentralized priors using Logarithmic Opinion Pooling.

Result: The framework produces valid, tractable priors and robustly aggregates them in multi-agent settings.

Conclusion: LLMPrior lowers the barrier to Bayesian modeling by automating and scaling prior specification.

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [39] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: The paper addresses the sim-to-real gap in RL by proposing an online distributionally robust RL algorithm for unknown environments, ensuring worst-case performance optimization with sublinear regret guarantees.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap in RL leads to underperformance in real-world deployments due to mismatched training and deployment conditions. Existing methods rely on impractical assumptions like generative models or broad offline datasets.

Method: The study introduces an online distributionally robust RL algorithm for unknown environments, focusing on $f$-divergence-based uncertainty sets (e.g., Chi-Square and KL divergence). It ensures computational efficiency and sublinear regret.

Result: The proposed algorithm achieves sublinear regret guarantees and is validated through extensive experiments, demonstrating robustness and efficiency across diverse environments.

Conclusion: The work provides a near-optimal solution for online distributionally robust RL, addressing the sim-to-real gap with theoretical and empirical validation.

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [40] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: GTPO improves GRPO by addressing conflicting gradient updates and policy collapse, enhancing stability and performance without KL-divergence regularization.


<details>
  <summary>Details</summary>
Motivation: GRPO has limitations like conflicting gradient updates and policy collapse, which degrade model performance.

Method: GTPO identifies conflict tokens, skips negative updates, amplifies positive ones, and filters high-entropy completions.

Result: GTPO outperforms GRPO on benchmarks (GSM8K, MATH, AIME 2024) with greater stability.

Conclusion: GTPO provides a more stable and effective policy optimization strategy than GRPO.

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [41] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: The paper introduces U-PINet, a physics-informed deep learning framework for efficient and accurate electromagnetic scattering modeling, addressing limitations of traditional solvers and pure data-driven methods.


<details>
  <summary>Details</summary>
Motivation: Overcoming the computational challenges and lack of physical constraints in traditional EM scattering modeling methods, while improving efficiency and generalization.

Method: U-PINet uses a hierarchical, physics-informed neural network architecture with multiscale processing and sparse graph representation to model near- and far-field interactions.

Result: U-PINet accurately predicts surface currents, matches traditional solver accuracy, reduces computational time, and outperforms conventional deep learning baselines.

Conclusion: U-PINet is a viable solution for efficient, physically consistent EM scattering modeling, validated by its performance in radar cross-section prediction tasks.

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [42] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: A Physics-Informed Neural Network (PINN) is proposed to estimate heat flux in the EAST nuclear fusion device, offering 40x faster computation than FEM while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional FEM is inefficient for real-time simulations in nuclear fusion devices. AI-powered methods like PINN can improve speed and accuracy.

Method: The PINN uses spatial coordinates, time stamps, and heat conduction equations to compute losses. Data-driven sampling enhances predictive capability.

Result: The PINN matches FEM accuracy and achieves 40x faster computation under uniform and non-uniform heating conditions.

Conclusion: PINN is a viable alternative to FEM for real-time heat flux estimation in fusion devices, with significant efficiency gains.

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [43] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Hauer,Felix Biemann*

Main category: cs.LG

TL;DR: SoilNet is a multimodal multitask model for soil horizon classification, addressing challenges like hierarchical labels and data imbalance by integrating image data and metadata in a structured pipeline.


<details>
  <summary>Details</summary>
Motivation: Soil horizon classification is critical for soil health monitoring, impacting agriculture, food security, and climate resilience, but remains challenging due to multimodal, multitask, and hierarchical label complexities.

Method: SoilNet uses a modular pipeline: predicts depth markers, segments soil profiles, extracts horizon-specific features, and predicts labels using a graph-based hierarchical representation.

Result: The method effectively handles large, imbalanced, and hierarchically structured labels, demonstrated on a real-world soil profile dataset.

Conclusion: SoilNet provides a robust solution for complex soil horizon classification, with potential applications in agriculture and environmental monitoring.

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [44] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtrik*

Main category: cs.LG

TL;DR: Bernoulli-LoRA introduces a probabilistic framework for parameter-efficient fine-tuning, unifying and extending existing LoRA methods with theoretical guarantees and practical validation.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of LoRA-based PEFT methods and provide a unified, theoretically tractable framework.

Method: Introduces a probabilistic Bernoulli mechanism for matrix updates, analyzing variants like Bernoulli-LoRA-GD, SGD, and others under non-convex and convex non-smooth assumptions.

Result: Convergence guarantees for multiple variants and empirical validation of practical efficacy across tasks.

Conclusion: Bernoulli-LoRA advances PEFT by combining theoretical rigor with practical effectiveness, bridging a gap in the field.

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [45] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO is a scalable neural network-based blackbox optimization method that avoids model uncertainty estimation, outperforming baselines in efficiency and runtime.


<details>
  <summary>Details</summary>
Motivation: Address scalability and computational challenges of Bayesian Optimization (BO) and NN-based BO in high-dimensional spaces.

Method: SNBO uses separate criteria for exploration and exploitation, adaptively controlling the sampling region without relying on model uncertainty estimation.

Result: SNBO achieves better function values than baselines, reduces function evaluations by 40-60%, and cuts runtime significantly.

Conclusion: SNBO is a highly efficient and scalable alternative to traditional BO and NN-based methods for high-dimensional optimization.

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [46] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: The paper introduces DP-NCB, a bandit algorithm ensuring both privacy and fairness, achieving optimal Nash regret while maintaining differential privacy.


<details>
  <summary>Details</summary>
Motivation: Address the gap in simultaneously achieving privacy and fairness in bandit algorithms, crucial for socially sensitive applications.

Method: Proposes DP-NCB, a framework ensuring differential privacy and optimal Nash regret, applicable under global and local privacy models.

Result: DP-NCB achieves order-optimal Nash regret and maintains privacy, outperforming baselines in simulations.

Conclusion: DP-NCB provides a principled solution for privacy-preserving and fair bandit algorithms, suitable for high-stakes applications.

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [47] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: A trainable-by-parts surrogate model (VAE-DNN) for solving nonlinear PDEs, offering independent training of components for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce training time and energy while improving accuracy in solving forward and inverse nonlinear PDEs compared to existing models like FNO and DeepONet.

Method: Uses an encoder, neural network, and decoder trained independently via VAEs for input and solution spaces.

Result: VAE-DNN outperforms FNO and DeepONet in efficiency and accuracy for nonlinear diffusion equations.

Conclusion: VAE-DNN is a promising alternative for solving PDEs with superior performance and reduced computational costs.

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [48] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: A spatio-temporal framework using crowdsourced KPIs and regulatory data improves spectrum demand prediction, outperforming traditional ITU models.


<details>
  <summary>Details</summary>
Motivation: Accurate spectrum demand prediction is essential for fair allocation, regulatory planning, and supporting emerging technologies like 5G, 6G, and IoT.

Method: The framework leverages crowdsourced KPIs and regulatory datasets, incorporating feature engineering, correlation analysis, and transfer learning.

Result: The method achieves superior accuracy and generalizability, outperforming ITU benchmarks with realistic, actionable predictions.

Conclusion: The framework is a robust tool for policymakers to enhance spectrum management and planning.

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [49] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: The paper proposes an information-theoretic method for intelligent data subsampling in offline learning from data streams, focusing on reducing uncertainty in predictions. It outperforms a prior technique and emphasizes careful model design for reliable performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of capturing relevant information from data streams while managing computational costs motivates the exploration of intelligent data subsampling methods.

Method: The study employs an information-theoretic approach centered on reducing uncertainty in downstream predictions for offline learning from data streams.

Result: The proposed prediction-oriented method outperforms a previously proposed information-theoretic technique on two widely studied problems.

Conclusion: Reliably achieving strong performance in practice requires careful model design, as demonstrated by the empirical results.

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [50] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE, a sparse intelligent curation framework, uses MaxEnt sampling to train models with less data, improving accuracy and reducing energy consumption by up to 38x.


<details>
  <summary>Details</summary>
Motivation: Efficient training is needed due to the end of Moore's law and Dennard scaling, prompting exploration of intelligent subsampling to reduce data volume.

Method: Developed SICKLE with MaxEnt sampling, compared it with random and phase-space sampling on turbulence DNS datasets, and evaluated scalability on Frontier.

Result: Subsampling as preprocessing improved model accuracy and reduced energy consumption significantly, with up to 38x reductions.

Conclusion: Intelligent subsampling (SICKLE) is effective for efficient learning, offering accuracy gains and energy savings.

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [51] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: A novel RL framework for T1DM combines impulse and switching control to manage insulin delivery, improving blood glucose control from 22.4% to 10.8% violations.


<details>
  <summary>Details</summary>
Motivation: Managing physiological variables in chronic conditions like T1DM is challenging due to delayed and heterogeneous treatment effects. RL can personalize treatment but needs better handling of temporal dynamics.

Method: The framework uses a constrained Markov decision process with physiological state features, unifying impulse control (fast-acting) and switching control (longer-acting) for realistic policy learning.

Result: Empirical tests show reduced blood glucose violations from 22.4% to 10.8%, with theoretical convergence guarantees.

Conclusion: The work lays a foundation for safe, temporally-aware RL in healthcare, though not yet ready for clinical use.

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [52] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: A hybrid model combining multi-task learning with a recurrent neural network improves grape phenology prediction, outperforming traditional biophysical and deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Accurate grape phenology prediction is crucial for vineyard management, but existing methods lack precision due to sparse datasets.

Method: Proposes a hybrid approach using multi-task learning and a recurrent neural network to parameterize a differentiable biophysical model.

Result: Outperforms conventional biophysical models and baseline deep learning in predicting phenological stages and other crop variables.

Conclusion: The hybrid approach enhances prediction accuracy and robustness by leveraging shared learning across cultivars while preserving biological structure.

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [53] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon Len Krug,Klaus-Robert Mller,Grgoire Montavon*

Main category: cs.LG

TL;DR: The paper reveals a hidden neural network structure in distance-based classifiers, enabling Explainable AI techniques like LRP, and demonstrates its advantages over baselines through evaluations and use cases.


<details>
  <summary>Details</summary>
Motivation: To enhance the explainability of distance-based classifiers (e.g., k-nearest neighbors, SVMs) by uncovering latent neural network structures, making them compatible with Explainable AI methods.

Method: Uncover a hidden neural network structure in distance-based classifiers (linear detection units + nonlinear pooling layers) and apply Explainable AI techniques like LRP.

Result: Quantitative evaluations show the novel explanation approach outperforms baselines, and practical use cases validate its usefulness.

Conclusion: The approach successfully bridges the gap between distance-based classifiers and Explainable AI, improving interpretability without sacrificing performance.

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [54] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: Combining active and transfer learning for anomaly detection in cross-domain time-series data shows limited performance improvement, with clustering often unnecessary and active learning benefits slowing over time.


<details>
  <summary>Details</summary>
Motivation: To explore the interaction between active learning and transfer learning for anomaly detection in cross-domain time-series data and evaluate their combined effectiveness.

Method: Combines active learning and transfer learning, tests clustering impact, and evaluates performance with distinct sampling and testing pools.

Result: Best performance without clustering; active learning improves performance linearly but slower than literature suggests; transfer learning performance tails off with more target points.

Conclusion: Active learning is effective but yields diminishing returns; clustering is often unnecessary, and performance improvement is linear and slower than expected.

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [55] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: The paper proposes a combined manifold and machine learning approach to bridge microscopic and macroscopic crowd dynamics, using agent-based simulations to learn a surrogate model for efficient and accurate crowd modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of connecting microscopic and macroscopic scales in crowd dynamics for better numerical analysis, optimization, and control.

Method: A four-stage approach: 1) Derive macroscopic fields from microscopic data using KDE. 2) Map to latent space via manifold learning and POD. 3) Learn reduced-order surrogate models (ROMs) with LSTMs and MVARs. 4) Reconstruct dynamics in high-dimensional space.

Result: High accuracy, robustness, and generalizability in modeling crowd dynamics, with mass conservation ensured.

Conclusion: The framework provides an effective solution operator for macroscopic PDEs, enabling fast and accurate crowd simulations from agent-based data.

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [56] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: Transformers can learn algorithms from context via next-token prediction, surpassing memorization when size and training data exceed thresholds.


<details>
  <summary>Details</summary>
Motivation: To explore if transformers can learn transition probabilities from context rather than memorizing training patterns.

Method: Train transformers on Markov chains with random transition matrices, varying model size and training data.

Result: Thresholds in model size and training data exist for learning transition probabilities; better encoding improves robustness.

Conclusion: Transformers can generalize beyond memorization with sufficient size and data, and better state encoding enhances adaptability.

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [57] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT is a model-agnostic post-processing framework using optimal transport to balance fairness and AUC performance by selectively transforming top quantiles of risk scores.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between enforcing fairness and maintaining AUC performance in high-stakes domains like healthcare and finance.

Method: Proposes FairPOT, leveraging optimal transport to align risk score distributions across groups, focusing on top quantiles for tunable fairness-performance trade-offs.

Result: Outperforms existing methods in global and partial AUC scenarios, achieving fairness with minimal AUC degradation or utility gains.

Conclusion: FairPOT is computationally efficient and adaptable, making it suitable for real-world applications requiring fairness in risk score evaluations.

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [58] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet is a physics-informed operator learning model for mapping pressure profiles to bubble radius responses, using adaptive activation to handle high-frequency features and tested in various bubble dynamics scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient surrogate model for simulating bubble dynamics, leveraging physics-informed deep learning to ensure physical fidelity.

Method: Built on PI-DeepONet, integrating Rowdy adaptive activation to address spectral bias, and evaluated in single and multiple initial radius scenarios using Rayleigh-Plesset and Keller-Miksis equations.

Result: BubbleONet effectively simulates bubble dynamics, offering a faster alternative to traditional numerical solvers.

Conclusion: BubbleONet is a promising surrogate model for bubble dynamics, combining efficiency with physical accuracy.

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [59] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP is a dynamic, user-controllable framework for privacy-preserving sensing, allowing users to specify privacy preferences and transform sensitive IMU data into privacy-compliant versions using contrastive learning and motion generation.


<details>
  <summary>Details</summary>
Motivation: User privacy preferences vary and evolve, especially with IMU sensors in devices like smartphones, but existing methods lack adaptability and user control.

Method: PrivCLIP uses multimodal contrastive learning to align IMU data with natural language descriptions, enabling few-shot sensitive activity detection and sanitization via IMU-GPT.

Result: PrivCLIP outperforms baselines in privacy protection and data utility on human activity recognition datasets.

Conclusion: PrivCLIP offers a flexible, user-centric approach to privacy-preserving sensing, balancing privacy and utility effectively.

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [60] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA addresses task interference in multi-task LoRA adapters by clustering training samples and disentangling task-specific factors via CP decomposition, improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Task interference in merged LoRA adapters degrades performance in multi-task settings.

Method: Clusters training samples for specialized adapters and uses CP decomposition to disentangle task-specific and shared factors.

Result: TC-LoRA improves accuracy by +1.4% on Phi-3 and +2.3% on Mistral-7B.

Conclusion: TC-LoRA effectively reduces interference and enhances LLM adaptation.

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [61] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: DCFL introduces a decoupled contrastive learning framework for federated learning, addressing data heterogeneity by independently optimizing alignment and uniformity without relying on asymptotic assumptions.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from performance degradation due to data heterogeneity, and existing contrastive learning methods violate finite-sample assumptions.

Method: DCFL decouples contrastive loss into alignment and uniformity objectives, enabling independent calibration of attraction and repulsion forces.

Result: DCFL achieves better alignment and uniformity, outperforming state-of-the-art methods on benchmarks like CIFAR-10, CIFAR-100, and Tiny-ImageNet.

Conclusion: DCFL provides an effective contrastive learning solution for federated learning with limited client data.

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [62] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: A comparative survey of TensorFlow and PyTorch, analyzing usability, performance, deployment, and ecosystem trade-offs, highlighting their distinct strengths for research and production.


<details>
  <summary>Details</summary>
Motivation: To provide practitioners with a clear understanding of the trade-offs between TensorFlow and PyTorch, aiding in framework selection for deep learning tasks.

Method: Comparative analysis of programming paradigms, performance benchmarks, deployment tools, and ecosystem support, supported by academic references and practical examples.

Result: PyTorch excels in research due to simplicity and flexibility, while TensorFlow offers a robust production ecosystem. Both frameworks have distinct advantages depending on use cases.

Conclusion: Understanding the trade-offs between PyTorch and TensorFlow is crucial for selecting the right framework, with PyTorch favored for research and TensorFlow for production.

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [63] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: The paper proposes Federated Dataset Learning (FeDaL) to address dataset-wise heterogeneity in Time Series Foundation Models (TSFMs) using federated learning, with mechanisms to mitigate local and global biases.


<details>
  <summary>Details</summary>
Motivation: Dataset-wise heterogeneity degrades generalization in TSFMs, and this challenge is underexplored.

Method: FeDaL uses federated learning to decompose heterogeneous datasets into shared and personalized knowledge, with Domain Bias Elimination (DBE) and Global Bias Elimination (GBE) mechanisms.

Result: FeDaL achieves strong cross-dataset generalization across eight tasks and outperforms 54 baselines.

Conclusion: FeDaL effectively addresses heterogeneity in TSFMs and scales well under decentralization.

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [64] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: The paper introduces QTFT, a quantum-enhanced version of the Temporal Fusion Transformer (TFT), showing improved or comparable performance in time series forecasting while being feasible on current quantum devices.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for enhancing the classical TFT model, aiming for better performance in multi-horizon time series forecasting.

Method: Proposes QTFT, a hybrid quantum-classical architecture based on variational quantum algorithms, compatible with NISQ devices.

Result: QTFT outperforms or matches classical TFT in some cases, with successful training and accurate predictions.

Conclusion: QTFT is a viable quantum-enhanced alternative for time series forecasting, suitable for current quantum hardware.

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [65] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: The paper evaluates fine-tuning methods (OpenAI's service and QLORA) vs. few-shot prompting for Automated Short Answer Grading (ASAG), finding limited utility for open-weight models but better performance for closed models. Synthetic data boosts open-weight model performance.


<details>
  <summary>Details</summary>
Motivation: To compare fine-tuning and few-shot prompting for ASAG, addressing accessibility and performance gaps in LLMs.

Method: Evaluated OpenAI's fine-tuning service and QLORA on open-weight models, measuring interaction with few-shot prompting for ASAG with JSON outputs.

Result: Fine-tuning with small data has limited utility for open-weight models but outperforms few-shot baselines for closed models. Synthetic data improves open-weight model performance.

Conclusion: Fine-tuning benefits vary by model type and domain; synthetic data can enhance open-weight model performance.

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [66] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT is a novel backdoor attack in federated learning using a latent-driven autoencoder to generate diverse, target-specific triggers, evading detection and achieving high success.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks in FL are inflexible and detectable due to fixed-pattern or single-target triggers. FLAT addresses this by enabling adaptive, multi-target attacks.

Method: FLAT employs a latent-driven conditional autoencoder to generate diverse, visually adaptive triggers, allowing arbitrary target selection without retraining.

Result: FLAT achieves high attack success, evades defenses, and introduces flexibility in backdoor attacks, demonstrating robustness against advanced FL defenses.

Conclusion: FLAT highlights the need for new defenses against latent-driven, multi-target backdoor threats in federated learning.

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [67] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: The paper proposes AFMVC, an adversarial fair multi-view clustering framework, to integrate fairness into representation learning, ensuring cluster assignments are unaffected by sensitive attributes while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view clustering methods overlook fairness, and current fairness-aware methods rely on impractical assumptions, degrading performance.

Method: AFMVC uses adversarial training to remove sensitive attribute information from features and aligns view-specific clustering assignments with a fairness-invariant consensus distribution via KL divergence.

Result: AFMVC achieves superior fairness and competitive clustering performance compared to existing methods.

Conclusion: The framework provides a practical and theoretically sound solution for fair multi-view clustering.

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [68] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: The paper explores model inversion (MI) attacks on vision-language models (VLMs), proposing novel token-based and sequence-based methods to reconstruct private training data, demonstrating VLMs' vulnerability to such attacks.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on unimodal DNNs, leaving VLMs' privacy risks underexplored. This study aims to assess VLMs' susceptibility to training data leakage.

Method: Proposes Token-based Model Inversion (TMI), Convergent TMI (TMI-C), Sequence-based Model Inversion (SMI), and SMI with Adaptive Token Weighting (SMI-AW). Evaluates on three VLMs and datasets.

Result: Sequence-based methods, especially SMI-AW, outperform token-based methods in reconstruction quality and attack accuracy (75.31% in human evaluation). Publicly released VLMs are also vulnerable.

Conclusion: VLMs are susceptible to MI attacks, posing significant privacy risks, especially in sensitive applications like healthcare and finance.

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [69] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: Proposes a consistency-aware policy optimization framework to address vanishing gradients in reinforcement learning for LLMs, improving training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of vanishing gradients when multiple responses under a single prompt converge to identical outcomes, limiting learning efficiency.

Method: Introduces a structured global reward based on outcome consistency and an entropy-based soft blending mechanism to balance local and global optimization.

Result: Achieves substantial performance gains on mathematical reasoning benchmarks, demonstrating robustness and general applicability.

Conclusion: The framework effectively mitigates vanishing gradients and enhances learning signals, validated by improved performance.

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [70] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: A semi-supervised deep domain adaptation framework improves solar generation prediction across diverse locations with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing domain shift in solar prediction due to varying weather conditions and lack of labeled data.

Method: Uses a teacher-student model with consistency and cross-entropy loss for semi-supervised adaptation.

Result: Achieves up to 11.36%, 6.65%, and 4.92% accuracy improvements in California, Florida, and New York, respectively.

Conclusion: The framework effectively adapts models to new locations with limited labeled data, enhancing prediction accuracy.

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [71] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: A two-stage pipeline (encoding mass spectra into fingerprints and decoding into structures) using MIST and MolForge with pretraining achieves a tenfold improvement over prior methods, generating accurate molecular structures from mass spectra.


<details>
  <summary>Details</summary>
Motivation: To improve de novo molecular generation from mass spectra by combining robust encoding (MIST) and decoding (MolForge) methods with pretraining.

Method: Uses MIST as the encoder and MolForge as the decoder, with pretraining. Thresholding fingerprint probabilities enhances substructure focus.

Result: Tenfold improvement over state-of-the-art, achieving top-1 28% and top-10 36% accuracy in molecular structure recovery.

Conclusion: The pipeline sets a strong baseline for future de novo molecule elucidation from mass spectra.

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [72] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: SAMT is a novel method for training deep neural networks using block-wise alternating updates and adaptive step sizes, improving stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the instability and high computational cost of standard SGD in nonconvex optimization for deep neural networks.

Method: Proposes Stochastic Alternating Minimization with Trainable Step Sizes (SAMT), updating parameters in blocks and using adaptive step sizes via meta-learning.

Result: SAMT achieves better generalization with fewer updates and provides theoretical convergence guarantees.

Conclusion: SAMT is effective and promising for neural network optimization, outperforming state-of-the-art methods.

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [73] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: Causal Reward Adjustment (CRA) mitigates reward hacking in external reasoning systems by correcting confounding semantic features, improving accuracy without retraining.


<details>
  <summary>Details</summary>
Motivation: Reward hacking in process reward models (PRMs) leads to incorrect answers due to confounding features, necessitating a solution.

Method: CRA uses sparse autoencoders to recover interpretable features from PRM activations and applies backdoor adjustment to correct confounding.

Result: Experiments show CRA reduces reward hacking and enhances accuracy on math solving tasks.

Conclusion: CRA effectively addresses reward hacking without modifying or retraining existing models.

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [74] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: Introduces symmetric divergences to BRPO for offline RL, addressing challenges with analytic policies and numerical issues via Taylor series, proposing S$f$-AC.


<details>
  <summary>Details</summary>
Motivation: Existing methods use asymmetric divergences like KL, but symmetric divergences pose challenges for analytic policies and numerical stability.

Method: Uses Taylor series of $f$-divergence to derive analytic policies and decompose symmetric divergences to address numerical issues.

Result: Proposes S$f$-AC, a practical BRPO algorithm with symmetric divergences, showing competitive performance in experiments.

Conclusion: S$f$-AC successfully integrates symmetric divergences into BRPO, offering a viable alternative to asymmetric methods.

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [75] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS, a data-centric agent for time series, improves forecasting by focusing on data quality, achieving a 6% error reduction.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML approaches prioritize feature engineering and model architecture, but lightweight models in time series forecasting suggest data quality improvement could be more impactful.

Method: DCATS leverages metadata to clean time series data while optimizing forecasting performance, tested with four models on a traffic volume dataset.

Result: DCATS reduces forecasting error by an average of 6% across all tested models and time horizons.

Conclusion: Data-centric approaches like DCATS show promise in AutoML for time series forecasting by enhancing data quality.

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [76] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: A deep learning-based method for automated Doppler angle estimation in ultrasound images is proposed, achieving clinical accuracy.


<details>
  <summary>Details</summary>
Motivation: Incorrect angle estimation in Doppler ultrasound leads to errors in blood velocity measurements, necessitating an automated solution.

Method: The approach uses 2100 carotid ultrasound images with augmentation, five pre-trained models for feature extraction, and a custom shallow network for angle estimation.

Result: The best model achieved a mean absolute error below the clinical threshold, avoiding misclassification of normal velocities as stenosis.

Conclusion: Deep learning shows promise for automating Doppler angle estimation in clinical ultrasound workflows.

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [77] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time is a trimodal framework for multivariate time series forecasting, combining time, spectral, and prompt branches with adaptive gating and cross-modal alignment to outperform state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current methods for MTSF lack adaptability across forecast horizons and fail to capture nuanced, horizon-specific relationships due to rigid inductive biases and static fusion strategies.

Method: T3Time introduces a trimodal framework with time, spectral, and prompt branches, a gating mechanism for feature prioritization, and adaptive cross-modal alignment heads.

Result: The model reduces MSE by 3.28% and MAE by 2.29% on average, with strong generalization in few-shot learning (e.g., 4.13% MSE reduction with 5% training data).

Conclusion: T3Time advances MTSF by dynamically capturing horizon-specific relationships and outperforming baselines, demonstrating robustness in few-shot scenarios.

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [78] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT is a Python tool for visually exploring ML models using local and global sensitivity analysis, supporting HITL workflows without programming.


<details>
  <summary>Details</summary>
Motivation: To help AI researchers and domain experts understand ML model behavior through interactive exploration and explanation.

Method: Automates model training/selection, provides global feature attribution (variance-based), and per-instance explanations (LIME/SHAP).

Result: Demonstrated on Titanic dataset, showing sensitivity analysis aids feature selection and data refinement.

Conclusion: SAInT effectively bridges the gap between ML models and human understanding through visualization and automation.

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [79] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: The paper introduces Mockingbird, a framework to adapt LLMs for general machine learning tasks, leveraging role-playing and self-reflection. While effective, it doesn't surpass domain-specific methods or human expert feedback.


<details>
  <summary>Details</summary>
Motivation: The study explores the potential of LLMs beyond chat bots, driven by curiosity about their reasoning capabilities for general machine learning tasks.

Method: Proposes Mockingbird, a framework where LLMs role-play functions and reflect on mistakes to improve performance on machine learning tasks.

Result: LLM-driven methods like Mockingbird achieve acceptable results but fall short of domain-specific approaches or human expert feedback.

Conclusion: While promising, LLMs' self-reflection alone isn't yet sufficient to outperform specialized methods or expert input in machine learning tasks.

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [80] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC, a lightweight RL algorithm, trains VLMs in cheap simulators and generalizes well to real-world tasks without degrading image understanding.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack the ability to convert visual observations into language-conditioned actions, and existing RL methods struggle with generalization and hyperparameter tuning.

Method: VL-DAC decouples PPO updates for action tokens from value learning at environment-step level, avoiding unstable weighting terms.

Result: VL-DAC achieves +50% on BALROG, +5% on VSI-Bench, and +2% on VisualWebBench, showing generalization without accuracy loss.

Conclusion: VL-DAC demonstrates that simple RL can train VLMs in synthetic worlds and improve real-world task performance.

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [81] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: The paper introduces WSS-CL, a two-phase machine unlearning method for image classification, using weight saliency and contrastive learning to improve unlearning efficacy with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning methods struggle with precision, stability, and domain applicability, prompting the need for a more efficient approach.

Method: WSS-CL involves a forgetting stage (maximizing KL divergence) and adversarial fine-tuning (contrastive learning with scaled features) to unlearn specific data.

Result: The method achieves improved unlearning efficacy with negligible performance loss, outperforming state-of-the-art approaches.

Conclusion: WSS-CL is effective for supervised and self-supervised settings, offering a practical solution for machine unlearning.

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [82] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: The paper introduces a token categorization method (positive/negative) for supervised fine-tuning (SFT) of LLMs to improve performance by focusing on useful tokens and forgetting misleading ones.


<details>
  <summary>Details</summary>
Motivation: SFT's effectiveness depends on data quality and volume; poor data can degrade performance. The goal is to reduce reliance on data quality by selectively training tokens.

Method: Tokens are categorized as positive (useful) or negative (misleading). Positive tokens are trained normally, while negative tokens are explicitly forgotten to shape a knowledge boundary.

Result: Experiments show the forgetting mechanism improves model performance and increases response diversity.

Conclusion: Token categorization and selective forgetting enhance SFT by refining what the model learns, leading to better performance and diversity.

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [83] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS introduces a distributed feature-sharing method for private inference, balancing privacy and efficiency by partitioning client data among non-colluding servers, with extensions (PrivDFS-AT and PrivDFS-KD) enhancing security.


<details>
  <summary>Details</summary>
Motivation: Addressing the privacy-efficiency trade-off in cloud-based MLaaS, where existing methods either compromise privacy (split inference) or efficiency (cryptographic approaches).

Method: PrivDFS partitions client input features into shares for non-colluding servers, aggregates outputs securely, and includes extensions (PrivDFS-AT for adversarial training and PrivDFS-KD for key-based diversification).

Result: Achieves privacy comparable to split inference with 100x less client computation and no accuracy loss, robust against attacks.

Conclusion: PrivDFS offers a scalable, efficient, and private inference solution for MLaaS, with extensions further strengthening security.

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [84] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: MMSFM extends flow matching to multi-marginal settings for high-dimensional data alignment without dimensionality reduction, handling irregular time points robustly.


<details>
  <summary>Details</summary>
Motivation: Traditional dimensionality reduction oversimplifies dynamics, missing transient behaviors in non-equilibrium systems. MMSFM addresses this by preserving high-dimensional data integrity.

Method: MMSFM uses measure-valued splines for irregular timing and score matching to prevent overfitting, aligning data without dimensionality reduction.

Result: Validated on synthetic and real datasets (e.g., gene expression, image progression), MMSFM shows versatility and robustness.

Conclusion: MMSFM effectively models high-dimensional system evolution from irregular snapshots, outperforming traditional methods.

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [85] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: Proposes a rehearsal-based continual learning method for Multiple Instance Learning (MIL) in leukemia detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of updating machine learning models in dynamic environments like labs, where data streams require continual learning without catastrophic forgetting, especially for MIL in hematologic disease diagnosis.

Method: Uses a rehearsal-based approach, selecting instances based on attention scores and distance from bag/class means to preserve data diversity in exemplary sets.

Result: Outperforms existing continual learning methods in class incremental scenarios, demonstrated using real-world leukemia lab data.

Conclusion: Introduces the first effective continual learning method for MIL, enabling model adaptation to shifting data distributions in medical diagnostics.

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [86] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ introduces INT6 quantization for LLMs, balancing accuracy and efficiency with algorithmic and hardware optimizations, achieving near-FP16 accuracy and significant speedups.


<details>
  <summary>Details</summary>
Motivation: LLMs face high memory and computational costs, and existing INT4/INT8 quantization methods degrade accuracy or lack efficiency. INT6 offers a better trade-off but lacks hardware support.

Method: FlexQ uses uniform 6-bit weight quantization and adaptive 8-bit activations, with a specialized GPU kernel for W6A6/W6A8 via Binary Tensor Core equivalents.

Result: FlexQ maintains near-FP16 accuracy (0.05 perplexity increase), achieves 1.39 speedup over ABQ-LLM, and 1.33 inference acceleration with 1.21 memory savings over SmoothQuant.

Conclusion: FlexQ effectively addresses LLM deployment challenges by combining INT6 quantization with hardware optimizations, offering a practical solution for efficient inference.

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [87] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sbastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: A systematic review of multimodal model explainability research (2020-2024) highlights gaps in evaluation methods and modality interaction understanding, recommending standardized practices for future XAI in multimodal AI.


<details>
  <summary>Details</summary>
Motivation: The rise of multimodal learning and XAI demand has created a need to understand and improve the explainability of complex models, especially in capturing cross-modality interactions.

Method: The review analyzes literature on multimodal model explainability, focusing on architecture, modalities, explanation algorithms, and evaluation methods.

Result: Most studies focus on vision-language and language-only models, using attention-based techniques, but lack systematic evaluation and fail to fully capture modality interactions.

Conclusion: The paper recommends standardized evaluation practices to enhance transparency and accountability in multimodal XAI research.

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [88] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: Proposes randomized algorithms for matrix norm estimation using matrix-vector multiplications, with applications in deep learning and recommender systems.


<details>
  <summary>Details</summary>
Motivation: To efficiently estimate matrix norms in a matrix-free setting, addressing needs in deep neural network training and adversarial attack mitigation.

Method: Modifies Hutchinson's diagonal estimator and Hutch++ version for matrix-vector multiplication-based norm estimation.

Result: Provides oracle complexity bounds and demonstrates utility in Jacobian-based regularization and adversarial attack mitigation.

Conclusion: The proposed algorithms are practical and effective for matrix norm estimation in relevant applications.

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [89] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE integrates the cloud model into WAE, improving latent space modeling and sample diversity by using a cloud model prior instead of Gaussian.


<details>
  <summary>Details</summary>
Motivation: To address homogenization in reconstructed samples and enhance latent space representation in generative models.

Method: Uses cloud model characteristic functions to regularize the latent space within the WAE framework.

Result: Outperforms existing models on MNIST, FashionMNIST, CIFAR-10, and CelebA in reconstruction quality and diversity.

Conclusion: CMCFAE successfully combines cloud model theory with MMD-based regularization, advancing autoencoder-based generative models.

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [90] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: A novel AI-driven red teaming approach for LLMs uses hierarchical RL to simulate multi-turn adversarial dialogues, outperforming brittle prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: Current automated red teaming methods for LLMs are limited by single-turn attacks or rigid templates, missing the complexity of real-world adversarial interactions.

Method: Formalizes red teaming as an MDP and employs hierarchical RL with token-level harm rewards to train an AI to strategically attack another AI.

Result: The generative agent learns coherent, multi-turn attack strategies, uncovering subtle vulnerabilities and setting a new state-of-the-art.

Conclusion: This approach reframes LLM red teaming as a dynamic, trajectory-based process, crucial for robust AI deployment.

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [91] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: The paper explores task-switching performance of attention-based models (transformers) versus traditional methods (LSTMs, MLPs). Only a modified transformer (cisformer) with extensive attention achieves high accuracy (95%), suggesting attention mechanisms can be improved by qualitative comparisons in such settings.


<details>
  <summary>Details</summary>
Motivation: To understand why attention-based models struggle with small-scale task-switching applications compared to traditional methods, and to explore improvements.

Method: Evaluated transformers, LSTMs, and MLPs on a task-switching framework (IARC tasks). Tested modified architectures like cisformer and extensive attention.

Result: Standard transformers, LSTMs, and MLPs performed similarly but modestly. Only the cisformer with extensive attention achieved ~95% accuracy.

Conclusion: Attention mechanisms can be better understood and improved by comparing different formulations in task-switching scenarios.

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [92] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: CARD introduces a cache-based parallel speculative decoding framework to accelerate LLM inference by decoupling drafting and verification, achieving up to 4.83x speedup.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods suffer from sequential execution and inefficient drafting due to token rejection, limiting performance and draft model size.

Method: CARD employs a 'query-and-correct' paradigm: the draft model populates a shared cache while the target model concurrently corrects the draft, enabling parallel execution.

Result: The approach achieves up to 4.83x speedup over vanilla decoding without fine-tuning the draft or target models.

Conclusion: CARD effectively addresses inefficiencies in speculative decoding, significantly improving inference speed while maintaining model integrity.

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [93] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: The paper introduces hierarchical scoring metrics for ML models, allowing partial credit for misclassifications based on a class taxonomy, providing finer-grained evaluation than traditional pass/fail methods.


<details>
  <summary>Details</summary>
Motivation: Traditional classification evaluation treats all misclassifications equally, ignoring hierarchical relationships or the impact of errors. This work aims to address this limitation.

Method: Develops hierarchical scoring metrics using scoring trees to encode class relationships, reflecting prediction-ground truth distances. Demonstrates on abstract use cases with three weighting strategies.

Result: Metrics capture errors with finer granularity, enabling tuning via scoring trees. They rank models by error impact, not just count.

Conclusion: Hierarchical metrics offer a nuanced evaluation of ML performance, aligning with class relationships and error significance. Python implementations will be open-sourced.

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [94] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal is a Transformer-based neural operator that combines global and local feature learning for solving PDEs, outperforming benchmarks by 15.2%.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to integrate local and global features in PDE solving, limiting accuracy and stability.

Method: GFocal uses Nystrm attention for global blocks and slices-based focal blocks for local features, fused via convolution-based gating.

Result: Achieves state-of-the-art performance with 15.2% relative gain in benchmarks and excels in industry-scale simulations.

Conclusion: GFocal effectively integrates multiscale features, improving accuracy and stability in PDE solving.

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [95] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM is a lightweight, efficient feature extractor for multivariate time-series classification, outperforming CNN and Transformer models with fewer parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Current Transformer- and CNN-based models for time-series classification are computationally heavy, lack frequency diversity, and require large parameter budgets.

Method: PRISM uses symmetric FIR filters at multiple temporal scales per channel, avoiding inter-channel convolutions for reduced complexity.

Result: PRISM matches or outperforms leading baselines in benchmarks (human-activity, sleep-stage, biomedical) with significantly fewer parameters and FLOPs.

Conclusion: PRISM combines classical signal processing with deep learning for an accurate, resource-efficient solution in time-series classification.

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [96] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP introduces a gradient-free, heterogeneity-invariant PFL scheme using closed-form solutions to address non-IID data challenges, outperforming baselines by 5.79%-20.97% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing PFL methods struggle with non-IID data due to gradient-based updates, which degrade performance. FedHiP aims to eliminate this issue by avoiding gradients.

Method: FedHiP uses a self-supervised pre-trained foundation model for feature extraction and an analytic classifier. It involves three phases: local training, global aggregation, and local personalization, all with closed-form solutions.

Result: FedHiP achieves heterogeneity invariance, meaning models remain consistent regardless of data distribution. It outperforms baselines by 5.79%-20.97% in accuracy.

Conclusion: FedHiP effectively addresses non-IID data challenges in PFL, offering a robust, gradient-free solution with superior performance.

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [97] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp trains graph foundation models (GFMs) by focusing on structural generalization, outperforming competitors in graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional GFMs lack structural cross-domain generalization, while graph structures provide more consistent cross-domain information.

Method: GraphProp trains a structural GFM by predicting graph invariants, then uses its representations as positional encodings for a comprehensive GFM.

Result: GraphProp excels in supervised and few-shot learning, especially for graphs without node attributes.

Conclusion: Emphasizing structural generalization in GFMs improves cross-domain performance.

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [98] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: Wall insulation reduces gas demand by up to 19% on average, but savings vary by energy burden groups. High-burden households reallocate savings to comfort, not consumption cuts.


<details>
  <summary>Details</summary>
Motivation: To understand the heterogeneous impact of energy efficiency interventions, especially wall insulation, on gas consumption across different energy burden subgroups.

Method: A causal machine learning model trained on nationally representative data of the English housing stock to estimate treatment effects.

Result: Low energy burden groups save significantly, while high-burden groups see little reduction due to reallocating savings to comfort.

Conclusion: Energy policies need a broader framework considering both climate impacts and equity, as behavioral responses reflect rational adjustments in deprived contexts.

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [99] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: A bio-inspired Spiking Neural Network (SNN) architecture for Network Intrusion Detection (NIDS) combines static and dynamic SNNs for lifelong learning, achieving 85.3% accuracy with low-power potential.


<details>
  <summary>Details</summary>
Motivation: Inspired by the brain's hierarchical processing and energy efficiency, the paper aims to create a lifelong learning NIDS using bio-plausible mechanisms.

Method: Uses a static SNN for initial intrusion detection and a dynamic SNN with GWR-inspired plasticity and Ad-STDP for attack classification.

Result: Achieves 85.3% accuracy on UNSW-NB15, with robust adaptation and reduced catastrophic forgetting. High sparsity suggests low-power neuromorphic deployment.

Conclusion: The SNN architecture is effective for lifelong NIDS, combining bio-plausible learning with practical efficiency.

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [100] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: A deep learning approach using cGANs for multimodal emotion detection, outperforming traditional unimodal methods.


<details>
  <summary>Details</summary>
Motivation: To improve emotion recognition by integrating text, audio, and facial expressions using cGANs for synthetic data generation.

Method: Proposes a cGAN-based multimodal framework combining text, audio, and facial data for emotion detection.

Result: Significant improvement in emotion recognition accuracy compared to baseline models.

Conclusion: Demonstrates cGANs' potential for nuanced emotional understanding in human-computer interaction.

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [101] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: The paper explores pretraining for Link Prediction (LP) in graph machine learning, addressing challenges like sparse supervision and poor generalization. It introduces a late fusion strategy and a Mixture-of-Experts (MoE) framework for transferability, achieving state-of-the-art results with low computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing LP methods using GNNs face issues like sparse supervision, sensitivity to initialization, and poor generalization. Pretraining is proposed to overcome these challenges.

Method: The study introduces a late fusion strategy for node- and edge-level information, a MoE framework for diverse pretraining data, and a parameter-efficient tuning strategy for fast adaptation.

Result: Experiments on 16 datasets show state-of-the-art performance in low-resource LP and competitive results with 10,000x lower computational overhead.

Conclusion: Pretraining with late fusion and MoE effectively improves LP performance, offering a scalable and efficient solution.

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [102] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: Causal Reflection framework enhances causal reasoning in agents by modeling causality dynamically and using a Reflect mechanism for self-correction.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs and traditional RL agents in robust causal reasoning and understanding delayed/nonlinear effects.

Method: Introduces Causal Reflection framework with dynamic causality modeling and a Reflect mechanism for hypothesis generation and model revision.

Result: Enables agents to reason about causality, adapt, self-correct, and communicate causal understanding.

Conclusion: The framework provides a theoretical foundation for more adaptive and explainable causal agents.

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [103] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: A novel federated traffic prediction method, Fed-CI, reduces communication overhead and improves accuracy by using local data without inter-client communication.


<details>
  <summary>Details</summary>
Motivation: Traffic data is distributed and privacy-sensitive, requiring federated methods. Current approaches have high communication costs and delays.

Method: Proposes Channel-Independent Paradigm (CIP) and Fed-CI framework, enabling local predictions without inter-client communication.

Result: Fed-CI outperforms existing methods, improving RMSE by 8%, MAE by 14%, and MAPE by 16%, while reducing communication costs.

Conclusion: Fed-CI offers a scalable, efficient, and privacy-compliant solution for federated traffic prediction.

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [104] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: The paper analyzes 5,000 identity theft cases to model privacy risks using a graph-based framework, predicting further disclosures when certain personal data is compromised.


<details>
  <summary>Details</summary>
Motivation: To help individuals and organizations protect personal information by understanding privacy risks through empirical data.

Method: Constructs an Identity Ecosystem graph with PII attributes as nodes and disclosure relationships as edges, then uses graph theory and neural networks to predict risks.

Result: The framework effectively predicts the likelihood of further disclosures when specific PII attributes are exposed.

Conclusion: The approach provides a foundational model for understanding and mitigating privacy risks in identity ecosystems.

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [105] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weienfels*

Main category: cs.LG

TL;DR: Physics-informed neural networks are used for non-invasive quality assessment in aluminum spot welding, addressing challenges with novel training strategies and a 2D model.


<details>
  <summary>Details</summary>
Motivation: Destructive testing for weld nugget diameter measurement limits efficient quality control; a non-invasive, model-based approach is needed.

Method: Two novel training strategies: fading-in experimental losses and conditional updates of material parameters. A 2D model is used for accuracy and efficiency.

Result: The network predicts displacement and nugget growth within experimental confidence, enabling fast quality control.

Conclusion: The approach shows strong potential for industrial applications, supporting non-invasive quality assessment.

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [106] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: A framework generalizing flow and diffusion models by replacing scalar time with vectors/matrices/operators, enabling versatile generative models for multiple tasks without task-specific training.


<details>
  <summary>Details</summary>
Motivation: To generalize the dynamics of flow and diffusion models and bridge probability distributions across diverse dimensional spaces.

Method: Uses operator-based interpolants to replace scalar time variables, unifying and extending existing generative models.

Result: Demonstrates zero-shot efficacy in tasks like conditional generation, inpainting, fine-tuning, posterior sampling, and multiscale modeling.

Conclusion: Proposes a generic, task-agnostic alternative to specialized models with broad applicability.

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [107] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: The paper introduces CaPulse, a causality-based framework for time series anomaly detection, addressing challenges like label scarcity and data imbalance using causal tools and periodical normalizing flows.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture anomaly generation mechanisms and struggle with data challenges like label scarcity and complex multi-periodicity.

Method: The authors build a structural causal model and propose Periodical Normalizing Flows with a mask mechanism and periodical learners for anomaly detection.

Result: CaPulse outperforms existing methods on seven datasets, achieving AUROC improvements of 3% to 17%, with enhanced interpretability.

Conclusion: The causality-based approach of CaPulse effectively addresses time series anomaly detection challenges, offering superior performance and interpretability.

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [108] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merrinboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0 is an advanced bioacoustics model trained on multi-taxa data, achieving state-of-the-art performance and strong transfer learning capabilities.


<details>
  <summary>Details</summary>
Motivation: To expand the model's applicability beyond avian species and improve performance in bioacoustics tasks.

Method: Trained with self-distillation, a prototype-learning classifier, and a new source-prediction criterion.

Result: Achieves top performance on BirdSet and BEANS benchmarks and excels in marine transfer learning tasks.

Conclusion: Fine-grained species classification is a robust pre-training task for bioacoustics.

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [109] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: A computationally efficient algorithm for learning Single-Index Models under adversarial label noise, achieving constant factor approximation for all monotone activations with bounded moment.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks efficiency or broad applicability for unknown activations, especially under adversarial noise. This paper aims to bridge this gap.

Method: Develops an optimization framework using a novel vector field, leveraging Gaussian space properties and monotone function regularity, avoiding traditional gradient methods.

Result: First efficient algorithm for this task, applicable to a wide class of monotone activations, including Lipschitz and discontinuous functions.

Conclusion: The approach provides a robust and efficient solution for learning Single-Index Models under adversarial noise, expanding beyond previous limitations.

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [110] [Forgive and Forget? An Industry 5.0 Approach to Trust-Fatigue Co-regulation in Human-Cobot Order Picking](https://arxiv.org/abs/2508.03765)
*Soumyadeep Dhar*

Main category: cs.MA

TL;DR: The paper explores trust and fatigue in human-cobot collaboration in logistics, proposing a Stackelberg game model. Simulations show a refined trust model boosts productivity by 100% and a proactive Trust-Repair Protocol cuts recovery time by 75%.


<details>
  <summary>Details</summary>
Motivation: To address challenges in human-robot collaboration in Logistics 5.0, focusing on trust and fatigue dynamics.

Method: Uses a dynamic Stackelberg game model with utility functions for fatigue and trust, validated via agent-based simulations.

Result: Refined trust model increases productivity by 100%; proactive Trust-Repair Protocol reduces recovery time by 75%.

Conclusion: Provides a framework for human-centric, sustainable, and resilient cobot behavior in Industry 5.0.

Abstract: This paper investigates the critical role of trust and fatigue in human-cobot
collaborative order picking, framing the challenge within the scope of
Logistics 5.0 -- the implementation of human-robot symbiosis in smart
logistics. We propose a dynamic, leader-follower Stackelberg game to model this
interaction, where utility functions explicitly account for human fatigue and
trust. Through agent-based simulations, we demonstrate that while a naive model
leads to a "trust death spiral," a refined trust model creates a "trust synergy
cycle," increasing productivity by nearly 100 percent. Finally, we show that a
cobot equipped with a proactive Trust-Repair Protocol can overcome system
brittleness, reducing trust recovery time after a severe failure by over 75
percent compared to a non-adaptive model. Our findings provide a framework for
designing intelligent cobot behaviors that fulfill the Industry 5.0 pillars of
human-centricity, sustainability, and resilience.

</details>


### [111] [When Agents Break Down in Multiagent Path Finding](https://arxiv.org/abs/2508.03777)
*Foivos Fioravantes,Duan Knop,Nikolaos Melissinos,Michal Opler*

Main category: cs.MA

TL;DR: The paper introduces a dynamic adaptation framework for Multiagent Path Finding (MAPF) to handle agent malfunctions without full replanning, ensuring bounded delays and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining optimal schedules in MAPF when agents experience malfunctions, avoiding computationally infeasible full replanning.

Method: Proposes two protocols: one for local agent coordination to adjust paths dynamically, and another offloading computations to network nodes for limited-capability agents.

Result: The primary protocol bounds makespan increase by k turns after k malfunctions; the secondary protocol ensures robustness without requiring enhanced agent processing.

Conclusion: The protocols offer a practical, scalable solution for resilient multiagent navigation despite agent failures.

Abstract: In Multiagent Path Finding (MAPF), the goal is to compute efficient,
collision-free paths for multiple agents navigating a network from their
sources to targets, minimizing the schedule's makespan-the total time until all
agents reach their destinations. We introduce a new variant that formally
models scenarios where some agents may experience delays due to malfunctions,
posing significant challenges for maintaining optimal schedules.
  Recomputing an entirely new schedule from scratch after each malfunction is
often computationally infeasible. To address this, we propose a framework for
dynamic schedule adaptation that does not rely on full replanning. Instead, we
develop protocols enabling agents to locally coordinate and adjust their paths
on the fly. We prove that following our primary communication protocol, the
increase in makespan after k malfunctions is bounded by k additional turns,
effectively limiting the impact of malfunctions on overall efficiency.
Moreover, recognizing that agents may have limited computational capabilities,
we also present a secondary protocol that shifts the necessary computations
onto the network's nodes, ensuring robustness without requiring enhanced agent
processing power. Our results demonstrate that these protocols provide a
practical, scalable approach to resilient multiagent navigation in the face of
agent failures.

</details>


### [112] [DRAMA: A Dynamic and Robust Allocation-based Multi-Agent System for Changing Environments](https://arxiv.org/abs/2508.04332)
*Naibo Wang,Yifan Zhang,Sai Liu,Xinkui Zhao,Guanjie Cheng,Yueshen Xu*

Main category: cs.MA

TL;DR: DRAMA is a dynamic multi-agent system designed for resilient collaboration in changing environments, featuring modular architecture and flexible task allocation.


<details>
  <summary>Details</summary>
Motivation: Existing MAS frameworks lack adaptability to dynamic environments, limiting robust multi-agent cooperation.

Method: DRAMA uses a modular architecture with control and worker planes, abstracting agents and tasks as resources with affinity-based allocation.

Result: Enables real-time monitoring, flexible task reassignment, and robust execution in dynamic scenarios.

Conclusion: DRAMA addresses adaptability challenges in MAS, ensuring efficient collaboration in unpredictable environments.

Abstract: Multi-agent systems (MAS) have demonstrated significant effectiveness in
addressing complex problems through coordinated collaboration among
heterogeneous agents. However, real-world environments and task specifications
are inherently dynamic, characterized by frequent changes, uncertainty, and
variability. Despite this, most existing MAS frameworks rely on static
architectures with fixed agent capabilities and rigid task allocation
strategies, which greatly limits their adaptability to evolving conditions.
This inflexibility poses substantial challenges for sustaining robust and
efficient multi-agent cooperation in dynamic and unpredictable scenarios. To
address these limitations, we propose DRAMA: a Dynamic and Robust
Allocation-based Multi-Agent System designed to facilitate resilient
collaboration in rapidly changing environments. DRAMA features a modular
architecture with a clear separation between the control plane and the worker
plane. Both agents and tasks are abstracted as resource objects with
well-defined lifecycles, while task allocation is achieved via an
affinity-based, loosely coupled mechanism. The control plane enables real-time
monitoring and centralized planning, allowing flexible and efficient task
reassignment as agents join, depart, or become unavailable, thereby ensuring
continuous and robust task execution. The worker plane comprises a cluster of
autonomous agents, each with local reasoning, task execution, the ability to
collaborate, and the capability to take over unfinished tasks from other agents
when needed.

</details>


### [113] [Position-Based Flocking for Robust Alignment](https://arxiv.org/abs/2508.04378)
*Hossein B. Jond*

Main category: cs.MA

TL;DR: A position-based flocking model for agents achieves stable collective motion by balancing cohesion-separation and alignment, outperforming velocity-based models in simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a robust flocking model that ensures stable collective motion by leveraging position-based interactions, addressing limitations of velocity-based approaches.

Method: Modifies a velocity-based approach by approximating velocity differences using initial and current positions, introducing a threshold weight for sustained alignment.

Result: Simulations with 50 agents in 2D show stronger alignment, more rigid formations, and better separation metrics compared to velocity-based models.

Conclusion: The position-based model ensures robust flocking behavior, with potential applications in robotics and collective dynamics.

Abstract: This paper presents a position-based flocking model for interacting agents,
balancing cohesion-separation and alignment to achieve stable collective
motion. The model modifies a velocity-based approach by approximating velocity
differences using initial and current positions, introducing a threshold weight
to ensure sustained alignment. Simulations with 50 agents in 2D demonstrate
that the position-based model produces stronger alignment and more rigid and
compact formations compared to the velocity-based model. The alignment metric
and separation distances highlight the efficacy of the proposed model in
achieving robust flocking behavior. The model's use of positions ensures robust
alignment, with applications in robotics and collective dynamics.

</details>
