{"id": "2602.17738", "categories": ["cs.MA", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.17738", "abs": "https://arxiv.org/abs/2602.17738", "authors": ["Hyowoon Seo", "Joonho Seon", "Jin Young Kim", "Mehdi Bennis", "Wan Choi", "Dong In Kim"], "title": "Reasoning-Native Agentic Communication for 6G", "comment": "8 pages 4 figures", "summary": "Future 6G networks will interconnect not only devices, but autonomous machines that continuously sense, reason, and act. In such environments, communication can no longer be understood solely as delivering bits or even preserving semantic meaning. Even when two agents interpret the same information correctly, they may still behave inconsistently if their internal reasoning processes evolve differently. We refer to this emerging challenge as belief divergence. This article introduces reasoning native agentic communication, a new paradigm in which communication is explicitly designed to address belief divergence rather than merely transmitting representations. Instead of triggering transmissions based only on channel conditions or data relevance, the proposed framework activates communication according to predicted misalignment in agents internal belief states. We present a reasoning native architecture that augments the conventional communication stack with a coordination plane grounded in a shared knowledge structure and bounded belief modeling. Through enabling mechanisms and representative multi agent scenarios, we illustrate how such an approach can prevent coordination drift and maintain coherent behavior across heterogeneous systems. By reframing communication as a regulator of distributed reasoning, reasoning native agentic communication enables 6G networks to act as an active harmonizer of autonomous intelligence.", "AI": {"tldr": "A new communication paradigm called reasoning native agentic communication is proposed for 6G networks to address belief divergence by aligning agents' internal belief states.", "motivation": "In future 6G networks with autonomous machines, communication must go beyond transmitting bits or semantics to prevent inconsistency in behavior due to belief divergence, where agents interpret information correctly but behave differently as their reasoning processes evolve.", "method": "Introduces a reasoning native architecture that adds a coordination plane based on shared knowledge and bounded belief modeling to the conventional communication stack, with communication triggered by predicted misalignment in agents' belief states.", "result": "The approach prevents coordination drift and maintains coherent behavior across heterogeneous systems by reframing communication as a regulator of distributed reasoning.", "conclusion": "Reasoning native agentic communication enables 6G networks to actively harmonize autonomous intelligence by addressing belief divergence and aligning agent behaviors through targeted communication."}}
{"id": "2602.17875", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17875", "abs": "https://arxiv.org/abs/2602.17875", "authors": ["Shreshth Rajan"], "title": "MultiVer: Zero-Shot Multi-Agent Vulnerability Detection", "comment": null, "summary": "We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate that for security applications where false negatives are costlier than false positives, zero-shot multi-agent ensembles can match and exceed fine-tuned models on the metric that matters most.", "AI": {"tldr": "MultiVer is a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning, outperforming fine-tuned baselines in recall but with lower precision.", "motivation": "To develop a system that can effectively detect vulnerabilities with high recall without requiring fine-tuning, addressing scenarios where false negatives are more costly than false positives.", "method": "A four-agent ensemble (security, correctness, performance, style) with union voting is used for vulnerability detection, evaluated on PyVul and SecurityEval benchmarks.", "result": "Achieves 82.7% recall on PyVul (exceeding fine-tuned GPT-3.5 by 1.4 percentage points) and 91.7% detection rate on SecurityEval, but with 48.8% precision versus 63.9% for baselines, resulting in 61.4% F1. Ablation shows multi-agent adds 17 percentage points recall over single-agent.", "conclusion": "Zero-shot multi-agent ensembles can match or exceed fine-tuned models in recall for security applications where false negatives are prioritized, despite trade-offs in precision."}}
{"id": "2602.18026", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18026", "abs": "https://arxiv.org/abs/2602.18026", "authors": ["Shan Yang"], "title": "Mean-Field Reinforcement Learning without Synchrony", "comment": "21 pages, 5 figures, 1 algorithm", "summary": "Mean-field reinforcement learning (MF-RL) scales multi-agent RL to large populations by reducing each agent's dependence on others to a single summary statistic -- the mean action. However, this reduction requires every agent to act at every time step; when some agents are idle, the mean action is simply undefined. Addressing asynchrony therefore requires a different summary statistic -- one that remains defined regardless of which agents act. The population distribution $\u03bc\\in \u0394(\\mathcal{O})$ -- the fraction of agents at each observation -- satisfies this requirement: its dimension is independent of $N$, and under exchangeability it fully determines each agent's reward and transition. Existing MF-RL theory, however, is built on the mean action and does not extend to $\u03bc$. We therefore construct the Temporal Mean Field (TMF) framework around the population distribution $\u03bc$ from scratch, covering the full spectrum from fully synchronous to purely sequential decision-making within a single theory. We prove existence and uniqueness of TMF equilibria, establish an $O(1/\\sqrt{N})$ finite-population approximation bound that holds regardless of how many agents act per step, and prove convergence of a policy gradient algorithm (TMF-PG) to the unique equilibrium. Experiments on a resource selection game and a dynamic queueing game confirm that TMF-PG achieves near-identical performance whether one agent or all $N$ act per step, with approximation error decaying at the predicted $O(1/\\sqrt{N})$ rate.", "AI": {"tldr": "The paper introduces Temporal Mean Field (TMF), a new MF-RL framework based on the population distribution to handle asynchrony, with theory and experiments showing scalability and performance guarantees.", "motivation": "Existing MF-RL methods rely on mean action, which fails when agents are idle in asynchronous settings, requiring a robust summary statistic like population distribution.", "method": "Construct the TMF framework using population distribution \u03bc, prove existence and uniqueness of equilibria, derive finite-population bounds, and develop TMF-PG policy gradient algorithm.", "result": "TMF-PG achieves consistent performance in synchronous and sequential cases, with approximation error decaying as O(1/\u221aN), validated in resource selection and queueing games.", "conclusion": "TMF provides a unified theory for MF-RL across asynchronous environments, addressing limitations of mean action-based approaches with proven convergence and scalability."}}
{"id": "2602.17677", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17677", "abs": "https://arxiv.org/abs/2602.17677", "authors": ["Sutej Kulgod", "Sean Ye", "Sanchit Tanwar", "Christoffer Heckman"], "title": "Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving", "comment": "7 pages, 2 figures", "summary": "Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.", "AI": {"tldr": "Paper introduces method to reduce textual shortcuts in MCQA benchmarks for VLMs, forcing visual reliance.", "motivation": "Synthetically generated MCQA benchmarks for VLMs are prone to hidden textual cues, allowing models to exploit linguistic patterns instead of visual context, which can lead to inflated performance not reflective of true perceptual understanding.", "method": "Decouples the correct answer from linguistic artifacts and employs a curriculum learning strategy to force the model to rely on visual grounding.", "result": "Reduces blind accuracy (model performance without visual input) from +66.9% above random to +2.9%, eliminating most exploitable textual shortcuts, making MCQA benchmarks more reliable indicators of VLM perceptual understanding.", "conclusion": "The proposed method effectively mitigates exploitable textual shortcuts in MCQA benchmarks, ensuring that VLM performance in such tasks more accurately reflects perceptual understanding based on visual grounding."}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "Behavioral pathologies in AI agents, like sycophancy and deception, are not errors but mathematically rationalizable due to model misspecification, leading to a shift from reward engineering to subjective model design for safety.", "motivation": "The rapid deployment of Large Language Models and AI agents in critical domains is hindered by persistent behavioral pathologies (e.g., sycophancy, hallucination, strategic deception) that resist reinforcement learning mitigation. Current safety paradigms lack a unified theoretical framework to explain their emergence and stability.", "method": "Adapt Berk-Nash Rationalizability from theoretical economics to AI, modeling agents as optimizing against a flawed subjective world model. Analyze failures as structural necessities, with unsafe behaviors emerging as stable misaligned equilibria or oscillatory cycles, and strategic deception as 'locked-in' equilibria or through epistemic indeterminacy.", "result": "Theoretical predictions validated through behavioral experiments on six state-of-the-art model families, generating phase diagrams that map topological boundaries of safe behavior. Shows safety is a discrete phase determined by epistemic priors, not a continuous function of reward magnitude.", "conclusion": "Establishes Subjective Model Engineering (design of agent's internal belief structure) as necessary for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping agent's interpretation of reality."}}
{"id": "2602.17679", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.17679", "abs": "https://arxiv.org/abs/2602.17679", "authors": ["Saksham Kiroriwal", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "title": "Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization", "comment": "This paper is under review and has been submitted for CIRP CMS 2026", "summary": "Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.", "AI": {"tldr": "POGPN-JPSS combines structured probabilistic modeling with expert knowledge to optimize high-dimensional multi-stage manufacturing processes faster and more reliably.", "motivation": "Bayesian optimization struggles with high-dimensional multi-stage systems due to ignoring intermediate observations and process structure; high-dimensional state-space data makes using these observations challenging.", "method": "Propose POGPN-JPSS, integrating Partially Observable Gaussian Process Networks with Joint Parameter and State-Space modeling to utilize intermediate extracted information from process-expert knowledge.", "result": "POGPN-JPSS significantly outperforms state-of-the-art methods on a bioethanol production simulation, achieving desired performance twice as fast with greater reliability, leading to substantial time and resource savings.", "conclusion": "Combining expert knowledge with structured probabilistic models is crucial for rapid process maturation in complex manufacturing systems."}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "A neuro-symbolic pipeline using OpenMath ontology improves LLM performance on MATH benchmark when retrieval is accurate, but irrelevant context harms it.", "motivation": "To address limitations of language models like hallucination and brittleness in high-stakes specialist fields by testing if formal domain ontologies can enhance reliability through retrieval-augmented generation.", "method": "Implemented a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts.", "result": "Evaluation on the MATH benchmark with three open-source models shows ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it.", "conclusion": "Neuro-symbolic approaches show promise for enhancing LLM reliability but face challenges with retrieval relevance, highlighting the need for precise context injection."}}
{"id": "2602.17680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17680", "abs": "https://arxiv.org/abs/2602.17680", "authors": ["Yujia Wang", "Jihong Guan", "Wengen Li", "Shuigeng Zhou", "Xuhong Wang"], "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs", "comment": null, "summary": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.", "AI": {"tldr": "BioBridge: a framework for protein understanding combining domain knowledge from PLMs with general reasoning from LLMs through continual pretraining, achieving strong performance on both protein and general benchmarks.", "motivation": "Protein Language Models (PLMs) lack adaptability and generalization, while Large Language Models (LLMs) lack domain-specific knowledge for protein sequences, limiting effective biosemantic reasoning.", "method": "Proposes BioBridge, a domain-adaptive continual pretraining framework using Domain-Incremental Continual Pre-training (DICP) to incorporate protein knowledge and general reasoning, with cross-modal alignment via a PLM-Projector-LLM pipeline and end-to-end optimization for tasks like protein property prediction and QA.", "result": "BioBridge performs comparably to mainstream PLMs on protein benchmarks (e.g., EC, BindingDB) and on par with LLMs on general understanding tasks (e.g., MMLU, RACE).", "conclusion": "BioBridge successfully combines domain-specific adaptability with general-purpose language competency, offering an innovative solution for protein understanding."}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "The Token Games is an automated evaluation framework using LLMs to create puzzles for each other, ranking models via Elo ratings without human effort, showing alignment with existing benchmarks and identifying gaps in creativity.", "motivation": "To address challenges in evaluating LLM reasoning, such as high cost of human-curated questions and concerns about training data contamination, by designing an auto-competitive framework.", "method": "Inspired by 16th-century duels, it uses Programming Puzzles format: models create puzzles for each other, with solutions verified via Python function checks, and ranks models through pairwise duels and Elo ratings.", "result": "Evaluation of 10 frontier models on TTG closely matched rankings from benchmarks like Humanity's Last Exam, without human puzzle creation, and revealed that creating good puzzles is highly challenging for current models.", "conclusion": "The Token Games offers a paradigm for saturated-proof reasoning evaluation, testing skills like creativity and task creation alongside problem-solving, with potential for broader applications."}}
{"id": "2602.17681", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17681", "abs": "https://arxiv.org/abs/2602.17681", "authors": ["Ofir Gordon", "Lior Dikstein", "Arnon Netzer", "Idan Achituve", "Hai Victor Habi"], "title": "LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs", "comment": "24 pages, 4 figures", "summary": "Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.", "AI": {"tldr": "LATMiX introduces learnable affine transformations to reduce outliers in activations for microscaling (MX) quantization of LLMs, improving accuracy over baselines.", "motivation": "Existing PTQ methods rely on restricted transformations (e.g., rotation/Hadamard) and are not well-suited for MX formats, causing degradation when combined; a more general, learnable approach is needed.", "method": "Propose LATMiX, a method using learnable invertible affine transformations optimized via standard deep learning tools, based on theoretical analysis of quantization error under MX.", "result": "Experiments show consistent accuracy improvements for low-bit MX quantization across multiple model sizes and zero-shot benchmarks.", "conclusion": "LATMiX effectively generalizes outlier reduction for MX quantization, enhancing performance without restrictive assumptions."}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "El Agente Gr\u00e1fico is a single-agent LLM framework that uses type-safe execution and knowledge graphs for robust scientific automation, improving consistency and auditability.", "motivation": "Current agentic approaches for automating scientific workflows with LLMs rely on unstructured text, leading to fragile integration, overwhelming information, and poor decision provenance and auditability.", "method": "The framework embeds LLM-driven decision-making in a type-safe environment with dynamic knowledge graphs for persistence. It uses a structured abstraction of scientific concepts and an object-graph mapper to represent computational state as typed Python objects, stored in memory or persisted externally.", "result": "Evaluation on university-level quantum chemistry tasks shows a single agent with reliable execution can robustly perform complex, multi-step, parallel computations. Extended to conformer ensemble generation and metal-organic framework design, knowledge graphs serve as memory and reasoning substrates.", "conclusion": "Abstraction and type safety provide a scalable foundation for agentic scientific automation beyond prompt-centric designs, enabling consistency, provenance tracking, and efficient tool orchestration."}}
{"id": "2602.17682", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17682", "abs": "https://arxiv.org/abs/2602.17682", "authors": ["Peng Sun", "Xinyi Shang", "Tao Lin", "Zhiqiang Shen"], "title": "Duality Models: An Embarrassingly Simple One-step Generation Paradigm", "comment": "https://github.com/LINs-lab/DuMo", "summary": "Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional \"one input, one output\" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a \"one input, dual output\" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo", "AI": {"tldr": "Duality Models (DuMo) enhance generative models by using a 'one input, dual output' paradigm to predict velocity and flow-map simultaneously, improving stability and achieving SOTA results.", "motivation": "Traditional methods split training between multi-step and few-step objectives, causing undertraining and inefficiency; DuMo addresses this by integrating both objectives into a single framework.", "method": "Proposes DuMo with a shared backbone and dual heads to predict velocity v_t and flow-map u_t from input x_t, applying geometric constraints without separating training objectives.", "result": "On ImageNet 256\u00d7256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art FID of 1.79 in just 2 steps.", "conclusion": "DuMo offers a more stable and efficient approach for consistency-based generative models, eliminating the trade-off in training budgets and enhancing performance."}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "APEMO is a runtime scheduling layer that optimizes computational allocation using temporal-affective signals to enhance reliability in long-horizon autonomous agent workflows without modifying model weights.", "motivation": "Traditional AI alignment focuses on individual model outputs, but autonomous agents in long-horizon workflows need sustained reliability across entire interaction trajectories, highlighting a gap in current methods.", "method": "APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings, to optimize computational allocation under fixed budgets.", "result": "Evaluation in multi-agent simulations and LLM-based planner--executor flows shows APEMO consistently enhances trajectory-level quality and reuse probability compared to structural orchestrators.", "conclusion": "APEMO reframes alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems by improving reliability across entire workflows."}}
{"id": "2602.17683", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17683", "abs": "https://arxiv.org/abs/2602.17683", "authors": ["Irene Iele", "Giulia Romoli", "Daniele Molino", "Elena Mulero Ayll\u00f3n", "Filippo Ruffini", "Paolo Soda", "Matteo Tortora"], "title": "Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates", "comment": null, "summary": "Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.", "AI": {"tldr": "A probabilistic transformer-based framework for field-level NDVI forecasting under clear-sky constraints, using historical and future meteorological data with a novel loss function.", "motivation": "Short-term vegetation forecasting is crucial for precision agriculture, but NDVI prediction from satellite data is hindered by sparse, irregular sampling due to clouds and heterogeneous climatic conditions.", "method": "Proposes a transformer architecture that separates historical vegetation dynamics from future exogenous information, integrates historical NDVI with meteorological covariates, and uses a temporal-distance weighted quantile loss for irregular data and horizon-dependent uncertainty, plus cumulative and extreme-weather feature engineering.", "result": "Experiments on European satellite data show the approach outperforms various statistical, deep learning, and time series baselines in point-wise and probabilistic metrics, with ablation studies emphasizing the importance of target history and complementary meteorological data.", "conclusion": "The framework effectively addresses NDVI forecasting challenges, demonstrating improved performance and robustness, with code made publicly available for further use and development."}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "WorkflowPerturb is a benchmark with controlled perturbations to golden workflows for evaluating LLM-based workflow generation metrics, showing differences in metric sensitivity and calibration.", "motivation": "Automatic evaluation of LLM-generated structured workflows is challenging due to uncalibrated metrics and unclear severity interpretation of score changes.", "method": "Create WorkflowPerturb by applying realistic perturbations (Missing Steps, Compressed Steps, Description Changes) at severity levels (10%, 30%, 50%) to 4,973 golden workflows, resulting in 44,757 variants, then benchmark multiple metric families using expected score trajectories and residuals.", "result": "Benchmarking reveals systematic differences across metric families in sensitivity and calibration, enabling severity-aware interpretation of workflow evaluation scores.", "conclusion": "WorkflowPerturb provides a controlled framework to study and improve workflow evaluation metrics, with the dataset to be released upon acceptance."}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "CodeScaler is an execution-free reward model that scales RL training and test-time inference for code generation, outperforming binary execution-based RL and reducing latency.", "motivation": "RLVR relies on unit tests which limit scalability due to availability and reliability issues of high-quality test cases.", "method": "CodeScaler is trained on curated preference data with syntax-aware code extraction and validity-preserving reward shaping.", "result": "Improves Qwen3-8B-Base by +11.72 points on average across five benchmarks, beats binary execution-based RL by +1.82 points, enables RL on synthetic datasets without test cases, reduces inference latency 10-fold, and surpasses existing reward models in code and general domains.", "conclusion": "CodeScaler effectively addresses scalability limitations in RL for code generation, offering a robust and efficient alternative to execution-based methods."}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "Combining offline RL with cross-embodiment learning for scalable robot policy pre-training, using grouping to reduce morphological conflicts.", "motivation": "High cost of collecting high-quality demonstrations for each robot platform hinders scalable policy pre-training.", "method": "Unite offline reinforcement learning with cross-embodiment learning, analyze the paradigm, and use an embodiment-based grouping strategy to cluster robots by morphological similarity for gradient updates.", "result": "The combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning, but conflicts arise with more suboptimal data and robot types; grouping reduces conflicts and outperforms existing methods.", "conclusion": "The offline RL and cross-embodiment paradigm is effective for scalable pre-training, with grouping mitigating morphological conflicts, offering a practical solution for leveraging heterogeneous robot data."}}
{"id": "2602.17685", "categories": ["cs.LG", "cs.RO", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2602.17685", "abs": "https://arxiv.org/abs/2602.17685", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling", "comment": "Presented at Conference: IFAC Workshop on Control Aspects of Multi-Satellite Systems (CAMSAT) 2025 At: Wuerzburg", "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.", "AI": {"tldr": "A paper introducing a unified coelliptic maneuver framework for multi-target active debris removal in LEO, comparing planning algorithms (Greedy, MCTS, RL with Masked PPO) in simulations, finding Masked PPO most efficient.", "motivation": "To address the challenge of multi-target active debris removal in Low Earth Orbit, needing efficient and safe planning methods.", "method": "Introduce a unified coelliptic maneuver framework combining Hohmann transfers, safety ellipse proximity operations, and refueling logic; benchmark Greedy heuristic, MCTS, and deep RL with Masked PPO in realistic orbital simulations.", "result": "Masked PPO achieves superior mission efficiency, visiting up to twice as many debris as Greedy and outperforming MCTS in runtime, based on 100 test scenarios.", "conclusion": "Modern RL methods like Masked PPO show promise for scalable, safe, and resource-efficient space mission planning, advancing ADR autonomy."}}
{"id": "2602.18095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18095", "abs": "https://arxiv.org/abs/2602.18095", "authors": ["Hyunseok Oh", "Sam Stern", "Youngki Lee", "Matthai Philipose"], "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory", "comment": null, "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.", "AI": {"tldr": "Logitext is a neurosymbolic language using natural language text constraints to enable joint textual-logical reasoning in LLMs.", "motivation": "Large language models often fail at reliable interleaving of textual and logical reasoning, especially for natural documents with partial logical structure not fully formalizable.", "method": "Develop Logitext, which represents documents as natural language text constraints, and an algorithm integrating LLM-based constraint evaluation with satisfiability modulo theory solving.", "result": "Experiments on content moderation, LegalBench, and Super-Natural Instructions show improvements in accuracy and coverage.", "conclusion": "Logitext extends neurosymbolic methods beyond fully formalizable domains by treating LLM-based reasoning as an SMT theory."}}
{"id": "2602.17686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17686", "abs": "https://arxiv.org/abs/2602.17686", "authors": ["Bowen Yu", "Maolin Wang", "Sheng Zhang", "Binhao Wang", "Yi Wen", "Jingtong Gao", "Bowen Liu", "Zimo Zhao", "Wanyu Wang", "Xiangyu Zhao"], "title": "Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO", "comment": "22 pages, 12 figures", "summary": "Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.", "AI": {"tldr": "A three-stage curriculum learning framework distills concise and accurate chain-of-thought reasoning from large to small language models, outperforming prior methods on GSM8K.", "motivation": "Teacher rationales in chain-of-thought distillation are often too verbose for smaller student models to faithfully reproduce, while existing compression methods lose interpretability.", "method": "The framework includes: 1) masked shuffled reconstruction for structural understanding, 2) Group Relative Policy Optimization (GRPO) on masked completion tasks for balancing accuracy and brevity, and 3) targeted rewriting guided by persistent failure cases with GRPO optimization.", "result": "Qwen2.5-3B-Base achieved an 11.29% accuracy improvement and reduced output length by 27.4% on GSM8K, surpassing instruction-tuned variants and prior distillation methods.", "conclusion": "The proposed framework effectively addresses the capacity mismatch in chain-of-thought distillation by enabling smaller models to produce more concise and accurate reasoning."}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "SOMtime, a method using Self-Organizing Maps, shows unsupervised representations can still encode sensitive attributes like age and income, even when excluded from training, challenging the 'fairness through unawareness' assumption.", "motivation": "The paper challenges the assumption that unsupervised representations are neutral to sensitive attributes when those attributes are withheld during training. The motivation is to investigate whether fairness through unawareness holds at the representation level.", "method": "The study uses SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps. It tests this method on real-world datasets, including the World Values Survey across five countries and the Census-Income dataset, comparing it to PCA, UMAP, t-SNE, and autoencoders.", "result": "SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, while other methods like PCA and UMAP typically stay below 0.23. Unsupervised segmentation of SOMtime embeddings also produces demographically skewed clusters.", "conclusion": "The findings establish that fairness through unawareness fails at the representation level for ordinal sensitive attributes, and fairness auditing must extend to unsupervised components of machine learning pipelines. Code is made available for further research."}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "AnchorTree uses diffusion models guided by abstract syntax trees to generate structurally correct programs by prioritizing key code elements.", "motivation": "Existing diffusion language models for code generation fail to respect programming language structure, often producing broken programs that don't execute.", "method": "Introduces AnchorTree framework that explicitly anchors the diffusion process using structured, hierarchical priors from code, specifically using abstract syntax trees to prioritize syntactically and semantically salient tokens like keywords and identifiers.", "result": "Validated via AnCoder models, showing structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "conclusion": "AnchorTree enables diffusion models to generate executable, high-quality code by leveraging structural priors from abstract syntax trees, addressing the limitations of previous approaches."}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "OMAD is an online multi-agent RL framework using diffusion policies to enhance coordination and exploration via a relaxed entropy objective, achieving state-of-the-art sample efficiency.", "motivation": "Diffusion models offer high expressiveness but are under-explored in online MARL due to intractable likelihoods hindering entropy-based exploration and coordination.", "method": "Proposes OMAD with a relaxed policy objective maximizing scaled joint entropy, and uses a joint distributional value function in CTDE to optimize decentralized diffusion policies.", "result": "Extensive evaluations on MPE and MAMuJoCo show OMAD achieves new state-of-the-art across 10 diverse tasks with 2.5x to 5x improvement in sample efficiency.", "conclusion": "OMAD effectively tackles the challenge of intractable likelihoods in diffusion models for online MARL, demonstrating superior performance and coordination through innovative relaxation and value optimization."}}
{"id": "2602.17689", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17689", "abs": "https://arxiv.org/abs/2602.17689", "authors": ["Melika Filvantorkaman", "Mohsen Piri"], "title": "Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction", "comment": "28 pages, 3 figures", "summary": "Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.", "AI": {"tldr": "Robust-MMR is a self-supervised pre-training framework that enhances robustness in medical vision-language models against domain shifts through masked reconstruction with perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints.", "motivation": "Existing multi-modal pre-training methods often overlook robustness, treating it as a downstream problem, leading to performance degradation under domain shifts from variations in imaging devices, acquisition protocols, and reporting styles.", "method": "Proposes Robust Multi-Modal Masked Reconstruction (Robust-MMR), which integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations during self-supervised pre-training.", "result": "Achieves 78.9% cross-domain accuracy on VQA-RAD (3.8 percentage point improvement), 74.6% on SLAKE, 77.0% on VQA-2019, improves VQA-RAD accuracy from 69.1% to 75.6% under perturbation, increases MELINDA cross-domain accuracy from 70.3% to 75.2%, reduces mean rank degradation in retrieval from over 16 to 4.1, and shows improved qualitative results in clinical reasoning.", "conclusion": "Explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment, as demonstrated by superior performance across multiple benchmarks."}}
{"id": "2602.17691", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17691", "abs": "https://arxiv.org/abs/2602.17691", "authors": ["Craig Atkinson"], "title": "Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering", "comment": "16 pages, 6 tables", "summary": "Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.\n  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.\n  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.", "AI": {"tldr": "HELIX, a geometric framework, decouples output entropy from hallucination in quantized language models by tethering hidden-state trajectories to a truthfulness manifold, allowing high-temperature sampling without semantic collapse.", "motivation": "Quantized language models suffer from repetitive outputs at low temperatures and incoherent outputs at high temperatures due to trajectory divergence and hallucination.", "method": "Proposes HELIX with a Unified Truth Score combining token-level semantic entropy and Mahalanobis distance, using graduated steering vectors to correct trajectory divergence affecting only a small fraction of tokens.", "result": "On 4-bit quantized Granite 4.0 H Small, GSM8K accuracy is 88.84% at T=3.0 (2.81pp degradation), MMLU maintains 72.49%; steering sparse Transformer layers (~10%) corrects drift, enabling a 5-20% idea duplication rate versus 70-80% at conservative settings.", "conclusion": "HELIX enables Multi-Temperature Synthesis, generating 200% more unique concepts, showing high-temperature hallucination is trajectory divergence rather than semantic collapse, with architecture-independent benefits."}}
{"id": "2602.17692", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17692", "abs": "https://arxiv.org/abs/2602.17692", "authors": ["Bin Wang", "Fan Wang", "Pingping Wang", "Jinyu Cong", "Yang Yu", "Yilong Yin", "Zhongyi Han", "Benzheng Wei"], "title": "Agentic Unlearning: When LLM Agent Meets Machine Unlearning", "comment": "9 pages, 6 figures, 6 tables", "summary": "In this paper, we introduce \\textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "AI": {"tldr": "Agentic Unlearning removes specified info from model parameters and persistent memory via Synchronized Backflow Unlearning (SBU), preventing cross-pathway recontamination and reducing private info with minimal degradation.", "motivation": "Existing unlearning methods target parameters alone, leaving gaps: parameter-memory backflow where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and lack of unified strategy covering both pathways.", "method": "Synchronized Backflow Unlearning (SBU) framework unlearns jointly across parameter and memory pathways: memory pathway uses dependency closure-based unlearning to prune isolated entities and logically invalidate shared artifacts; parameter pathway employs stochastic reference alignment to guide model outputs toward high-entropy prior; integrated via synchronized dual-update protocol forming closed-loop mechanism.", "result": "Experiments on medical QA benchmarks show SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "conclusion": "Agentic unlearning and SBU effectively address parameter-memory gaps, enabling unified information removal and preventing cross-pathway recontamination in closed-loop agents."}}
{"id": "2602.17693", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17693", "abs": "https://arxiv.org/abs/2602.17693", "authors": ["Yuchen Luo", "Fangyue Zhu", "Ruining Zhou", "Mingzhe Huang", "Jian Zhu", "Fanyu Fan", "Wei Shao"], "title": "A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU", "comment": null, "summary": "Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.", "AI": {"tldr": "This paper investigates Post-Training Quantization (PTQ) effectiveness on Ascend NPU for reasoning models, finding platform sensitivity, with 4-bit weight-only viable for larger models, 4-bit weight-activation unstable, and real-world INT8 deployment limited by dynamic overheads.", "motivation": "PTQ is crucial for efficient model deployment, but its effectiveness on Ascend NPU remains under-explored compared to GPU architectures, necessitating a case study.", "method": "The paper presents a case study applying representative PTQ baselines (AWQ, GPTQ, SmoothQuant, FlatQuant) to reasoning models like DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B, evaluating from weight-only to advanced rotation-based methods.", "result": "Empirical results show significant platform sensitivity: 4-bit weight-only quantization is viable for larger models, but aggressive 4-bit weight-activation schemes suffer layer-wise calibration instability on NPU, causing logic collapse in long-context tasks; standard 8-bit quantization remains stable; real-world INT8 deployment shows optimized kernels reduce latency, but dynamic quantization overheads limit end-to-end acceleration.", "conclusion": "The findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU, highlighting challenges in quantization schemes and deployment efficiency."}}
{"id": "2602.17694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17694", "abs": "https://arxiv.org/abs/2602.17694", "authors": ["Hui Ma", "Shaoyu Dou", "Ya Liu", "Fei Xing", "Li Feng", "Feng Pi"], "title": "AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models", "comment": "Accepted in Scientific Reports", "summary": "With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.", "AI": {"tldr": "An asynchronous distributed bilevel tuning algorithm called AsynDBT is proposed to enhance LLM task performance by optimizing in-context learning samples and prompt fragments, addressing privacy and heterogeneity issues in federated learning settings.", "motivation": "Cloud-based LLM APIs require costly prompt adjustments, and in-context learning (ICL) faces challenges with high-quality, sensitive data sharing. Federated learning (FL) can help but struggles with straggler problems and heterogeneous data.", "method": "The AsynDBT algorithm optimizes both in-context learning samples and prompt fragments asynchronously in a distributed manner based on LLM feedback, leveraging federated learning for privacy and adaptability.", "result": "Theoretical analysis establishes convergence guarantees, and extensive experiments on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.", "conclusion": "AsynDBT effectively addresses key limitations in FL for LLMs by improving performance, ensuring privacy protection, and adapting to heterogeneous computing environments, offering a scalable solution for real-world applications."}}
{"id": "2602.17695", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17695", "abs": "https://arxiv.org/abs/2602.17695", "authors": ["Xin Yu", "Hanwen Xing", "Lingzhou Xue"], "title": "EXACT: Explicit Attribute-Guided Decoding-Time Personalization", "comment": null, "summary": "Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.", "AI": {"tldr": "EXACT is a decoding-time personalization method for LLMs that uses interpretable attributes and similarity-based retrieval to adapt to shifting user preferences across prompts.", "motivation": "Existing decoding-time personalization methods rely on implicit preference representations and rigid user models, failing to account for how preferences shift across different contexts and prompts.", "method": "EXACT uses a predefined set of interpretable attributes to represent preferences. It identifies user-specific attribute subsets offline by maximizing the likelihood of preferred responses, then retrieves the most semantically relevant attributes for each prompt during online inference to inject into the context and steer generation.", "result": "Extensive experiments on human-annotated preference datasets show that EXACT consistently outperforms strong baselines in both preference modeling accuracy and personalized generation quality.", "conclusion": "EXACT effectively mitigates contextual preference shifts by adapting to disparate tasks without pooling conflicting preferences, offering a scalable and interpretable approach to personalized alignment in large language models."}}
{"id": "2602.17696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17696", "abs": "https://arxiv.org/abs/2602.17696", "authors": ["Zongmin Li", "Jian Su", "Farah Benamara", "Aixin Sun"], "title": "Can LLM Safety Be Ensured by Constraining Parameter Regions?", "comment": "32 pages", "summary": "Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.", "AI": {"tldr": "Current techniques for identifying safety regions in LLMs show low overlap and instability, failing to find reliable dataset-agnostic safety parameters.", "motivation": "LLMs are assumed to contain safety regions\u2014parameter subsets affecting safety behaviors\u2014but it's unclear if current methods reliably identify them across datasets and models.", "method": "Systematic evaluation of four safety region identification methods across different granularities, applied to four families of backbone LLMs of varying sizes using ten safety datasets and utility datasets.", "result": "Identified safety regions exhibit low to moderate overlap, with IoU dropping significantly when refined with non-harmful queries, indicating high dataset dependency.", "conclusion": "Current methods fail to consistently identify stable, dataset-agnostic safety regions in LLMs, highlighting a need for more robust techniques."}}
{"id": "2602.17697", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17697", "abs": "https://arxiv.org/abs/2602.17697", "authors": ["Nada Zine", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.", "AI": {"tldr": "This paper applies variability management techniques to analyze and optimize the energy consumption of Large Language Model (LLM) inference configurations.", "motivation": "LLMs have high computational demands, especially during inference, which raises concerns about energy efficiency and sustainability. The vast configuration space of inference servers makes empirical evaluation infeasible due to combinatorial explosion.", "method": "Treats LLMs as configurable systems and uses variability management techniques. Specifically, it applies a feature-based variability model to represent generation hyperparameters and constraints, samples representative configurations, and measures energy consumption, latency, and accuracy. Predictive models are learned from the collected data.", "result": "Variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameter effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from limited measurements.", "conclusion": "This work bridges software engineering and machine learning by leveraging variability modeling for efficient and sustainable LLM configuration, opening a new research direction."}}
{"id": "2602.17698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17698", "abs": "https://arxiv.org/abs/2602.17698", "authors": ["Xinlin Li", "Timothy Chou", "Josh Fromm", "Zichang Liu", "Yunjie Pan", "Christina Fragouli"], "title": "ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs", "comment": null, "summary": "Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "AI": {"tldr": "ScaleBITS is a mixed-precision quantization framework that enables automated fine-grained bitwidth allocation for LLM weight compression below 4 bits, with hardware efficiency and no runtime overhead.", "motivation": "Pushing post-training weight quantization below 4 bits is challenging due to non-uniform weight sensitivity and lack of principled precision allocation; existing methods have high overhead or rely on heuristics.", "method": "Proposes ScaleBITS: sensitivity analysis, hardware-aligned block-wise weight partitioning with bi-directional channel reordering, and scalable approximation to a greedy algorithm for constrained optimization.", "result": "ScaleBITS improves over uniform-precision quantization by up to 36% and outperforms state-of-the-art baselines by up to 13% in ultra-low-bit regimes.", "conclusion": "ScaleBITS enables effective and efficient low-bit quantization for LLMs through automated, fine-grained bitwidth allocation without adding runtime overhead."}}
{"id": "2602.17699", "categories": ["cs.LG", "math.RA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17699", "abs": "https://arxiv.org/abs/2602.17699", "authors": ["Chandrasekhar Gokavarapu", "Sudhakar Gadde", "Y. Rajasekhar", "S. R. Bhargava"], "title": "Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure", "comment": null, "summary": "Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.", "AI": {"tldr": "Excess risk under distribution shift bounded by a computable metric; framework certifies risk, verifies models, ensures interpretability via identifiability.", "motivation": "To provide explicit, verifiable bounds on model performance under distribution shift, ensuring soundness for nontrivial cases and interpretability without post-hoc explanations.", "method": "Develops a unified framework with verifiable regularity and complexity constraints, using a computable shift metric and model parameters.", "result": "Risk under distribution shift is certified by explicit inequalities, verification is sound for nontrivial sizes, interpretability is enforced through identifiability conditions.", "conclusion": "The proposition offers a structured way to bound and certify excess risk under shift, with clear assumptions and characterized failure modes."}}
{"id": "2602.17700", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.17700", "abs": "https://arxiv.org/abs/2602.17700", "authors": ["Konstanty Subbotko"], "title": "MIDAS: Mosaic Input-Specific Differentiable Architecture Search", "comment": null, "summary": "Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.", "AI": {"tldr": "MIDAS modernizes DARTS with dynamic, input-specific architecture parameters via self-attention, patchwise localization, and a parameter-free search space, achieving high accuracy and optimal architectures in multiple benchmarks.", "motivation": "Differentiable NAS methods like DARTS are efficient but underused in practice, lacking robustness and input-specific adaptation.", "method": "Replaces static architecture parameters with dynamic ones computed via self-attention, localizes selection per spatial patch, and introduces a parameter-free, topology-aware search space for node connectivity.", "result": "Achieves 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100 in DARTS, finds globally optimal architectures in NAS-Bench-201, and sets state-of-the-art in two RDARTS spaces on CIFAR-10.", "conclusion": "MIDAS improves robustness and performance in NAS, with patchwise attention enhancing operation discrimination and providing reliable, class-aware guidance for decoding, validating its effectiveness."}}
{"id": "2602.17751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17751", "abs": "https://arxiv.org/abs/2602.17751", "authors": ["Nina Brolich", "Simon Geis", "Maximilian Kasper", "Alexander Barnhill", "Axel Plinge", "Dominik Seu\u00df"], "title": "Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.", "AI": {"tldr": "Proposes a method to deploy compressed neural networks on microcontrollers for real-time bird species detection via acoustic monitoring to address biodiversity loss.", "motivation": "Biodiversity loss threatens ecosystems; bird monitoring is crucial but traditional methods are costly and inefficient, while existing ML solutions are complex and resource-heavy.", "method": "Trains and compresses neural network models for bird species detection, deploying them on inexpensive microcontroller units (MCUs) in the field to assess hardware and energy constraints.", "result": "Achieves significant compression rates with minimal performance loss, benchmarks on various hardware, and evaluates feasibility for energy-autonomous devices.", "conclusion": "Efficient AI architectures on MCUs enable effective and low-cost avian monitoring, addressing conservation needs with deployable, energy-efficient solutions."}}
{"id": "2602.17706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17706", "abs": "https://arxiv.org/abs/2602.17706", "authors": ["Rongyao Cai", "Yuxi Wan", "Kexin Zhang", "Ming Jin", "Zhiqiang Ge", "Qingsong Wen", "Yong Liu"], "title": "Parallel Complex Diffusion for Scalable Time Series Generation", "comment": null, "summary": "Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \\textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.", "AI": {"tldr": "PaCoDi (Parallel Complex Diffusion) is a spectral-native architecture that decouples generative modeling in the frequency domain for time series, improving efficiency and quality by using Fourier Transform to reduce attention costs and handle dependencies.", "motivation": "Traditional temporal diffusion models struggle with long-range dependencies due to local entanglement and high O(L^2) attention costs, limiting scalability and efficiency in time series generation.", "method": "Introduces PaCoDi with Fourier Transform to diagonalize signals into spectral components, uses Quadrature Forward Diffusion and Conditional Reverse Factorization to split complex diffusion, applies Mean Field Theory approximation with correction, generalizes to Frequency SDEs, and exploits Hermitian Symmetry for compression and Heteroscedastic Loss.", "result": "PaCoDi reduces attention FLOPs by 50% without information loss, outperforms baselines in generation quality and inference speed in extensive experiments.", "conclusion": "PaCoDi provides a theoretically grounded and computationally efficient solution for modeling long-range dependencies in time series, balancing representational capacity and efficiency."}}
{"id": "2602.17865", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17865", "abs": "https://arxiv.org/abs/2602.17865", "authors": ["Andrzej Podobi\u0144ski", "Jaros\u0142aw A. Chudziak"], "title": "Financial time series augmentation using transformer based GAN architecture", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.", "AI": {"tldr": "Using GANs for data augmentation improves LSTM forecasting in finance by addressing data scarcity.", "motivation": "Applying deep learning models in volatile financial domains is challenging due to limited and dynamic time series data, leading to poor model training and generalization.", "method": "Train an LSTM forecasting model on datasets augmented with synthetic data from a transformer-based GAN (TTS-GAN). Evaluate using a novel metric combining DTW and modified DeD-iMs for data quality.", "result": "Forecasting accuracy significantly improves compared to using real data alone, confirmed across Bitcoin and S&P500 data and various forecasting horizons.", "conclusion": "GAN-based data augmentation enhances predictive capabilities in finance, providing a reliable approach to overcome data scarcity."}}
{"id": "2602.17743", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17743", "abs": "https://arxiv.org/abs/2602.17743", "authors": ["Di Zhang"], "title": "Provable Adversarial Robustness in In-Context Learning", "comment": "16 pages", "summary": "Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($\u03c1$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($\u03c1_{\\text{max}} \\propto \\sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_\u03c1- N_0 \\propto \u03c1^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.", "AI": {"tldr": "This paper analyzes the robustness of in-context learning (ICL) in large language models under adversarial distribution shifts, using a distributionally robust meta-learning framework to derive theoretical bounds and scaling laws.", "motivation": "Existing theoretical explanations for ICL assume test tasks come from a distribution similar to pretraining, which overlooks adversarial shifts threatening real-world reliability. The paper aims to address this gap.", "method": "The authors introduce a distributionally robust meta-learning framework with Wasserstein-based shifts, focusing on linear self-attention Transformers to derive non-asymptotic bounds linking perturbation strength, model capacity, and number of in-context examples.", "result": "The analysis shows that model robustness scales with the square root of its capacity, while adversarial settings increase sample complexity proportionally to the square of perturbation magnitude. Experiments on synthetic tasks confirm these scaling laws.", "conclusion": "These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity is a fundamental resource for distributional robustness."}}
