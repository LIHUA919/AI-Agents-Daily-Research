<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-α Optimization](https://arxiv.org/abs/2511.09563)
*Qilong Yuan*

Main category: cs.AI

TL;DR: Novel PPR solver for JRA optimization achieves near-optimal solutions with 0.00% average deviation while maintaining computational efficiency, applicable to TSP and related problems.


<details>
  <summary>Details</summary>
Motivation: Previous exact methods for JRA optimization become computationally inefficient for large-scale instances, requiring more efficient high-accuracy approaches.

Method: Proposed Partial Path Reconstructon (PPR) solver identifies key item-placeholder pairs to form reduced subproblems, integrates global Large-α constraint, and iteratively polishes solutions along optimization path.

Result: Achieves almost optimal solutions with 0.00% average deviation from ground truth on benchmark datasets (n=300,500,1000) while maintaining high computational efficiency.

Conclusion: The PJAR framework with PPR solver provides highly accurate JRA solutions and exhibits strong potential for broader applications in TSP and related optimization problems.

Abstract: The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-α constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.

</details>


### [2] [Variable Neighborhood Search for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2511.09570)
*David Woller,Viktor Kozák,Miroslav Kulich,Libor Přeučil*

Main category: cs.AI

TL;DR: This paper presents the competition-winning Variable Neighborhood Search (VNS) approach for solving the Capacitated Green Vehicle Routing Problem (CGVRP), achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The growing use of electric vehicles in logistics creates complex routing problems with various constraints, making it difficult to compare different solution approaches across problem variants. The Capacitated Green Vehicle Routing Problem (CGVRP) provides a standardized minimalistic variant for fair comparison.

Method: The authors developed a solution based on the Variable Neighborhood Search (VNS) metaheuristic, which systematically explores different neighborhood structures to escape local optima and find high-quality solutions.

Result: The proposed VNS-based method achieved the best results in the CEC-12 competition dataset and outperformed a more recent algorithm published after the competition.

Conclusion: The Variable Neighborhood Search approach proves highly effective for solving the CGVRP, demonstrating superior performance and establishing a strong benchmark for this electric vehicle routing problem variant.

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning](https://arxiv.org/abs/2511.09792)
*Tianmeng Hu,Yongzheng Cui,Rui Tang,Biao Luo,Ke Li*

Main category: cs.LG

TL;DR: Non-monotonic value decomposition in MARL can achieve IGM consistency without constraints, outperforming monotonic approaches through dynamical systems analysis.


<details>
  <summary>Details</summary>
Motivation: Existing value decomposition methods face trade-offs between expressive power (monotonic constraints) and complexity (softer surrogates).

Method: Dynamical systems analysis using continuous-time gradient flow to model learning dynamics, proving unstable equilibria for IGM violations.

Result: Experiments on matrix games and MARL benchmarks show non-monotonic factorization reliably achieves IGM-optimal solutions and beats monotonic baselines.

Conclusion: Unconstrained factorization is viable for IGM consistency, with insights on TD targets and exploration strategies for future MARL algorithms.

Abstract: Value decomposition is a central approach in multi-agent reinforcement learning (MARL), enabling centralized training with decentralized execution by factorizing the global value function into local values. To ensure individual-global-max (IGM) consistency, existing methods either enforce monotonicity constraints, which limit expressive power, or adopt softer surrogates at the cost of algorithmic complexity. In this work, we present a dynamical systems analysis of non-monotonic value decomposition, modeling learning dynamics as continuous-time gradient flow. We prove that, under approximately greedy exploration, all zero-loss equilibria violating IGM consistency are unstable saddle points, while only IGM-consistent solutions are stable attractors of the learning dynamics. Extensive experiments on both synthetic matrix games and challenging MARL benchmarks demonstrate that unconstrained, non-monotonic factorization reliably recovers IGM-optimal solutions and consistently outperforms monotonic baselines. Additionally, we investigate the influence of temporal-difference targets and exploration strategies, providing actionable insights for the design of future value-based MARL algorithms.

</details>


### [4] [Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding](https://arxiv.org/abs/2511.09559)
*Tianlei Chen,Yuxiao Chen,Yang Li,Feifei Wang*

Main category: cs.LG

TL;DR: A directed bipartite graph encoder method for ICD coding that leverages co-occurrence relationships between common and rare codes, enhanced by LLM-generated descriptions, achieving SOTA performance with significant Macro-F1 improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of large label space (10,000-20,000 codes) and long-tail distribution in automated ICD coding, where rare codes lack sufficient training data while a few common codes dominate.

Method: A Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes, using probability-based co-occurrence relationships and attention mechanism with Co-occurrence Encoding, combined with LLM-generated code descriptions for initial embeddings.

Result: Achieves state-of-the-art performance on three automated ICD coding benchmark datasets with particularly notable improvements in Macro-F1, the key metric for long-tail classification.

Conclusion: The proposed method achieves state-of-the-art performance in automated ICD coding, with significant improvements in Macro-F1 scores across three benchmark datasets, validating the effectiveness of modeling fine-grained co-occurrence relationships and leveraging LLM-generated descriptions.

Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.

</details>


### [5] [Towards Emotionally Intelligent and Responsible Reinforcement Learning](https://arxiv.org/abs/2511.10573)
*Garapati Keerthana,Manik Gupta*

Main category: cs.LG

TL;DR: Proposes a Responsible Reinforcement Learning (RRL) framework that incorporates emotional context and ethical constraints into personalized decision systems, using constrained MDPs to balance engagement with user well-being.


<details>
  <summary>Details</summary>
Motivation: Current personalized decision systems in healthcare rely on static or engagement-maximizing heuristics that ignore emotional context and ethical considerations, risking insensitive or unsafe interventions.

Method: Formulates personalization as a Constrained Markov Decision Process (CMDP) with multi-objective rewards balancing short-term engagement and long-term well-being, using emotion-informed state representations and safety-augmented RL algorithms.

Result: A conceptual framework that operationalizes empathy and responsibility in ML policy optimization, bridging safe RL, affective computing, and responsible AI for domains like behavioral health.

Conclusion: This work initiates a methodological conversation toward ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems, with simulation-based validation proposed.

Abstract: Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.

</details>


### [6] [Let the Experts Speak: Improving Survival Prediction & Calibration via Mixture-of-Experts Heads](https://arxiv.org/abs/2511.09567)
*Todd Morrill,Aahlad Puli,Murad Megjhani,Soojin Park,Richard Zemel*

Main category: cs.LG

TL;DR: Expressive deep mixture-of-experts for survival analysis can cluster patients while maintaining calibration and accuracy by tailoring predictions per patient rather than using fixed group prototypes.


<details>
  <summary>Details</summary>
Motivation: To discover patient group structure while improving calibration and predictive accuracy, overcoming the restrictive inductive bias of traditional mixture-of-experts where predictions must resemble group assignments.

Method: Introduced several discrete-time deep mixture-of-experts (MoE) architectures for survival analysis, with varying expressiveness of experts.

Result: Found that more expressive experts which tailor predictions per patient outperform experts that rely on fixed group prototypes, with one MoE architecture achieving all desiderata: clustering, calibration, and predictive accuracy.

Conclusion: Deep mixture-of-experts models can successfully achieve clustering, calibration, and predictive accuracy simultaneously when experts are expressive enough to tailor predictions per patient rather than relying on fixed group prototypes.

Abstract: Deep mixture-of-experts models have attracted a lot of attention for survival analysis problems, particularly for their ability to cluster similar patients together. In practice, grouping often comes at the expense of key metrics such calibration error and predictive accuracy. This is due to the restrictive inductive bias that mixture-of-experts imposes, that predictions for individual patients must look like predictions for the group they're assigned to. Might we be able to discover patient group structure, where it exists, while improving calibration and predictive accuracy? In this work, we introduce several discrete-time deep mixture-of-experts (MoE) based architectures for survival analysis problems, one of which achieves all desiderata: clustering, calibration, and predictive accuracy. We show that a key differentiator between this array of MoEs is how expressive their experts are. We find that more expressive experts that tailor predictions per patient outperform experts that rely on fixed group prototypes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [7] [Multi-agent In-context Coordination via Decentralized Memory Retrieval](https://arxiv.org/abs/2511.10030)
*Tao Jiang,Zichuan Lin,Lihe Li,Yi-Chen Li,Cong Guan,Lei Yuan,Zongzhang Zhang,Yang Yu,Deheng Ye*

Main category: cs.MA

TL;DR: MAICC is a new multi-agent RL method that improves coordination by using decentralized memory retrieval and hybrid utility scores for better credit assignment, enabling faster adaptation to unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Decentralized policy deployment in cooperative MARL causes task alignment mismatches and reward assignment issues, limiting policy adaptation efficiency.

Method: Trains a centralized embedding model for trajectory representations, then uses decentralized models to approximate it. Retrieves relevant trajectories as context combined with current sub-trajectories for decision-making. Introduces a memory mechanism balancing online and offline data, using a hybrid utility score for credit assignment.

Result: Extensive experiments on LBF and SMAC benchmarks show MAICC enables faster adaptation to unseen tasks compared to existing methods.

Conclusion: MAICC effectively addresses coordination challenges in cooperative MARL through in-context coordination and decentralized memory retrieval, demonstrating superior adaptation performance.

Abstract: Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.

</details>


### [8] [Behavior Modeling for Training-free Building of Private Domain Multi Agent System](https://arxiv.org/abs/2511.10283)
*Won Ik Cho,Woonghee Han,Kyung Seo Ki,Young Min Kim*

Main category: cs.MA

TL;DR: Introduces a framework for private-domain multi-agent systems using behavior modeling and documentation to avoid training, enabling scalable tool integration with orchestrator, tool-caller, and chat agent components.


<details>
  <summary>Details</summary>
Motivation: Applying open-domain agent systems to private domains is challenging due to tool heterogeneity, jargon, API restrictions, and governance; existing fine-tuning methods are inefficient and fragile.

Method: Uses a framework with orchestrator, tool-calling agent, and general chat agent, integrating tools via structured specifications and domain instructions without training or data generation.

Result: Enables scalable adaptation to private tools and contexts, supports deployment, API specification retrieval, and synthetic dialogue generation for evaluation.

Conclusion: Provides a sustainable method to align agent behavior with domain expertise in private conversational ecosystems without continual retraining.

Abstract: The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.

</details>


### [9] [Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance](https://arxiv.org/abs/2511.10400)
*Lifan Zheng,Jiawei Chen,Qinghong Yin,Jingyuan Zhang,Xinyi Zeng,Yu Tian*

Main category: cs.MA

TL;DR: Proposes CP-WBFT, a weighted Byzantine Fault Tolerant mechanism leveraging LLM agents' skepticism to enhance multi-agent system reliability under extreme fault conditions.


<details>
  <summary>Details</summary>
Motivation: The reliability implications of shifting from traditional agents to LLM-based agents in multi-agent systems remain unexplored, despite LLM advancements enabling breakthroughs in complex problem solving.

Method: Designs CP-WBFT, a confidence probe-based weighted consensus mechanism that uses LLMs' reflective capabilities through weighted information flow transmission.

Result: CP-WBFT achieves superior performance across diverse topologies under extreme Byzantine conditions (85.7% fault rate), surpassing traditional methods in accuracy and reliability.

Conclusion: LLM-based agents demonstrate stronger skepticism against erroneous messages, making them effective for enhancing MAS reliability through mechanisms like CP-WBFT.

Abstract: Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.

</details>
