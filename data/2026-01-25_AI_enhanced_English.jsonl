{"id": "2601.16091", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16091", "abs": "https://arxiv.org/abs/2601.16091", "authors": ["Saar Cohen"], "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals", "comment": "To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026", "summary": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.", "AI": {"tldr": "Novel framework for online non-centroid clustering with delays, where decisions can be postponed at a cost. Under stochastic arrivals, the algorithm achieves constant competitive ratio.", "motivation": "Traditional online clustering requires immediate assignment, but allowing delays can optimize costs. In worst-case models, no good competitive ratio exists, motivating study of stochastic models for better theoretical guarantees.", "method": "Introduced framework where points arrive sequentially, revealing locations. Algorithm can assign to existing clusters or create new ones, but can delay assignments, incurring delay costs. Goal is minimizing distance and delay costs. Used stochastic arrival model with points from unknown distribution.", "result": "In worst-case arbitrary arrivals, algorithms have only sublogarithmic competitive ratios. In contrast, under stochastic arrivals from a fixed distribution, an algorithm achieves a constant competitive ratio, bounding expected costs versus optimal offline clustering.", "conclusion": "By incorporating delays and stochastic assumptions, it overcomes impossibility results of worst-case models, enabling constant-competitive online clustering. Highlights benefit of beyond worst-case analysis."}}
{"id": "2601.16187", "categories": ["cs.MA", "cs.GT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16187", "abs": "https://arxiv.org/abs/2601.16187", "authors": ["Pan-Yang Su", "Arwa Alanqary", "Bryce L. Ferguson", "Manxi Wu", "Alexandre M. Bayen", "Shankar Sastry"], "title": "Average Unfairness in Routing Games", "comment": "14 pages, 5 figures, 1 table. Accepted for publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.", "AI": {"tldr": "Introduces average unfairness as a new fairness measure in routing games, compares it with existing measures, shows worst-case values coincide, proves advantages in minimizing total latency under unfairness constraints, and provides theoretical insights.", "motivation": "To develop a complementary fairness measure (average unfairness) for routing games that addresses limitations of existing metrics like loaded and UE unfairness, enabling better evaluation of fairness-efficiency tradeoffs.", "method": "Theoretical analysis defining average unfairness mathematically, comparing it analytically with loaded and UE unfairness, studying worst-case bounds via steepness parameters, and analyzing the constrained system optimum problem under unfairness constraints.", "result": "Average unfairness is always \u2264 loaded unfairness, with equality only in fully fair flows. Worst-case values for all three measures coincide. Under average unfairness constraints, optimal flows achieve lower total latency than under loaded constraints, especially in parallel-link networks.", "conclusion": "Average unfairness provides a robust and effective fairness measure with theoretical advantages over existing ones, offering practical insights for network routing design to balance fairness and efficiency, supported by proofs and numerical examples."}}
{"id": "2601.15487", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15487", "abs": "https://arxiv.org/abs/2601.15487", "authors": ["Chandan Kumar Sahu", "Premith Kumar Chilukuri", "Matthew Hetrich"], "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation", "comment": "12 pages, 2 figures, Submitted to ACL", "summary": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.", "AI": {"tldr": "MiRAGE is a multiagent framework for generating domain-specific, multimodal, multi-hop QA datasets to evaluate RAG systems, addressing the lack of specialized benchmarks.", "motivation": "The evolution of RAG systems into multimodal enterprise applications has outstripped the development of evaluation benchmarks, as existing datasets use general-domain corpora or textual-only retrieval, failing to capture the complexity of specialized technical documents where information is multimodal and reasoning requires synthesizing disparate evidence.", "method": "Introduce MiRAGE, a multiagent framework with specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent for factual grounding, and an agent to recognize expert personas and domains, automating the generation of verified datasets.", "result": "Empirical evaluation across four domains (regulations, finance, quantitative biology, journalism) shows MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness; ablation studies indicate capability with LLMs if image descriptions are available, though visual grounding remains challenging.", "conclusion": "MiRAGE provides automated infrastructure for creating gold standard evaluation datasets that reflect proprietary corpora structures, enabling rigorous benchmarking of next-generation information retrieval systems."}}
{"id": "2601.15551", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15551", "abs": "https://arxiv.org/abs/2601.15551", "authors": ["Bismack Tokoli", "Luis Jaimes", "Ayesha S. Dina"], "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance", "comment": "35 pages", "summary": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "AI": {"tldr": "ALIGNAgent is an AI-based multi-agent framework that provides personalized learning by integrating knowledge estimation, skill-gap identification, and resource recommendation to enhance student outcomes.", "motivation": "Existing personalized learning systems are often fragmented, focusing separately on knowledge tracing, diagnostics, or recommendations without a cohesive adaptive cycle, limiting effectiveness.", "method": "ALIGNAgent uses multiple agents: a Skill Gap Agent processes quiz performance, gradebook data, and preferences for proficiency estimation and misconception identification, and a Recommender Agent provides targeted materials in a feedback loop before advancing topics.", "result": "Empirical tests on undergraduate computer science courses show high performance, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge estimation validated against exam performance.", "conclusion": "ALIGNAgent effectively integrates key components into a unified adaptive system, demonstrating improved personalized learning through data-driven diagnostics and recommendations, with potential for broader educational applications."}}
{"id": "2601.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15305", "abs": "https://arxiv.org/abs/2601.15305", "authors": ["Alfred Shen", "Aaron Shen"], "title": "Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models", "comment": "15 pages, 1 figure, attention mechanism, sparse attention, gating, long-context", "summary": "The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.", "AI": {"tldr": "Gated Sparse Attention (GSA) combines sparse and gated attention mechanisms to reduce computational cost, improve training stability, and enhance performance in long-context language models.", "motivation": "Traditional attention mechanisms in long-context models are computationally heavy, while sparse and gated attention address separate issues (complexity vs. stability). GSA aims to unify their benefits to overcome both weaknesses.", "method": "GSA uses a gated scorer with sigmoid activations for bounded token selection, an adaptive sparsity controller to adjust attended tokens based on uncertainty, and dual gating at value and output stages, backed by theoretical analysis.", "result": "Experiments with 1.7B models on 400B tokens show GSA achieves 12-16x speedup at 128K context, reduces perplexity from 6.03 to 5.70, nearly doubles RULER scores, cuts attention to first token from 47% to under 4%, and reduces loss spikes by 98%.", "conclusion": "Gated Sparse Attention effectively integrates sparse and gated attention, offering computational efficiency, improved model quality, and enhanced training stability for long-context language processing."}}
{"id": "2601.15333", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.15333", "abs": "https://arxiv.org/abs/2601.15333", "authors": ["Xuanning Hu", "Anchen Li", "Qianli Xing", "Jinglong Ji", "Hao Tuo", "Bo Yang"], "title": "Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference", "comment": null, "summary": "Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.", "AI": {"tldr": "ELILLM enhances LLMs for drug design by adding latent exploration and knowledge-guided decoding to generate valid molecules with high binding affinity.", "motivation": "Large Language Models have strong abilities but struggle in structure-based drug design due to poor understanding of protein structures and unpredictable molecular generation.", "method": "Proposes ELILLM framework, reinterpreting LLM generation as encoding, latent space exploration with Bayesian optimization and a position-aware surrogate model, and decoding with chemical validity constraints.", "result": "Tested on CrossDocked2020 benchmark, ELILLM shows strong controlled exploration and achieves high binding affinity scores, outperforming seven baseline methods.", "conclusion": "ELILLM effectively enhances LLMs capabilities for structure-based drug design by addressing key limitations in protein understanding and molecular generation."}}
