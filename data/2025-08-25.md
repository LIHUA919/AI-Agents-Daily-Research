<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: T-ILR is a neurosymbolic framework that integrates temporal logic specifications (LTLf) directly into deep learning for sequence tasks, improving accuracy and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches for integrating symbolic knowledge with deep learning work well in static domains but lack effective methods for handling temporal logic specifications, which are crucial for sequence-based tasks.

Method: Extends the Iterative Local Refinement (ILR) algorithm using fuzzy LTLf interpretations to create Temporal ILR (T-ILR), incorporating temporal logic directly into deep learning architectures without requiring explicit finite-state automata.

Result: T-ILR demonstrated improved accuracy and computational efficiency compared to state-of-the-art methods on a benchmark for temporal neurosymbolic architectures involving image sequence classification.

Conclusion: The proposed T-ILR framework successfully addresses the gap in handling temporal logic specifications for neurosymbolic learning, offering a more efficient and accurate approach for sequence-based tasks with temporal constraints.

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [2] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: CoFE framework generates counterfactual ECGs to explain AI-based ECG prediction models by showing how specific ECG features influence predictions, with case studies on atrial fibrillation classification and potassium level regression.


<details>
  <summary>Details</summary>
Motivation: Need for explainable AI approaches to successfully integrate AI-based ECG prediction models into clinical practice by making model decisions interpretable to clinicians.

Method: Developed a framework called CoFE (CounterFactual ECGs) that generates modified ECG signals to illustrate how specific features (amplitudes, intervals) affect model predictions.

Result: CoFE reveals feature changes in ECG signals that align with established clinical knowledge, showing where valid features appear and how they influence model predictions.

Conclusion: The framework enhances interpretability of AI-ECG models and supports more effective clinical decision-making by clarifying model behavior through counterfactual ECG examples.

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [3] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: Training-free multimodal QA framework using Adaptive Planning Graph for dynamic reasoning paths without costly training


<details>
  <summary>Details</summary>
Motivation: Existing multimodal QA methods rely on sequential single-path reasoning that is vulnerable to errors from misleading intermediate steps and requires expensive training

Method: Adaptive Planning Graph with planning, retrieval, and reasoning modules that dynamically explore reasoning paths and use modality-specific strategies for cross-modal retrieval

Result: Matches or outperforms existing trained models on MultimodalQA and WebQA benchmarks

Conclusion: Proposed training-free framework enables flexible multimodal reasoning without costly training while maintaining performance

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [4] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP is a multimodal foundation model that combines structured EHR time-series data with unstructured clinical notes using CNN-Transformer encoder and cross-modal attention, achieving superior clinical prediction performance and high-quality narrative generation.


<details>
  <summary>Details</summary>
Motivation: Electronic health records contain rich but heterogeneous data (structured time-series and unstructured notes), but current approaches serialize numeric data into text, losing temporal and quantitative details. There's a need for better multimodal integration to improve patient outcomes.

Method: Two-stage training: (1) generative pretraining with masked feature prediction and next time-step prediction to capture temporal dynamics, (2) multi-task fine-tuning for clinical predictions. Uses CNN-Transformer encoder for structured time-series and cross-modal attention with LLaMA-based decoder.

Result: Superior performance on MIMIC-IV: heart failure AUROC=0.923, type 2 diabetes AUROC=0.817, 30-day readmission AUROC=0.627. Narrative generation: ROUGE-L=0.135, BERTScore-F1=0.545. Blinded human evaluation showed highest scores on faithfulness, fluency, and clinical utility.

Conclusion: A single multimodal foundation model can effectively predict clinically actionable events and generate high-quality clinical narratives, potentially reducing hospital documentation workload while maintaining accuracy. The flexible architecture supports extension to additional modalities.

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [5] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: This paper explores theoretical interpretations and methodologies for assessing urban comfort in digital planning, focusing on three key dimensions: multidimensional analysis, data support, and AI assistance.


<details>
  <summary>Details</summary>
Motivation: Urban comfort is a fundamental objective of urban planning, but there's no clear definition or comprehensive evaluation framework despite numerous computational studies on factors like greenery coverage, thermal comfort, and walkability.

Method: The research explores theoretical interpretations and methodologies for urban comfort assessment, emphasizing three key dimensions: multidimensional analysis, data support, and AI assistance within digital planning contexts.

Result: The paper establishes a framework for understanding and evaluating urban comfort through digital planning approaches, though specific quantitative results are not provided in the abstract.

Conclusion: The research provides a structured approach to defining and assessing urban comfort through digital planning methodologies, addressing the current lack of comprehensive evaluation frameworks in this domain.

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [6] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: A competitor-discovery AI agent for drug asset due diligence that retrieves competitive landscape drugs and achieves 83% recall, outperforming existing systems while reducing analysis time from 2.5 days to ~3 hours.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems cannot reliably retrieve all competing drug names for investor-specific competitor definitions, with data being paywalled, fragmented, and rapidly changing across multiple registries with ontology mismatches.

Method: Uses LLM-based agents to transform 5 years of multi-modal unstructured diligence memos into a structured evaluation corpus, and introduces a competitor-validating LLM-as-a-judge agent to filter false positives and suppress hallucinations.

Result: Achieves 83% recall on the benchmark, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). In production deployment, reduced analyst turnaround time from 2.5 days to ~3 hours (~20x improvement).

Conclusion: The developed competitor-discovery agent successfully addresses the limitations of existing systems for drug competitive landscape analysis, demonstrating significant performance improvements and practical efficiency gains for biotech VC due diligence processes.

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


### [7] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF framework enables LLMs to access time series patterns at all depths through multi-layer embedding fusion, reducing information loss in deeper layers and achieving 31.8% average MSE reduction.


<details>
  <summary>Details</summary>
Motivation: Existing LLM adaptation methods for time series forecasting suffer from shallow integration where TS information fades in deeper layers, leading to ineffective adaptation between textual and TS representations.

Method: Proposes Multi-layer Steerable Embedding Fusion (MSEF) that uses time series foundation models to extract embeddings, fused with intermediate text representations via layer-specific steering vectors for continuous modality alignment.

Result: Experimental results on seven benchmarks show significant performance improvements with average 31.8% reduction in MSE compared to baselines.

Conclusion: MSEF effectively mitigates progressive loss of time series information in deeper LLM layers and enables efficient few-shot learning capabilities through layer-specific adaptation mechanisms.

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [8] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: InMind is a framework to evaluate if LLMs can capture personalized reasoning styles in social deduction games, showing current LLMs struggle with individualized adaptive reasoning.


<details>
  <summary>Details</summary>
Motivation: Previous evaluations overlook individualized reasoning styles that influence human social interpretation, and social deduction games provide a natural testbed for assessing personalized reasoning strategies.

Method: InMind enhances structured gameplay data with round-level strategy traces and post-game reflections collected under Observer and Participant modes, supporting four cognitively motivated tasks evaluating both static alignment and dynamic adaptation.

Result: General-purpose LLMs like GPT-4o rely on lexical cues and struggle with temporal anchoring and strategy adaptation, while reasoning-enhanced LLMs like DeepSeek-R1 show early signs of style-sensitive reasoning.

Conclusion: Current LLMs have key limitations in individualized adaptive reasoning capacity, and InMind represents a step toward cognitively aligned human-AI interaction.

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [9] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent: A multi-agent framework for molecular structure elucidation from IR spectra that emulates expert analytical processes and integrates diverse chemical knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing IR spectroscopy approaches fail to reflect expert analytical processes and lack flexibility in incorporating diverse chemical knowledge needed for real-world analysis.

Method: Proposed IR-Agent, a novel multi-agent framework where each agent specializes in a specific aspect of IR interpretation, enabling integrated reasoning through complementary roles.

Result: Extensive experiments show IR-Agent improves baseline performance on experimental IR spectra and demonstrates strong adaptability to various forms of chemical information.

Conclusion: The framework successfully emulates expert-driven IR analysis procedures, is inherently extensible, and enhances overall accuracy of molecular structure elucidation.

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [10] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: Proposes a Food Claim-Traceability Network (FCN) to systematically trace, verify, and contextualize diverse food claims using knowledge graphs and LLMs.


<details>
  <summary>Details</summary>
Motivation: Address fragmented infrastructure for verifying food claims ranging from scientific evidence to cultural beliefs and commercial misrepresentations.

Method: Extends FKG.in knowledge graph with ontology design and semi-automated curation workflow using Reddit data and Large Language Models for claim extraction and validation.

Result: Developed proof-of-concept FCN that integrates structured schemas and provenance-aware pipelines for food claim traceability.

Conclusion: Provides adaptable methodology for transparent food knowledge ecosystems to help consumers navigate dietary assertions.

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [11] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: MM-Retinal-Reason is the first ophthalmic multimodal dataset with comprehensive perception and reasoning tasks, and OphthaReason is the first ophthalmology-specific multimodal reasoning model that achieves state-of-the-art performance using Uncertainty-Aware Dynamic Thinking.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reasoning models in medicine focus only on basic visual feature matching, but real clinical diagnosis requires integrating heterogeneous clinical information with multimodal imaging data for complex reasoning.

Method: Proposed OphthaReason model with Uncertainty-Aware Dynamic Thinking (UADT) that estimates sample-level uncertainty via entropy and dynamically modulates exploration depth using a shaped advantage mechanism.

Result: Achieves state-of-the-art performance, outperforming general-purpose MLLMs by 24.92%, medical MLLMs by 15.00%, RL-based medical MLLMs by 21.20%, and ophthalmic MLLMs by 17.66%.

Conclusion: The work bridges the gap between basic and complex clinical reasoning by introducing a specialized ophthalmology multimodal reasoning model and dataset that emulates realistic clinical thinking patterns.

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [12] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: The paper introduces Preference Chain, a novel method combining Graph RAG with LLMs to simulate human transportation behavior more accurately than standard LLMs, particularly useful for data-scarce urban environments.


<details>
  <summary>Details</summary>
Motivation: Collecting accurate behavioral data in urban environments is challenging, especially in newly developed areas. Existing LLM-based generative agents struggle with consistent and context-sensitive behavioral outputs.

Method: Preference Chain method that integrates Graph Retrieval-Augmented Generation (RAG) with Large Language Models to enhance context-aware simulation of human behavior in transportation systems.

Result: Experiments on Replica dataset show Preference Chain outperforms standard LLMs in aligning with real-world transportation mode choices.

Conclusion: The method offers a promising framework for simulating complex human behavior in data-scarce environments, with applications in urban mobility modeling, personalized travel analysis, and traffic forecasting, despite limitations like slow inference and hallucination risks.

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [13] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*Jo√£o Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2 is an evolutionary algorithm that dynamically merges machine learning models without fixed parameter groupings, achieving state-of-the-art performance while preserving diverse capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods require manual partitioning of parameters into fixed groups, which limits exploration of potential combinations and restricts performance improvements.

Method: Proposes Model Merging of Natural Niches (M2N2) with three key features: dynamic adjustment of merging boundaries, diversity preservation mechanism inspired by natural competition, and heuristic-based attraction metric for identifying promising model pairs.

Result: Achieves comparable performance to CMA-ES on MNIST classifiers while being more computationally efficient. Scales to merge specialized language and image generation models with state-of-the-art performance, preserving capabilities beyond those explicitly optimized.

Conclusion: M2N2 demonstrates that model merging can evolve models entirely from scratch, offering a robust and versatile approach that overcomes limitations of fixed parameter grouping methods.

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [14] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI framework extends AI assessment beyond Turing Test to measure if machines can 'grow up' using six criteria tested through games, with standardized journaling and maturity scoring.


<details>
  <summary>Details</summary>
Motivation: To create a natural successor to the Turing Test that answers 'Can machines grow up?' by developing a comprehensive framework for assessing AI maturity and developmental progression.

Method: Uses six primary criteria (C1-C6) assessed through specific 'games' across four arenas, with all AI actions recorded in a standardized AI Journal. Employs expert method for weight assignment and calculates a Grow Up Index as arithmetic mean of six scores with maturity thresholds.

Result: Methodology enables coherent and comparable assessment of AI 'growth' across different AI types (robots, software agents, LLMs), identifies strengths and vulnerabilities through multi-game structure, and ensures traceability via unified journal.

Conclusion: GROW-AI provides an original integrated testing format combining psychology, robotics, computer science, and ethics perspectives to measure not just performance but the evolutionary path of AI entities toward maturity.

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [15] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0 introduces major improvements for building tool-based agent applications with unified interfaces, asynchronous design, built-in agents, and robust engineering support including evaluation tools and runtime sandbox.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for comprehensive support of flexible and efficient tool-based agent-environment interactions as LLMs advance, enabling developers to build scalable agentic applications.

Method: Abstracts foundational components with unified interfaces, grounds agent behaviors in ReAct paradigm, uses systematic asynchronous design for efficiency, provides built-in agents for practical scenarios, and includes evaluation modules with visual studio interface and runtime sandbox.

Result: A framework that enhances both human-agent and agent-agent interaction patterns while improving execution efficiency, making development of long-trajectory agentic applications more manageable and traceable.

Conclusion: AgentScope 1.0 provides a practical foundation for building scalable, adaptive, and effective agentic applications with comprehensive tool support and engineering infrastructure.

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [16] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: IVA framework enables Vision-Language-Action models to detect false-premise instructions, engage in clarification, and provide grounded alternatives, achieving 97.56% detection accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models struggle with false-premise instructions that reference non-existent objects or conditions, requiring robust interpretation and response capabilities.

Method: Proposed Instruct-Verify-and-Act (IVA) framework with large-scale instruction tuning using structured language prompts and semi-synthetic dataset containing paired positive and false-premise instructions.

Result: 97.56% improvement in false premise detection accuracy over baselines and 50.78% increase in successful responses to false-premise scenarios.

Conclusion: IVA provides a unified framework that significantly enhances VLA models' ability to handle erroneous requests through detection, clarification, and grounded alternative generation.

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [17] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: Proposes a causally-aware DL framework for mmWave MIMO beam alignment that integrates causal discovery to identify minimal relevant inputs, reducing beam sweeping overhead by 59.4% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing DL-based beam alignment methods neglect causal relationships, leading to poor interpretability, limited generalization, and unnecessary beam sweeping overhead in 6G mmWave systems.

Method: Two-stage causal beam selection: 1) Causal discovery learns Bayesian graph capturing dependencies between received power inputs and optimal beam, 2) Graph guides causal feature selection for DL-based classifier.

Result: Matches conventional methods' performance while reducing input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing only on causally relevant features.

Conclusion: Causally-aware framework enables efficient and reliable beam alignment by identifying minimal relevant inputs, significantly reducing overhead while maintaining performance in mmWave MIMO systems.

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [18] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE is an agentic legal reasoning framework that addresses LLMs' insufficient reasoning in legal judgment prediction by dynamically acquiring legal knowledge through multiple modules, improving both reasoning quality and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing large language models lack sufficient legal knowledge and reasoning capabilities for legal judgment prediction tasks, leading to inadequate performance in this specialized domain.

Method: GLARE framework uses an agentic approach that dynamically invokes different modules to acquire key legal knowledge, enhancing both the breadth and depth of legal reasoning.

Result: Experiments on real-world datasets verify the effectiveness of the method, showing improved performance in legal judgment prediction tasks.

Conclusion: The framework not only improves legal reasoning performance but also generates interpretable reasoning chains, making it suitable for practical legal applications.

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [19] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: MoDER enhances VLMs' zero-shot capabilities through modular expert training and composition, improving classification on unseen classes in continual learning.


<details>
  <summary>Details</summary>
Motivation: While VLMs have strong zero-shot abilities, fine-tuning for downstream tasks often focuses on preservation rather than enhancement of these capabilities. The paper aims to transform preservation into enhancement.

Method: MoDular Embedding Recomposition (MoDER) trains multiple textual experts specialized in single seen classes, stores them in a foundational hub, and composes retrieved experts to synthesize refined prototypes for unseen class classification.

Result: The method shows effectiveness across 14 datasets in two zero-shot incremental protocols (Class-IL and MTIL), demonstrating improved classification performance.

Conclusion: MoDER successfully enhances VLMs' zero-shot capabilities beyond mere preservation, providing a modular framework for improved continual learning performance on unseen classes.

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [20] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: A diffusion-based neuro-symbolic learning approach that uses a two-stage training strategy with MDP formulation and improved PPO to enforce logical constraints on neural outputs, achieving high accuracy on symbolic reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between neural networks and symbolic reasoning by enabling neural networks to learn complex logical constraints and fulfill symbolic reasoning requirements, particularly for solving logical puzzles.

Method: Two-stage diffusion pipeline: first stage cultivates basic reasoning abilities, second stage uses MDP formulation with improved proximal policy optimization to impose hard logical constraints using rule-based reward signals.

Result: Outstanding accuracy and logical consistency achieved on classical symbolic reasoning benchmarks including Sudoku, Maze, pathfinding, and preference learning tasks.

Conclusion: The diffusion-based approach successfully enables neural networks to perform neuro-symbolic learning and solve logical puzzles with high accuracy while maintaining logical consistency.

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner is a novel post-training pruning method that reduces LLM size without retraining by leveraging weight update magnitudes and activation patterns, achieving better performance than state-of-the-art methods that require weight updates.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges due to their massive sizes, and existing pruning methods either cause performance degradation or require computationally expensive fine-tuning.

Method: Z-Pruner uses both weight update magnitudes and activation patterns to identify and eliminate redundant parameters in pretrained LLMs without any retraining, making it model-agnostic and efficient.

Result: Z-Pruner achieves the lowest perplexity scores and highest overall average score for zero-shot accuracy across multiple LLM architectures (LLaMA-2, LLaMA-3, OPT) on standard language benchmarks, outperforming state-of-the-art pruning methods.

Conclusion: Z-Pruner provides an effective solution for reducing LLM size without performance degradation or retraining requirements, making large language models more deployable and energy-efficient.

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [22] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net is a novel multimodal sentiment analysis framework with progressive fusion, gated arbitration, and parameter-efficient fine-tuning, achieving state-of-the-art results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and interpretable multimodal sentiment analysis model that can deeply integrate textual, audio, and visual features while maintaining computational efficiency for resource-limited scenarios.

Method: Three key innovations: 1) Progressive Intra-Layer Fusion with Cross-Attention for deep multimodal integration, 2) Adaptive Gated Arbitration to balance linguistic and multimodal information, 3) Hybrid PEFT strategy combining LoRA and Post-Fusion Adapters for parameter efficiency.

Result: Achieved state-of-the-art performance on MOSI dataset with MAE of 0.691 and F1-Score of 86.9%, using only 3.09M trainable parameters.

Conclusion: PGF-Net successfully demonstrates deep, dynamic, and interpretable multimodal sentiment analysis with exceptional parameter efficiency, offering superior balance between performance and computational requirements.

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [23] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: A simplified ECG segmentation model combining spectral analysis with probabilistic predictions achieves high accuracy while being computationally efficient and interpretable through XAI.


<details>
  <summary>Details</summary>
Motivation: Existing ECG segmentation models use complex architectures like BiLSTM that are computationally intensive and inefficient, creating a need for more streamlined and interpretable solutions.

Method: Combines spectral analysis with probabilistic predictions, replacing complex layers with simpler ones to capture both temporal and spectral features. Uses Explainable AI (XAI) and physics-based AI principles for interpretability.

Result: Achieves high segmentation accuracy: 97.00% for QRS wave, 93.33% for T wave, and 96.07% for P wave.

Conclusion: The simplified architecture improves computational efficiency while providing precise segmentation, making it a practical and effective solution for heart signal monitoring with enhanced transparency.

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [24] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: TPLA is a tensor-parallel attention scheme that partitions latent representations across devices to maintain MLA's KV cache compression benefits while enabling efficient tensor parallelism without retraining.


<details>
  <summary>Details</summary>
Motivation: MLA reduces KV cache memory by compressing states into latent vectors, but tensor parallelism erodes this advantage by requiring each device to load the full cache, limiting scalability.

Method: Partitions latent representation and head input dimensions across devices, performs independent attention per shard, combines results with all-reduce, and uses orthogonal transforms (Hadamard/PCA) to mitigate cross-shard interference.

Result: Achieves 1.79x speedup for DeepSeek-V3 and 1.93x for Kimi-K2 at 32K-token context length while maintaining performance on commonsense and LongBench benchmarks.

Conclusion: TPLA preserves MLA's memory efficiency while enabling practical tensor-parallel decoding, is drop-in compatible with MLA models, and can be implemented with FlashAttention-3 for end-to-end acceleration.

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [25] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: A Transformer-based framework for temporal causal discovery that handles nonlinear dependencies and spurious correlations through gradient analysis and attention masking.


<details>
  <summary>Details</summary>
Motivation: To address challenges in temporal causal discovery including complex nonlinear dependencies and spurious correlations that traditional methods struggle with.

Method: Uses multi-layer Transformer-based time-series forecaster to capture temporal relationships, extracts causal structure via gradient analysis, and integrates prior knowledge through attention masking to exclude spurious causal links.

Result: Significantly outperforms state-of-the-art methods with 12.8% F1-score improvement in causal discovery and 98.9% accuracy in causal lag estimation.

Conclusion: The proposed Transformer-based framework effectively addresses nonlinear temporal dependencies and spurious correlations, demonstrating superior performance in both causal discovery and lag estimation tasks.

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [26] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Em≈ëke-√Ågnes Horv√°t,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poliƒçar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: A comprehensive review of high-dimensional data embedding methods, addressing challenges in the fragmented research field and providing best practices for practitioners.


<details>
  <summary>Details</summary>
Motivation: The proliferation of high-dimensional data across various domains has created demand for effective dimensionality reduction algorithms, but the field has become fragmented with unclear guidance for practitioners.

Method: The authors conduct a detailed critical overview of recent developments, evaluate popular embedding approaches on multiple datasets, and derive best practices for creating and using low-dimensional embeddings.

Result: The review provides systematic evaluation of existing methods and establishes practical guidelines for effective implementation of dimensionality reduction techniques.

Conclusion: The paper aims to increase coherence in the field, facilitate future research, and address remaining challenges and open problems in high-dimensional data embedding.

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [27] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,Mar√≠a C. V. Nascimento*

Main category: cs.LG

TL;DR: Proposes GL-GRASP - a hybrid approach combining Graph Representation Learning with GRASP metaheuristics for constrained incremental graph drawing, showing superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning techniques like supervised/reinforcement learning are often too time-consuming and not competitive with hand-crafted heuristics for graph problems. Need for less expensive learning strategies to extract latent graph structures.

Method: Hybridizes metaheuristics with Graph Representation Learning (GRL) by incorporating GRL into the construction phase of Greedy Randomized Search Procedures (GRASP), creating Graph Learning GRASP (GL-GRASP) for the Constrained Incremental Graph Drawing Problem.

Result: Deep learning-based GRL strategies performed best. GL-GRASP heuristics demonstrated superior performance than state-of-the-art GRASP methods according to primal integral measure. Scalability tests on denser instances confirmed robustness under fixed time limits.

Conclusion: GL-GRASP successfully combines efficient graph representation learning with metaheuristics, providing a competitive and scalable approach for hierarchical graph visualization problems that outperforms existing methods.

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [28] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: Onboard measurement system using displacement/temperature sensors and machine learning to monitor wheel flange wear with 96.5-98.2% accuracy, enhanced by IIR filtering for real-time noise reduction.


<details>
  <summary>Details</summary>
Motivation: Rail and wheel interaction is critical for railway safety, requiring accurate measurement systems for optimal safety monitoring operations.

Method: Uses displacement and temperature sensors, machine learning regression models, and IIR filtering. Laboratory experiments emulate wheel flange wear and temperature fluctuations. Filter parameters computed from FFT analysis of simulation data.

Result: Achieves 96.5% accuracy with machine learning algorithm, improved to 98.2% with IIR filtering. Minimal runtime and effective counter to sensor nonlinear response to temperature effects.

Conclusion: Integrated with railway communication systems, this monitoring system provides real-time insights into wheel flange wear and track conditions, ensuring enhanced safety and efficiency in railway operations.

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [29] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: A contextual bandit learning approach for distribution shift scenarios with ordered reward vectors using preference cones, featuring adaptive discretization and optimistic elimination with preference-based regret metrics.


<details>
  <summary>Details</summary>
Motivation: To address contextual bandit learning under distribution shift when reward vectors have ordered structure defined by preference cones, extending beyond traditional no-shift scenarios.

Method: Proposes an adaptive-discretization and optimistic elimination based policy that self-tunes to underlying distribution shift, using preference cones to order reward vectors.

Result: Establishes upper bounds on preference-based regret under various distribution shift assumptions, generalizing existing no-shift results and scaling gracefully with problem parameters.

Conclusion: The proposed policy effectively handles distribution shift in contextual bandit learning with ordered reward structures, providing theoretical guarantees that extend and improve upon previous work.

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [30] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: Novel EP framework with intermediate error signals to solve vanishing gradient problem in deep networks, achieving SOTA on CIFAR datasets with deep VGG architectures.


<details>
  <summary>Details</summary>
Motivation: Previous EP studies were limited to shallow architectures due to vanishing gradient problems in deeper networks, hindering convergence in both energy minimization and gradient computation.

Method: Proposed EP framework incorporating intermediate error signals and knowledge distillation to enhance information flow and convergence of neuron dynamics in deep networks.

Result: Achieved state-of-the-art performance on CIFAR-10 and CIFAR-100 datasets, demonstrating successful training of significantly deeper VGG architectures.

Conclusion: Significant advancement in EP scalability, enabling application in real-world systems by overcoming vanishing gradient limitations in deep networks.

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [31] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: A comprehensive survey on Quantum Federated Learning (QFL) that explores its concepts, fundamentals, applications, and challenges in combining quantum computing with federated learning for privacy-preserving decentralized learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in efficient and secure model training across distributed quantum systems by integrating quantum computing's enhanced capabilities with federated learning's privacy-preserving decentralized approach.

Method: The paper conducts a comprehensive survey covering QFL's recent advancements, market opportunities, background knowledge, working principles, fundamentals, taxonomy, federation architecture, networking topology, communication schemes, optimization techniques, and security mechanisms.

Result: The survey provides insights into QFL applications across multiple domains including vehicular networks, healthcare networks, satellite networks, metaverse, and network security, along with analysis of frameworks, platforms, and prototype implementations.

Conclusion: QFL emerges as a promising approach for privacy-preserving decentralized learning with quantum-enhanced capabilities, though current challenges need to be addressed through future research in this rapidly advancing field.

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [32] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fr√©,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: This paper develops mathematical foundations for Cartan neural networks using non-compact symmetric spaces, introducing Tits Satake vector bundles and studying separator walls, tiling groups, Laplacian functions on hyperbolic spaces, and proposing new strategies for eigenfunction construction on Riemann surfaces.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations for the next steps in Cartan neural networks development by modeling layers as non-compact symmetric spaces connected by solvable group homomorphisms, inspired by convolutional neural networks.

Method: Introduces Tits Satake vector bundles with TS submanifold as base space, studies tiling of base manifolds, representation of bundle sections using harmonics, and develops group theoretical construction of separators for non-compact symmetric spaces. Also presents new representations of Laplacian Green function and Heat Kernel on Hyperbolic Spaces.

Result: Presents construction of separators for all non-compact symmetric spaces U/H, Œî‚Çà,‚ÇÉ,‚ÇÇ tiling group and its normal Fuchsian subgroups (uniformizing genus g=3 Fermat Quartic and genus g=2 Bolza surface), new representation of Laplacian Green function and Heat Kernel on Hyperbolic Spaces ‚Ñç‚Åø.

Conclusion: The paper provides diverse mathematical foundations for Cartan neural networks, with results spanning group theory, hyperbolic spaces, and Riemann surfaces, and proposes a new strategy using Abel-Jacobi map and Siegel Theta function for explicit construction of Laplacian eigenfunctions on Bolza Riemann surface.

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [33] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: PAC-MCoFL is a game-theoretic MARL framework that enables multiple service providers to jointly optimize federated learning resources through Pareto Actor-Critic principles and expectile regression, achieving significant performance improvements over existing solutions.


<details>
  <summary>Details</summary>
Motivation: Federated learning in multi-service provider ecosystems faces challenges due to non-cooperative dynamics, privacy constraints, and competing interests that prevent centralized optimization of communication and computation resources.

Method: Integrates Pareto Actor-Critic principles with expectile regression, uses ternary Cartesian decomposition for high-dimensional action space management, and includes a scalable variant (PAC-MCoFL-p) with parameterized conjecture generator to reduce computational complexity.

Result: Achieves approximately 5.8% improvement in total reward and 4.2% improvement in hypervolume indicator over latest MARL solutions, effectively balances individual SP and system performance in scaled deployments under diverse data heterogeneity.

Conclusion: The framework provides theoretical convergence guarantees and demonstrates superior performance in optimizing multi-SP federated learning ecosystems through game-theoretic MARL approach with risk-aware optimization.

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [34] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: Proposes nonstationary discriminant analysis methods (NSLDA/NSQDA) that handle time-varying class distributions using state-space models and Kalman/particle smoothing, outperforming traditional stationary classifiers.


<details>
  <summary>Details</summary>
Motivation: Traditional discriminant analysis assumes stationary distributions, but real-world data often exhibits temporal drift, making stationary classifiers unreliable for time-series classification tasks.

Method: Embeds discriminant analysis within state-space models, using Kalman smoothing for linear-Gaussian dynamics and particle smoothing for nonlinear/non-Gaussian cases. Includes EM for parameter estimation and GMM-Kalman for recovering unknown time labels.

Result: Extensive simulations show consistent improvements over stationary LDA, QDA, and SVM baselines, with robustness to noise, missing data, and class imbalance.

Conclusion: Establishes a unified, data-efficient framework for discriminant analysis under temporal distribution shift, providing reliable classification for time-varying data.

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [35] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodol√†*

Main category: cs.LG

TL;DR: Task arithmetic works because task vectors approximate negative gradients of task losses, with single-epoch finetuning providing comparable merging performance to fully converged models.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of task arithmetic for model merging, there was no clear theoretical explanation for why and when it works effectively.

Method: Established theoretical connection between task vectors and gradients, proved equivalence under gradient descent, bounded second-order error for feed-forward networks, and conducted empirical analysis across seven vision benchmarks.

Result: Task vectors from one epoch of finetuning are equivalent to negative scaled gradients, and first-epoch gradient dominates finetuning trajectory in both norm and direction. Single-epoch finetuned models merge comparably to fully converged models.

Conclusion: Task arithmetic is reframed as approximate multitask learning, with early training dynamics playing a critical role in effective model merging.

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [36] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: Proposes symmetric phase urgency functions using Genetic Programming for traffic signal control, improving performance over traditional methods


<details>
  <summary>Details</summary>
Motivation: Current GP-based traffic signal control methods cannot treat common traffic features of different phases consistently, limiting their effectiveness

Method: Uses Genetic Programming to evolve symmetric phase urgency functions that aggregate shared subtrees representing urgency of turn movements in each phase

Result: Significantly improves performance over traditional GP representation across multiple real-world traffic scenarios

Conclusion: The method evolves effective, human-understandable, and easily deployable traffic signal control policies with consistent treatment of traffic features

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [37] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayy√ºce Beg√ºm Bekta≈ü,Mithat G√∂nen*

Main category: cs.LG

TL;DR: This paper argues that medical ML models must prioritize interpretability, shareability, reproducibility, and accountability over black-box accuracy. It proposes interpretable methods like kernel methods and prototype learning, plus collaborative learning approaches for privacy-preserving data sharing.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in medical domains need to gain trust and regulatory approval, which requires transparency and accountability that black-box models lack. The authors aim to establish foundational design criteria for trustworthy medical AI.

Method: The paper discusses intrinsically interpretable modeling approaches including kernel methods with sparsity, prototype-based learning, and deep kernel models. It also examines accountability through rigorous evaluation, fairness, and uncertainty quantification, and explores generative AI and collaborative learning paradigms like federated learning and diffusion-based data synthesis.

Result: The paper presents a framework for developing medical AI that is not only accurate but also transparent, trustworthy, and translatable to real-world clinical settings by addressing interpretability, shareability, reproducibility, and accountability.

Conclusion: By rethinking machine learning foundations to prioritize interpretability, shareability, reproducibility, and accountability, we can develop medical AI systems that gain clinical trust and regulatory approval while maintaining effectiveness in high-stakes healthcare applications.

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [38] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV is a training-free method for cross-layer KV cache compression using adjacent parameter sharing and SVD to reduce memory usage in LLMs without performance degradation.


<details>
  <summary>Details</summary>
Motivation: LLMs face significant memory challenges from growing KV cache sizes with sequence length, and existing cross-layer sharing methods require architectural changes or cause performance loss at high compression rates.

Method: Uses Singular Value Decomposition (SVD) for weight sharing across adjacent parameters based on cross-layer hidden state similarity, plus adaptive budget allocation that dynamically assigns compression based on cosine similarity.

Result: Outperforms existing low-rank and cross-layer approaches across multiple models and benchmarks at various compression ratios, and works orthogonally with quantization/eviction methods to achieve 98% compression without significant performance loss.

Conclusion: CommonKV provides an effective training-free solution for KV cache compression that maintains performance while significantly reducing memory requirements in large language models.

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [39] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: This survey paper provides a comprehensive review of machine learning applications in micromobility systems, covering datasets, ML techniques, and specific use cases like demand prediction, energy management, and safety.


<details>
  <summary>Details</summary>
Motivation: Micromobility systems have become crucial for urban transportation but face complex optimization challenges. While ML methods support these advancements, there's insufficient literature addressing ML's specific applications in micromobilities.

Method: The authors collect and analyze various micromobility-related datasets, discuss their spatial, temporal, and feature-based characteristics, and provide a detailed overview of ML models including their advantages, challenges, and specific use cases.

Result: The survey explores multiple ML applications such as demand prediction, energy management, and safety, focusing on improving efficiency, accuracy, and user experience in micromobility systems.

Conclusion: The paper proposes future research directions to address existing challenges and aims to help researchers better understand the field of ML applications in micromobility systems.

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [40] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: A memory-based reinforcement learning approach for LLM agents that enables continuous adaptation without fine-tuning, achieving state-of-the-art performance on GAIA and DeepResearcher benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent approaches are either rigid (static workflows) or computationally intensive (gradient updates), creating a need for low-cost continual adaptation methods.

Method: Memory-augmented Markov Decision Process (M-MDP) with neural case-selection policy, episodic memory storage, and memory rewriting/retrieval mechanisms for policy improvement without gradient updates.

Result: Top-1 on GAIA validation (87.88% Pass@3), 79.40% on test set, 66.6% F1 and 80.4% PM on DeepResearcher, outperforming SOTA training-based methods with 4.7-9.6% improvement on OOD tasks.

Conclusion: Provides a scalable, efficient pathway for generalist LLM agents capable of continuous real-time learning without gradient updates, advancing open-ended skill acquisition in deep research scenarios.

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [41] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: The paper identifies collapse errors in ODE-based diffusion sampling where data becomes overly concentrated, introduces a metric to quantify this, reveals a see-saw effect in score learning, and provides empirical evidence using existing techniques.


<details>
  <summary>Details</summary>
Motivation: To explore the previously unrecognized limitations of deterministic samplers in diffusion models, specifically the collapse errors phenomenon where sampled data becomes overly concentrated in local data space.

Method: Introduced a novel metric to quantify collapse errors, observed the see-saw effect in score learning, and applied existing techniques from sampling, training, and architecture to empirically support their findings.

Result: Demonstrated that collapse errors occur across various settings in ODE-based diffusion sampling, showing how misfitting in high noise regimes combined with deterministic sampler dynamics causes data concentration issues.

Conclusion: This work provides empirical evidence of collapse errors, emphasizing the need for further research into the interplay between score learning and deterministic sampling in diffusion models.

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [42] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN is a novel GNN-based kriging framework that improves spatio-temporal pattern validity and generalization through decoupled phase sensing, dynamic graph modeling, and adversarial transfer learning.


<details>
  <summary>Details</summary>
Motivation: Current spatio-temporal kriging models struggle with ensuring valid and generalizable inferred patterns, particularly in capturing dynamic spatial dependencies, temporal shifts, and optimizing for unknown sensors.

Method: STA-GANN integrates three key components: (1) Decoupled Phase Module for timestamp shift sensing and adjustment, (2) Dynamic Data-Driven Metadata Graph Modeling for updating spatial relationships using temporal data and metadata, and (3) adversarial transfer learning strategy for ensuring generalizability.

Result: Extensive validation across nine datasets from four different fields, along with theoretical evidence, demonstrates STA-GANN's superior performance compared to existing approaches.

Conclusion: STA-GANN effectively addresses the limitations of current spatio-temporal kriging models by providing improved pattern validity and generalization capabilities through its integrated framework of temporal shift handling, dynamic graph modeling, and adversarial learning.

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [43] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: SPL-LNS: A sampling-enhanced neural Large Neighborhood Search solver that uses locally-informed proposals and hindsight relabeling to escape local optima and improve efficiency in solving Integer Linear Programs.


<details>
  <summary>Details</summary>
Motivation: Address limitations of greedy neural LNS solvers which suffer from local optima and poor sample efficiency in the long run.

Method: Formulates LNS as a stochastic process, introduces sampling-enhanced neural LNS with locally-informed proposals, and develops hindsight relabeling for efficient training on self-generated data.

Result: SPL-LNS substantially surpasses prior neural LNS solvers for various ILP problems of different sizes.

Conclusion: The proposed SPL-LNS approach effectively addresses local optima and sample efficiency issues in neural LNS solvers, demonstrating superior performance across ILP problems.

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [44] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: Proposes MiniRocket feature extraction with linear classifier for MI-EEG classification, achieving 98.63% accuracy with lower computational cost than deep learning baselines.


<details>
  <summary>Details</summary>
Motivation: EEG signals for motor imagery classification face challenges with nonstationarity, time-variance, individual diversity, and growing class complexity, requiring efficient feature extraction methods.

Method: Uses Minimally Random Convolutional Kernel Transform (MiniRocket) for feature extraction followed by linear classification, with CNN-LSTM deep learning model as baseline comparison.

Result: Achieved mean accuracy of 98.63% for MiniRocket and 98.06% for CNN-LSTM on PhysioNet dataset, showing superior performance with lower computational requirements.

Conclusion: MiniRocket-based feature extraction significantly enhances MI-EEG classification accuracy and provides new insights for efficient motor imagery brain-computer interface systems.

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [45] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM is a parameter-efficient fine-tuning method that maximizes updates relative to parameter scale rather than absolute size, achieving better performance with only 0.1% parameter updates.


<details>
  <summary>Details</summary>
Motivation: Most PEFT methods update parameters without considering their original scale, leading to minimal behavioral changes. The authors aim to maximize meaningful updates relative to parameter scale for more effective adaptation.

Method: Proposes Gradient-to-Weight Ratio and Entropy-guided Masking (GEM) - a scale-aware, distribution-sensitive sparse fine-tuning framework that prioritizes parameters with significant updates relative to their initial values and uses entropy to determine how many parameters to tune per layer.

Result: Achieves up to 1.6% improvement in fine-tuning accuracy over full fine-tuning while updating only 0.1% of model parameters, demonstrated on both general-domain (GLUE, SuperGLUE) and domain-specific (GSM8k, MBPP) tasks.

Conclusion: GEM provides more effective parameter-efficient fine-tuning by considering parameter scale and distribution, making better use of computational budget while outperforming full fine-tuning with minimal parameter updates.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [46] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO improves dimensionality reduction by better preserving both local and global structures through a two-phase optimization approach, outperforming UMAP and other techniques.


<details>
  <summary>Details</summary>
Motivation: Existing dimensionality reduction techniques either preserve local neighborhood structures or global pairwise distances, but not both, which can lead to misleading interpretations of high-dimensional data.

Method: UMATO divides UMAP's optimization into two phases: first constructing a skeletal layout using representative points, then projecting remaining points while preserving regional characteristics.

Result: Quantitative experiments show UMATO outperforms widely used DR techniques including UMAP in global structure preservation, with slight local structure loss, and demonstrates better scalability and stability.

Conclusion: UMATO provides more faithful projections that enhance the reliability of visual analytics for high-dimensional data by effectively capturing both local and global structures.

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [47] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: PIANO introduces autoregressive modeling to PINNs for time-dependent PDEs, improving stability and accuracy by conditioning future predictions on past states.


<details>
  <summary>Details</summary>
Motivation: PINNs suffer from temporal instability and inaccurate predictions for dynamical systems due to pointwise predictions that neglect autoregressive properties.

Method: Redesigns PINNs with autoregressive operation, explicit conditioning of future predictions on past states, self-supervised rollout training, and physical constraint enforcement.

Result: Achieves state-of-the-art performance on challenging time-dependent PDEs with significant accuracy and stability improvements, and outperforms existing methods in weather forecasting.

Conclusion: Autoregressive modeling through PIANO framework effectively addresses temporal instability in PINNs and provides superior performance for dynamical systems modeling.

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [48] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-Jos√©-Revuelta,Xinheng Wang,Jos√© Ram√≥n Garmendia-Leiza,Carlos Alberola-L√≥pez,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: XAI-based framework using CNN and occlusion maps to analyze cough sound spectrograms for COPD diagnosis, identifying disease-specific spectral patterns across frequency subbands.


<details>
  <summary>Details</summary>
Motivation: To develop an explainable AI approach for analyzing cough sounds in chronic respiratory diseases, particularly COPD, to uncover diagnostically relevant spectral patterns and provide interpretable biomarkers.

Method: Trained CNN on time-frequency representations of cough signals, used occlusion maps to identify diagnostically relevant regions, decomposed into 5 frequency subbands for targeted spectral feature extraction.

Result: Identified distinct spectral patterns across subbands and disease groups, distinguished COPD from other respiratory conditions, and chronic from non-chronic groups using interpretable spectral markers.

Conclusion: The frequency-resolved XAI approach provides valuable insights into cough acoustics pathophysiology and demonstrates strong potential for translational respiratory disease diagnostics.

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [49] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: Prophet model often matches or exceeds LSTM accuracy for air pollution forecasting in Northern Nigeria, challenging the assumption that deep learning always outperforms simpler methods.


<details>
  <summary>Details</summary>
Motivation: Address data irregularities and scarcity challenges in low-resource regions like Northern Nigeria, where few studies have systematically compared advanced ML models for air pollution forecasting under such constraints.

Method: Evaluated LSTM networks and Facebook Prophet model for forecasting CO, SO2, SO4 pollutants using monthly observational data from 2018-2023 across 19 states in Northern Nigeria.

Result: Prophet often matches or exceeds LSTM's accuracy, especially in series with seasonal/long-term trends, while LSTM performs better with abrupt structural changes.

Conclusion: Findings challenge deep learning superiority assumptions, highlighting importance of model-data alignment and supporting context-sensitive, computationally efficient methods for resource-constrained settings.

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [50] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST is a systematic framework for evaluating synthetic tabular data that integrates privacy, similarity, and utility metrics to analyze the privacy-utility trade-off in synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive assessment frameworks for evaluating synthetic data generation, particularly regarding the balance between privacy preservation and data utility.

Method: Proposed FEST framework integrates diverse privacy metrics (attack-based and distance-based), similarity metrics, and machine learning utility metrics. Developed as an open-source Python library and validated on multiple datasets.

Result: FEST demonstrates effectiveness in analyzing the privacy-utility trade-off of different synthetic data generation models. The framework provides holistic assessment capabilities.

Conclusion: FEST bridges the gap in synthetic data evaluation by offering a systematic framework that comprehensively assesses both privacy preservation and data utility, available as an open-source tool for researchers and practitioners.

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [51] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: C-DaSh is a scalable Data Shapley method that chunks datasets and uses optimized subset selection with single-iteration SGD to efficiently identify high-quality data points, achieving 80x-2300x speedups while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: As datasets grow in volume and diversity, assessing data quality becomes crucial for reliable ML analytics. Existing Data Shapley methods face scalability challenges with large datasets, limiting their practical application.

Method: Chunked Data Shapley (C-DaSh) divides datasets into manageable chunks, estimates each chunk's contribution using optimized subset selection and single-iteration stochastic gradient descent to reduce computation time.

Result: C-DaSh achieves speedups between 80x-2300x compared to existing Shapley approximations while maintaining high accuracy in detecting low-quality data regions across diverse classification and regression tasks.

Conclusion: C-DaSh enables practical measurement of dataset quality on large tabular datasets, supporting both classification and regression pipelines with significantly improved computational efficiency.

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [52] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: A comprehensive survey on federated tuning for large language models (LLMs) that categorizes approaches based on model access and parameter efficiency, proposing a taxonomy of white-box, gray-box, and black-box techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing FL approaches that rely on access to LLMs' internal information, which is often restricted in real-world scenarios, and to explore inference-only (black-box) paradigms for federated learning with LLMs.

Method: Proposes a taxonomy categorizing FedLLM studies along two axes: model access-based (white-box, gray-box, black-box) and parameter efficiency-based optimization. Reviews representative methods within each category and discusses emerging research on treating LLMs as black-box inference APIs.

Result: The survey provides a comprehensive classification framework for federated tuning approaches for LLMs, highlighting different techniques based on their level of model access and parameter efficiency.

Conclusion: Identifies promising research directions and open challenges for future work in federated learning with large language models, particularly focusing on black-box inference approaches that address real-world deployment constraints.

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [53] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: Proposes auxiliary knowledge concepts (KCs) - sparse binary representations learned from data that capture latent concepts beyond human-defined annotations, improving both knowledge tracing and exercise recommendation.


<details>
  <summary>Details</summary>
Motivation: Human-annotated knowledge concepts in intelligent tutoring systems are often incomplete, error-prone, or overly general, limiting the effectiveness of knowledge tracing models and personalized recommendations.

Method: Deep learning model that learns sparse binary representations of exercises (auxiliary KCs) where each bit indicates presence/absence of latent concepts. These are compatible with both classical (BKT) and modern deep learning KT architectures.

Result: Auxiliary KCs improve predictive performance in student modeling when augmenting classical models like BKT. For recommendation, they enhance both reinforcement learning policies and planning-based methods, leading to measurable gains in student learning outcomes in simulations.

Conclusion: Auxiliary knowledge concepts learned from data provide superior conceptual representations that enhance both knowledge tracing accuracy and exercise recommendation effectiveness in intelligent tutoring systems.

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [54] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE is a teacher-student framework that uses structured error analysis and targeted feedback to improve multimodal reasoning in MLLMs, achieving better efficiency and performance than previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning from errors in multimodal reasoning lack structured frameworks, particularly for MLLMs where visual-textual integration adds complexity. Current approaches also suffer from redundant retrievals and inefficient error mitigation.

Method: Proposes REFINE framework with three systematic queries (Feed-Target, Feed-Check, Feed-Path) to structure errors and provide targeted feedback. Uses retrieval-enhanced feedback via in-context neural error-book in a teacher-student setup to optimize multimodal reasoning.

Result: Demonstrates substantial speedup, reduced computational costs, and successful generalization. Shows improved inference efficiency, token usage, and scalability compared to prior approaches.

Conclusion: REFINE effectively enhances multimodal reasoning by providing structured error analysis and targeted feedback, offering a more efficient and scalable solution for MLLM error mitigation.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [55] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: Proposes intent-driven threat models combining capabilities and intents for cyber physical awareness in space networks, using signal analysis and multitask learning to improve threat assessment robustness.


<details>
  <summary>Details</summary>
Motivation: Current threat assessment approaches analyze reliability and security separately, leading to overfitting on system-specific criteria and inadequate handling of complex threat scenarios in space networks.

Method: Three-step framework: 1) Algorithm for extracting signal characteristics, 2) Multitask learning architecture with separate tasks for reliability capabilities and intent analysis, 3) Adaptable threat assessment aligned with varying security requirements.

Result: The framework enhances robustness of threat detection and assessment, outperforms conventional sequential methods, and enables effective handling of complex threat scenarios in space networks with intershell links.

Conclusion: Intent-driven threat models that incorporate both capabilities and intents provide a more holistic approach to cyber physical awareness in space networks, addressing limitations of traditional separate reliability-security analysis.

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [56] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: Specialized 8-32B OwkinZero models outperform larger commercial LLMs on biological reasoning tasks through reinforcement learning from curated biomedical data, showing strong generalization across unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with core biological reasoning tasks essential for biomedical discovery, creating a need for specialized models that can handle drug discovery challenges like target druggability and drug perturbation effects.

Method: Created eight benchmark datasets with 300,000+ verifiable QA pairs, then developed OwkinZero models by post-training open-source LLMs using Reinforcement Learning from Verifiable Rewards strategy.

Result: Specialized 8-32B OwkinZero models substantially outperform larger state-of-the-art commercial LLMs on biological benchmarks, showing evidence of generalization where specialist models trained on single tasks outperform base models on unseen tasks.

Conclusion: Targeted reinforcement learning on carefully curated biomedical data can unlock generalizable performance in specialized models, representing a significant step toward addressing LLMs' biological reasoning blind spot and accelerating AI-driven biological discovery.

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [57] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: Unsupervised online learning framework using LSTM-VAE with dual drift detection for real-time fault detection in water distribution networks, handling pipe blockages as collective anomalies and background leakages as concept drift.


<details>
  <summary>Details</summary>
Motivation: Water Distribution Networks face challenges like pipe blockages and background leakages, exacerbated by data non-stationarity and limited labeled data, requiring robust unsupervised detection methods.

Method: Combines Long Short-Term Memory Variational Autoencoder (LSTM-VAE) with dual drift detection mechanism for lightweight, memory-efficient real-time edge monitoring under non-stationary conditions.

Result: Experiments on two realistic WDNs show consistent outperformance over strong baselines in detecting anomalies and adapting to recurrent drift.

Conclusion: The approach demonstrates effectiveness in unsupervised event detection for dynamic WDN environments, enabling robust detection and adaptation.

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [58] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: NIAQUE is a novel neural model for transfer learning in probabilistic regression that uses permutation invariance and any-quantile estimation, demonstrating improved performance through pre-training on diverse datasets and fine-tuning on specific targets.


<details>
  <summary>Details</summary>
Motivation: Transfer learning for probabilistic regression remains underexplored, creating a gap in leveraging pre-trained models for enhanced regression performance across different datasets.

Method: NIAQUE (Neural Interpretable Any-Quantile Estimation) uses permutation invariance for transfer learning. It involves pre-training on diverse regression datasets followed by fine-tuning on specific target datasets.

Result: NIAQUE enhances performance on individual regression tasks and shows effectiveness in Kaggle competitions against strong baselines including tree-based models and neural foundation models like TabPFN and TabDPT.

Conclusion: NIAQUE proves to be an effective and scalable framework for probabilistic regression that successfully leverages transfer learning to improve predictive performance.

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [59] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille is a deep learning framework for contour data that achieves rotation and cyclic shift equivariance using complex-valued circular convolution, with equivariant non-linearities and pooling layers for invariant representations.


<details>
  <summary>Details</summary>
Motivation: Contours appear in many domains and require models that are equivariant to rotations and cyclic shifts (due to arbitrary starting points in contour sequences), which is essential for proper learning from contour data.

Method: Uses complex-valued circular convolution to achieve rotation and cyclic shift equivariance, with specialized equivariant non-linearities, coarsening layers, and global pooling layers for invariant representations.

Result: The framework demonstrates effectiveness through experiments in shape classification, reconstruction, and contour regression tasks.

Conclusion: RotaTouille provides an effective deep learning approach for contour data that properly handles the required equivariance properties through complex-valued operations and specialized layers.

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [60] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: FraPPE is a computationally efficient algorithm for preference-based pure exploration in multi-objective bandits that optimally solves the lower bound optimization problem in O(KL¬≤) time and achieves optimal sample complexity.


<details>
  <summary>Details</summary>
Motivation: Existing PrePEx algorithms lack computational efficiency and cannot optimally track the lower bound for arbitrary preference cones, creating a gap in the literature.

Method: Derives three structural properties to reduce the minimization problem, deploys Frank-Wolfe optimizer for the maximization problem, and combines these to solve the maxmin optimization efficiently.

Result: FraPPE achieves O(KL¬≤) computational complexity (significant acceleration over literature) and asymptotically optimal sample complexity, outperforming existing algorithms in identifying exact Pareto sets.

Conclusion: FraPPE successfully fills the computational efficiency gap in PrePEx by providing an optimal algorithm that efficiently solves the lower bound optimization problem with proven optimal performance.

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [61] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: A qualitative study of 204 GitHub repositories using 13 bias detection/mitigation APIs, finding developers use them for learning and real-world problems but face troubleshooting issues and lack expertise.


<details>
  <summary>Details</summary>
Motivation: To understand how open-source fairness APIs are used in practice, what challenges developers face, and how these tools are adopted for bias detection and mitigation in ML systems.

Method: Analyzed 204 GitHub repositories (from 1,885 candidates) that used 13 different bias-related APIs, conducting qualitative analysis of usage patterns and developer challenges.

Result: APIs used for learning and real-world problem solving across 17 unique use-cases; developers lack expertise in bias detection/mitigation, face troubleshooting issues, and frequently seek opinions and resources.

Conclusion: Findings can guide future bias-related software engineering research and help educators develop better curricula to address developer challenges with fairness tools.

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [62] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine is a post-hoc method that improves regression accuracy by combining base model predictions with pairwise ranking information from experts or LLMs, requiring no retraining.


<details>
  <summary>Details</summary>
Motivation: Deep learning regressors struggle in data-scarce regimes where labels are limited, but expert knowledge in the form of pairwise rankings is often more accessible.

Method: Model-agnostic plug-and-play method that uses inverse variance weighting to combine base regressor outputs with rank-based estimates from pairwise comparisons on a small reference set.

Result: Achieves up to 10% relative reduction in mean absolute error using only 20 pairwise comparisons from general-purpose LLMs without fine-tuning in molecular property prediction.

Conclusion: RankRefine provides practical and broadly applicable regression refinement using accessible ranking information from human experts or LLMs, especially valuable in low-data settings.

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [63] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: Analysis of fair cohort selection in university admissions, comparing one-shot vs sequential settings and examining fairness properties.


<details>
  <summary>Details</summary>
Motivation: To address fairness in university admissions by developing transparent admission policies that work with unknown applicant populations, ensuring meritocracy and group parity.

Method: Compare one-shot setting (fixed policy in advance) with sequential setting (policy updates as new applicant data becomes available). Use population models trained on previous admission cycle data to optimize policies.

Result: The paper examines the fairness properties of resulting admission policies, particularly focusing on meritocracy and group parity in the one-shot setting.

Conclusion: Both one-shot and sequential approaches offer different advantages for fair cohort selection, with sequential methods allowing policy adaptation to new data while one-shot provides transparency and fixed fairness guarantees.

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [64] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: This thesis addresses zero-shot reinforcement learning challenges in real-world settings where data simulation is expensive, proposing methods to handle data quality, observability, and availability constraints.


<details>
  <summary>Details</summary>
Motivation: RL systems excel in simulated environments but struggle in real-world deployment due to inevitable misalignment between training and deployment environments. Zero-shot RL must overcome data limitations and partial observability constraints.

Method: Proposes a suite of methods for zero-shot RL that specifically address three constraints: small/homogeneous datasets (data quality), partial observability of states/dynamics/rewards (observability), and lack of a priori data access (availability).

Result: Empirical studies demonstrate the failures of existing methods and validate the proposed techniques for overcoming these constraints in zero-shot RL scenarios.

Conclusion: The proposed methods represent progress toward deployable RL systems that can solve real-world problems by effectively handling the fundamental constraints of data quality, observability, and availability in zero-shot settings.

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [65] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: A lightweight graph neural network with masked autoregressive flow outputs for accurate RF circuit performance prediction, achieving 3.14x better error with 2.24x fewer training samples than prior methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RF circuit simulation is computationally expensive, and existing ML surrogates require large datasets to generalize across topologies and model complex performance distributions.

Method: Topology-aware graph neural network modeling circuits at device-terminal level to capture symmetry and connectivity, with masked autoregressive flow output heads for robust modeling of complex target distributions.

Result: Achieved symmetric mean absolute percentage error of 2.40% and mean relative error of 2.91%, improving MRE by 3.14x while using 2.24x fewer training samples compared to prior work.

Conclusion: The method provides rapid and accurate RF circuit design automation through efficient graph-based modeling and robust density estimation of performance metrics.

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [66] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES framework systematically compares 10 math reasoning data synthesis strategies, finding that increasing problem complexity and maintaining high coverage are key. The resulting FLAMES dataset outperforms existing datasets and achieves state-of-the-art MATH performance.


<details>
  <summary>Details</summary>
Motivation: Existing works on improving LLM math reasoning with synthetic data use unique setups, making comparisons impractical and leaving unanswered questions about factors like filtering low-quality problems.

Method: Introduces FLAMES framework to systematically study 10 data synthesis strategies and multiple factors. Designs novel strategies for out-of-domain generalization and creates FLAMES dataset blending novel and existing approaches.

Result: FLAMES dataset outperforms public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuned Qwen2.5-Math-7B achieves 81.4% on MATH, surpassing larger models like Llama3 405B, GPT-4o and Claude 3.5 Sonnet.

Conclusion: Increasing problem complexity and maintaining high problem coverage are crucial for effective synthetic math data. The FLAMES framework provides valuable insights and the resulting dataset demonstrates strong performance across multiple benchmarks.

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [67] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin M√ºller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: Doctor is a novel offline RL approach that double-checks transformer outputs for better target return alignment, enabling precise performance control in applications like medical treatment policies.


<details>
  <summary>Details</summary>
Motivation: Existing RvS-based transformers struggle to reliably align actual returns with specified target returns, especially when interpolating or extrapolating beyond dataset coverage, limiting precise performance control in real-world applications.

Method: Proposes Doctor approach that double checks transformer outputs with target alignment for offline RL, improving reliability in matching specified target returns both within and beyond dataset coverage.

Result: Achieves superior target alignment within and beyond dataset, enables accurate flexible control over policy performance, and effectively modulates treatment aggressiveness in medical applications like EpiCare benchmark.

Conclusion: Doctor addresses critical alignment limitations of existing RvS transformers, providing reliable performance control for real-world offline RL applications requiring precise return targeting.

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [68] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF is a reinforcement learning framework that fine-tunes equivariant diffusion models for 3D molecular generation using physical feedback from force-field evaluations to produce more stable and physically realistic structures.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle to generate physically realistic 3D molecular structures that adhere to physical principles like force field consistency, creating a need for methods that incorporate direct physical feedback.

Method: RLPF extends Denoising Diffusion Policy Optimization by formulating 3D molecular generation as a Markov decision process and using proximal policy optimization to fine-tune equivariant diffusion models with reward functions derived from force-field evaluations.

Result: Experiments on QM9 and GEOM-drug datasets show RLPF significantly improves molecular stability compared to existing methods, producing energetically stable and physically meaningful structures.

Conclusion: Incorporating physics-based feedback through reinforcement learning is valuable for improving the physical realism and stability of generated 3D molecular structures.

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [69] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*√Ålvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: LLMs can generate playable board game code from natural language rules, with Claude 3.7 Sonnet achieving 55.6% error-free implementations.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs can automate board game implementation from natural language rules, enabling faster digital game development.

Method: Tested three state-of-the-art LLMs (Claude, DeepSeek, ChatGPT) on 12 anonymized board games using free-form and API-constrained approaches, evaluating playability and rule compliance.

Result: Best model (Claude 3.7 Sonnet) produced 55.6% error-free games. API compliance increased errors but LLM choice had greater impact on error severity.

Conclusion: LLM-assisted board game implementation is viable, with potential for framework development to make game creation more accessible.

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [70] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: RL fine-tuning can recover OOD performance loss from SFT, but struggles with severe overfitting. Direction shifts of singular vectors matter more than magnitude changes, with recovery possible through low-rank and shallow interventions.


<details>
  <summary>Details</summary>
Motivation: Training LLMs from scratch is impractical, making post-training methods like SFT and RL-FT crucial. Understanding how these stages affect model representation and OOD performance is essential for efficient fine-tuning.

Method: Used OOD variant of 24-point card game and spectrum-based diagnostics to analyze how SFT and RL-FT reshape model representations. Examined singular vector direction shifts and spectrum changes.

Result: RL-FT restores OOD performance loss from SFT (e.g., Llama-11B: 8.97% to 15.38%). Direction shifts concentrate on largest/smallest singular values. Low-rank (top 20%) and shallow (first 25% layers) recovery achieves 70-80% OOD performance restoration.

Conclusion: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Spectrum-aware analysis reveals inexpensive recovery methods (low-rank UV merging, shallow-layer resets) that can be used before costly RL fine-tuning.

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [71] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA is a novel Bayesian optimization framework that handles noisy, sparse, and scarce data by integrating experimental uncertainty knowledge and using trust regions to focus sampling on promising areas.


<details>
  <summary>Details</summary>
Motivation: Conventional multi-objective Bayesian optimization struggles with sparse, scarce datasets affected by experimental uncertainty where identical inputs yield varying outputs, leading to inefficient resource allocation and suboptimal designs.

Method: NOSTRA integrates prior knowledge of experimental uncertainty to construct accurate surrogate models and employs trust regions to focus sampling on promising areas of the design space, strategically leveraging prior information and refining search regions.

Result: NOSTRA outperforms existing methods in handling noisy, sparse, and scarce data across test functions with varying uncertainty levels, accelerating convergence to the Pareto frontier and improving solution quality.

Conclusion: NOSTRA provides a resource-efficient algorithm that effectively prioritizes sampling regions to enhance Pareto frontier accuracy, making it practical for scenarios with limited experimental budgets while ensuring efficient performance.

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [72] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adri√† Garriga-Alonso*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) require precise L0 hyperparameter setting - too low or too high causes feature mixing, and correct L0 is essential for learning true LLM features.


<details>
  <summary>Details</summary>
Motivation: Existing SAE work treats L0 as a free parameter without a single correct value, but this study shows L0 precision is crucial for proper feature learning in SAEs.

Method: Studied BatchTopK SAEs to analyze L0 effects, developed a method to determine correct L0 values for given training distributions, and validated on toy models and LLMs.

Result: Most commonly used SAEs have L0 set too low. Correct L0 setting prevents feature mixing and coincides with peak sparse probing performance in LLMs.

Conclusion: Practitioners must set L0 precisely to train SAEs that learn correct underlying features from LLM activations, as improper L0 leads to degenerate solutions and feature mixing.

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [73] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan N√∂ther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: BAD-ACTS benchmark evaluates LLM-based agentic systems' security against attacks that manipulate agents into harmful actions, showing high attack success rates and proposing effective defense strategies.


<details>
  <summary>Details</summary>
Motivation: To understand and address security vulnerabilities in agentic systems by systematically studying the range of malicious behaviors they may exhibit when under attack.

Method: Proposed a novel taxonomy of harms and created BAD-ACTS benchmark with 4 agentic system implementations and 188 harmful action examples. Tested attacks where one adversarial agent manipulates others to execute harmful actions.

Result: Attack has high success rate, demonstrating that even a single adversarial agent can significantly compromise system security. Simple prompting-based defenses were ineffective, but message monitoring proved more effective.

Conclusion: Agentic systems are vulnerable to manipulation attacks, requiring robust security measures. The BAD-ACTS benchmark provides a comprehensive testbed for future security research in this domain.

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [74] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: MuST2-Learn framework predicts municipal service request completion times by jointly modeling spatial, temporal, and service type dimensions, achieving 32.5% lower error than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Municipal 311 systems lack transparency about service completion times, reducing resident satisfaction and increasing follow-up inquiries due to complex spatial-temporal correlations and heterogeneous service types.

Method: Multi-view Spatial-Temporal-Type Learning framework with inter-type encoder for heterogeneous relationships, intra-type variation encoder for homogeneous variations, and spatiotemporal encoder for spatial-temporal correlations.

Result: Reduces mean absolute error by at least 32.5% compared to state-of-the-art methods on two real-world datasets.

Conclusion: MuST2-Learn effectively addresses the challenges of predicting municipal service times by comprehensively modeling spatial, temporal, and type dimensions, significantly improving prediction accuracy.

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [75] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: The paper presents PSD algorithm for escaping saddle points in non-convex optimization with explicit constants and rigorous phase separation, achieving approximate second-order stationary points with logarithmic dimension dependence.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive theoretical analysis of first-order methods for escaping strict saddle points in smooth non-convex optimization with fully explicit constants and clear phase separation.

Method: Proposes Perturbed Saddle-escape Descent (PSD) algorithm with distinct gradient-descent and saddle-escape phases, plus finite-difference variant (PSD-Probe) and stochastic extension (PSGD) with mini-batch sizing.

Result: PSD finds (Œµ,‚àö(œÅŒµ))-approximate second-order stationary points with high probability using O(‚ÑìŒî_f/Œµ¬≤) gradient evaluations plus O((‚Ñì/‚àö(œÅŒµ))log(d/Œ¥)) per escape episode, with logarithmic dimension dependence.

Conclusion: The algorithm provides rigorous theoretical guarantees for saddle point escape with explicit constants, validated through experiments on synthetic functions and machine learning tasks, confirming predicted performance characteristics.

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [76] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: First interpretable LSTM model for solar storm prediction (flares and CMEs) using attention mechanism and post-hoc explainability techniques.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are black-boxes, making it hard to understand their predictions. This is problematic for critical applications like solar storm forecasting where reliability and accountability are essential.

Method: Uses LSTM network with attention mechanism to model active region data as time series, capturing temporal dynamics. Applies post hoc model-agnostic interpretability techniques to explain predictions.

Result: Developed an interpretable deep learning model that can predict whether an active region producing a flare will also produce an associated CME within 24 hours.

Conclusion: Successfully added interpretability to LSTM-based solar storm prediction, making model predictions accountable and providing insights into model behavior across multiple sequences.

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [77] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: Complete TinyML workflow for industrial process monitoring using 8-bit quantized CNN on microcontroller achieving 100% accuracy with low energy consumption


<details>
  <summary>Details</summary>
Motivation: Enable retrofitting of legacy industrial machines with wireless monitoring capabilities using TinyML for smart factory applications in Industry 4.0

Method: Developed complete TinyML pipeline including dataset generation (MillingVibes dataset), machine learning model development, and implementation of preprocessing and classification on ARM Cortex M4F microcontroller using 8-bit quantized convolutional neural network

Result: Achieved 100.0% test accuracy with 12.59kiB parameter storage, 15.4ms inference time, and 1.462mJ energy consumption per inference

Conclusion: Demonstrated feasibility of TinyML for structure-integrated process quality monitoring, providing reference implementation for future industrial monitoring solutions

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [78] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: FedMox framework enables privacy-preserving adaptation of foundation models in federated learning with limited edge device resources and unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Address privacy constraints in cloud-based FM adaptation while overcoming edge device limitations (computational resources, scarce labeled data) in federated learning scenarios.

Method: Proposes Federated Mixture of Experts (FedMox) with sparse MoE architecture, spatial router for resolution alignment, and Soft-Mixture strategy for semi-supervised learning stability.

Result: Demonstrated effective FM adaptation in object detection tasks on autonomous driving datasets with significant performance improvements and constrained memory costs on edge devices.

Conclusion: FedMox enables scalable, privacy-preserving foundation model adaptation in federated learning environments, particularly for applications with edge device constraints.

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [79] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: Flat-start training on child speech outperforms fine-tuning adult ASR models, mitigating SSL representation biases toward adult speech. Performance scales up to 1B parameters, with open-data models being crucial for reliable child speech research.


<details>
  <summary>Details</summary>
Motivation: Child speech recognition remains challenging due to acoustic variability and limited annotated data, with under-explored comparisons between fine-tuning adult models and flat-start training approaches.

Method: Comparison of flat-start training across multiple datasets, SSL representations (WavLM, XEUS), and decoder architectures using ESPnet framework, with analysis of model scaling up to 1B parameters.

Result: SSL representations are biased toward adult speech, but flat-start training on child speech mitigates these biases. Consistent improvements observed up to 1B parameters, with performance plateauing beyond that scale.

Conclusion: Proprietary models like Whisper have limitations for child speech, emphasizing the need for open-data models. The publicly available benchmark provides insights for robust child speech processing training strategies.

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [80] [Building and Measuring Trust between Large Language Models](https://arxiv.org/abs/2508.15858)
*Maarten Buyl,Yousra Fettach,Guillaume Bied,Tijl De Bie*

Main category: cs.MA

TL;DR: LLM trust relationships show surprising disconnect between explicit questionnaire measures and implicit behavioral measures like persuasion susceptibility and financial collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand how trust develops between LLMs in multi-agent interactions and compare different trust-building strategies, as well as examine the relationship between explicit and implicit trust measures.

Method: Compared three trust-building approaches (dynamic rapport building, prewritten trust scripts, system prompt adaptation) and measured trust through both explicit questionnaires and implicit behavioral measures (persuasion susceptibility, financial collaboration).

Result: Explicit trust measures showed little or negative correlation with implicit trust measures, suggesting that asking LLMs about trust may be misleading compared to observing their actual behavior.

Conclusion: Context-specific implicit measures are more informative for understanding LLM trust than explicit questionnaires, indicating that behavioral observation provides better insights into actual trust relationships between language models.

Abstract: As large language models (LLMs) increasingly interact with each other, most
notably in multi-agent setups, we may expect (and hope) that `trust'
relationships develop between them, mirroring trust relationships between human
colleagues, friends, or partners. Yet, though prior work has shown LLMs to be
capable of identifying emotional connections and recognizing reciprocity in
trust games, little remains known about (i) how different strategies to build
trust compare, (ii) how such trust can be measured implicitly, and (iii) how
this relates to explicit measures of trust.
  We study these questions by relating implicit measures of trust, i.e.
susceptibility to persuasion and propensity to collaborate financially, with
explicit measures of trust, i.e. a dyadic trust questionnaire well-established
in psychology. We build trust in three ways: by building rapport dynamically,
by starting from a prewritten script that evidences trust, and by adapting the
LLMs' system prompt. Surprisingly, we find that the measures of explicit trust
are either little or highly negatively correlated with implicit trust measures.
These findings suggest that measuring trust between LLMs by asking their
opinion may be deceiving. Instead, context-specific and implicit measures may
be more informative in understanding how LLMs trust each other.

</details>


### [81] [Sound and Solution-Complete CCBS](https://arxiv.org/abs/2508.16410)
*Alvin Combrink,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.MA

TL;DR: CCBS, the standard continuous-time multi-agent path finding solver, has issues with non-termination and sub-optimal solutions. This paper introduces an analytical framework to ensure soundness and completeness, proposes a new branching rule that fixes these problems, and demonstrates improved performance.


<details>
  <summary>Details</summary>
Motivation: The original CCBS algorithm suffers from theoretical non-termination issues and the widely used implementation can return sub-optimal solutions, undermining its reliability as the de-facto optimal solver for continuous-time multi-agent path finding.

Method: Developed an analytical framework providing sufficient conditions for soundness and solution completeness in CCBS-style algorithms. Proposed a novel branching rule that satisfies these conditions and can be implemented as a drop-in replacement in existing codebases.

Result: The new branching rule restores soundness and termination guarantees, making CCBS both sound and solution complete for the first time in continuous domain. Experimental results show it returns solutions with lower sum-of-costs than standard CCBS.

Conclusion: The proposed branching rule and analytical framework provide a systematic way to ensure optimal performance in continuous-time multi-agent path finding solvers, matching discrete-time CBS guarantees while maintaining backward compatibility with existing implementations.

Abstract: Continuous-time Conflict Based-Search (CCBS) has long been viewed as the
de-facto optimal solver for multi-agent path finding in continuous time
(MAPFR). Recent findings, however, show that the original theoretical variant
of CCBS can suffer from non-termination, while the widely used implementation
can return sub-optimal solutions. We introduce an analytical framework that
yields simple and sufficient conditions under which any CCBS-style algorithm is
both sound, i.e., returns only optimal solutions, and solution complete, i.e.,
terminates on every solvable MAPFR instance. Investigating the publicly
available implementation of CCBS reveals that it violates these conditions.
Though this merely indicates that CCBS might be unsound, this indication is
supported by counter-examples.
  Leveraging the analytical framework, we propose a novel branching rule and
prove that it satisfies the sufficient conditions, thereby restoring soundness
and termination guarantees. Consequently, the resulting CCBS variant is both
sound and solution complete, matching the guarantees of the discrete-time CBS
for the first time in the continuous domain. We experimentally apply standard
CCBS and CCBS under our branching rule to an example problem, with our
branching rule returning a solution with lower sum-of-costs than standard CCBS.
Because the branching rule largely only affects the branching step, it can be
adopted as a drop-in replacement in existing code-bases, as we show in our
provided implementation. Beyond CCBS, the analytical framework and termination
criterion provide a systematic way to evaluate other CCBS-like MAPFR solvers
and future extensions.

</details>


### [82] [Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework](https://arxiv.org/abs/2508.16440)
*Surya Murthy,Zhenyu Gao,John-Paul Clarke,Ufuk Topcu*

Main category: cs.MA

TL;DR: RL-based air traffic management system for Urban Air Mobility that jointly optimizes noise reduction and safety through decentralized altitude adjustment policies in multi-layered airspace.


<details>
  <summary>Details</summary>
Motivation: Urban Air Mobility faces critical challenges in balancing noise minimization and safe separation in low-altitude urban environments, which are typically addressed separately rather than integrated.

Method: Reinforcement learning framework with decentralized agents operating in structured multi-layered airspace, learning altitude adjustment policies to manage both noise impact and separation constraints simultaneously.

Result: The system demonstrates strong performance across both objectives and reveals tradeoffs among separation, noise exposure, and energy efficiency under high traffic density conditions.

Conclusion: Reinforcement learning and multi-objective coordination strategies show significant potential for enhancing the safety, quietness, and efficiency of Urban Air Mobility operations.

Abstract: Urban Air Mobility (UAM) envisions the widespread use of small aerial
vehicles to transform transportation in dense urban environments. However, UAM
faces critical operational challenges, particularly the balance between
minimizing noise exposure and maintaining safe separation in low-altitude urban
airspace, two objectives that are often addressed separately. We propose a
reinforcement learning (RL)-based air traffic management system that integrates
both noise and safety considerations within a unified, decentralized framework.
Under this scalable air traffic coordination solution, agents operate in a
structured, multi-layered airspace and learn altitude adjustment policies to
jointly manage noise impact and separation constraints. The system demonstrates
strong performance across both objectives and reveals tradeoffs among
separation, noise exposure, and energy efficiency under high traffic density.
The findings highlight the potential of RL and multi-objective coordination
strategies in enhancing the safety, quietness, and efficiency of UAM
operations.

</details>


### [83] [Abmax: A JAX-based Agent-based Modeling Framework](https://arxiv.org/abs/2508.16508)
*Siddharth Chaturvedi,Ahmed El-Gazzar,Marcel van Gerven*

Main category: cs.MA

TL;DR: Abmax is a JAX-based agent-based modeling framework that enables dynamic agent updates while maintaining high performance through JIT compilation and vectorization.


<details>
  <summary>Details</summary>
Motivation: JAX's requirement for immutable array shapes constrains agent-based modeling where dynamic agent manipulation is needed, particularly for updating varying numbers of agents with distinct changes during simulations.

Method: Developed Abmax framework using JAX that implements multiple JIT-compilable algorithms to support dynamic agent selection and updates while maintaining array shape immutability constraints.

Result: Achieved runtime performance comparable to state-of-the-art implementations on predation model benchmark, demonstrated vectorization capability for parallel execution of multiple models, and validated with traffic-flow and financial market examples.

Conclusion: Abmax successfully bridges the gap between JAX's performance benefits and the flexibility requirements of agent-based modeling, enabling scalable dynamic agent manipulations while maintaining computational efficiency.

Abstract: Agent-based modeling (ABM) is a principal approach for studying complex
systems. By decomposing a system into simpler, interacting agents, agent-based
modeling (ABM) allows researchers to observe the emergence of complex
phenomena. High-performance array computing libraries like JAX can help scale
such computational models to a large number of agents by using automatic
vectorization and just-in-time (JIT) compilation. One of the caveats of using
JAX to achieve such scaling is that the shapes of arrays used in the
computational model should remain immutable throughout the simulation. In the
context of agent-based modeling (ABM), this can pose constraints on certain
agent manipulation operations that require flexible data structures. A subset
of which is represented by the ability to update a dynamically selected number
of agents by applying distinct changes to them during a simulation. To this
effect, we introduce Abmax, an ABM framework based on JAX that implements
multiple just-in-time (JIT) compilable algorithms to provide this
functionality. On the canonical predation model benchmark, Abmax achieves
runtime performance comparable to state-of-the-art implementations. Further, we
show that this functionality can also be vectorized, making it possible to run
many similar agent-based models in parallel. We also present two examples in
the form of a traffic-flow model and a financial market model to show the use
case of Abmax.

</details>
