<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: A self-improving, cache-optional Planner-Auditor framework using LLMs enhances safety and reliability in clinical discharge planning by decoupling generation from validation and enabling feedback-driven replay.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for clinical discharge planning but are constrained by hallucination, omissions, and miscalibrated confidence, necessitating improved safety and reliability mechanisms.

Method: The framework includes a Planner (LLM) generating structured discharge action plans with confidence estimates, and an Auditor as a deterministic module evaluating multi-task coverage, calibration (Brier score, ECE proxies), and action-distribution drift, with two-tier self-improvement via within-episode regeneration and cross-episode discrepancy buffering with replay.

Result: Self-improvement loop increased task coverage from 32% to 86%, improved calibration with reduced Brier/ECE and fewer high-confidence misses, and discrepancy buffering corrected persistent omissions during replay; context caching provided additional performance benefits.

Conclusion: The Planner-Auditor framework offers a practical pathway for safer automated discharge planning using interoperable FHIR data access and deterministic auditing, with feedback-driven regeneration and targeted replay reducing omissions and improving confidence reliability without model retraining.

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [2] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: LLM-assisted reviews show leniency toward lower quality papers, creating spurious interaction effects rather than preferential treatment of LLM-generated content. Fully LLM-generated reviews exhibit severe rating compression, while human reviewers using LLMs reduce this leniency.


<details>
  <summary>Details</summary>
Motivation: There are increasing indications that LLMs are being used in both scientific paper production and peer review processes, raising questions about potential interaction effects between LLM-assisted papers and LLM-assisted reviews.

Method: Analyzed over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML conferences. Used observational analysis controlling for paper quality, and augmented findings with fully LLM-generated reviews to examine rating patterns and decision-making.

Result: LLM-assisted reviews appear kinder to LLM-assisted papers initially, but this effect disappears when controlling for paper quality. Fully LLM-generated reviews show severe rating compression that fails to discriminate paper quality. LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores.

Conclusion: LLMs interact with peer review processes in complex ways: they don't show preferential treatment of LLM-generated content but exhibit systematic biases like leniency toward lower quality papers and rating compression. These findings provide important input for developing policies governing LLM use in peer review.

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [3] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross is a benchmark and agent framework for analyzing structured data and visual documents to bridge the gap in real-world data analytics.


<details>
  <summary>Details</summary>
Motivation: Critical information is fragmented between structured sources and unstructured visual documents, creating a gap in analytics that current agents fail to address.

Method: DataCross includes a benchmark of 200 tasks with a human-in-the-loop synthesis pipeline and an agent framework with sub-agents and a reReAct mechanism for code generation and debugging.

Result: DataCrossAgent achieves 29.7% improvement in factuality over GPT-4o and shows superior robustness on high-difficulty tasks.

Conclusion: DataCross effectively activates 'zombie data' from visual documents, enabling insightful cross-modal analysis to meet industrial needs.

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [4] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: EPDDL provides a standardized PDDL-like language for representing epistemic planning tasks, addressing fragmentation in the field by capturing the full semantics of Dynamic Epistemic Logic (DEL).


<details>
  <summary>Details</summary>
Motivation: The field of epistemic planning suffers from fragmentation due to different planners targeting different DEL fragments and using ad hoc or no standardized representation languages, making comparison, reuse, and systematic benchmark development difficult.

Method: The authors introduce EPDDL, which includes: 1) formal development of abstract event models as a novel representation for epistemic actions; 2) formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3) demonstration of practical applicability by identifying useful fragments for current planners and showing how to represent them in EPDDL.

Result: EPDDL provides a uniform specification language that captures the entire DEL semantics, enabling interoperability between planners, reproducible evaluation, and systematic benchmark development through examples of representative benchmarks.

Conclusion: EPDDL addresses the fragmentation problem in epistemic planning by providing a standardized language that facilitates comparison, reuse, and future advances in the field while maintaining the full expressive power of DEL.

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [5] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: ECL improves multi-agent robustness by using historical interactions to model peer reliability, enabling small models to outperform larger ones.


<details>
  <summary>Details</summary>
Motivation: Agents in multi-agent systems often lack robustness and blindly follow misleading peers due to sycophancy and poor reliability assessment.

Method: Formalized history-aware reference, introduced historical interactions as input, developed Epistemic Context Learning (ECL) with explicit peer profiles, optimized via reinforcement learning with auxiliary rewards.

Result: ECL enables Qwen 3-4B to outperform Qwen 3-30B, boosts frontier models to near 100% performance, generalizes across MA configurations, and shows high correlation between trust modeling accuracy and answer quality.

Conclusion: ECL effectively addresses sycophancy and reliability issues, enhancing agent robustness through trust-based learning from historical data.

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: DASH is a framework that co-optimizes solver mechanisms and schedules using convergence-aware metrics, improving runtime efficiency and solution quality while reducing LLM adaptation costs via warm-starts.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-Driven Heuristic Design frameworks face endpoint-only evaluation ignoring convergence and high adaptation costs for new instances.

Method: Proposes Dynamics-Aware Solver Heuristics with convergence-aware metric guidance and incorporates Profiled Library Retrieval to archive solvers for warm-starts.

Result: Experiments show DASH improves runtime efficiency by over 3 times, surpasses baseline solution quality, and cuts LLM adaptation costs by over 90%.

Conclusion: DASH effectively addresses limitations of current frameworks, enhancing performance and cost-efficiency in combinatorial optimization.

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [7] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: FIP is a model-agnostic pretraining method that biases representation learning toward a target modality, improving downstream fine-tuning performance without extra data or compute.


<details>
  <summary>Details</summary>
Motivation: Standard multimodal pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters during downstream fine-tuning.

Method: FIP biases representation learning toward a designated target modality by combining higher masking difficulty, stronger loss weighting, and increased decoder capacity for that modality, without modifying the shared encoder or requiring additional supervision.

Result: When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute.

Conclusion: FIP is a simple, effective, and broadly applicable method for improving downstream fine-tuned performance in multimodal pretraining when only one modality is heavily used at fine-tuning time.

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [8] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: A generative ML framework using causal discovery creates novel metal hydride candidates for hydrogen storage, identifying six new formulas, four validated by DFT, offering a scalable approach to accelerate materials discovery.


<details>
  <summary>Details</summary>
Motivation: Existing materials databases, like the Materials Project, contain a limited number of well-characterized hydrides, which restricts the discovery of optimal hydrogen storage materials needed for carbon-neutral energy systems.

Method: The framework integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates not present in existing databases. Using a dataset of 450 samples (split into 270 training, 90 validation, and 90 testing), the model generates 1,000 candidates, which are then ranked and filtered.

Result: The model generated 1,000 candidates, leading to the identification of six previously unreported chemical formulas and crystal structures. Four of these were validated by density functional theory simulations and show strong potential for future experimental investigation.

Conclusion: The framework offers a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating the discovery of novel metal hydrides, with validated candidates showing strong potential for experimental investigation.

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping](https://arxiv.org/abs/2601.21126)
*Kooktae Lee,Julian Martinez*

Main category: cs.MA

TL;DR: AI-augmented decentralized framework for multi-agent environmental mapping under limited sensing/communication, using adaptive optimal transport to refine density estimates and achieve robust alignment with ground-truth.


<details>
  <summary>Details</summary>
Motivation: Conventional coverage formulations perform poorly under uncertain or biased priors, necessitating a method that ensures theoretical consistency and scalability in multi-agent mapping.

Method: Adaptive self-correcting mechanism within an optimal transport framework, enhanced by a dual MLP module for local statistics inference and virtual uncertainty regulation to avoid local minima.

Result: Theoretical convergence under the Wasserstein metric; simulations show consistent robust and precise alignment with ground-truth density, yielding higher-fidelity reconstruction than baselines.

Conclusion: The proposed framework effectively addresses limitations in multi-agent mapping under uncertainty, providing a scalable and adaptive solution for complex spatial distributions.

Abstract: This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.

</details>


### [10] [Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions](https://arxiv.org/abs/2601.21477)
*Tobias Schmidt,Kai Cui*

Main category: cs.MA

TL;DR: A framework for mean-field control on sparse graphs using neighborhood distributions, with horizon-dependent locality enabling scalable RL via GNNs.


<details>
  <summary>Details</summary>
Motivation: Traditional mean-field control assumes exchangeability via dense, all-to-all interactions, which is restrictive for real-world network structures with sparse connections.

Method: Redefine system state as probability measure over decorated rooted neighborhoods, prove horizon-dependent locality (optimal policy depends on (T-t)-hop neighborhood), develop Dynamic Programming Principle on lifted space of neighborhood distributions, and use Graph Neural Networks for actor-critic algorithms.

Result: Established theoretical foundation for scalable reinforcement learning on sparse graphs, with horizon-dependent locality making infinite-dimensional control tractable, and demonstrated practical viability of GNN-based actor-critic methods.

Conclusion: The framework bridges mean-field control to real-world sparse network structures, provides theoretical guarantees for scalable RL, recovers classical MFC as degenerate case, and enables efficient control on complex sparse topologies.

Abstract: Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.

</details>


### [11] [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](https://arxiv.org/abs/2601.22041)
*Naomi Pitzer,Daniela Mihai*

Main category: cs.MA

TL;DR: Emergent communication in heterogeneous multimodal systems shows agents develop shared representations despite perceptual misalignment, with unimodal systems being more efficient than multimodal ones, and meaning encoded distributionally rather than compositionally.


<details>
  <summary>Details</summary>
Motivation: Most emergent communication research assumes homogeneous modalities or aligned representational spaces, overlooking real-world perceptual heterogeneity. The study aims to understand how agents develop shared structured representations across different modalities without perceptual grounding.

Method: Researchers designed a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. They analyzed communication patterns, efficiency, and conducted bit perturbation experiments and interoperability analyses.

Result: Multimodal systems converge to class-consistent messages grounded in perceptual input despite perceptual misalignment. Unimodal systems communicate more efficiently (fewer bits, lower entropy). Bit perturbation shows meaning is encoded distributionally. Systems from different perceptual worlds can't directly communicate but limited fine-tuning enables cross-system communication.

Conclusion: Emergent communication serves as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for theory and experimentation in multimodal AI systems.

Abstract: Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.

</details>
