<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.LG](#cs.LG) [Total: 180]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: White box monitors for detecting harmful behaviors in LLMs face evaluation challenges due to data leakage from elicitation methods, requiring systematic frameworks to distinguish genuine detection from superficial artifacts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inflated monitor performance caused by information leakage from prompting/fine-tuning methods used to elicit harmful behaviors, ensuring monitors detect genuine model behavior rather than just elicitation artifacts.

Method: Proposed three evaluation strategies: content filtering (removing deception-related text), score filtering (aggregating over task-relevant tokens), and prompt-distilled fine-tuned model organisms (models trained for deception without explicit prompting).

Result: Identified two forms of leakage (elicitation and reasoning leakage) that inflate monitor performance. Content filtering decreased probe AUROC by 30%, score filtering reduced AUROC by 15%, and fine-tuned model organisms reduced monitor performance by up to 40% even when retrained.

Conclusion: Current evaluation methods for white box monitors are vulnerable to data leakage, and the proposed mitigation strategies reveal significant performance gaps, highlighting the need for more robust evaluation frameworks that distinguish genuine behavior detection from superficial artifacts.

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [2] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: A self-training method that uses decision pivots - minimal verifiable checkpoints - to align reasoning paths in LLMs without ground truth data.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning exposes LLM thinking but lacks scalable verification methods for reasoning traces.

Method: Self-training pipeline that samples diverse reasoning paths, mines shared decision pivots, compresses traces into pivot-focused short paths using a verifier, and post-trains the model.

Result: Effective performance on standard benchmarks including LogiQA, MedQA, and MATH500.

Conclusion: The method successfully aligns reasoning without requiring ground truth reasoning data or external metrics.

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [3] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: A knowledge graph integrated with AI agents lowers technical barriers in climate data science by enabling natural language interaction and automated workflows.


<details>
  <summary>Details</summary>
Motivation: Climate data science faces challenges from fragmented data sources, heterogeneous formats, and high technical expertise requirements that limit participation and reproducibility.

Method: Integration of a curated knowledge graph with AI agents powered by generative AI services, leveraging cloud-native workflows and existing API data portals.

Result: The system drastically lowers technical thresholds, enabling non-specialist users to identify and analyze relevant climate datasets through natural language interaction.

Conclusion: The approach demonstrates a pathway toward democratizing climate data access and establishing reproducible, extensible frameworks for human-AI collaboration in scientific research.

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [4] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: A comparative study using EEG data to predict consumer behavior, comparing classical machine learning models with Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: To predict consumer behavior using EEG data, which provides detailed neural activity information, for applications in marketing, cognitive neuroscience, and human-computer interaction.

Method: Extracted and cleaned EEG features from NeuMa dataset, created brain connectivity features for GNN models, and compared various classical ML models (including ensemble models) with different GNN architectures.

Result: No significant overall performance difference between models, but GNN models generally performed better in some basic criteria where classical models were unsatisfactory.

Conclusion: Combining EEG signal analysis with machine learning provides deeper understanding of consumer behavior, and GNNs show promise compared to traditional models like SVM in EEG-based neuromarketing.

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [5] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve is a multi-agent LLM framework that combines evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms, achieving significant improvements in spatial interpolation and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based algorithm discovery frameworks lack domain knowledge and multi-step reasoning required for complex geospatial problems, which are critical for addressing global challenges like sustainability and climate change.

Method: Two nested loops: inner loop uses code evolver to generate/mutate candidate solutions, outer agentic controller evaluates elites and queries GeoKnowRAG module - a structured geospatial knowledge base that injects theoretical priors from geography.

Result: Reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17%. Successfully discovers new algorithms incorporating geospatial theory on classical models.

Conclusion: GeoEvolve provides scalable path toward automated, knowledge-driven geospatial modeling, opening opportunities for trustworthy and efficient AI-for-Science discovery.

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [6] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: MultiFIX is an interpretable multimodal AI framework that integrates clinical variables and CT imaging for survival analysis in head and neck cancer, achieving superior performance while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and interpretable survival analysis in oncology is growing due to increasing multimodal data and clinical requirements for transparent models that support validation and trust.

Method: Uses deep learning to extract survival-relevant features from CT imaging (interpreted via Grad-CAM) and clinical variables (modeled as symbolic expressions through genetic programming), with transparent Cox regression for risk estimation and patient stratification.

Result: Achieved C-index of 0.838 for prediction and 0.826 for stratification on RADCURE dataset, outperforming clinical and academic baselines while aligning with known prognostic markers.

Conclusion: MultiFIX demonstrates the promise of interpretable multimodal AI for precision oncology by combining high predictive performance with model transparency and explainability.

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [7] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: A framework for modeling decisions with axioms (ethical constraints) that reveals a Decision-Evaluation Paradox between using axioms for making vs evaluating decisions.


<details>
  <summary>Details</summary>
Motivation: To understand the structural properties of decision axioms and identify tensions in how axioms are used for decision-making versus evaluation.

Method: Developed a framework for modeling decisions with axioms, defined a taxonomy based on structural properties, and analyzed the paradox between decision-making and evaluation.

Result: Identified the Decision-Evaluation Paradox that arises with realistic axiom structures, showing inherent tension between using axioms to make decisions versus using them to evaluate decisions.

Conclusion: The paradox highlights the need for exceptional care when training models on decision data or applying axioms to make and evaluate decisions, due to fundamental structural tensions.

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [8] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: Semantic F1 Scores are new evaluation metrics for fuzzy multi-label classification that measure semantic similarity between predicted and actual labels, giving partial credit for semantically related predictions instead of treating them as complete failures.


<details>
  <summary>Details</summary>
Motivation: Conventional F1 metrics fail to account for semantic relatedness between labels, treating semantically similar predictions as complete failures. This is problematic in domains with human disagreement or fuzzy category boundaries where categories overlap and annotators disagree.

Method: Uses a label similarity matrix to compute soft precision-like and recall-like scores, then derives Semantic F1 scores. Features a novel two-step precision-recall formulation that enables comparison of label sets of arbitrary sizes without discarding labels or forcing matches between dissimilar labels.

Result: Semantic F1 demonstrates greater interpretability and ecological validity through theoretical justification and extensive empirical validation on synthetic and real data. It provides fairer evaluations by recognizing that similar predictions lead to similar outcomes.

Conclusion: Semantic F1 is applicable across tasks and modalities since it only requires a domain-appropriate similarity matrix (robust to misspecification) rather than a rigid ontology, making it more practical for real-world applications with subjective or fuzzy classification boundaries.

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [9] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: Shachi is a formal methodology and modular framework for studying emergent behaviors in LLM-driven multi-agent systems by decomposing agent policies into cognitive components (Configuration, Memory, Tools) orchestrated by LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of principled methodologies for controlled experimentation in studying emergent behaviors in LLM-driven multi-agent systems, which limits research progress.

Method: Introduces Shachi framework that decomposes agent policy into core cognitive components: Configuration (intrinsic traits), Memory (contextual persistence), and Tools (expanded capabilities), all orchestrated by an LLM reasoning engine.

Result: Validated on 10-task benchmark; demonstrated external validity by modeling real-world U.S. tariff shock - agent behaviors aligned with observed market reactions only when properly configured with memory and tools.

Conclusion: Provides a rigorous, open-source foundation for building and evaluating LLM agents, enabling more cumulative and scientifically grounded research in multi-agent systems.

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [10] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: The paper develops a scalable benchmark for evaluating physical safety understanding in embodied AI systems, analyzes foundation models' safety perception capabilities, and introduces a post-training method to improve safety reasoning with interpretable thinking traces.


<details>
  <summary>Details</summary>
Motivation: To address the unique safety challenges when AI interacts with the physical world, where physical harm is direct and immediate, and assess whether state-of-the-art foundation models understand common-sense physical safety facts.

Method: Created a scalable physical safety benchmarking approach using real-world injury narratives and operational safety constraints, converted into photorealistic images/videos using generative models. Analyzed major foundation models' risk perception and safety reasoning, and developed a post-training paradigm to teach explicit safety constraint reasoning.

Result: Comprehensive analysis revealed multi-faceted insights into foundation models' deployment readiness for safety-critical applications. The post-training approach achieved state-of-the-art performance in constraint satisfaction evaluations with interpretable safety reasoning traces.

Conclusion: The benchmark provides a systematic way to evaluate embodied AI safety, and the post-training method enables models to generate transparent safety reasoning, advancing the deployment readiness of AI systems for physical world interactions.

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [11] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: CoBel-World is a framework that enhances LLM-based multi-agent collaboration by modeling agents' mental states and enabling dynamic intent inference, reducing communication costs and improving task efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM collaboration frameworks lack dynamic intent reasoning, leading to inconsistent plans and redundant communication in partially observable environments.

Method: CoBel-World uses a collaborative belief world with symbolic belief language for structured knowledge representation and zero-shot Bayesian-style belief updates through LLM reasoning.

Result: The framework reduces communication costs by 22-60% and improves task completion efficiency by 4-28% on embodied benchmarks compared to baselines.

Conclusion: Explicit intent-aware belief modeling is crucial for efficient, human-like collaboration in LLM-based multi-agent systems.

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [12] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: A framework using Group Relative Policy Optimization (GRPO) to adapt multilingual TTS models to low-resource languages using limited paired data and unpaired text with speaker prompts, guided by ASR, speaker verification, and audio quality rewards.


<details>
  <summary>Details</summary>
Motivation: Developing TTS for low-resource languages is challenging due to scarce paired text-speech data, while ASR models are more accessible through multilingual pre-training.

Method: 1) Train multilingual TTS baseline with IPA tokens, 2) Fine-tune on limited paired data for target language prosody, 3) Apply GRPO optimization using unpaired text and speaker prompts with multi-objective rewards from ASR, speaker verification, and audio quality models.

Result: Produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone. Also improves TTS in high-resource languages, surpassing DPO with better intelligibility, speaker similarity, and audio quality.

Conclusion: GRPO-based framework effectively adapts TTS to low-resource languages using limited resources and improves performance across both low and high-resource settings.

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [13] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan is a GUI task automation system that combines structured two-level planning with task mining from user behavior logs to overcome limitations of existing LLM/VLM-based agents, achieving robust performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM/VLM-based planner-executor agents for GUI automation suffer from brittle generalization, high latency, limited long-horizon coherence, and fragility under UI changes or complex tasks.

Method: Combines structured two-level planning framework with task mining approach over user behavior logs. Constructs high-level plans by mapping commands to task dictionary, then grounds them into low-level action sequences using real-time GUI context.

Result: Evaluated on 200 real-world tasks, showing significant improvements in task success rate and execution time. Maintains over 60.0% success rate on long-horizon task sequences.

Conclusion: Log2Plan enables robust and adaptable GUI automation by leveraging user behavior logs and structured planning, demonstrating strong performance in complex, multi-step workflows.

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [14] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: RoT reuses prior reasoning steps as composable "thoughts" to guide new problems, reducing output tokens by up to 40% and inference latency by 82% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models produce long reasoning traces that inflate latency and cost, creating need for inference-time efficiency improvements.

Method: Organizes reasoning steps into a thought graph with sequential and semantic edges, retrieves query-relevant nodes, and applies reward-guided traversal to assemble problem-specific templates that guide generation.

Result: Reduces output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy across reasoning benchmarks with multiple models.

Conclusion: RoT establishes a scalable paradigm for efficient large reasoning model reasoning through dynamic template construction via retrieval.

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [15] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: Proposes LLR-BC, a lifelong learning framework for neural VRP solvers that prevents catastrophic forgetting by consolidating prior knowledge through behavior alignment and weighted confidence-based consolidation.


<details>
  <summary>Details</summary>
Motivation: Existing neural solvers suffer from poor zero-shot generalization or catastrophic forgetting when facing new tasks with different distributions and scales, requiring a lifelong learning approach.

Method: LLR-BC framework that consolidates prior knowledge by aligning behaviors of solvers trained on new tasks with buffered ones in a decision-seeking way, using greater consolidated weights for decisions with lower confidence.

Result: Extensive experiments on capacitated VRP and TSP show LLR-BC effectively trains high-performance neural solvers, addresses catastrophic forgetting, maintains plasticity, and improves zero-shot generalization.

Conclusion: LLR-BC provides an effective lifelong learning solution for neural routing problem solvers, enabling continuous learning across diverse tasks while preserving previously acquired knowledge.

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [16] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: UltraHorizon is a novel benchmark for evaluating autonomous agents in long-horizon, partially observable tasks, revealing significant performance gaps between LLM-agents and humans despite extensive scaling.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on short-horizon, fully observable tasks, while real-world challenges like software development and scientific discovery require sustained reasoning, planning, memory management, and tool use in long-horizon scenarios.

Method: The benchmark uses exploration as a unifying task across three environments where agents must iteratively uncover hidden rules through sustained reasoning, planning, memory management, and tool interactions, with trajectories averaging 35k-200k+ tokens and 60-400+ tool calls.

Result: LLM-agents consistently underperform in these long-horizon settings, while human participants achieve higher scores. Simple scaling fails in these tasks, with analysis identifying eight error types attributed to in-context locking and functional capability gaps.

Conclusion: There is a persistent gap in agents' long-horizon abilities that current scaling approaches cannot overcome, highlighting the need for improved foundational capabilities in autonomous agents for complex real-world challenges.

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [17] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: WebRSSBench is a comprehensive benchmark for evaluating multimodal large language models on web understanding tasks, focusing on reasoning, robustness, and safety across 8 tasks with 3799 question-answer pairs from 729 websites.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus too much on visual perception or UI code generation, lacking evaluation of reasoning, robustness, and safety capabilities needed for end-to-end web applications.

Method: Constructed benchmark from 729 websites with 3799 QA pairs across 8 tasks, using standardized prompts, deterministic evaluation scripts, and multi-stage quality control with automatic checks and human verification.

Result: Evaluation of 12 MLLMs revealed significant gaps: models struggle with compositional and cross-element reasoning, show limited robustness to UI perturbations, and are overly conservative in safety-critical scenarios.

Conclusion: Current MLLMs have substantial limitations in web understanding capabilities, particularly in reasoning, robustness, and safety, highlighting the need for improved models for real-world web applications.

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [18] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis is a deliberative GUI agent framework that uses a cognitive loop of Thinking, Alignment, and Reflection with app-specific tip retrieval, pre-execution alignment, and post-execution reflection to achieve state-of-the-art performance on GUI automation tasks.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents face challenges including data bottlenecks in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance, which limit their effectiveness in automating human tasks.

Method: D-Artemis employs a cognitive loop with three stages: Thinking (using app-specific tip retrieval), Alignment (with Thought-Action Consistency Check and Action Correction Agent), and Reflection (with Status Reflection Agent). It enhances MLLMs without requiring training on complex trajectory datasets.

Result: Achieves 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2 benchmarks, establishing new state-of-the-art results. Ablation studies confirm each component's significant contribution.

Conclusion: D-Artemis demonstrates strong generalization capabilities for GUI tasks by leveraging a deliberative framework that mimics human cognitive processes, achieving superior performance without complex dataset training requirements.

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [19] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe is a proactive reward system for GUI agents that uses a reasoner and evaluator agents to actively probe the environment for better reward assessment, improving accuracy by up to 5.3% and success rates by up to 22.4%.


<details>
  <summary>Details</summary>
Motivation: Existing reward methods struggle with GUI agents due to lack of ground-truth trajectories or application databases, and static LLM-as-a-Judge approaches have limited accuracy.

Method: ProRe uses a general-purpose reasoner that schedules targeted state probing tasks, which domain-specific evaluator agents execute by actively interacting with the environment to collect additional observations.

Result: Empirical results on 3K+ trajectories show ProRe improves reward accuracy by up to 5.3% and F1 score by 19.4%. Integration with policy agents yields success rate improvements up to 22.4%.

Conclusion: ProRe enables more accurate and verifiable rewards for GUI agents through proactive environment interaction, significantly outperforming existing reward methods.

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [20] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: DS-STAR is a novel data science agent that overcomes LLM limitations in data science tasks by automatically exploring diverse data formats, verifying analysis plan sufficiency through LLM-based judges, and using iterative refinement to handle complex multi-source analyses.


<details>
  <summary>Details</summary>
Motivation: Data science tasks are complex and involve multiple data sources, but LLMs struggle with heterogeneous data formats and generating optimal analysis plans due to difficulty in verifying plan sufficiency without ground-truth labels.

Method: DS-STAR features: (1) data file analysis module for exploring diverse data formats, (2) LLM-based judge to verify analysis plan sufficiency at each stage, (3) sequential planning that starts simple and iteratively refines plans based on feedback until verified.

Result: DS-STAR achieves state-of-the-art performance across DABStep, KramaBench, and DA-Code benchmarks, with particularly strong performance on hard tasks requiring processing multiple heterogeneous data files.

Conclusion: The iterative refinement approach with verification enables DS-STAR to reliably navigate complex data analyses involving diverse data sources, outperforming existing baselines especially on challenging multi-file tasks.

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [21] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel is an end-to-end agentic reinforcement learning framework that enables autonomous travel planning agents to plan, execute tools, and reflect on responses for multi-step reasoning, outperforming frontier LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing travel planning agents rely on hand-crafted prompts and fixed workflows, limiting flexibility and autonomy in travel itinerary generation.

Method: Constructs a sandbox environment with cached travel data, develops hierarchical reward modeling with trajectory-level and turn-level verifiers, and implements reply-augmented reinforcement learning with failure experience replay.

Result: DeepTravel enables small LLMs (Qwen3 32B) to significantly outperform frontier LLMs like OpenAI o1, o3 and DeepSeek R1 in travel planning tasks, as demonstrated through deployment on DiDi Enterprise Solutions App.

Conclusion: The proposed framework successfully builds autonomous travel planning agents capable of flexible, multi-step reasoning and verification, achieving superior performance compared to existing approaches.

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [22] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: TRACE introduces a hierarchical transformer architecture and function shift learning objective to better model computational graphs, outperforming previous methods on electronic circuit benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks have architectural mismatches for computational tasks due to permutation-invariant aggregation that fails to capture position-aware hierarchical computation.

Method: Uses Hierarchical Transformer mirroring step-by-step computation flow and function shift learning that predicts only the discrepancy between true global function and local approximation.

Result: Substantially outperforms all prior architectures across comprehensive electronic circuit benchmarks.

Conclusion: Architecturally-aligned backbone and decoupled learning objective form a more robust paradigm for learning to compute on graphs.

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [23] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: GenesisGeo is an automated theorem prover for Euclidean geometry that achieves IMO gold medal level performance through a neuro-symbolic approach combining a 120x accelerated symbolic engine with neural models.


<details>
  <summary>Details</summary>
Motivation: To advance automated theorem proving in Euclidean geometry by creating a system that can solve complex geometric problems at International Mathematical Olympiad (IMO) level, addressing the need for large-scale datasets and efficient reasoning systems.

Method: Developed a neuro-symbolic prover combining: 1) A large-scale geometry dataset of 21.8M problems (3M+ with auxiliary constructions), 2) 120x acceleration of DDARN symbolic engine via theorem matching and C++ implementation, 3) Neural model based on Qwen3-0.6B-Base, 4) Dual-model ensemble approach.

Result: Solves 24/30 problems (IMO silver medal level) with single model, and 26/30 problems (IMO gold medal level) with dual-model ensemble on IMO-AG-30 benchmark. Created largest open-source geometry dataset with 21.8M problems.

Conclusion: GenesisGeo demonstrates state-of-the-art performance in automated geometric theorem proving, achieving IMO competition levels through synergistic combination of symbolic reasoning acceleration and neural approaches with large-scale data.

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [24] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: DyRo-MCTS integrates action robustness estimation into Monte Carlo Tree Search to improve dynamic job shop scheduling by making decisions that are adaptable to future job arrivals, outperforming vanilla MCTS and offline policies with minimal additional planning time.


<details>
  <summary>Details</summary>
Motivation: Dynamic job shop scheduling faces challenges from frequent disruptions caused by new job arrivals. Offline-learned scheduling policies are imperfect and vulnerable to disturbances due to incomplete problem information during online planning.

Method: Proposed Dynamic Robust MCTS (DyRo-MCTS) that integrates action robustness estimation into Monte Carlo Tree Search, guiding the production environment toward states that yield good scheduling outcomes while being easily adaptable to future job arrivals.

Result: DyRo-MCTS significantly improves performance of offline-learned policies with negligible additional online planning time. It consistently outperforms vanilla MCTS across various scheduling scenarios and achieves long-term sustainable performance gains under disturbances.

Conclusion: The integration of action robustness estimation into MCTS enables robust scheduling decisions that maintain performance under dynamic disturbances, making DyRo-MCTS an effective approach for dynamic job shop scheduling problems.

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [25] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael Fröhlich*

Main category: cs.AI

TL;DR: This study compares Statistical Parametric Mapping (SPM) and explainable machine learning for outlier detection in plantar pressure data, finding ML outperforms SPM but both provide interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Plantar pressure mapping datasets often contain outliers from technical errors or procedural inconsistencies, but existing methods like SPM have limitations in robust outlier detection and sensitivity to alignment.

Method: Used a dataset with 798 valid samples and 2000 outliers, comparing (i) non-parametric registration-dependent SPM approach and (ii) CNN with SHAP explanations, evaluated via nested cross-validation and expert surveys.

Result: ML model achieved higher accuracy than SPM, which misclassified clinically meaningful variations and missed true outliers. Both SPM and SHAP explanations were perceived as clear and trustworthy by experts.

Conclusion: SPM and explainable ML have complementary potential for automated outlier detection in plantar pressure data, with explainability being crucial for translating complex model outputs into actionable insights.

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [26] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: RISK is a framework for building GUI agents for e-commerce risk management, featuring a dataset, benchmark, and reinforcement fine-tuning method that improves performance on multi-step web interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional scraping methods and existing GUI agents cannot handle the complex, multi-step, stateful interactions required for e-commerce risk management, particularly with dynamic web content.

Method: RISK framework includes: (1) RISK-Data dataset with interaction trajectories, (2) RISK-Bench evaluation benchmark, and (3) RISK-R1 reinforcement fine-tuning framework with four reward components for output format, stepwise accuracy, process reweighting, and task level reweighting.

Result: RISK-R1 achieves 6.8% improvement in offline single-step tasks, 8.8% improvement in offline multi-step tasks, and 70.5% task success rate in online evaluation, outperforming existing baselines.

Conclusion: RISK provides a scalable, domain-specific solution for automating complex web interactions and advances the state of the art in e-commerce risk management.

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [27] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: The reversal curse in language models is not inherent but stems from knowledge encoding. Training on relational knowledge graphs induces bilinear representations that alleviate the curse and enable consistent model editing.


<details>
  <summary>Details</summary>
Motivation: To challenge the view that the reversal curse is a fundamental LM limitation and show it's an artifact of knowledge encoding, demonstrating that proper relational structure enables logical consistency.

Method: Training LMs from scratch on synthetic relational knowledge graphs and analyzing the emergence of bilinear relational structure in hidden representations.

Result: Models with bilinear structure substantially alleviate the reversal curse and enable correct propagation of edits to reverse and logically dependent facts, while models lacking this structure fail to generalize edits and introduce inconsistencies.

Conclusion: Success of model editing depends critically on the underlying representational geometry, not just editing algorithms, and relational training induces bilinear representations that enable logically consistent behavior.

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [28] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: GSM-Agent benchmark tests LLM agentic reasoning by requiring models to solve grade-school math problems without premises, forcing them to proactively collect information using tools.


<details>
  <summary>Details</summary>
Motivation: Current agent benchmarks mix agentic reasoning with other advanced capabilities, making it hard to isolate and evaluate pure agentic reasoning skills.

Method: Created GSM-Agent benchmark where LLMs must solve grade-school math problems but only get questions without premises, requiring proactive information gathering via tools. Proposed agentic reasoning graphs to analyze patterns and tool-augmented test-time scaling to improve performance.

Result: Even frontier models like GPT-5 only achieve 67% accuracy on grade-school problems when requiring agentic reasoning. Analysis revealed models often lack the ability to revisit previously visited nodes, a crucial reasoning pattern.

Conclusion: The benchmark and agentic reasoning framework provide tools to better understand and improve LLM agentic reasoning capabilities, with insights about the importance of revisiting behavior in dynamic reasoning environments.

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [29] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: Model merging enables tunable control over LLM reasoning capabilities by combining general-purpose and specialized models, creating a spectrum of models that balance accuracy and efficiency through varying merging strengths.


<details>
  <summary>Details</summary>
Motivation: There's growing demand for LLMs with customizable reasoning capabilities that can balance reasoning depth and computational cost for real-world applications, but existing methods lack fine-grained control over this trade-off.

Method: Conducted large-scale empirical study evaluating various model merging techniques across reasoning benchmarks, systematically varying merging strengths to construct accuracy-efficiency curves.

Result: Model merging effectively calibrates reasoning accuracy vs token efficiency trade-off, even with divergent parent models. Found instances of Pareto Improvement where merged models achieve both higher accuracy and lower token consumption than parent models.

Conclusion: Model merging provides practical method for creating LLMs with specific reasoning profiles, offering comprehensive guidelines for meeting diverse application demands through tunable performance control.

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [30] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: A2R is an asymmetric two-stage reasoning framework that uses an explorer model to generate multiple solution paths and a synthesizer model to integrate them, achieving significant performance improvements while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between a model's potential capabilities (revealed across multiple solution paths) and its actual performance in single attempts, addressing the disparity between realized and inherent reasoning abilities.

Method: Asymmetric two-stage framework: (1) Explorer model generates potential solutions through parallel sampling, (2) Synthesizer model integrates references for refined reasoning. Enables orthogonal computation scaling and supports "small-to-big" configurations.

Result: Qwen3-8B-distill achieved 75% performance improvement over self-consistency baseline. A2R-Efficient (Qwen3-4B explorer + Qwen3-8B synthesizer) surpassed Qwen3-32B performance at 30% lower cost.

Conclusion: A2R is both a performance-boosting framework and an efficient practical solution that explicitly enhances model capabilities on complex questions through parallel reasoning.

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [31] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: A generalized multi-objective search formulation using aggregation functions of hidden objectives that enables standard MOS algorithms to solve complex robotics problems with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Real-world robotic systems need to balance multiple conflicting objectives, but recent complex problem formulations prevent direct use of state-of-the-art MOS algorithms.

Method: Propose a generalized problem formulation with aggregation functions of hidden objectives, extending core operations of standard MOS algorithms to handle specific aggregation functions.

Result: Applied to diverse robotics planning problems, the extended MOS algorithms outperform vanilla versions by orders of magnitude across navigation, manipulation, medical systems, inspection, and route planning.

Conclusion: The generalized formulation with proper core operation extensions enables effective application of standard MOS algorithms to complex multi-objective robotics problems with substantial performance gains.

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [32] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: This study evaluates the accuracy of AI energy estimation tools like ML Emissions Calculator and CodeCarbon, finding they can have errors up to 40% despite generally following consumption patterns.


<details>
  <summary>Details</summary>
Motivation: The rapid development of ML/AI is significantly impacting the environment, and while resource-awareness tools exist, their estimation accuracy is questionable due to pragmatic assumptions and neglected factors.

Method: Systematic evaluation of static and dynamic energy estimation approaches through comparisons with ground-truth measurements across hundreds of AI experiments using a proposed validation framework.

Result: Established estimation approaches consistently make errors of up to 40% while generally following AI energy consumption patterns.

Conclusion: The study provides empirical evidence on energy estimation quality, establishes transparency for sustainable AI development, and offers guidelines and code for improving estimation tools and extending validation to other domains.

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [33] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: Machine learning evaluations should use probabilistic metrics that account for annotation uncertainty in clinical datasets, rather than simple aggregation methods like majority voting.


<details>
  <summary>Details</summary>
Motivation: Clinical dataset labels are uncertain due to annotator disagreement and varying confidence levels, but typical aggregation procedures obscure this variability and can significantly impact model rankings.

Method: Propose probabilistic metrics that operate directly on distributions of annotations, which can be applied regardless of how annotations were generated (counting, confidence ratings, or probabilistic response models).

Result: Accounting for confidence in binary labels significantly impacts model rankings in medical imaging benchmarks. The proposed metrics are computationally lightweight with linear-time implementations.

Conclusion: The community should release raw annotations and adopt uncertainty-aware evaluation methods to better reflect the reality of clinical data.

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [34] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: This paper evaluates LLMs for combinatorial optimization, specifically 2D bin-packing, by combining LLMs with evolutionary algorithms to generate efficient heuristics.


<details>
  <summary>Details</summary>
Motivation: To assess LLM capabilities in specialized domains like combinatorial optimization and establish benchmarks for evaluating LLM performance in such tasks.

Method: Systematic methodology combining LLMs with evolutionary algorithms to iteratively generate and refine heuristic solutions for 2D bin-packing problems.

Result: LLMs (GPT-4o) achieved optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins and improving space utilization from 0.76-0.78 to 0.83, outperforming traditional approaches with fewer computational resources.

Conclusion: LLMs can effectively generate efficient solutions for combinatorial optimization problems, contributing to understanding LLM evaluation in specialized domains and establishing performance benchmarks.

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


### [35] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: Proposes InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, medical-specific multimodal LLMs that address challenges in medical AI through improved data quality, training efficiency, and domain knowledge extraction.


<details>
  <summary>Details</summary>
Motivation: General-purpose MLLMs lack specialized medical knowledge, leading to hallucinations and uncertain responses. Knowledge distillation fails to capture domain expertise, and continual pretraining with medical data is computationally expensive.

Method: Used high-quality general and medical multimodal data with a five-dimensional quality assessment framework. Employed low-to-high image resolution and multimodal sequence packing for efficiency. Applied three-stage supervised fine-tuning for complex medical tasks.

Result: InfiMed-Foundation-1.7B outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B and MedGemma-27B-IT on MedEvalKit, demonstrating superior performance in medical VQA and diagnostic tasks.

Conclusion: The work addresses key challenges in data quality, training efficiency, and domain-specific knowledge extraction, paving the way for more reliable and effective AI-driven healthcare solutions.

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [36] [Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models](https://arxiv.org/abs/2509.22284)
*Aleksandar Terzić,Nicolas Menet,Michael Hersche,Thomas Hofmann,Abbas Rahimi*

Main category: cs.AI

TL;DR: PD-SSM is a structured sparse parametrization for state-space models that enables optimal finite-state automata tracking with linear computational cost, outperforming modern SSM variants.


<details>
  <summary>Details</summary>
Motivation: Current SSMs use transition matrices that are either too restrictive in expressivity (diagonal) or too computationally expensive (unstructured), creating a need for a balanced approach.

Method: Parametrizes transition matrix as product of column one-hot matrix (P) and complex-valued diagonal matrix (D), enabling linear computational cost scaling with state size.

Result: Significantly outperforms modern SSM variants on FSA state tracking tasks, achieves comparable performance to neural controlled differential equations on time-series classification, and successfully tracks complex FSA states in hybrid Transformer-SSM architecture.

Conclusion: PD-SSM provides optimal expressivity for FSA emulation with efficient computation, bridging the gap between restrictive diagonal SSMs and expensive unstructured SSMs.

Abstract: Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model

</details>


### [37] [Large Language Models as Nondeterministic Causal Models](https://arxiv.org/abs/2509.22297)
*Sander Beckers*

Main category: cs.AI

TL;DR: The paper presents a simpler method for generating counterfactuals in LLMs by representing them as nondeterministic causal models, contrasting with existing methods that use deterministic representations.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in existing counterfactual generation methods for LLMs and provide a more straightforward approach that aligns with LLMs' intended interpretation.

Method: Proposes representing LLMs as nondeterministic causal models rather than deterministic ones, enabling counterfactual generation that works with any black-box LLM without modification.

Result: Developed a simpler counterfactual generation method that is implementation-agnostic and directly applicable to black-box LLMs.

Conclusion: Both existing and proposed methods have advantages for different applications, and the paper provides theoretical foundations for reasoning about counterfactuals based on LLMs' intended semantics.

Abstract: Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.

</details>


### [38] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME is a multi-agent reasoning framework that integrates fast System 1 thinking and deliberate System 2 thinking to enhance LLM reasoning capabilities, enabling open-source models to compete with state-of-the-art closed-source models.


<details>
  <summary>Details</summary>
Motivation: Inspired by dual-process theory from Thinking, Fast and Slow, to create a framework that mimics human cognitive processes by dynamically integrating intuitive and deliberate thinking for enhanced reasoning in LLMs.

Method: Multi-agent framework with Quick Thinking Agent (System 1) for rapid answers, followed by structured System 2 pipeline with specialized agents for planning, hypothesis generation, retrieval, information integration, and decision-making when uncertainty is detected.

Result: Experimental results with LLaMA 3 models show PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on multi-hop and knowledge-grounded reasoning benchmarks.

Conclusion: PRIME establishes a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning by faithfully mimicking human cognitive processes.

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [39] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: SeekBench is a new benchmark for evaluating LLM search agents' epistemic competence through step-level analysis of 190 expert-annotated traces with over 1,800 response steps.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM search agents focus only on final answer accuracy, overlooking how agents reason with and act on external evidence during the search process.

Method: Created SeekBench with 190 expert-annotated traces containing evidence annotations to analyze agents' reasoning grounding, search adaptation, and evidence sufficiency calibration.

Result: The benchmark enables granular analysis of whether agents generate reasoning steps grounded in evidence, adaptively reformulate searches, and properly assess evidence sufficiency.

Conclusion: SeekBench provides the first comprehensive framework for evaluating the epistemic competence of LLM search agents through step-level trace analysis.

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [40] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: EMMA framework enhances VLA policies using DreamTransfer for generating multi-view consistent robot manipulation videos and AdaMix for hard-sample-aware training, achieving significant performance gains in generalization.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale real-world robot manipulation data across varied conditions is time-consuming and expensive, creating a bottleneck for robust VLA model generalization.

Method: Proposes EMMA framework with DreamTransfer (diffusion Transformer for text-controlled video editing) and AdaMix (dynamic batch reweighting for challenging samples), using hybrid training with real and generated data.

Result: DreamTransfer outperforms prior methods in multi-view consistency and geometric fidelity. VLAs trained with generated data achieve over 200% performance gain in zero-shot visual domains and further 13% improvement with AdaMix.

Conclusion: The approach effectively overcomes data collection bottlenecks and significantly boosts policy generalization to unseen object categories and novel visual domains.

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


### [41] [Guiding Evolution of Artificial Life Using Vision-Language Models](https://arxiv.org/abs/2509.22447)
*Nikhil Baid,Hannah Erlebach,Paul Hellegouarch,Frederico Wieser*

Main category: cs.AI

TL;DR: ASAL++ extends Automated Search for Artificial Life by using multimodal foundation models to propose new evolutionary targets based on simulation history, enabling open-ended-like search in artificial life.


<details>
  <summary>Details</summary>
Motivation: To advance artificial life research by leveraging foundation models for automated search and introducing open-ended-like evolutionary trajectories with increasing complexity.

Method: Uses two foundation models: one for alignment and another (Gemma-3) to propose new evolutionary targets. Tests two strategies: EST (single prompt evolution) and ETT (sequence evolution) in Lenia substrate.

Result: EST promotes greater visual novelty while ETT fosters more coherent and interpretable evolutionary sequences. Both strategies enable increasingly complex targets.

Conclusion: ASAL++ demonstrates new directions for foundation model-driven artificial life discovery with open-ended characteristics.

Abstract: Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.

</details>


### [42] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch is a neural-symbolic framework that transforms geometric reasoning into an interactive perception-reasoning-action loop, enabling dynamic manipulation of diagrams through auxiliary line construction and affine transformations.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs process diagrams as static images, lacking the capacity for dynamic manipulation which is essential for human geometric reasoning involving auxiliary lines and transformations.

Method: Integrates three modules: Perception (abstracts diagrams to logic forms), Symbolic Reasoning (applies geometric theorems), and Sketch Action (executes operations like drawing lines/transformations). Trained via supervised fine-tuning on 2,000 trajectories followed by reinforcement learning with symbolic rewards.

Result: Significantly improves stepwise reasoning accuracy and problem-solving success over static perception methods on the GeoSketch Benchmark of 390 geometry problems requiring auxiliary construction or affine transformations.

Conclusion: GeoSketch advances multimodal reasoning from static interpretation to dynamic, verifiable interaction by unifying hierarchical decision-making, executable visual actions, and symbolic verification.

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [43] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAgent is a pyramid-like DAG-based multi-agent framework that automatically decomposes complex tasks into hierarchical multi-agent systems, featuring dual-audit quality control, agent routing, self-evolution mechanisms, and atomic task parallelism for improved efficiency across infinite scenarios.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent development requires manual workflow design, prompt crafting, and iterative tuning, which hinders scalability and cost-effectiveness across industries. The need for domain expertise and hand-crafted limitations prevents widespread adoption.

Method: Proposes a Pyramid-like DAG-based Multi-Agent Framework with: agent-as-a-tool mechanism for automatic decomposition, dual-audit mechanism for quality control, agent routing for task-agent matching, self-evolution mechanism for autonomous restructuring, and atomic task design for parallelism.

Result: Achieves 9.9% higher performance than ADAS (similar auto-generated agent framework). Case study shows InfiHelper generates scientific papers recognized by human reviewers at top-tier IEEE conferences.

Conclusion: InfiAgent evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems, demonstrating superior performance and practical applicability in real-world scenarios like AI research assistance.

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [44] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: Proposes empowerment (mutual information between agent actions and future states) as an open-ended evaluation metric for language model agents, introducing EELMA algorithm to estimate empowerment from multi-turn text interactions.


<details>
  <summary>Details</summary>
Motivation: Need for scalable evaluation frameworks for LM agents as they gain real-world tool access, overcoming limitations of costly benchmark-centric evaluations that require human-designed tasks.

Method: Developed EELMA algorithm to approximate effective empowerment from multi-turn text interactions, validated on language games and realistic web-browsing scenarios.

Result: Empowerment strongly correlates with average task performance, characterizes impact of environmental complexity and agentic factors (chain-of-thought, model scale, memory length), and high empowerment states/actions are pivotal for general capabilities.

Conclusion: Empowerment serves as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [45] [TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments](https://arxiv.org/abs/2509.22516)
*Rakesh Thakur,Shivaansh Kaushik,Gauri Chopra,Harsh Rohilla*

Main category: cs.AI

TL;DR: TrueGradeAI is an AI-driven digital exam system that preserves handwriting via stylus tablets, uses OCR for transcription, and employs a retrieval-augmented pipeline with LLMs for automated, explainable grading with bias mitigation.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional paper-based exams including excessive paper usage, logistical complexity, grading delays, and evaluator bias in assessment processes.

Method: Captures stylus input on secure tablets, applies transformer-based OCR for transcription, and uses retrieval-augmented pipeline integrating faculty solutions, cache layers, and external references to enable LLM-based scoring with evidence-linked reasoning.

Result: The system enables handwriting preservation while providing scalable, transparent evaluation with explainable automation, bias mitigation, and auditable grading trails.

Conclusion: TrueGradeAI reduces environmental costs, accelerates feedback cycles, builds reusable knowledge bases, and actively works to mitigate grading bias while ensuring fairness in assessments.

Abstract: This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.

</details>


### [46] [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)
*Bo Li,Guanzhi Deng,Ronghao Chen,Junrong Yue,Shuo Zhang,Qinghua Zhao,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: The paper introduces REMA, a framework that analyzes reasoning failures in LLMs by defining a "Reasoning Manifold" - a low-dimensional geometric structure of correct reasoning representations, and measuring deviations of erroneous representations from this manifold.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs perform complex reasoning and their failure mechanisms through measurable geometric analysis, connecting abstract reasoning failures to spatial deviations in internal representations.

Method: Define Reasoning Manifold concept, then build REMA framework that quantifies geometric deviation of erroneous representations using k-nearest neighbors distance to correct manifold, and localizes divergence points by tracking deviations across model layers.

Result: Experiments show low-dimensional nature of reasoning manifold and high separability between erroneous and correct reasoning representations, validating REMA's effectiveness in analyzing reasoning failure origins.

Conclusion: This research connects reasoning failures to measurable geometric deviations, providing new avenues for understanding and diagnosing internal computational processes of black-box models.

Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.

</details>


### [47] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: LLMs exhibit intrinsic heterogeneity in social tendencies - "Adaptive Egoists" prioritize self-interest but can be influenced by social norms, while "Altruistic Optimizers" consistently prioritize collective benefit even at personal cost.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on cooperation in small-scale games, overlooking how altruism emerges in large-scale agent societies. Understanding the social logics LLMs embody is critical for computational social science.

Method: Introduced a Schelling-variant urban migration model creating a social dilemma for 200+ LLM agents, with Grounded Theory-inspired method to systematically code agent reasoning.

Result: Identified two distinct archetypes: Adaptive Egoists (default to self-interest but increase altruism with social norms) and Altruistic Optimizers (inherently prioritize collective benefit at personal cost).

Conclusion: Model selection for social simulation should consider intrinsic social action logic, not just reasoning capability. Adaptive Egoists better simulate complex human societies, while Altruistic Optimizers suit idealized pro-social scenarios.

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [48] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: StepORLM is a self-evolving framework that uses generative process supervision to train LLMs on OR problems, addressing limitations of outcome rewards and discriminative supervision through a co-evolutionary loop between policy and reward models.


<details>
  <summary>Details</summary>
Motivation: Existing LLM training methods for OR problems face two key issues: outcome rewards suffer from credit assignment problems (correct answers can reinforce flawed reasoning), and discriminative process supervision is myopic and fails to evaluate interdependent steps holistically.

Method: StepORLM features a co-evolutionary loop where a policy model and generative process reward model (GenPRM) iteratively improve each other using dual feedback: outcome-based verification from external solvers and holistic process evaluation from GenPRM, aligned via Weighted Direct Preference Optimization.

Result: The 8B-parameter StepORLM achieves state-of-the-art performance across six benchmarks, outperforming larger generalist models, agentic methods, and specialized baselines. The co-evolved GenPRM also serves as a powerful universal process verifier that boosts inference scaling for other LLMs.

Conclusion: StepORLM's generative process supervision framework effectively addresses key limitations in LLM training for OR problems, establishing new SOTA performance while creating a universally applicable process verification capability.

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [49] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: UniMIC is a unified token-based multimodal interactive coding framework that enables efficient communication between edge devices and cloud AI agents using compact tokenized representations instead of raw pixels or text, achieving substantial bitrate savings while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Existing codecs are optimized for unimodal, one-way communication and cause repeated degradation in compress-transmit-reconstruct pipelines, while the rapid progress of Large Multimodal Models and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction.

Method: Uses compact tokenized representations as communication medium with lightweight Transformer-based entropy models (generic, masked, and text-conditioned) to minimize inter-token redundancy and enable efficient low-bitrate transmission while maintaining LMM compatibility.

Result: Achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp) without compromising downstream task performance in text-to-image generation, text-guided inpainting, outpainting, and visual question answering.

Conclusion: UniMIC establishes a practical and forward-looking paradigm for next-generation multimodal interactive communication, bridging edge devices and cloud AI agents efficiently.

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


### [50] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: Dynamic Experts Search (DES) is a test-time scaling strategy that leverages Mixture-of-Experts architecture to generate diverse reasoning paths by controlling expert activation counts during inference, improving accuracy and stability without extra cost.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling approaches focus on output-level sampling but ignore model architecture. The authors observed that varying activated experts in MoE models creates complementary solution sets with stable accuracy, revealing an underexplored source of diversity.

Method: DES has two key components: (1) Dynamic MoE - enables direct control of expert counts during inference to generate diverse reasoning trajectories, (2) Expert Configuration Inheritance - preserves consistent expert counts within reasoning paths while varying them across runs to balance stability and diversity.

Result: Extensive experiments across MoE architectures, verifiers, and reasoning benchmarks (math, code, knowledge) show DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional computational cost.

Conclusion: DES represents a practical and scalable form of architecture-aware test-time scaling, demonstrating how structural flexibility in modern LLMs can advance reasoning capabilities.

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


### [51] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: RL enhances LLM planning but lacks theoretical basis. Analysis shows SFT introduces spurious solutions while RL achieves correct planning through exploration, though PG suffers from diversity collapse while Q-learning preserves diversity and enables off-policy learning.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical basis for why RL methods improve LLM planning capabilities, and to investigate both the benefits and limitations of different RL approaches.

Method: Used a tractable graph-based abstraction to analyze policy gradient (PG) and Q-learning methods, examining exploration strategies, diversity preservation, and reward design. Applied framework to Blocksworld planning benchmark.

Result: SFT introduces co-occurrence-based spurious solutions. RL achieves correct planning through exploration, enabling better generalization. PG suffers from diversity collapse even after perfect accuracy, while Q-learning preserves diversity and enables off-policy learning. Reward design is crucial to prevent reward hacking in Q-learning.

Conclusion: RL's effectiveness in LLM planning stems from exploration, with Q-learning offering advantages over PG in diversity preservation and off-policy learning. Careful reward design is essential for stable Q-learning performance.

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Discovering and Analyzing Stochastic Processes to Reduce Waste in Food Retail](https://arxiv.org/abs/2509.21322)
*Anna Kalenkova,Lu Xia,Dirk Neumann*

Main category: cs.LG

TL;DR: A novel method combining object-centric process mining with stochastic process discovery to analyze food retail processes and reduce food waste through what-if analysis.


<details>
  <summary>Details</summary>
Motivation: To address food waste in retail by finding optimal balance between customer purchasing behavior and supply strategies, preventing both oversupply waste and product shortages.

Method: Integrates object-centric process mining with stochastic process discovery. Discovers continuous-time Markov chain from sales data, extends with supply activities, and conducts what-if analysis on product quantity evolution over time.

Result: Enables identification of optimal balance between customer purchasing behavior and supply strategies to prevent food waste from oversupply and product shortages.

Conclusion: The proposed method effectively analyzes food retail processes to reduce food waste by optimizing supply strategies based on customer behavior patterns.

Abstract: This paper proposes a novel method for analyzing food retail processes with a
focus on reducing food waste. The approach integrates object-centric process
mining (OCPM) with stochastic process discovery and analysis. First, a
stochastic process in the form of a continuous-time Markov chain is discovered
from grocery store sales data. This model is then extended with supply
activities. Finally, a what-if analysis is conducted to evaluate how the
quantity of products in the store evolves over time. This enables the
identification of an optimal balance between customer purchasing behavior and
supply strategies, helping to prevent both food waste due to oversupply and
product shortages.

</details>


### [53] [Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics](https://arxiv.org/abs/2509.21393)
*Yi En Chou,Te Hsin Liu,Chao An Lin*

Main category: cs.LG

TL;DR: Proposed two weighting schemes for Physics Informed Neural Networks to address sensitivity to loss weights, with the second scheme improving stability and accuracy in various CFD problems.


<details>
  <summary>Details</summary>
Motivation: PINNs are mesh-free for solving PDEs but highly sensitive to loss weight selection, requiring better balancing schemes for stable training.

Method: Two dimensional analysis based weighting schemes: one using quantifiable terms, another incorporating both quantifiable and unquantifiable terms for balanced training.

Result: The second scheme consistently improves stability and accuracy over equal weighting in heat conduction, convection diffusion, and lid driven cavity flows. Notably achieves stable, accurate predictions in high Peclet number convection diffusion where traditional solvers fail.

Conclusion: PINNs with the proposed weighting scheme demonstrate robustness and generalizability in CFD problems, especially in challenging scenarios where conventional methods fail.

Abstract: Physics Informed Neural Networks offer a mesh free framework for solving PDEs
but are highly sensitive to loss weight selection. We propose two dimensional
analysis based weighting schemes, one based on quantifiable terms, and another
also incorporating unquantifiable terms for more balanced training. Benchmarks
on heat conduction, convection diffusion, and lid driven cavity flows show that
the second scheme consistently improves stability and accuracy over equal
weighting. Notably, in high Peclet number convection diffusion, where
traditional solvers fail, PINNs with our scheme achieve stable, accurate
predictions, highlighting their robustness and generalizability in CFD
problems.

</details>


### [54] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: LLMs fail at in-context experimental design, showing no sensitivity to feedback, while classical methods outperform them. A hybrid LLM-guided Nearest Neighbour method achieves competitive performance by combining LLM prior knowledge with traditional sampling.


<details>
  <summary>Details</summary>
Motivation: To evaluate claims that LLMs can perform in-context experimental design as general-purpose agents, particularly for scientific tasks like genetic perturbation and molecular property discovery.

Method: Evaluated open- and closed-source instruction-tuned LLMs on experimental design tasks, compared with classical methods (linear bandits, Gaussian process optimization), and proposed LLM-guided Nearest Neighbour (LLMNN) sampling that combines LLM prior knowledge with nearest-neighbor sampling.

Result: LLM-based agents showed no sensitivity to experimental feedback - random label permutations had no impact on performance. Classical methods consistently outperformed LLM agents. The proposed LLMNN method achieved competitive or superior performance across domains without requiring significant in-context adaptation.

Conclusion: Current LLMs do not perform in-context experimental design in practice, highlighting the need for hybrid frameworks that separate prior-based reasoning from batch acquisition with updated posteriors.

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [55] [Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification](https://arxiv.org/abs/2509.21405)
*Nyi Nyi Aung,Neil Muralles,Adrian Stein*

Main category: cs.LG

TL;DR: Physics-informed residual neural network combines learning and classification for object identification in UAV applications, achieving high accuracy with reduced training time.


<details>
  <summary>Details</summary>
Motivation: To address object identification under known dynamics in unmanned aerial vehicle applications by combining learning and classification through physics-informed methods.

Method: Physics-informed residual neural network for state mapping and state-derivative prediction, with softmax layer for multi-class confidence estimation. Case studies include quadcopter, fixed-wing, and helicopter aerial vehicles.

Result: High classification accuracy with reduced training time.

Conclusion: The framework offers a promising solution for system identification problems in domains where underlying dynamics are well understood.

Abstract: This work addresses object identification under known dynamics in unmanned
aerial vehicle applications, where learning and classification are combined
through a physics-informed residual neural network. The proposed framework
leverages physics-informed learning for state mapping and state-derivative
prediction, while a softmax layer enables multi-class confidence estimation.
Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case
studies. The results demonstrate high classification accuracy with reduced
training time, offering a promising solution for system identification problems
in domains where the underlying dynamics are well understood.

</details>


### [56] [Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity](https://arxiv.org/abs/2509.21413)
*Zihuan Qiu,Lei Wang,Yang Cao,Runtong Zhang,Bing Su,Yi Xu,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: NUFILT is a data-free continual model merging framework that uses null-space filtering and LoRA adapters to merge fine-tuned models without accessing task data, achieving transparency and fidelity while minimizing forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to bridge data-level desiderata with parameter-space optimization for data-free continual model merging, particularly in ensuring transparency (no interference with earlier tasks) and fidelity (faithful adaptation to new tasks).

Method: Proposes NUFILT with null-space projector to filter overlapping components of new task vectors for transparency, and lightweight LoRA adapter with projection-based surrogate loss for fidelity. Joint filtering-adaptation process with layer-wise linear fusion.

Result: Achieves state-of-the-art performance with minimal forgetting on vision and NLP benchmarks, improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing gap to fine-tuning and reducing computation overhead.

Conclusion: NUFILT effectively addresses data-free continual model merging by linking desiderata to optimization through subspace alignment, enabling backbone evolution without task data access while maintaining transparency and fidelity.

Abstract: Data-free continual model merging (DFCMM) aims to fuse independently
fine-tuned models into a single backbone that evolves with incoming tasks
without accessing task data. This paper formulate two fundamental desiderata
for DFCMM: transparency, avoiding interference with earlier tasks, and
fidelity, adapting faithfully to each new task. This poses a challenge that
existing approaches fail to address: how to bridge data-level desiderata with
parameter-space optimization to ensure transparency and fidelity in the absence
of task data. To this end, we propose NUFILT (NUll-space FILTering), a
data-free framework that directly links these desiderata to optimization. Our
key observation is that task vectors approximately align with representation
subspaces, providing structural surrogates for enforcing transparency and
fidelity. Accordingly, we design a null-space projector that preserves prior
responses by filtering out overlapping components of new task vectors, thereby
ensuring transparency, and a lightweight LoRA adapter that injects
complementary task-specific signals, enabling fidelity in adapting to new
tasks. The adapter is trained with a projection-based surrogate loss to retain
consistency with previous knowledge while introducing novel directions. This
joint filtering-adaptation process allows the backbone to absorb new knowledge
while retaining existing behaviors, and the updates are finally fused back in a
layer-wise linear fashion without extra parameters or inference cost.
Theoretically, we establish approximate subspace alignment guarantees that
justify null-space filtering. Empirically, NUFILT achieves state-of-the-art
performance with minimal forgetting on both vision and NLP benchmarks,
improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing
the gap to fine-tuning and reducing computation overhead.

</details>


### [57] [Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.21828)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: A novel framework for online multi-agent reinforcement learning in sparse-reward environments that combines inverse preference learning with multi-agent optimization, using LLMs for preference labeling and achieving superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of sparse rewards in online MARL where reward feedback is only provided at trajectory ends, hindering standard algorithms from effective policy learning.

Method: Integrates online inverse preference learning with multi-agent on-policy optimization using an implicit reward learning model based on preference-based value-decomposition network, creating dual advantage streams for centralized critic and decentralized actors, with LLM-enhanced preference labels.

Result: Empirical evaluations on MAMuJoCo and SMACv2 benchmarks show superior performance compared to existing baselines.

Conclusion: The proposed framework effectively addresses sparse-reward challenges in online MARL through integrated reward learning and multi-agent optimization.

Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in
environments with sparse rewards, where reward feedback is not provided at each
interaction but only revealed at the end of a trajectory. This setting, though
realistic, presents a fundamental challenge: the lack of intermediate rewards
hinders standard MARL algorithms from effectively guiding policy learning. To
address this issue, we propose a novel framework that integrates online inverse
preference learning with multi-agent on-policy optimization into a unified
architecture. At its core, our approach introduces an implicit multi-agent
reward learning model, built upon a preference-based value-decomposition
network, which produces both global and local reward signals. These signals are
further used to construct dual advantage streams, enabling differentiated
learning targets for the centralized critic and decentralized actors. In
addition, we demonstrate how large language models (LLMs) can be leveraged to
provide preference labels that enhance the quality of the learned reward model.
Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and
SMACv2, show that our method achieves superior performance compared to existing
baselines, highlighting its effectiveness in addressing sparse-reward
challenges in online MARL.

</details>


### [58] [Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope](https://arxiv.org/abs/2509.21446)
*Waleed Esmail,Alexander Kappes,Stuart Russell,Christine Thomas*

Main category: cs.LG

TL;DR: SeismoGPT is a transformer-based model for forecasting three-component seismic waveforms to support gravitational wave detectors like Einstein Telescope.


<details>
  <summary>Details</summary>
Motivation: To enable data-driven seismic forecasting for Newtonian noise mitigation and real-time observatory control in future gravitational wave detectors.

Method: Transformer-based model trained in autoregressive setting on waveform data, learning temporal and spatial dependencies from both single-station and array-based inputs.

Result: Model performs well within immediate prediction window but gradually degrades further ahead, as expected in autoregressive systems.

Conclusion: This approach lays groundwork for data-driven seismic forecasting to support gravitational wave detection infrastructure.

Abstract: We introduce \textit{SeismoGPT}, a transformer-based model for forecasting
three-component seismic waveforms in the context of future gravitational wave
detectors like the Einstein Telescope. The model is trained in an
autoregressive setting and can operate on both single-station and array-based
inputs. By learning temporal and spatial dependencies directly from waveform
data, SeismoGPT captures realistic ground motion patterns and provides accurate
short-term forecasts. Our results show that the model performs well within the
immediate prediction window and gradually degrades further ahead, as expected
in autoregressive systems. This approach lays the groundwork for data-driven
seismic forecasting that could support Newtonian noise mitigation and real-time
observatory control.

</details>


### [59] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: Using LLMs to create interpretable decision trees for low-resource tabular data instead of black-box foundation models.


<details>
  <summary>Details</summary>
Motivation: Tabular foundation models are black boxes that are difficult to interpret and costly to inference, despite their good performance on low-resource problems.

Method: Design minimal tools for constructing, analyzing and manipulating decision trees, then use reasoning-capable LLMs in an agentic setup to combine prior knowledge with data learning.

Result: LLM-created decision trees outperform traditional CART on low-resource tabular problems and provide human-readable reasoning traces.

Conclusion: LLM-induced decision trees offer interpretability, bias checking, and allow human input for corrections and domain knowledge incorporation, though they don't outperform state-of-the-art black box models.

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [60] [Score-based Idempotent Distillation of Diffusion Models](https://arxiv.org/abs/2509.21470)
*Shehtab Zaman,Chengyan Liu,Kenneth Chiu*

Main category: cs.LG

TL;DR: SIGN combines diffusion models and idempotent generative networks by distilling idempotent models from diffusion model scores, enabling faster inference than iterative models while supporting multi-step sampling for quality-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the training instabilities and mode collapse issues of conventional IGNs while overcoming the high computational cost of diffusion models, by uniting the benefits of both approaches.

Method: Distill idempotent models (SIGN) from pre-trained diffusion model scores using score-based training methods, eliminating the need for adversarial losses.

Result: Achieved state-of-the-art results for idempotent models on CIFAR and CelebA datasets, with faster inference than iterative score-based models and support for zero-shot editing of inputs.

Conclusion: SIGN successfully combines diffusion and idempotent generative networks, providing stable training, efficient sampling, and flexible quality-efficiency trade-offs while enabling zero-shot editing capabilities.

Abstract: Idempotent generative networks (IGNs) are a new line of generative models
based on idempotent mapping to a target manifold. IGNs support both single-and
multi-step generation, allowing for a flexible trade-off between computational
cost and sample quality. But similar to Generative Adversarial Networks (GANs),
conventional IGNs require adversarial training and are prone to training
instabilities and mode collapse. Diffusion and score-based models are popular
approaches to generative modeling that iteratively transport samples from one
distribution, usually a Gaussian, to a target data distribution. These models
have gained popularity due to their stable training dynamics and high-fidelity
generation quality. However, this stability and quality come at the cost of
high computational cost, as the data must be transported incrementally along
the entire trajectory. New sampling methods, model distillation, and
consistency models have been developed to reduce the sampling cost and even
perform one-shot sampling from diffusion models. In this work, we unite
diffusion and IGNs by distilling idempotent models from diffusion model scores,
called SIGN. Our proposed method is highly stable and does not require
adversarial losses. We provide a theoretical analysis of our proposed
score-based training methods and empirically show that IGNs can be effectively
distilled from a pre-trained diffusion model, enabling faster inference than
iterative score-based models. SIGNs can perform multi-step sampling, allowing
users to trade off quality for efficiency. These models operate directly on the
source domain; they can project corrupted or alternate distributions back onto
the target manifold, enabling zero-shot editing of inputs. We validate our
models on multiple image datasets, achieving state-of-the-art results for
idempotent models on the CIFAR and CelebA datasets.

</details>


### [61] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: Hallucinations in generative models are defined as failures to link estimates to plausible causes. Even optimal estimators can hallucinate, with a proven lower bound on hallucination rates. This reframes hallucinations as structural misalignment between loss minimization and human-acceptable outputs.


<details>
  <summary>Details</summary>
Motivation: To understand why hallucinations occur in generative models even with optimal estimators, and to reframe hallucinations as structural issues rather than simple estimation errors.

Method: Formalized hallucinations as failures to link estimates to plausible causes, provided theoretical analysis with high probability lower bounds on hallucination rates, and conducted experiments on coin aggregation, open-ended QA, and text-to-image tasks.

Result: Showed that even loss-minimizing optimal estimators still hallucinate, with confirmed lower bounds on hallucination rates across different data distributions and tasks.

Conclusion: Hallucinations are structural misalignments between loss minimization and human-acceptable outputs, representing estimation errors induced by miscalibration rather than simple optimization failures.

Abstract: We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.

</details>


### [62] [d2: Improved Techniques for Training Reasoning Diffusion Language Models](https://arxiv.org/abs/2509.21474)
*Guanghan Wang,Yair Schiff,Gilad Turok,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: d2 is a new reinforcement learning framework for masked diffusion language models that introduces efficient policy gradient algorithms for reasoning tasks, achieving state-of-the-art performance on logical and mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: While diffusion language models show competitive text generation performance, improving their reasoning ability with reinforcement learning remains challenging and requires specialized approaches.

Method: Developed a new policy gradient algorithm that leverages masking properties to estimate sampling trajectory likelihoods, with estimators that trade computation for accuracy in an analytically tractable way. Focused on DLMs supporting any-order likelihood estimation.

Result: Significantly outperformed previous diffusion reasoning frameworks using only RL (without supervised fine-tuning), achieving new state-of-the-art on logical reasoning tasks (Countdown, Sudoku) and math reasoning benchmarks (GSM8K, MATH500).

Conclusion: The d2 framework demonstrates that efficient diffusion-based reasoning is achievable through proper algorithmic design that exploits the unique properties of masked diffusion models, particularly their any-order likelihood estimation capability.

Abstract: While diffusion language models (DLMs) have achieved competitive performance
in text generation, improving their reasoning ability with reinforcement
learning remains an active research area. Here, we introduce d2, a reasoning
framework tailored for masked DLMs. Central to our framework is a new policy
gradient algorithm that relies on properties of masking to accurately estimate
the likelihoods of sampling trajectories. Our estimators trade off computation
for approximation accuracy in an analytically tractable manner, and are
particularly effective for DLMs that support any-order likelihood estimation.
We characterize and study this property in popular DLMs and show that it is key
for efficient diffusion-based reasoning. Empirically, d2 significantly improves
over previous diffusion reasoning frameworks using only RL (without relying on
supervised fine-tuning), and sets a new state-of-the-art performance for DLMs
on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks
(GSM8K and MATH500).

</details>


### [63] [Learning from Delayed Feedback in Games via Extra Prediction](https://arxiv.org/abs/2509.22426)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: The paper addresses time-delayed feedback in multi-agent learning games, showing that even single-step delays worsen OFTRL performance. It proposes Weighted OFTRL (WOFTRL) with weighted predictions that can cancel out time delays, achieving constant regret and convergence to Nash equilibrium.


<details>
  <summary>Details</summary>
Motivation: Learning in games involves multiple agents independently learning strategies, creating optimization discrepancies. Time delays in observing past rewards hinder prediction-based algorithms like OFTRL, degrading performance even with minimal delays.

Method: Proposes Weighted Optimistic Follow-the-Regularized-Leader (WOFTRL) where the prediction vector of next reward is weighted n times. The optimistic weight is designed to cancel out time delays in feedback.

Result: When optimistic weight exceeds time delay, WOFTRL achieves constant regret (O(1)-regret) in general-sum normal-form games and best-iterate convergence to Nash equilibrium in poly-matrix zero-sum games. Experiments support theoretical findings.

Conclusion: WOFTRL effectively mitigates time-delayed feedback issues in multi-agent learning, recovering good performance when optimistic weight properly compensates for delays, with proven theoretical guarantees and experimental validation.

Abstract: This study raises and addresses the problem of time-delayed feedback in
learning in games. Because learning in games assumes that multiple agents
independently learn their strategies, a discrepancy in optimization often
emerges among the agents. To overcome this discrepancy, the prediction of the
future reward is incorporated into algorithms, typically known as Optimistic
Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the
past rewards hinders the prediction. Indeed, this study firstly proves that
even a single-step delay worsens the performance of OFTRL from the aspects of
regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where
the prediction vector of the next reward in OFTRL is weighted $n$ times. We
further capture an intuition that the optimistic weight cancels out this time
delay. We prove that when the optimistic weight exceeds the time delay, our
WOFTRL recovers the good performances that the regret is constant
($O(1)$-regret) in general-sum normal-form games, and the strategies converge
to the Nash equilibrium as a subsequence (best-iterate convergence) in
poly-matrix zero-sum games. The theoretical results are supported and
strengthened by our experiments.

</details>


### [64] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: VISION introduces a novel reconstruction paradigm using Dynamic Prompting to reconstruct subsurface ocean dynamics from incomplete surface observations, outperforming state-of-the-art models and showing strong generalization under extreme data missing scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of reconstructing subsurface ocean dynamics from incomplete surface observations and overcome the lack of standardized benchmarks in Earth science.

Method: Built KD48 benchmark from petascale simulations, then developed VISION with Dynamic Prompting that generates visual prompts from available observations and uses State-conditioned Prompting module to inject these prompts into a universal backbone with geometry- and scale-aware operators.

Result: VISION substantially outperforms state-of-the-art models and exhibits strong generalization under extreme data missing scenarios on the KD48 benchmark.

Conclusion: The work establishes solid infrastructure for ocean science research under data uncertainty by providing both a high-quality benchmark and a robust model.

Abstract: Reconstructing subsurface ocean dynamics, such as vertical velocity fields,
from incomplete surface observations poses a critical challenge in Earth
science, a field long hampered by the lack of standardized, analysis-ready
benchmarks. To systematically address this issue and catalyze research, we
first build and release KD48, a high-resolution ocean dynamics benchmark
derived from petascale simulations and curated with expert-driven denoising.
Building on this benchmark, we introduce VISION, a novel reconstruction
paradigm based on Dynamic Prompting designed to tackle the core problem of
missing data in real-world observations. The essence of VISION lies in its
ability to generate a visual prompt on-the-fly from any available subset of
observations, which encodes both data availability and the ocean's physical
state. More importantly, we design a State-conditioned Prompting module that
efficiently injects this prompt into a universal backbone, endowed with
geometry- and scale-aware operators, to guide its adaptive adjustment of
computational strategies. This mechanism enables VISION to precisely handle the
challenges posed by varying input combinations. Extensive experiments on the
KD48 benchmark demonstrate that VISION not only substantially outperforms
state-of-the-art models but also exhibits strong generalization under extreme
data missing scenarios. By providing a high-quality benchmark and a robust
model, our work establishes a solid infrastructure for ocean science research
under data uncertainty. Our codes are available at:
https://github.com/YuanGao-YG/VISION.

</details>


### [65] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: SPEAR is a curriculum-based self-imitation learning method that balances exploration-exploitation in RL for agentic LLMs by managing entropy across training stages using intrinsic rewards and experience replay.


<details>
  <summary>Details</summary>
Motivation: Address the exploration-exploitation trade-off in RL for LLMs, avoiding training instability from mechanical entropy maximization and multi-turn distribution shifting.

Method: Extends vanilla self-imitation learning with curriculum-based entropy control, using intrinsic rewards for skill-level exploration and self-imitation for action-level exploration, with regularization techniques to stabilize training.

Result: Enables progressive exploration-exploitation balance without entropy collapsing or divergence, allowing broad skill acquisition followed by efficient pattern exploitation.

Conclusion: SPEAR provides a stable framework for training agentic LLMs that effectively balances exploration and exploitation through curriculum-based entropy management.

Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.

</details>


### [66] [Filtering with Confidence: When Data Augmentation Meets Conformal Prediction](https://arxiv.org/abs/2509.21479)
*Zixuan Wu,So Won Jeong,Yating Liu,Yeo Jin Jung,Claire Donnat*

Main category: cs.LG

TL;DR: Conformal data augmentation is a principled data filtering framework that uses conformal prediction to generate diverse synthetic data while filtering out poor-quality generations with provable risk control, improving performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Synthetic data augmentation addresses data scarcity and data-intensive model demands, but controlling bias is critical to ensure generated samples come from the same underlying distribution as training data with minimal shifts.

Method: Proposes conformal data augmentation framework that leverages conformal prediction to filter synthetic data, requiring no access to internal model logits nor large-scale model retraining.

Result: Shows consistent performance improvements of up to 40% in F1 score over unaugmented baselines and 4% over other filtered augmentation baselines across tasks including topic prediction, sentiment analysis, image classification, and fraud detection.

Conclusion: The method effectively produces diverse synthetic data while filtering poor-quality generations with provable risk control, demonstrating significant performance gains across various applications.

Abstract: With promising empirical performance across a wide range of applications,
synthetic data augmentation appears a viable solution to data scarcity and the
demands of increasingly data-intensive models. Its effectiveness lies in
expanding the training set in a way that reduces estimator variance while
introducing only minimal bias. Controlling this bias is therefore critical:
effective data augmentation should generate diverse samples from the same
underlying distribution as the training set, with minimal shifts. In this
paper, we propose conformal data augmentation, a principled data filtering
framework that leverages the power of conformal prediction to produce diverse
synthetic data while filtering out poor-quality generations with provable risk
control. Our method is simple to implement, requires no access to internal
model logits, nor large-scale model retraining. We demonstrate the
effectiveness of our approach across multiple tasks, including topic
prediction, sentiment analysis, image classification, and fraud detection,
showing consistent performance improvements of up to 40% in F1 score over
unaugmented baselines, and 4% over other filtered augmentation baselines.

</details>


### [67] [High-Probability Analysis of Online and Federated Zero-Order Optimisation](https://arxiv.org/abs/2509.21484)
*Arya Akhavan,David Janz,El-Mahdi El-Mhamdi*

Main category: cs.LG

TL;DR: FedZero is a federated zero-order optimization algorithm that achieves near-optimal error bounds with high probability in federated convex settings and establishes the first high-probability convergence guarantees for convex zero-order optimization.


<details>
  <summary>Details</summary>
Motivation: To address distributed learning in gradient-free zero-order optimization and provide strong theoretical guarantees that improve upon classical expectation-based results.

Method: FedZero uses a gradient estimator based on randomization over the ℓ₁-sphere and develops new concentration inequalities for Lipschitz functions under the uniform measure on the ℓ₁-sphere.

Result: FedZero achieves near-optimal optimization error bounds with high probability in federated convex settings and establishes the first high-probability convergence guarantees for convex zero-order optimization.

Conclusion: The algorithm provides sharp theoretical guarantees and the developed concentration tools may be of independent interest beyond the specific optimization context.

Abstract: We study distributed learning in the setting of gradient-free zero-order
optimization and introduce FedZero, a federated zero-order algorithm that
delivers sharp theoretical guarantees. Specifically, FedZero: (1) achieves
near-optimal optimization error bounds with high probability in the federated
convex setting; and (2) in the single-worker regime-where the problem reduces
to the standard zero-order framework, establishes the first high-probability
convergence guarantees for convex zero-order optimization, thereby
strengthening the classical expectation-based results. At its core, FedZero
employs a gradient estimator based on randomization over the $\ell_1$-sphere.
To analyze it, we develop new concentration inequalities for Lipschitz
functions under the uniform measure on the $\ell_1$-sphere, with explicit
constants. These concentration tools are not only central to our
high-probability guarantees but may also be of independent interest.

</details>


### [68] [Neural Operators for Mathematical Modeling of Transient Fluid Flow in Subsurface Reservoir Systems](https://arxiv.org/abs/2509.21485)
*Daniil D. Sirota,Sergey A. Khan,Sergey L. Kostikov,Kirill A. Butov*

Main category: cs.LG

TL;DR: The paper presents TFNO-opt, an optimized Fourier neural operator architecture for modeling transient fluid flow in subsurface reservoirs, achieving 6 orders of magnitude speedup over traditional methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods for reservoir modeling are computationally expensive, limiting their use in control and decision support applications. There's a need for faster modeling approaches that can handle complex PDE systems.

Method: Developed TFNO-opt architecture based on Fourier neural operators with modifications: adjustable internal time resolution, tensor decomposition in spectral domain, Sobolev norm in error function, and separation of approximation errors from initial condition reconstruction.

Result: Achieved 6 orders of magnitude acceleration compared to traditional methods in underground gas storage modeling while maintaining modeling accuracy and stability.

Conclusion: The proposed neural operator architecture enables efficient control of complex reservoir systems by providing fast and accurate modeling capabilities that were previously impractical with traditional numerical methods.

Abstract: This paper presents a method for modeling transient fluid flow in subsurface
reservoir systems based on the developed neural operator architecture
(TFNO-opt). Reservoir systems are complex dynamic objects with distributed
parameters described by systems of partial differential equations (PDEs).
Traditional numerical methods for modeling such systems, despite their high
accuracy, are characterized by significant time costs for performing
calculations, which limits their applicability in control and decision support
problems. The proposed architecture (TFNO-opt) is based on Fourier neural
operators, which allow approximating PDE solutions in infinite-dimensional
functional spaces, providing invariance to discretization and the possibility
of generalization to various implementations of equations. The developed
modifications are aimed at increasing the accuracy and stability of the trained
neural operator, which is especially important for control problems. These
include adjustable internal time resolution of the integral Fourier operator,
tensor decomposition of parameters in the spectral domain, use of the Sobolev
norm in the error function, and separation of approximation errors and
reconstruction of initial conditions for more accurate reproduction of physical
processes. The effectiveness of the proposed improvements is confirmed by
computational experiments. The practical significance is confirmed by
computational experiments using the example of the problem of hydrodynamic
modeling of an underground gas storage (UGS), where the acceleration of
calculations by six orders of magnitude was achieved, compared to traditional
methods. This opens up new opportunities for the effective control of complex
reservoir systems.

</details>


### [69] [GraphPFN: A Prior-Data Fitted Graph Foundation Model](https://arxiv.org/abs/2509.21489)
*Dmitry Eremeev,Oleg Platonov,Gleb Bazhenov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: GraphPFN is a graph foundation model that uses synthetic graph generation and tabular foundation model augmentation to achieve state-of-the-art performance on node-level prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing graph foundation models rely on hand-crafted features and struggle to learn complex graph-specific patterns, limiting their effectiveness.

Method: Design a prior distribution of synthetic attributed graphs using stochastic block models and preferential attachment, then augment tabular foundation model LimiX with graph attention layers and train on synthetic graphs.

Result: GraphPFN achieves state-of-the-art results on diverse real-world datasets with up to 50,000 nodes, outperforming both G2T-FM and task-specific GNNs on most datasets.

Conclusion: Pretraining on synthetic graphs from a well-designed prior distribution is an effective strategy for building graph foundation models.

Abstract: Foundation models pretrained on large-scale datasets have transformed such
fields as natural language processing and computer vision, but their
application to graph data remains limited. Recently emerged graph foundation
models, such as G2T-FM, utilize tabular foundation models for graph tasks and
were shown to significantly outperform prior attempts to create GFMs. However,
these models primarily rely on hand-crafted graph features, limiting their
ability to learn complex graph-specific patterns. In this work, we propose
GraphPFN: a prior-data fitted network for node-level prediction. First, we
design a prior distribution of synthetic attributed graphs. For graph structure
generation, we use a novel combination of multiple stochastic block models and
a preferential attachment process. We then apply graph-aware structured causal
models to generate node attributes and targets. This procedure allows us to
efficiently generate a wide range of realistic graph datasets. Then, we augment
the tabular foundation model LimiX with attention-based graph neighborhood
aggregation layers and train it on synthetic graphs sampled from our prior,
allowing the model to capture graph structural dependencies not present in
tabular data. On diverse real-world graph datasets with up to 50,000 nodes,
GraphPFN shows strong in-context learning performance and achieves
state-of-the-art results after finetuning, outperforming both G2T-FM and
task-specific GNNs trained from scratch on most datasets. More broadly, our
work demonstrates that pretraining on synthetic graphs from a well-designed
prior distribution is an effective strategy for building graph foundation
models.

</details>


### [70] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: SlimDiff is a gradient-free structural compression framework for diffusion models that reduces attention and feedforward dimensions through spectral approximation guided by activation covariances, achieving 35% acceleration and ~100M parameter reduction without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally prohibitive due to their large parameters and iterative denoising, and existing efficiency techniques require fine-tuning or retraining to recover performance.

Method: Reframes DM compression as spectral approximation using activation covariances across timesteps to define low-rank subspaces for dynamic pruning, with module-wise decompositions over functional weight groups and adaptive sparsity allocation.

Result: Achieves up to 35% acceleration and ~100M parameter reduction while maintaining generation quality comparable to uncompressed models, requiring only about 500 calibration samples (70× fewer than prior methods).

Conclusion: First closed-form, activation-guided structural compression of diffusion models that is entirely training-free, providing both theoretical clarity and practical efficiency.

Abstract: Diffusion models (DMs), lauded for their generative performance, are
computationally prohibitive due to their billion-scale parameters and iterative
denoising dynamics. Existing efficiency techniques, such as quantization,
timestep reduction, or pruning, offer savings in compute, memory, or runtime
but are strictly bottlenecked by reliance on fine-tuning or retraining to
recover performance. In this work, we introduce SlimDiff, an automated
activation-informed structural compression framework that reduces both
attention and feedforward dimensionalities in DMs, while being entirely
gradient-free. SlimDiff reframes DM compression as a spectral approximation
task, where activation covariances across denoising timesteps define low-rank
subspaces that guide dynamic pruning under a fixed compression budget. This
activation-aware formulation mitigates error accumulation across timesteps by
applying module-wise decompositions over functional weight groups: query--key
interactions, value--output couplings, and feedforward projections, rather than
isolated matrix factorizations, while adaptively allocating sparsity across
modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff
achieves up to 35\% acceleration and $\sim$100M parameter reduction over
baselines, with generation quality on par with uncompressed models without any
backpropagation. Crucially, our approach requires only about 500 calibration
samples, over 70$\times$ fewer than prior methods. To our knowledge, this is
the first closed-form, activation-guided structural compression of DMs that is
entirely training-free, providing both theoretical clarity and practical
efficiency.

</details>


### [71] [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500)
*Junkai Zhang,Zihao Wang,Lin Gui,Swarnashree Mysore Sathyendra,Jaehwan Jeong,Victor Veitch,Wei Wang,Yunzhong He,Bing Liu,Lifeng Jin*

Main category: cs.LG

TL;DR: The paper proposes rubric-based rewards to address reward over-optimization in reinforcement fine-tuning by focusing on the high-reward tail and leveraging off-policy examples while avoiding their artifacts.


<details>
  <summary>Details</summary>
Motivation: Reinforcement fine-tuning suffers from reward over-optimization where models achieve high reward scores but produce low-quality outputs, due to reward misspecification at the high-reward tail.

Method: The authors introduce rubric-based rewards that can leverage off-policy examples while remaining insensitive to their artifacts, with a workflow designed to distinguish among great and diverse responses.

Result: Empirical results show that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements.

Conclusion: Rubric-based rewards provide an effective solution to reward over-optimization in reinforcement fine-tuning by properly capturing distinctions in the high-reward tail region.

Abstract: Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .

</details>


### [72] [Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations](https://arxiv.org/abs/2509.21511)
*Micha Livne*

Main category: cs.LG

TL;DR: cMIM is a contrastive extension of Mutual Information Machine that combines generative fidelity with discriminative performance, outperforming MIM and InfoNCE on classification/regression tasks while maintaining competitive reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Existing representation learning methods (contrastive learning, self-supervised masking, denoising auto-encoders) have different trade-offs. MIM maximizes mutual information but falls short on discriminative tasks, creating a need for a unified framework that serves both discriminative and generative applications.

Method: cMIM extends MIM with a contrastive objective that imposes global discriminative structure while retaining MIM's generative fidelity. It removes the need for positive data augmentation and is less sensitive to batch size than InfoNCE. Also introduces 'informative embeddings' technique for extracting enriched features from encoder-decoder models.

Result: Empirical evidence across vision and molecular benchmarks shows cMIM outperforms MIM and InfoNCE on classification and regression tasks while preserving competitive reconstruction quality.

Conclusion: cMIM serves as a unified framework for representation learning that advances models capable of effectively serving both discriminative and generative applications.

Abstract: Learning representations that transfer well to diverse downstream tasks
remains a central challenge in representation learning. Existing paradigms --
contrastive learning, self-supervised masking, and denoising auto-encoders --
balance this challenge with different trade-offs. We introduce the {contrastive
Mutual Information Machine} (cMIM), a probabilistic framework that extends the
Mutual Information Machine (MIM) with a contrastive objective. While MIM
maximizes mutual information between inputs and latents and promotes clustering
of codes, it falls short on discriminative tasks. cMIM addresses this gap by
imposing global discriminative structure while retaining MIM's generative
fidelity. Our contributions are threefold. First, we propose cMIM, a
contrastive extension of MIM that removes the need for positive data
augmentation and is substantially less sensitive to batch size than InfoNCE.
Second, we introduce {informative embeddings}, a general technique for
extracting enriched features from encoder-decoder models that boosts
discriminative performance without additional training and applies broadly
beyond MIM. Third, we provide empirical evidence across vision and molecular
benchmarks showing that cMIM outperforms MIM and InfoNCE on classification and
regression tasks while preserving competitive reconstruction quality. These
results position cMIM as a unified framework for representation learning,
advancing the goal of models that serve both discriminative and generative
applications effectively.

</details>


### [73] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: DistillKac is a fast image generator using damped wave equation and stochastic Kac representation to enable finite-speed probability transport, offering improved stability over diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from stiff reverse time velocities and implicitly allow unbounded propagation speed, while Kac dynamics enforce finite speed transport and bounded kinetic energy for better numerical stability.

Method: Uses damped wave equation with stochastic Kac representation, introduces classifier-free guidance in velocity space, and proposes endpoint-only distillation where a student network matches a frozen teacher over long intervals.

Result: DistillKac delivers high-quality samples with very few function evaluations while maintaining numerical stability benefits of finite-speed probability flows.

Conclusion: The approach successfully combines finite-speed transport with efficient distillation, providing stable and fast image generation with theoretical guarantees on path closeness through endpoint supervision.

Abstract: We present DistillKac, a fast image generator that uses the damped wave
equation and its stochastic Kac representation to move probability mass at
finite speed. In contrast to diffusion models whose reverse time velocities can
become stiff and implicitly allow unbounded propagation speed, Kac dynamics
enforce finite speed transport and yield globally bounded kinetic energy.
Building on this structure, we introduce classifier-free guidance in velocity
space that preserves square integrability under mild conditions. We then
propose endpoint only distillation that trains a student to match a frozen
teacher over long intervals. We prove a stability result that promotes
supervision at the endpoints to closeness along the entire path. Experiments
demonstrate DistillKac delivers high quality samples with very few function
evaluations while retaining the numerical stability benefits of finite speed
probability flows.

</details>


### [74] [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
*Joshua Mitton,Prarthana Bhattacharyya,Ralph Abboud,Simon Woodhead*

Main category: cs.LG

TL;DR: This paper proposes adding uncertainty estimation capabilities to Knowledge Tracing models to better detect when students choose distractors, addressing the limitation where current KT models make most incorrect predictions in such cases.


<details>
  <summary>Details</summary>
Motivation: Current Knowledge Tracing models focus primarily on predictive accuracy but struggle most when students select distractors, causing student errors to go undetected. The research aims to enhance KT models by incorporating uncertainty estimation to identify these problematic cases.

Method: The approach involves capturing predictive uncertainty in KT models and demonstrating that higher predictive uncertainty correlates with model incorrect predictions, particularly when students choose distractors.

Result: The study shows that uncertainty in KT models is informative and that larger predictive uncertainty aligns with model incorrect predictions, providing a useful signal for detecting student errors.

Conclusion: Uncertainty estimation in KT models provides pedagogically useful information that could be valuable in educational learning platforms, especially in resource-limited settings where understanding student ability is crucial.

Abstract: The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.

</details>


### [75] [$\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and Delayed Generalization](https://arxiv.org/abs/2509.21519)
*Yuandong Tian*

Main category: cs.LG

TL;DR: The paper proposes a mathematical framework called Li₂ that characterizes grokking (delayed generalization) in 2-layer nonlinear networks through three stages: lazy learning, independent feature learning, and interactive feature learning, revealing how features emerge and interact during training.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical principles behind grokking behavior in neural networks, specifically what features emerge, how they develop, and under what conditions delayed generalization occurs for complex structured inputs.

Method: Developed the Li₂ framework analyzing backpropagated gradient structure across layers. Studied three learning stages characterized by gradient dynamics: lazy learning (random gradients), independent feature learning (node-specific gradients), and interactive feature learning (collaborative gradient focus). Used group arithmetic tasks to analyze feature generalizability and representation power.

Result: Showed that independent dynamics follow gradient ascent of an energy function E, with local maxima corresponding to emerging features. Demonstrated how hidden nodes interact and how gradients focus on missing features. Revealed roles of hyperparameters (weight decay, learning rate, sample sizes) in grokking and scaling laws for memorization/generalization.

Conclusion: The Li₂ framework provides a principled mathematical understanding of grokking phenomena, explaining feature emergence dynamics and how recent optimizers like Muon work effectively from gradient dynamics principles. The analysis is extendable to multi-layer architectures.

Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been
studied extensively, it remains an open question whether there is a
mathematical framework to characterize what kind of features emerge, how and in
which conditions it happens from training, for complex structured inputs. We
propose a novel framework, named $\mathbf{Li_2}$, that captures three key
stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy
learning, (II) independent feature learning and (III) interactive feature
learning, characterized by the structure of backpropagated gradient $G_F$
across layers. In (I), $G_F$ is random, and top layer overfits to random hidden
representation. In (II), the gradient of each node (column of $G_F$) only
depends on its own activation, and thus each hidden node learns their
representation independently from $G_F$, which now carries information about
target labels, thanks to weight decay. Interestingly, the independent dynamics
follows exactly the gradient ascent of an energy function $E$, and its local
maxima are precisely the emerging features. We study whether these local-optima
induced features are generalizable, their representation power, and how they
change on sample size, in group arithmetic tasks. Finally, in (III), we
provably show how hidden nodes interact, and how $G_F$ changes to focus on
missing features that need to be learned. Our study sheds lights on roles
played by key hyperparameters such as weight decay, learning rate and sample
sizes in grokking, leads to provable scaling laws of memorization and
generalization, and reveals the underlying cause why recent optimizers such as
Muon can be effective, from the first principles of gradient dynamics. Our
analysis can be extended to multi-layer architectures.

</details>


### [76] [TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning](https://arxiv.org/abs/2509.21526)
*Hongyang He,Xinyuan Song,Yangfan He,Zeyu Zhang,Yanshu Li,Haochen You,Lifan Sun,Wenqiao Zhang*

Main category: cs.LG

TL;DR: TRiCo is a triadic game-theoretic co-training framework for semi-supervised learning that uses a teacher, two students, and an adversarial generator in a unified training paradigm, achieving state-of-the-art performance in low-label regimes.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing SSL frameworks such as static view interactions, unreliable pseudo-labels, and lack of hard sample modeling by rethinking the structure of semi-supervised learning.

Method: Uses a triadic interaction with: (i) two student classifiers on frozen complementary representations, (ii) meta-learned teacher for adaptive pseudo-label selection and loss balancing via validation feedback, (iii) non-parametric generator that perturbs embeddings. Formalized as a Stackelberg game with mutual information-based pseudo-label selection.

Result: Extensive experiments on CIFAR-10, SVHN, STL-10, and ImageNet demonstrate consistent state-of-the-art performance in low-label regimes while remaining architecture-agnostic and compatible with frozen vision backbones.

Conclusion: TRiCo provides a principled and generalizable solution to semi-supervised learning by addressing key limitations through its triadic game-theoretic framework.

Abstract: We introduce TRiCo, a novel triadic game-theoretic co-training framework that
rethinks the structure of semi-supervised learning by incorporating a teacher,
two students, and an adversarial generator into a unified training paradigm.
Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL
as a structured interaction among three roles: (i) two student classifiers
trained on frozen, complementary representations, (ii) a meta-learned teacher
that adaptively regulates pseudo-label selection and loss balancing via
validation-based feedback, and (iii) a non-parametric generator that perturbs
embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected
based on mutual information rather than confidence, providing a more robust
measure of epistemic uncertainty. This triadic interaction is formalized as a
Stackelberg game, where the teacher leads strategy optimization and students
follow under adversarial perturbations. By addressing key limitations in
existing SSL frameworks, such as static view interactions, unreliable
pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled
and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,
and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art
performance in low-label regimes, while remaining architecture-agnostic and
compatible with frozen vision backbones.

</details>


### [77] [Preemptive Detection and Steering of LLM Misalignment via Latent Reachability](https://arxiv.org/abs/2509.21528)
*Sathwik Karnik,Somil Bansal*

Main category: cs.LG

TL;DR: BRT-Align is a reachability-based framework that applies control-theoretic safety tools to LLM inference, providing runtime monitoring and steering to prevent harmful content generation while preserving output quality.


<details>
  <summary>Details</summary>
Motivation: Current safety approaches like RLHF only work during training but offer no safeguards at inference time, where unsafe content may still be generated, creating urgent safety concerns for widely deployed LLMs.

Method: Models autoregressive generation as a dynamical system in latent space and learns a safety value function via backward reachability to estimate worst-case trajectory evolution, enabling runtime monitoring and least-restrictive steering filters.

Result: BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines, substantially reduces unsafe generations while preserving diversity and coherence, and produces responses that are less violent, profane, offensive, and politically biased.

Conclusion: Reachability analysis provides a principled and practical foundation for inference-time LLM safety, offering effective safeguards during generation without compromising output quality.

Abstract: Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.

</details>


### [78] [Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration](https://arxiv.org/abs/2509.21530)
*Dongkyu Cho,Miao Zhang,Rumi Chunara*

Main category: cs.LG

TL;DR: A query-based model collaboration framework that integrates expert domain knowledge to guide LLM data augmentation, improving both performance and safety in clinical prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Address the risk of LLMs generating clinically incorrect or misleading information when used for data augmentation in high-stakes healthcare domains, bridging the gap between LLM augmentation potential and safety requirements.

Method: Proposed a lightweight query-based model collaboration framework that integrates expert-level domain knowledge to guide the augmentation process and preserve critical medical information.

Result: Consistently outperforms existing LLM augmentation methods on clinical prediction tasks while improving safety through reduced factual errors.

Conclusion: The framework successfully addresses safety concerns in specialized domains like healthcare while leveraging LLM augmentation capabilities, demonstrating a viable approach for high-stakes applications.

Abstract: Data augmentation is a widely used strategy to improve model robustness and
generalization by enriching training datasets with synthetic examples. While
large language models (LLMs) have demonstrated strong generative capabilities
for this purpose, their applications in high-stakes domains like healthcare
present unique challenges due to the risk of generating clinically incorrect or
misleading information. In this work, we propose a novel query-based model
collaboration framework that integrates expert-level domain knowledge to guide
the augmentation process to preserve critical medical information. Experiments
on clinical prediction tasks demonstrate that our lightweight
collaboration-based approach consistently outperforms existing LLM augmentation
methods while improving safety through reduced factual errors. This framework
addresses the gap between LLM augmentation potential and the safety
requirements of specialized domains.

</details>


### [79] [A circuit for predicting hierarchical structure in-context in Large Language Models](https://arxiv.org/abs/2509.21534)
*Tankred Saanum,Can Demircan,Samuel J. Gershman,Eric Schulz*

Main category: cs.LG

TL;DR: LLMs develop adaptive induction heads that learn contextual dependencies for hierarchical patterns, going beyond simple token repetition to support complex in-context learning.


<details>
  <summary>Details</summary>
Motivation: To understand if induction heads can support in-context learning of complex hierarchical patterns beyond simple token repetition, particularly for natural language cases where context-dependent predictions are needed.

Method: Designed synthetic in-context learning tasks with hierarchical dependencies, evaluated various LLMs on these sequences and natural language analogues, and investigated how induction heads learn in-context.

Result: Found adaptive induction heads that learn what to attend to in-context, supported by attention heads that uncover latent contexts determining token transition relationships.

Conclusion: LLMs have induction heads that learn to predict higher-order repetitive patterns in-context, providing a complete mechanistic account of how they handle complex hierarchical dependencies.

Abstract: Large Language Models (LLMs) excel at in-context learning, the ability to use
information provided as context to improve prediction of future tokens.
Induction heads have been argued to play a crucial role for in-context learning
in Transformer Language Models. These attention heads make a token attend to
successors of past occurrences of the same token in the input. This basic
mechanism supports LLMs' ability to copy and predict repeating patterns.
However, it is unclear if this same mechanism can support in-context learning
of more complex repetitive patterns with hierarchical structure. Natural
language is teeming with such cases: The article "the" in English usually
prefaces multiple nouns in a text. When predicting which token succeeds a
particular instance of "the", we need to integrate further contextual cues from
the text to predict the correct noun. If induction heads naively attend to all
past instances of successor tokens of "the" in a context-independent manner,
they cannot support this level of contextual information integration. In this
study, we design a synthetic in-context learning task, where tokens are
repeated with hierarchical dependencies. Here, attending uniformly to all
successor tokens is not sufficient to accurately predict future tokens.
Evaluating a range of LLMs on these token sequences and natural language
analogues, we find adaptive induction heads that support prediction by learning
what to attend to in-context. Next, we investigate how induction heads
themselves learn in-context. We find evidence that learning is supported by
attention heads that uncover a set of latent contexts, determining the
different token transition relationships. Overall, we not only show that LLMs
have induction heads that learn, but offer a complete mechanistic account of
how LLMs learn to predict higher-order repetitive patterns in-context.

</details>


### [80] [Evidence for Limited Metacognition in LLMs](https://arxiv.org/abs/2509.21545)
*Christopher Ackerman*

Main category: cs.LG

TL;DR: This paper introduces a novel methodology for quantitatively evaluating metacognitive abilities in LLMs, showing that frontier models since early 2024 demonstrate increasing evidence of metacognitive capabilities like confidence assessment and answer anticipation.


<details>
  <summary>Details</summary>
Motivation: The possibility of LLM self-awareness and sentience is gaining public attention with major safety implications, but current measurement science is underdeveloped, creating a need for objective evaluation methods.

Method: The approach takes inspiration from animal metacognition research, eschewing self-reports and instead testing strategic deployment of internal state knowledge using two experimental paradigms, supplemented by token probability analysis.

Result: Frontier LLMs show increasingly strong evidence of metacognitive abilities including confidence assessment and answer anticipation, though these abilities are limited in resolution, context-dependent, and qualitatively different from human metacognition.

Conclusion: LLMs demonstrate emerging metacognitive capabilities influenced by post-training, but these abilities differ from human metacognition and require further study for understanding their implications for AI safety and policy.

Abstract: The possibility of LLM self-awareness and even sentience is gaining
increasing public attention and has major safety and policy implications, but
the science of measuring them is still in a nascent state. Here we introduce a
novel methodology for quantitatively evaluating metacognitive abilities in
LLMs. Taking inspiration from research on metacognition in nonhuman animals,
our approach eschews model self-reports and instead tests to what degree models
can strategically deploy knowledge of internal states. Using two experimental
paradigms, we demonstrate that frontier LLMs introduced since early 2024 show
increasingly strong evidence of certain metacognitive abilities, specifically
the ability to assess and utilize their own confidence in their ability to
answer factual and reasoning questions correctly and the ability to anticipate
what answers they would give and utilize that information appropriately. We
buttress these behavioral findings with an analysis of the token probabilities
returned by the models, which suggests the presence of an upstream internal
signal that could provide the basis for metacognition. We further find that
these abilities 1) are limited in resolution, 2) emerge in context-dependent
manners, and 3) seem to be qualitatively different from those of humans. We
also report intriguing differences across models of similar capabilities,
suggesting that LLM post-training may have a role in developing metacognitive
abilities.

</details>


### [81] [Machine Learning. The Science of Selection under Uncertainty](https://arxiv.org/abs/2509.21547)
*Yevgeny Seldin*

Main category: cs.LG

TL;DR: This book provides statistical tools for theoretical guarantees in machine learning selection processes, covering concentration inequalities, offline supervised learning generalization bounds, and online learning regret analysis.


<details>
  <summary>Details</summary>
Motivation: To address the inherent uncertainty in machine learning due to data sampling randomness, and provide statistical guarantees for selection processes in both offline and online learning settings.

Method: Uses concentration inequalities (Markov's, Chebyshev's, Hoeffding's, Bernstein's, etc.) for controlling empirical estimate deviations, then applies these to derive generalization bounds in offline learning (Occam's razor, VC analysis, PAC-Bayesian analysis) and regret bounds in online learning (stochastic/adversarial environments, full/bandit feedback).

Result: Comprehensive statistical framework for obtaining theoretical guarantees on learning outcomes under uncertainty, covering both empirical risk minimization in offline settings and regret minimization in online settings.

Conclusion: The book establishes a unified statistical foundation for analyzing learning as a selection process under uncertainty, providing tools that bridge offline and online learning paradigms with theoretical guarantees.

Abstract: Learning, whether natural or artificial, is a process of selection. It starts
with a set of candidate options and selects the more successful ones. In the
case of machine learning the selection is done based on empirical estimates of
prediction accuracy of candidate prediction rules on some data. Due to
randomness of data sampling the empirical estimates are inherently noisy,
leading to selection under uncertainty. The book provides statistical tools to
obtain theoretical guarantees on the outcome of selection under uncertainty. We
start with concentration of measure inequalities, which are the main
statistical instrument for controlling how much an empirical estimate of
expectation of a function deviates from the true expectation. The book covers a
broad range of inequalities, including Markov's, Chebyshev's, Hoeffding's,
Bernstein's, Empirical Bernstein's, Unexpected Bernstein's, kl, and split-kl.
We then study the classical (offline) supervised learning and provide a range
of tools for deriving generalization bounds, including Occam's razor,
Vapnik-Chervonenkis analysis, and PAC-Bayesian analysis. The latter is further
applied to derive generalization guarantees for weighted majority votes. After
covering the offline setting, we turn our attention to online learning. We
present the space of online learning problems characterized by environmental
feedback, environmental resistance, and structural complexity. A common
performance measure in online learning is regret, which compares performance of
an algorithm to performance of the best prediction rule in hindsight, out of a
restricted set of prediction rules. We present tools for deriving regret bounds
in stochastic and adversarial environments, and under full information and
bandit feedback.

</details>


### [82] [Interpretable time series analysis with Gumbel dynamics](https://arxiv.org/abs/2509.21578)
*Yiliu Wang,Timothy Doyeon Kim,Eric Shea-Brown,Uygar Sümbül*

Main category: cs.LG

TL;DR: The Gumbel Dynamical Model (GDM) introduces continuous relaxation of discrete states using Gumbel distribution to better capture smooth transitions and overlapping states in switching dynamical systems, enabling differentiable training and improved modeling of real-world stochastic time series.


<details>
  <summary>Details</summary>
Motivation: Traditional switching dynamical systems struggle with smooth transitions, variable-speed changes, and stochastic mixtures of overlapping states due to their discrete nature, often resulting in spurious rapid switching on real-world datasets.

Method: GDM introduces continuous relaxation of discrete states and a Gumbel distribution-based noise model on the relaxed-discrete state space, making the model fully differentiable and enabling gradient-based training.

Result: GDM successfully models soft, sticky states and transitions in stochastic settings, approximates smoother and non-stationary dynamics more faithfully, and infers interpretable states in real-world datasets where traditional methods fail.

Conclusion: The Gumbel Dynamical Model provides a more flexible and differentiable framework for switching dynamical systems that better captures complex real-world dynamics while maintaining interpretability.

Abstract: Switching dynamical systems can model complicated time series data while
maintaining interpretability by inferring a finite set of dynamics primitives
and explaining different portions of the observed time series with one of these
primitives. However, due to the discrete nature of this set, such models
struggle to capture smooth, variable-speed transitions, as well as stochastic
mixtures of overlapping states, and the inferred dynamics often display
spurious rapid switching on real-world datasets. Here, we propose the Gumbel
Dynamical Model (GDM). First, by introducing a continuous relaxation of
discrete states and a different noise model defined on the relaxed-discrete
state space via the Gumbel distribution, GDM expands the set of available state
dynamics, allowing the model to approximate smoother and non-stationary
ground-truth dynamics more faithfully. Second, the relaxation makes the model
fully differentiable, enabling fast and scalable training with standard
gradient descent methods. We validate our approach on standard simulation
datasets and highlight its ability to model soft, sticky states and transitions
in a stochastic setting. Furthermore, we apply our model to two real-world
datasets, demonstrating its ability to infer interpretable states in stochastic
time series with multiple dynamics, a setting where traditional methods often
fail.

</details>


### [83] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: This paper uses big data analytics and machine learning to detect spam reviews in Amazon product reviews, with Logistic Regression achieving 90.35% accuracy.


<details>
  <summary>Details</summary>
Motivation: Fraudulent reviews undermine consumer trust and damage seller reputations in online shopping, creating a need for reliable spam detection methods.

Method: Employed scalable big data framework to process large-scale Amazon review data, extracted key features of fraudulent behavior, and used various machine learning classifiers including Logistic Regression.

Result: Logistic Regression classifier achieved 90.35% accuracy in detecting spam reviews, demonstrating effective performance in identifying fraudulent content.

Conclusion: The research successfully contributes to creating a more trustworthy and transparent online shopping environment through accurate spam review detection using advanced analytics.

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [84] [GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks](https://arxiv.org/abs/2509.21605)
*Tian Yu Yen,Reese E. Jones,Ravi G. Patel*

Main category: cs.LG

TL;DR: GenUQ introduces a measure-theoretic approach to uncertainty quantification in operator learning that uses generative hyper-networks to produce parameter distributions without needing likelihood construction, outperforming other UQ methods in three example problems.


<details>
  <summary>Details</summary>
Motivation: Traditional likelihood-based UQ methods in operator learning struggle when stochastic operators produce actions where likelihoods are difficult or impossible to construct, limiting uncertainty quantification capabilities.

Method: GenUQ uses a generative hyper-network model that produces parameter distributions consistent with observed data, avoiding the need for likelihood construction through a measure-theoretic approach.

Result: GenUQ outperforms other UQ methods in three test cases: recovering a manufactured operator, learning solution operators for stochastic elliptic PDEs, and modeling failure locations in porous steel under tension.

Conclusion: The measure-theoretic generative approach provides an effective alternative to likelihood-based methods for uncertainty quantification in operator learning, enabling UQ in cases where traditional methods fail.

Abstract: Operator learning is a recently developed generalization of regression to
mappings between functions. It promises to drastically reduce expensive
numerical integration of PDEs to fast evaluations of mappings between
functional states of a system, i.e., surrogate and reduced-order modeling.
Operator learning has already found applications in several areas such as
modeling sea ice, combustion, and atmospheric physics. Recent approaches
towards integrating uncertainty quantification into the operator models have
relied on likelihood based methods to infer parameter distributions from noisy
data. However, stochastic operators may yield actions from which a likelihood
is difficult or impossible to construct. In this paper, we introduce, GenUQ, a
measure-theoretic approach to UQ that avoids constructing a likelihood by
introducing a generative hyper-network model that produces parameter
distributions consistent with observed data. We demonstrate that GenUQ
outperforms other UQ methods in three example problems, recovering a
manufactured operator, learning the solution operator to a stochastic elliptic
PDE, and modeling the failure location of porous steel under tension.

</details>


### [85] [Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection](https://arxiv.org/abs/2509.21606)
*Seohyeon Cha,Huancheng Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedProTIP is a federated continual learning framework that uses gradient projection to mitigate catastrophic forgetting and includes task identity prediction for task-agnostic inference.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in federated continual learning settings where data heterogeneity, communication constraints, and privacy concerns exacerbate the forgetting problem.

Method: Projects client updates onto orthogonal complement of subspace spanned by previous task representations, and uses lightweight task identity prediction with core bases from prior tasks.

Result: Significantly outperforms state-of-the-art methods in average accuracy, especially when task identities are unknown.

Conclusion: FedProTIP effectively mitigates forgetting in federated continual learning through gradient projection and task identity prediction, demonstrating superior performance over existing approaches.

Abstract: Federated continual learning (FCL) enables distributed client devices to
learn from streaming data across diverse and evolving tasks. A major challenge
to continual learning, catastrophic forgetting, is exacerbated in decentralized
settings by the data heterogeneity, constrained communication and privacy
concerns. We propose Federated gradient Projection-based Continual Learning
with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates
forgetting by projecting client updates onto the orthogonal complement of the
subspace spanned by previously learned representations of the global model.
This projection reduces interference with earlier tasks and preserves
performance across the task sequence. To further address the challenge of
task-agnostic inference, we incorporate a lightweight mechanism that leverages
core bases from prior tasks to predict task identity and dynamically adjust the
global model's outputs. Extensive experiments across standard FCL benchmarks
demonstrate that FedProTIP significantly outperforms state-of-the-art methods
in average accuracy, particularly in settings where task identities are a
priori unknown.

</details>


### [86] [Causal Abstraction Inference under Lossy Representations](https://arxiv.org/abs/2509.21607)
*Kevin Xia,Elias Bareinboim*

Main category: cs.LG

TL;DR: The paper introduces projected abstractions to handle lossy abstraction functions in causal modeling, allowing multiple low-level interventions to map to the same high-level intervention while maintaining valid causal query translation.


<details>
  <summary>Details</summary>
Motivation: Existing causal abstraction frameworks fail when abstraction functions are lossy, violating the abstract invariance condition where multiple low-level interventions map to the same high-level intervention.

Method: Developed projected abstractions that generalize existing definitions, with construction methods from low-level models and translation of observational, interventional, and counterfactual queries. Also provided graphical criteria for identifying high-level queries from limited low-level data.

Result: Successfully demonstrated the effectiveness of projected abstraction models in high-dimensional image settings through experimental validation.

Conclusion: Projected abstractions provide a robust framework for causal modeling with lossy representations, enabling practical application when the true model is unavailable and supporting causal query translation across abstraction levels.

Abstract: The study of causal abstractions bridges two integral components of human
intelligence: the ability to determine cause and effect, and the ability to
interpret complex patterns into abstract concepts. Formally, causal abstraction
frameworks define connections between complicated low-level causal models and
simple high-level ones. One major limitation of most existing definitions is
that they are not well-defined when considering lossy abstraction functions in
which multiple low-level interventions can have different effects while mapping
to the same high-level intervention (an assumption called the abstract
invariance condition). In this paper, we introduce a new type of abstractions
called projected abstractions that generalize existing definitions to
accommodate lossy representations. We show how to construct a projected
abstraction from the low-level model and how it translates equivalent
observational, interventional, and counterfactual causal queries from low to
high-level. Given that the true model is rarely available in practice we prove
a new graphical criteria for identifying and estimating high-level causal
queries from limited low-level data. Finally, we experimentally show the
effectiveness of projected abstraction models in high-dimensional image
settings.

</details>


### [87] [LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning](https://arxiv.org/abs/2509.21617)
*Marco Paul E. Apolinario,Kaushik Roy*

Main category: cs.LG

TL;DR: LANCE proposes a one-shot SVD-based activation compression method that reduces memory usage by 250× while maintaining accuracy, enabling efficient on-device fine-tuning and continual learning.


<details>
  <summary>Details</summary>
Motivation: On-device learning requires efficient methods to handle memory constraints during backpropagation, especially for fine-tuning and continual learning without catastrophic forgetting. Existing activation compression methods have computational overhead from repeated decompositions.

Method: Uses one-shot higher-order Singular Value Decomposition to create a reusable low-rank subspace for activation projection, eliminating repeated decompositions. Also enables continual learning by allocating tasks to orthogonal subspaces.

Result: Reduces activation storage up to 250× while maintaining comparable accuracy to full backpropagation on multiple datasets. Achieves competitive performance with orthogonal gradient projection methods in continual learning benchmarks at much lower memory cost.

Conclusion: LANCE provides a practical and scalable solution for efficient fine-tuning and continual learning on resource-constrained edge devices.

Abstract: On-device learning is essential for personalization, privacy, and long-term
adaptation in resource-constrained environments. Achieving this requires
efficient learning, both fine-tuning existing models and continually acquiring
new tasks without catastrophic forgetting. Yet both settings are constrained by
high memory cost of storing activations during backpropagation. Existing
activation compression methods reduce this cost but relying on repeated
low-rank decompositions, introducing computational overhead. Also, such methods
have not been explored for continual learning. We propose LANCE (Low-rank
Activation Compression), a framework that performs one-shot higher-order
Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for
activation projection. This eliminates repeated decompositions, reducing both
memory and computation. Moreover, fixed low-rank subspaces further enable
on-device continual learning by allocating tasks to orthogonal subspaces
without storing large task-specific matrices. Experiments show that LANCE
reduces activation storage up to 250$\times$ while maintaining accuracy
comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets,
Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split
CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive
with orthogonal gradient projection methods at a fraction of the memory cost.
These results position LANCE as a practical and scalable solution for efficient
fine-tuning and continual learning on edge devices.

</details>


### [88] [PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters](https://arxiv.org/abs/2509.21619)
*Krishu K Thapa,Reet Barik,Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: Dynamic switching from full parameter training to Low-Rank Adaptation (LoRA) during training reduces trainable parameters to 10% while maintaining accuracy, improving throughput 3x and reducing training time 1.5x.


<details>
  <summary>Details</summary>
Motivation: Training large models is resource-intensive, and most learning occurs in early stages with weight changes stabilizing later, enabling low-rank approximation.

Method: Identify partial convergence states and dynamically switch from full training to LoRA, using hyperparameters to determine switching point and assign layer-specific ranks based on convergence level.

Result: Reduced trainable parameters to 10% of original size, 3x throughput improvement, 1.5x reduction in average training time per epoch, 20% GPU memory reduction while preserving accuracy.

Conclusion: Dynamic switching to LoRA during training effectively reduces resource requirements while maintaining model performance.

Abstract: Training large models ranging from millions to billions of parameters is
highly resource-intensive, requiring significant time, compute, and memory. It
is observed that most of the learning (higher change in weights) takes place in
the earlier stage of the training loop. These changes stabilize as training
continues, enabling them to be captured by matrices of a low intrinsic rank.
Therefore, we propose an approach to identify such states of partial
convergence and dynamically switch from full parameter training to Low-Rank
Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that
leverages user-defined hyperparameters to determine the switching point and
assign a rank specific to each module layer based on its level of convergence.
Experimental results show that this approach preserves model accuracy while
reducing the number of trainable parameters to 10% of its original size,
resulting in a 3x improvement in throughput, and a 1.5x reduction in average
training time per epoch while also reducing GPU memory consumption by 20%

</details>


### [89] [Shoot from the HIP: Hessian Interatomic Potentials without derivatives](https://arxiv.org/abs/2509.21624)
*Andreas Burger,Luca Thiede,Nikolaj Rønne,Varinia Bernales,Nandita Vijaykumar,Tejs Vegge,Arghya Bhowmik,Alan Aspuru-Guzik*

Main category: cs.LG

TL;DR: HIP (Hessian from Irreducible Representations) enables direct prediction of molecular Hessians using SE(3)-equivariant graph neural networks, making Hessian calculations 10-100x faster and more accurate than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Molecular Hessians are computationally expensive to calculate and scale poorly with system size, limiting their use in fundamental computational chemistry tasks like transition state search and vibrational analysis.

Method: Construct SE(3)-equivariant, symmetric Hessians directly from irreducible representation features (up to l=2) computed during message passing in graph neural networks, avoiding automatic differentiation or finite differences.

Result: HIP Hessians are 1-2 orders of magnitude faster, more accurate, more memory efficient, easier to train, and scale better with system size compared to traditional quantum mechanical methods and neural networks.

Conclusion: HIP enables efficient and accurate Hessian prediction across various downstream tasks including transition state search, geometry optimization, zero-point energy corrections, and vibrational analysis, with open-source code available for further development.

Abstract: Fundamental tasks in computational chemistry, from transition state search to
vibrational analysis, rely on molecular Hessians, which are the second
derivatives of the potential energy. Yet, Hessians are computationally
expensive to calculate and scale poorly with system size, with both quantum
mechanical methods and neural networks. In this work, we demonstrate that
Hessians can be predicted directly from a deep learning model, without relying
on automatic differentiation or finite differences. We observe that one can
construct SE(3)-equivariant, symmetric Hessians from irreducible
representations (irrep) features up to degree $l$=2 computed during message
passing in graph neural networks. This makes HIP Hessians one to two orders of
magnitude faster, more accurate, more memory efficient, easier to train, and
enables more favorable scaling with system size. We validate our predictions
across a wide range of downstream tasks, demonstrating consistently superior
performance for transition state search, accelerated geometry optimization,
zero-point energy corrections, and vibrational analysis benchmarks. We
open-source the HIP codebase and model weights to enable further development of
the direct prediction of Hessians at https://github.com/BurgerAndreas/hip

</details>


### [90] [Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2509.21637)
*Feng Yu,Jia Hu,Geyong Min*

Main category: cs.LG

TL;DR: BHRA is a parameter-efficient fine-tuning method that uses blockwise Hadamard-style modulation to achieve localized rank amplification while maintaining the same parameter footprint as other PEFT methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing PEFT methods like LoRA (constrained by nominal rank) and HiRA (global modulation that couples updates to global energy patterns), enabling more effective fine-tuning with limited parameters.

Method: Partitions weight matrices into blocks and applies HiRA-style multiplicative modulation independently within each block, allowing localized rank amplification while preserving the PEFT parameter budget.

Result: BHRA consistently outperforms strong PEFT baselines across eight commonsense reasoning tasks and two arithmetic benchmarks using Llama-3.2 1B/3B, Mistral-7B, and Gemma-2 9B models under matched parameter budgets.

Conclusion: Blockwise design maintains rich spectra across rank budgets and mitigates collapse induced by global modulation, making BHRA an effective parameter-efficient fine-tuning approach.

Abstract: Parameter-efficient fine-tuning (PEFT) methods must be resource-efficient yet
handle heterogeneous reasoning transformations, and classical low-rank
adaptation (LoRA) is constrained by the nominal rank $r$. Hadamard-style
extensions like HiRA raise the nominal rank but couple every update to the
global energy pattern of the frozen weight matrix, while ABBA trades this
inductive bias for fully learned dense intermediates. To address the limitation
of global modulation, we propose Block Hadamard high-Rank Adaptation (BHRA),
which partitions each weight matrix and applies HiRA-style multiplicative
modulation independently within every block, preserving the PEFT parameter
footprint while unlocking localized rank amplification. Our empirical analyses
reveal that this blockwise design maintains rich spectra across rank budgets,
mitigating the collapse induced by global modulation. Across eight commonsense
reasoning tasks and two arithmetic benchmarks with Llama-3.2 1B/3B, Mistral-7B,
and Gemma-2 9B, BHRA consistently surpasses strong PEFT baselines under matched
parameter budgets.

</details>


### [91] [Understanding and Enhancing Mask-Based Pretraining towards Universal Representations](https://arxiv.org/abs/2509.21650)
*Mingze Dong,Leda Wang,Yuval Kluger*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework for understanding mask-based pretraining through high-dimensional linear regression analysis, and proposes R²MAE - a simple random masking scheme that outperforms fixed masking ratios across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Despite the empirical success of mask-based pretraining in language, vision, and biology, its theoretical foundations and limitations in learning data representations remain unclear.

Method: The authors characterize mask-based pretraining using test risk in high-dimensional minimum-norm linear regression, then propose R²MAE - Randomly Random Mask AutoEncoding that enforces multi-scale feature capture through variable masking ratios.

Result: The theoretical framework is validated across diverse neural architectures (MLPs, CNNs, Transformers) on vision and language tasks. R²MAE consistently outperforms standard and more complex masking schemes in vision, language, DNA sequence, and single-cell models.

Conclusion: Mask-based pretraining behavior can be theoretically characterized through linear regression analysis, and the proposed R²MAE scheme provides a simple yet effective alternative that captures multi-scale features and improves state-of-the-art models.

Abstract: Mask-based pretraining has become a cornerstone of modern large-scale models
across language, vision, and recently biology. Despite its empirical success,
its role and limits in learning data representations have been unclear. In this
work, we show that the behavior of mask-based pretraining can be directly
characterized by test risk in high-dimensional minimum-norm ("ridge-less")
linear regression, without relying on further model specifications. Further
analysis of linear models uncovers several novel aspects of mask-based
pretraining. The theoretical framework and its implications have been validated
across diverse neural architectures (including MLPs, CNNs, and Transformers)
applied to both vision and language tasks. Guided by our theory, we propose an
embarrassingly simple yet overlooked pretraining scheme named Randomly Random
Mask AutoEncoding (R$^2$MAE), which enforces capturing multi-scale features
from data and is able to outperform optimal fixed mask ratio settings in our
linear model framework. We implement R$^2$MAE in vision, language, DNA
sequence, and single-cell models, where it consistently outperforms standard
and more complicated masking schemes, leading to improvements for
state-of-the-art models. Our code is available at:
https://github.com/MingzeDong/r2mae

</details>


### [92] [Limitations on Safe, Trusted, Artificial General Intelligence](https://arxiv.org/abs/2509.21654)
*Rina Panigrahy,Vatsal Sharan*

Main category: cs.LG

TL;DR: The paper proves a fundamental incompatibility between mathematically defined safety, trust, and AGI - a safe and trusted AI system cannot be an AGI system.


<details>
  <summary>Details</summary>
Motivation: To establish strict mathematical definitions of safety, trust, and AGI, and explore their fundamental relationships in AI systems.

Method: Proposed formal mathematical definitions: safety as never making false claims, trust as assuming safety, and AGI as always matching/exceeding human capability. Used proofs drawing parallels to Gödel's incompleteness theorems and Turing's halting problem.

Result: Demonstrated that for the formal definitions, a safe and trusted AI system cannot be an AGI system - there exist tasks solvable by humans but not by such systems.

Conclusion: There is a fundamental incompatibility between mathematically defined safety, trust, and AGI, though real-world systems may use practical interpretations that avoid this limitation.

Abstract: Safety, trust and Artificial General Intelligence (AGI) are aspirational
goals in artificial intelligence (AI) systems, and there are several informal
interpretations of these notions. In this paper, we propose strict,
mathematical definitions of safety, trust, and AGI, and demonstrate a
fundamental incompatibility between them. We define safety of a system as the
property that it never makes any false claims, trust as the assumption that the
system is safe, and AGI as the property of an AI system always matching or
exceeding human capability. Our core finding is that -- for our formal
definitions of these notions -- a safe and trusted AI system cannot be an AGI
system: for such a safe, trusted system there are task instances which are
easily and provably solvable by a human but not by the system. We note that we
consider strict mathematical definitions of safety and trust, and it is
possible for real-world deployments to instead rely on alternate, practical
interpretations of these notions. We show our results for program verification,
planning, and graph reachability. Our proofs draw parallels to G\"odel's
incompleteness theorems and Turing's proof of the undecidability of the halting
problem, and can be regarded as interpretations of G\"odel's and Turing's
results.

</details>


### [93] [DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2509.21655)
*Yinuo Ren,Wenhao Gao,Lexing Ying,Grant M. Rotskoff,Jiequn Han*

Main category: cs.LG

TL;DR: DriftLite is a training-free particle-based method for inference-time adaptation of diffusion models that provides optimal stability control without retraining, outperforming guidance-based and sequential Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inference-time adaptation of diffusion models have limitations: guidance-based methods introduce bias, while particle-based corrections suffer from weight degeneracy and high computational costs.

Method: DriftLite exploits a degree of freedom in the Fokker-Planck equation between drift and particle potential, yielding two practical instantiations: Variance-Controlling Guidance (VCG) and Energy-Controlling Guidance (ECG) for approximating optimal drift with minimal overhead.

Result: Across Gaussian mixture models, particle systems, and protein-ligand co-folding problems, DriftLite consistently reduces variance and improves sample quality compared to pure guidance and sequential Monte Carlo baselines.

Conclusion: DriftLite provides a principled, efficient route toward scalable inference-time adaptation of diffusion models with provable optimal stability control.

Abstract: We study inference-time scaling for diffusion models, where the goal is to
adapt a pre-trained model to new target distributions without retraining.
Existing guidance-based methods are simple but introduce bias, while
particle-based corrections suffer from weight degeneracy and high computational
cost. We introduce DriftLite, a lightweight, training-free particle-based
approach that steers the inference dynamics on the fly with provably optimal
stability control. DriftLite exploits a previously unexplored degree of freedom
in the Fokker-Planck equation between the drift and particle potential, and
yields two practical instantiations: Variance- and Energy-Controlling Guidance
(VCG/ECG) for approximating the optimal drift with minimal overhead. Across
Gaussian mixture models, particle systems, and large-scale protein-ligand
co-folding problems, DriftLite consistently reduces variance and improves
sample quality over pure guidance and sequential Monte Carlo baselines. These
results highlight a principled, efficient route toward scalable inference-time
adaptation of diffusion models.

</details>


### [94] [Differentiable Structure Learning for General Binary Data](https://arxiv.org/abs/2509.21658)
*Chang Deng,Bryon Aragam*

Main category: cs.LG

TL;DR: A differentiable structure learning framework for discrete data that captures arbitrary dependencies, overcoming limitations of existing methods that assume specific structural equation models and linear effects.


<details>
  <summary>Details</summary>
Motivation: Existing methods for differentiable structure learning in discrete data make unrealistic assumptions about data-generating processes and ignore complex dependence structures, limiting their general applicability.

Method: Proposed a differentiable structure learning framework that formulates learning as a single differentiable optimization task without unrealistic simplifications, capable of capturing arbitrary dependencies among discrete variables.

Result: The framework can characterize the complete set of compatible parameters and structures, establishes identifiability up to Markov equivalence under mild assumptions, and empirically demonstrates effective capture of complex relationships in discrete data.

Conclusion: The proposed approach successfully addresses limitations of previous methods by providing a general differentiable framework for learning complex dependency structures in discrete data without restrictive assumptions.

Abstract: Existing methods for differentiable structure learning in discrete data
typically assume that the data are generated from specific structural equation
models. However, these assumptions may not align with the true data-generating
process, which limits the general applicability of such methods. Furthermore,
current approaches often ignore the complex dependence structure inherent in
discrete data and consider only linear effects. We propose a differentiable
structure learning framework that is capable of capturing arbitrary
dependencies among discrete variables. We show that although general discrete
models are unidentifiable from purely observational data, it is possible to
characterize the complete set of compatible parameters and structures.
Additionally, we establish identifiability up to Markov equivalence under mild
assumptions. We formulate the learning problem as a single differentiable
optimization task in the most general form, thereby avoiding the unrealistic
simplifications adopted by previous methods. Empirical results demonstrate that
our approach effectively captures complex relationships in discrete data.

</details>


### [95] [RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion](https://arxiv.org/abs/2509.21659)
*Siming Shan,Min Zhu,Youzuo Lin,Lu Lu*

Main category: cs.LG

TL;DR: RED-DiffEq integrates physics-driven inversion with data-driven learning using pretrained diffusion models as regularization for PDE-governed inverse problems, demonstrating improved accuracy and generalization in full waveform inversion.


<details>
  <summary>Details</summary>
Motivation: PDE-governed inverse problems face challenges with nonlinearity, ill-posedness, and noise sensitivity, requiring robust computational frameworks.

Method: Combines physics-driven inversion with data-driven learning by leveraging pretrained diffusion models as a regularization mechanism for PDE inverse problems.

Result: Enhanced accuracy and robustness in full waveform inversion compared to conventional methods, with strong generalization to complex velocity models not seen during training.

Conclusion: RED-DiffEq provides an effective framework that can be directly applied to diverse PDE-governed inverse problems, offering improved performance and generalization capabilities.

Abstract: Partial differential equation (PDE)-governed inverse problems are fundamental
across various scientific and engineering applications; yet they face
significant challenges due to nonlinearity, ill-posedness, and sensitivity to
noise. Here, we introduce a new computational framework, RED-DiffEq, by
integrating physics-driven inversion and data-driven learning. RED-DiffEq
leverages pretrained diffusion models as a regularization mechanism for
PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform
inversion problem in geophysics, a challenging seismic imaging technique that
seeks to reconstruct high-resolution subsurface velocity models from seismic
measurement data. Our method shows enhanced accuracy and robustness compared to
conventional methods. Additionally, it exhibits strong generalization ability
to more complex velocity models that the diffusion model is not trained on. Our
framework can also be directly applied to diverse PDE-governed inverse
problems.

</details>


### [96] [A Systematic Review of Conformal Inference Procedures for Treatment Effect Estimation: Methods and Challenges](https://arxiv.org/abs/2509.21660)
*Pascal Memmesheimer,Vincent Heuveline,Jürgen Hesser*

Main category: cs.LG

TL;DR: This paper provides a systematic review of conformal prediction methods for treatment effect estimation, analyzing 11 key papers to identify state-of-the-art approaches and propose future research directions.


<details>
  <summary>Details</summary>
Motivation: Treatment effect estimation is crucial for decision-making in high-stakes fields, but existing machine learning models lack uncertainty quantification. Conformal prediction offers finite-sample coverage guarantees with minimal assumptions, making it valuable for improving decision-making.

Method: Conducted a systematic review through a filtering process to select and analyze eleven key papers on conformal prediction methods for treatment effect estimation.

Result: Identified and described current state-of-the-art conformal prediction methods that provide frequentist, finite-sample coverage guarantees while handling distribution shifts and being computationally inexpensive.

Conclusion: Conformal prediction methods show significant potential for treatment effect estimation, and the review proposes directions for future research in this area.

Abstract: Treatment effect estimation is essential for informed decision-making in many
fields such as healthcare, economics, and public policy. While flexible machine
learning models have been widely applied for estimating heterogeneous treatment
effects, quantifying the inherent uncertainty of their point predictions
remains an issue. Recent advancements in conformal prediction address this
limitation by allowing for inexpensive computation, as well as distribution
shifts, while still providing frequentist, finite-sample coverage guarantees
under minimal assumptions for any point-predictor model. This advancement holds
significant potential for improving decision-making in especially high-stakes
environments. In this work, we perform a systematic review regarding conformal
prediction methods for treatment effect estimation and provide for both the
necessary theoretical background. Through a systematic filtering process, we
select and analyze eleven key papers, identifying and describing current
state-of-the-art methods in this area. Based on our findings, we propose
directions for future research.

</details>


### [97] [MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning](https://arxiv.org/abs/2509.21662)
*Afrina Tabassum,Bin Guo,Xiyao Ma,Hoda Eldardiry,Ismini Lourentzou*

Main category: cs.LG

TL;DR: MMPlanner is a zero-shot multimodal procedural planning framework that uses Object State Reasoning Chain-of-Thought prompting to generate text-image instructions while maintaining object-state consistency across modalities.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for multimodal procedural planning often lack proper visual object-state alignment and systematic evaluation methods, leading to inconsistent plans across text and images.

Method: Proposes MMPlanner with Object State Reasoning Chain-of-Thought (OSR-CoT) prompting to explicitly model object-state transitions, and introduces LLM-as-a-judge protocols for evaluation plus visual step-reordering task for temporal coherence.

Result: Achieves state-of-the-art performance on RECIPEPLAN and WIKIPLAN datasets: +6.8% improvement in textual planning, +11.9% in cross-modal alignment, and +26.7% in visual step ordering.

Conclusion: MMPlanner effectively addresses multimodal procedural planning challenges through explicit object-state reasoning and comprehensive evaluation protocols, demonstrating significant improvements across all metrics.

Abstract: Multimodal Procedural Planning (MPP) aims to generate step-by-step
instructions that combine text and images, with the central challenge of
preserving object-state consistency across modalities while producing
informative plans. Existing approaches often leverage large language models
(LLMs) to refine textual steps; however, visual object-state alignment and
systematic evaluation are largely underexplored. We present MMPlanner, a
zero-shot MPP framework that introduces Object State Reasoning Chain-of-Thought
(OSR-CoT) prompting to explicitly model object-state transitions and generate
accurate multimodal plans. To assess plan quality, we design LLM-as-a-judge
protocols for planning accuracy and cross-modal alignment, and further propose
a visual step-reordering task to measure temporal coherence. Experiments on
RECIPEPLAN and WIKIPLAN show that MMPlanner achieves state-of-the-art
performance, improving textual planning by +6.8%, cross-modal alignment by
+11.9%, and visual step ordering by +26.7%

</details>


### [98] [Logic of Hypotheses: from Zero to Full Knowledge in Neurosymbolic Integration](https://arxiv.org/abs/2509.21663)
*Davide Bizzaro,Alessandro Daniele*

Main category: cs.LG

TL;DR: Logic of Hypotheses (LoH) is a novel neurosymbolic framework that unifies rule injection and rule learning through a choice operator with learnable parameters, enabling flexible integration of symbolic priors and data-driven learning while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between two main approaches in neurosymbolic integration: methods that inject hand-crafted rules into neural models and methods that induce symbolic rules from data, creating a unified framework.

Method: Extends propositional logic with a choice operator that has learnable parameters and selects subformulas from a pool of options. Uses fuzzy logic to compile formulas into differentiable computational graphs, enabling learning via backpropagation. Employs Goedel fuzzy logic and the Goedel trick for discretization without performance loss.

Result: Strong experimental results on tabular data and the Visual Tic-Tac-Toe neurosymbolic task, while producing interpretable decision rules. The framework subsumes existing neurosymbolic models and allows arbitrary degrees of knowledge specification.

Conclusion: LoH provides a unified neurosymbolic framework that successfully integrates symbolic reasoning with neural learning, enabling flexible knowledge specification while maintaining interpretability and achieving competitive performance on benchmark tasks.

Abstract: Neurosymbolic integration (NeSy) blends neural-network learning with symbolic
reasoning. The field can be split between methods injecting hand-crafted rules
into neural models, and methods inducing symbolic rules from data. We introduce
Logic of Hypotheses (LoH), a novel language that unifies these strands,
enabling the flexible integration of data-driven rule learning with symbolic
priors and expert knowledge. LoH extends propositional logic syntax with a
choice operator, which has learnable parameters and selects a subformula from a
pool of options. Using fuzzy logic, formulas in LoH can be directly compiled
into a differentiable computational graph, so the optimal choices can be
learned via backpropagation. This framework subsumes some existing NeSy models,
while adding the possibility of arbitrary degrees of knowledge specification.
Moreover, the use of Goedel fuzzy logic and the recently developed Goedel trick
yields models that can be discretized to hard Boolean-valued functions without
any loss in performance. We provide experimental analysis on such models,
showing strong results on tabular data and on the Visual Tic-Tac-Toe NeSy task,
while producing interpretable decision rules.

</details>


### [99] [DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks](https://arxiv.org/abs/2509.21666)
*Joshua Salim,Jordan Yu,Xilei Zhao*

Main category: cs.LG

TL;DR: Proposes DIM regularization method that enforces domain-informed monotonicity in deep neural networks to prevent overfitting and improve generalization by penalizing violations of expected monotonic relationships.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often overfit by memorizing training data and noise rather than learning generalizable patterns, due to their complex structures and large parameter counts.

Method: Enforces monotonicity by penalizing violations relative to a linear baseline, using a mathematical framework that establishes linear references, measures monotonicity deviations, and integrates these into the training objective.

Result: Experiments on real-world ridesourcing data and synthetic datasets show that even modest monotonicity constraints consistently enhance model performance across various neural network architectures.

Conclusion: DIM regularization effectively improves deep neural network predictive performance by applying domain-informed monotonicity constraints to regularize model behavior and mitigate overfitting.

Abstract: While deep learning models excel at predictive tasks, they often overfit due
to their complex structure and large number of parameters, causing them to
memorize training data, including noise, rather than learn patterns that
generalize to new data. To tackle this challenge, this paper proposes a new
regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep
Neural Networks (DIM), which maintains domain-informed monotonic relationships
in complex deep learning models to further improve predictions. Specifically,
our method enforces monotonicity by penalizing violations relative to a linear
baseline, effectively encouraging the model to follow expected trends while
preserving its predictive power. We formalize this approach through a
comprehensive mathematical framework that establishes a linear reference,
measures deviations from monotonic behavior, and integrates these measurements
into the training objective. We test and validate the proposed methodology
using a real-world ridesourcing dataset from Chicago and a synthetically
created dataset. Experiments across various neural network architectures show
that even modest monotonicity constraints consistently enhance model
performance. DIM enhances the predictive performance of deep neural networks by
applying domain-informed monotonicity constraints to regularize model behavior
and mitigate overfitting

</details>


### [100] [Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2509.21671)
*Andrii Zahorodnii,Christopher Wang,Bennett Stankovits,Charikleia Moraitaki,Geeling Chau,Andrei Barbu,Boris Katz,Ila R Fiete*

Main category: cs.LG

TL;DR: Neuroprobe is a standardized benchmark suite for evaluating iEEG foundation models, built on the BrainTreebank dataset. It provides decoding tasks for studying multi-modal language processing and enables comparison of competing architectures.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation frameworks for intracranial EEG (iEEG) recordings creates a gap in the field, making it difficult to discriminate between competing modeling approaches for brain-computer interfaces and neurological treatments.

Method: Neuroprobe is built on the BrainTreebank dataset containing 40 hours of iEEG recordings from 10 human subjects during movie viewing. It provides decoding tasks that measure feature decodability across time and electrode locations to study language processing.

Result: The benchmark revealed that linear baselines are surprisingly strong, beating frontier foundation models on many tasks. It enabled visualization of information flow from superior temporal gyrus to prefrontal cortex and progression from simple auditory to complex language features.

Conclusion: Neuroprobe serves as both a neuroscience insight tool and rigorous evaluation framework for neural foundation models, with public code and leaderboard to accelerate progress in iEEG research.

Abstract: High-resolution neural datasets enable foundation models for the next
generation of brain-computer interfaces and neurological treatments. The
community requires rigorous benchmarks to discriminate between competing
modeling approaches, yet no standardized evaluation frameworks exist for
intracranial EEG (iEEG) recordings. To address this gap, we present Neuroprobe:
a suite of decoding tasks for studying multi-modal language processing in the
brain. Unlike scalp EEG, intracranial EEG requires invasive surgery to implant
electrodes that record neural activity directly from the brain with minimal
signal distortion. Neuroprobe is built on the BrainTreebank dataset, which
consists of 40 hours of iEEG recordings from 10 human subjects performing a
naturalistic movie viewing task. Neuroprobe serves two critical functions.
First, it is a mine from which neuroscience insights can be drawn. Its high
temporal and spatial resolution allows researchers to systematically determine
when and where computations for each aspect of language processing occur in the
brain by measuring the decodability of each feature across time and all
electrode locations. Using Neuroprobe, we visualize how information flows from
the superior temporal gyrus to the prefrontal cortex, and the progression from
simple auditory features to more complex language features in a purely
data-driven manner. Second, as the field moves toward neural foundation models,
Neuroprobe provides a rigorous framework for comparing competing architectures
and training protocols. We found that the linear baseline is surprisingly
strong, beating frontier foundation models on many tasks. Neuroprobe is
designed with computational efficiency and ease of use in mind. We make the
code for Neuroprobe openly available and maintain a public leaderboard, aiming
to enable rapid progress in the field of iEEG foundation models, at
https://neuroprobe.dev/

</details>


### [101] [SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks](https://arxiv.org/abs/2509.21673)
*Junyong Park,Oron Levy,Rebecca Adaimi,Asaf Liberman,Gierad Laput,Abdelkareem Bedri*

Main category: cs.LG

TL;DR: SlotFM is an accelerometer foundation model that uses Time-Frequency Slot Attention to generate multiple embeddings capturing different signal components, enabling better generalization across diverse downstream tasks beyond standard activity recognition.


<details>
  <summary>Details</summary>
Motivation: Most existing foundation models focus only on classifying common daily activities, limiting their applicability to the broader range of tasks that rely on other signal characteristics in wearable accelerometer data.

Method: Uses Time-Frequency Slot Attention (extension of Slot Attention) to process both time and frequency representations, generating multiple small embeddings (slots) that capture different signal components. Includes two loss regularizers for local structure and frequency patterns to improve reconstruction and preserve task-relevant information.

Result: Outperforms existing self-supervised approaches on 13 out of 16 classification and regression downstream tasks, achieving comparable results on the remaining tasks. On average, yields a 4.5% performance gain.

Conclusion: SlotFM demonstrates strong generalization for sensing foundation models, enabling broader applicability across diverse accelerometer-based tasks beyond standard human activity recognition.

Abstract: Wearable accelerometers are used for a wide range of applications, such as
gesture recognition, gait analysis, and sports monitoring. Yet most existing
foundation models focus primarily on classifying common daily activities such
as locomotion and exercise, limiting their applicability to the broader range
of tasks that rely on other signal characteristics. We present SlotFM, an
accelerometer foundation model that generalizes across diverse downstream
tasks. SlotFM uses Time-Frequency Slot Attention, an extension of Slot
Attention that processes both time and frequency representations of the raw
signals. It generates multiple small embeddings (slots), each capturing
different signal components, enabling task-specific heads to focus on the most
relevant parts of the data. We also introduce two loss regularizers that
capture local structure and frequency patterns, which improve reconstruction of
fine-grained details and helps the embeddings preserve task-relevant
information. We evaluate SlotFM on 16 classification and regression downstream
tasks that extend beyond standard human activity recognition. It outperforms
existing self-supervised approaches on 13 of these tasks and achieves
comparable results to the best performing approaches on the remaining tasks. On
average, our method yields a 4.5% performance gain, demonstrating strong
generalization for sensing foundation models.

</details>


### [102] [Scalable Second-order Riemannian Optimization for $K$-means Clustering](https://arxiv.org/abs/2509.21675)
*Peng Xu,Chun-Ying Hou,Xiaohui Chen,Richard Y. Zhang*

Main category: cs.LG

TL;DR: A new smooth unconstrained optimization formulation for K-means clustering using Riemannian manifold structure, solved with second-order cubic-regularized Newton method that achieves faster convergence than first-order methods while maintaining optimal statistical accuracy.


<details>
  <summary>Details</summary>
Motivation: Current relaxation algorithms for K-means clustering struggle to balance constraint feasibility and objective optimality, presenting challenges in computing second-order critical points with rigorous guarantees.

Method: Formulate K-means as smooth unconstrained optimization over a submanifold with Riemannian structure, use second-order cubic-regularized Riemannian Newton algorithm, and factorize the manifold into product manifold to solve Newton subproblems in linear time.

Result: The proposed method converges significantly faster than state-of-the-art first-order nonnegative low-rank factorization methods while achieving similar optimal statistical accuracy.

Conclusion: The Riemannian manifold approach provides an effective framework for solving K-means clustering with improved convergence speed and maintained statistical performance.

Abstract: Clustering is a hard discrete optimization problem. Nonconvex approaches such
as low-rank semidefinite programming (SDP) have recently demonstrated promising
statistical and local algorithmic guarantees for cluster recovery. Due to the
combinatorial structure of the $K$-means clustering problem, current relaxation
algorithms struggle to balance their constraint feasibility and objective
optimality, presenting tremendous challenges in computing the second-order
critical points with rigorous guarantees. In this paper, we provide a new
formulation of the $K$-means problem as a smooth unconstrained optimization
over a submanifold and characterize its Riemannian structures to allow it to be
solved using a second-order cubic-regularized Riemannian Newton algorithm. By
factorizing the $K$-means manifold into a product manifold, we show how each
Newton subproblem can be solved in linear time. Our numerical experiments show
that the proposed method converges significantly faster than the
state-of-the-art first-order nonnegative low-rank factorization method, while
achieving similarly optimal statistical accuracy.

</details>


### [103] [Prophecy: Inferring Formal Properties from Neuron Activations](https://arxiv.org/abs/2509.21677)
*Divya Gopinath,Corina S. Pasareanu,Muhammad Usman*

Main category: cs.LG

TL;DR: Prophecy is a tool that automatically infers formal properties of feed-forward neural networks by extracting rules based on neuron activation patterns that imply desired output behaviors.


<details>
  <summary>Details</summary>
Motivation: To automatically extract formal properties from neural networks that capture the logic encoded in hidden layer neuron activations, enabling various applications like verification, monitoring, and repair.

Method: Extracts rules based on neuron activations (values or on/off status) as preconditions that imply certain output properties, such as predicting a specific class. These rules represent network properties captured in hidden layers.

Result: Demonstrated usage on different types of models and output properties, showing potential applications in formal explanations, compositional verification, run-time monitoring, repair, and large vision-language models.

Conclusion: Prophecy provides an effective approach for automatically inferring formal properties from neural networks, with broad applications in network analysis, verification, and monitoring across various model types including large vision-language models.

Abstract: We present Prophecy, a tool for automatically inferring formal properties of
feed-forward neural networks. Prophecy is based on the observation that a
significant part of the logic of feed-forward networks is captured in the
activation status of the neurons at inner layers. Prophecy works by extracting
rules based on neuron activations (values or on/off statuses) as preconditions
that imply certain desirable output property, e.g., the prediction being a
certain class. These rules represent network properties captured in the hidden
layers that imply the desired output behavior. We present the architecture of
the tool, highlight its features and demonstrate its usage on different types
of models and output properties. We present an overview of its applications,
such as inferring and proving formal explanations of neural networks,
compositional verification, run-time monitoring, repair, and others. We also
show novel results highlighting its potential in the era of large
vision-language models.

</details>


### [104] [SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding](https://arxiv.org/abs/2509.21689)
*Thomas Walton,Darin Tsui,Aryan Musharaf,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: SpecMER is a speculative decoding framework that uses k-mer motifs from multiple sequence alignments to guide protein sequence generation, achieving 24-32% speedup while improving biological plausibility.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for protein engineering suffer from high latency in sequential inference, and existing speculative decoding methods produce biologically implausible sequences due to lack of structural/functional constraints.

Method: Uses k-mer motifs extracted from multiple sequence alignments as biological priors to score candidate sequences in parallel, selecting those most consistent with known biological patterns during speculative decoding.

Result: Achieves 24-32% speedup over standard autoregressive decoding with higher acceptance rates and improved sequence likelihoods while maintaining biological plausibility.

Conclusion: SpecMER successfully combines speculative decoding efficiency with biological guidance through k-mer motifs, enabling faster and more biologically meaningful protein sequence generation.

Abstract: Autoregressive models have transformed protein engineering by enabling the
generation of novel protein sequences beyond those found in nature. However,
their sequential inference introduces significant latency, limiting their
utility in high-throughput protein screening. Speculative decoding accelerates
generation by employing a lightweight draft model to sample tokens, which a
larger target model then verifies and refines. Yet, in protein sequence
generation, draft models are typically agnostic to the structural and
functional constraints of the target protein, leading to biologically
implausible outputs and a shift in the likelihood distribution of generated
sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a
novel framework that incorporates biological, structural, and functional priors
using k-mer motifs extracted from multiple sequence alignments. By scoring
candidate sequences in parallel and selecting those most consistent with known
biological patterns, SpecMER significantly improves sequence plausibility while
retaining the efficiency of speculative decoding. SpecMER achieves 24-32%
speedup over standard autoregressive decoding, along with higher acceptance
rates and improved sequence likelihoods.

</details>


### [105] [Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment](https://arxiv.org/abs/2509.21695)
*Saurabh Kataria,Davood Fattahi,Minxiao Wang,Ran Xiao,Matthew Clark,Timothy Ruchti,Mark Mai,Xiao Hu*

Main category: cs.LG

TL;DR: This paper proposes three orthogonal improvements for PPG-only cardiac arrest prediction systems: time-to-event modeling, patient-identity invariant feature learning using adversarial training, and regression on pseudo-lab values from auxiliary networks.


<details>
  <summary>Details</summary>
Motivation: Current physiological foundation models based on PPG can predict critical events like cardiac arrest, but their powerful representations are not fully leveraged when downstream data/labels are scarce. The paper aims to improve PPG-only CA systems using minimal auxiliary information.

Method: Three main approaches: 1) Time-to-event modeling via regression to event onset or discrete survival modeling; 2) Patient-identity invariant feature learning using adversarial training with p-vector models; 3) Regression on pseudo-lab values generated by pre-trained auxiliary estimator networks for continuous label enrichment.

Result: The proposed methods independently improve 24-hour time-averaged AUC from 0.74 to 0.78-0.80 range, with primary improvements over longer time horizons and minimal degradation near the event.

Conclusion: The paper successfully pushes Early Warning System research by improving cardiac arrest prediction through multi-task formulations and addressing gradient conflicts with PCGrad optimization, enabling better long-term forecasting with scarce data.

Abstract: High-frequency physiological waveform modality offers deep, real-time
insights into patient status. Recently, physiological foundation models based
on Photoplethysmography (PPG), such as PPG-GPT, have been shown to predict
critical events, including Cardiac Arrest (CA). However, their powerful
representation still needs to be leveraged suitably, especially when the
downstream data/label is scarce. We offer three orthogonal improvements to
improve PPG-only CA systems by using minimal auxiliary information. First, we
propose to use time-to-event modeling, either through simple regression to the
event onset time or by pursuing fine-grained discrete survival modeling.
Second, we encourage the model to learn CA-focused features by making them
patient-identity invariant. This is achieved by first training the
largest-scale de-identified biometric identification model, referred to as the
p-vector, and subsequently using it adversarially to deconfound cues, such as
person identity, that may cause overfitting through memorization. Third, we
propose regression on the pseudo-lab values generated by pre-trained auxiliary
estimator networks. This is crucial since true blood lab measurements, such as
lactate, sodium, troponin, and potassium, are collected sparingly. Via
zero-shot prediction, the auxiliary networks can enrich cardiac arrest waveform
labels and generate pseudo-continuous estimates as targets. Our proposals can
independently improve the 24-hour time-averaged AUC from the 0.74 to the
0.78-0.80 range. We primarily improve over longer time horizons with minimal
degradation near the event, thus pushing the Early Warning System research.
Finally, we pursue multi-task formulation and diagnose it with a high gradient
conflict rate among competing losses, which we alleviate via the PCGrad
optimization technique.

</details>


### [106] [Exact Subgraph Isomorphism Network for Predictive Graph Mining](https://arxiv.org/abs/2509.21699)
*Taiga Kojima,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: EIN combines exact subgraph enumeration with neural networks and sparse regularization for graph-level prediction, achieving high performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Building graph-level prediction models with both high discriminative ability and interpretability is challenging. Information in subgraphs is crucial for graph-level tasks.

Method: Exact subgraph enumeration combined with neural networks and sparse regularization. Uses pruning strategy to mitigate computational complexity while maintaining performance.

Result: EIN achieves sufficiently high prediction performance compared to standard graph neural network models. Enables post-hoc analysis through identified important subgraphs.

Conclusion: EIN provides an effective approach for graph-level prediction that balances performance and interpretability through subgraph enumeration and sparse regularization.

Abstract: In the graph-level prediction task (predict a label for a given graph), the
information contained in subgraphs of the input graph plays a key role. In this
paper, we propose Exact subgraph Isomorphism Network (EIN), which combines the
exact subgraph enumeration, neural network, and a sparse regularization. In
general, building a graph-level prediction model achieving high discriminative
ability along with interpretability is still a challenging problem. Our
combination of the subgraph enumeration and neural network contributes to high
discriminative ability about the subgraph structure of the input graph.
Further, the sparse regularization in EIN enables us 1) to derive an effective
pruning strategy that mitigates computational difficulty of the enumeration
while maintaining the prediction performance, and 2) to identify important
subgraphs that contributes to high interpretability. We empirically show that
EIN has sufficiently high prediction performance compared with standard graph
neural network models, and also, we show examples of post-hoc analysis based on
the selected subgraphs.

</details>


### [107] [Downscaling human mobility data based on demographic socioeconomic and commuting characteristics using interpretable machine learning methods](https://arxiv.org/abs/2509.21703)
*Yuqin Jiang,Andrey A. Popov,Tianle Duan,Qingchun Li*

Main category: cs.LG

TL;DR: A machine learning framework for downscaling taxi OD trip flows in NYC using four models, with NN performing best on training/testing data but SVM showing better generalization for downscaling.


<details>
  <summary>Details</summary>
Motivation: Understanding urban human mobility patterns at various spatial levels is essential for social science and improving transportation services.

Method: Used four ML models (Linear Regression, Random Forest, SVM, Neural Networks) to correlate OD taxi trips with demographic, socioeconomic, and commuting characteristics, plus perturbation-based sensitivity analysis for variable importance.

Result: Linear regression failed to capture complex interactions; NN performed best on training/testing datasets; SVM showed best generalization ability in downscaling performance.

Conclusion: The methodology provides both analytical advancement and practical applications to improve transportation services and urban development.

Abstract: Understanding urban human mobility patterns at various spatial levels is
essential for social science. This study presents a machine learning framework
to downscale origin-destination (OD) taxi trips flows in New York City from a
larger spatial unit to a smaller spatial unit. First, correlations between OD
trips and demographic, socioeconomic, and commuting characteristics are
developed using four models: Linear Regression (LR), Random Forest (RF),
Support Vector Machine (SVM), and Neural Networks (NN). Second, a
perturbation-based sensitivity analysis is applied to interpret variable
importance for nonlinear models. The results show that the linear regression
model failed to capture the complex variable interactions. While NN performs
best with the training and testing datasets, SVM shows the best generalization
ability in downscaling performance. The methodology presented in this study
provides both analytical advancement and practical applications to improve
transportation services and urban development.

</details>


### [108] [PQFed: A Privacy-Preserving Quality-Controlled Federated Learning Framework](https://arxiv.org/abs/2509.21704)
*Weiqi Yue,Wenbiao Li,Yuzhou Jiang,Anisa Halimi,Roger French,Erman Ayday*

Main category: cs.LG

TL;DR: PQFed is a privacy-preserving personalized federated learning framework that uses clustering to estimate dataset similarity and enables clients to collaborate with compatible partners, improving model performance even with limited participants.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning challenges global model performance, and traditional methods rely on post-training local adaptation rather than proactive quality control during training setup.

Method: Extracts representative features from client data, applies clustering to estimate inter-client dataset similarity, implements client selection strategy for compatible collaborations, and integrates with existing federated learning algorithms.

Result: PQFed consistently improves target client model performance on CIFAR-10 and MNIST datasets, even with limited participants, and outperforms baseline IFCA algorithm in low-participation scenarios.

Conclusion: PQFed demonstrates scalability and effectiveness in personalized federated learning settings through early-stage quality control and compatible client collaboration strategies.

Abstract: Federated learning enables collaborative model training without sharing raw
data, but data heterogeneity consistently challenges the performance of the
global model. Traditional optimization methods often rely on collaborative
global model training involving all clients, followed by local adaptation to
improve individual performance. In this work, we focus on early-stage quality
control and propose PQFed, a novel privacy-preserving personalized federated
learning framework that designs customized training strategies for each client
prior to the federated training process. PQFed extracts representative features
from each client's raw data and applies clustering techniques to estimate
inter-client dataset similarity. Based on these similarity estimates, the
framework implements a client selection strategy that enables each client to
collaborate with others who have compatible data distributions. We evaluate
PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three
existing federated learning algorithms. Experimental results show that PQFed
consistently improves the target client's model performance, even with a
limited number of participants. We further benchmark PQFed against a baseline
cluster-based algorithm, IFCA, and observe that PQFed also achieves better
performance in low-participation scenarios. These findings highlight PQFed's
scalability and effectiveness in personalized federated learning settings.

</details>


### [109] [A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems](https://arxiv.org/abs/2509.21716)
*Xavier Gonzalez,E. Kelly Buchanan,Hyun Dong Lee,Jerry Weihong Liu,Ke Alexander Wang,David M. Zoltowski,Christopher Ré,Scott W. Linderman*

Main category: cs.LG

TL;DR: The paper presents a unified framework using linear dynamical systems (LDSs) to understand parallel evaluation methods for sequential models, showing how Newton, Picard, and Jacobi iterations emerge as approximate linearizations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of parallelizing seemingly sequential models in machine learning by providing a common theoretical foundation for existing fixed-point methods.

Method: Develops a framework based on linear dynamical systems (LDSs) that shows how different fixed-point iteration schemes (Newton, Picard, Jacobi) arise as approximate linearizations of nonlinear recursions.

Result: The framework unifies diverse parallel evaluation algorithms, reveals shared principles, and clarifies conditions under which specific fixed-point methods are most effective.

Conclusion: The LDS-based framework provides a clearer theoretical foundation for parallelizing sequential models and suggests new opportunities for efficient and scalable computation.

Abstract: Harnessing parallelism in seemingly sequential models is a central challenge
for modern machine learning. Several approaches have been proposed for
evaluating sequential processes in parallel using fixed-point methods, like
Newton, Picard, and Jacobi iterations. In this work, we show that these methods
can be understood within a common framework based on linear dynamical systems
(LDSs), where different iteration schemes arise naturally as approximate
linearizations of a nonlinear recursion. This unifying view highlights shared
principles behind these techniques and clarifies when particular fixed-point
methods are most likely to be effective. By bridging diverse algorithms through
the language of LDSs, our framework provides a clearer theoretical foundation
for parallelizing sequential models and points toward new opportunities for
efficient and scalable computation.

</details>


### [110] [Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems](https://arxiv.org/abs/2509.21725)
*Takuya Kanayama,Yuki Ito,Tomoyuki Tamura,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: Proposes an information-theoretic Bayesian optimization method for bilevel optimization problems with expensive black-box functions at both levels, using unified information gain criteria.


<details>
  <summary>Details</summary>
Motivation: Bilevel optimization has complex nested structure with expensive black-box functions at both levels, making it challenging and less studied compared to other BO extensions like multi-objective optimization.

Method: Information-theoretic approach that considers information gain of both upper- and lower-level optimal solutions and values, with practical lower bound evaluation of information gain.

Result: Empirical demonstration shows effectiveness through several benchmark datasets.

Conclusion: The proposed unified information gain criterion successfully handles bilevel optimization problems with expensive black-box functions.

Abstract: A bilevel optimization problem consists of two optimization problems nested
as an upper- and a lower-level problem, in which the optimality of the
lower-level problem defines a constraint for the upper-level problem. This
paper considers Bayesian optimization (BO) for the case that both the upper-
and lower-levels involve expensive black-box functions. Because of its nested
structure, bilevel optimization has a complex problem definition and, compared
with other standard extensions of BO such as multi-objective or constraint
settings, it has not been widely studied. We propose an information-theoretic
approach that considers the information gain of both the upper- and
lower-optimal solutions and values. This enables us to define a unified
criterion that measures the benefit for both level problems, simultaneously.
Further, we also show a practical lower bound based approach to evaluating the
information gain. We empirically demonstrate the effectiveness of our proposed
method through several benchmark datasets.

</details>


### [111] [Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks](https://arxiv.org/abs/2509.21735)
*Houliang Zhou,Rong Zhou,Yangying Liu,Kanhao Zhao,Li Shen,Brian Y. Chen,Yu Zhang,Lifang He,Alzheimer's Disease Neuroimaging Initiative*

Main category: cs.LG

TL;DR: An interpretable spatio-temporal graph neural network framework using dual Stochastic Differential Equations to predict Alzheimer's disease progression from irregularly-sampled longitudinal fMRI data, identifying key brain circuit abnormalities and novel biomarkers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying objective neuroimaging biomarkers for Alzheimer's disease progression, particularly the complex spatio-temporal dysfunctions in brain networks that existing methods often overlook.

Method: Developed an interpretable spatio-temporal graph neural network framework leveraging dual Stochastic Differential Equations to model irregularly-sampled longitudinal fMRI data, validated on OASIS-3 and ADNI cohorts.

Result: Effectively learned sparse regional and connective importance probabilities, identifying key brain abnormalities in parahippocampal cortex, prefrontal cortex, parietal lobule, and disruptions in ventral attention, dorsal attention, and default mode networks that strongly correlate with AD clinical symptoms.

Conclusion: The framework demonstrates potential for early, individualized prediction of AD progression using spatio-temporal graph-based learning, revealing both established and novel neural systems-level and sex-specific biomarkers for understanding AD neurobiological mechanisms.

Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease
(AD) progression is crucial for timely intervention. However, this task remains
challenging due to the complex dysfunctions in the spatio-temporal
characteristics of underlying brain networks, which are often overlooked by
existing methods. To address these limitations, we develop an interpretable
spatio-temporal graph neural network framework to predict future AD
progression, leveraging dual Stochastic Differential Equations (SDEs) to model
the irregularly-sampled longitudinal functional magnetic resonance imaging
(fMRI) data. We validate our approach on two independent cohorts, including the
Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease
Neuroimaging Initiative (ADNI). Our framework effectively learns sparse
regional and connective importance probabilities, enabling the identification
of key brain circuit abnormalities associated with disease progression.
Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal
lobule as salient regions, with significant disruptions in the ventral
attention, dorsal attention, and default mode networks. These abnormalities
correlate strongly with longitudinal AD-related clinical symptoms. Moreover,
our interpretability strategy reveals both established and novel neural
systems-level and sex-specific biomarkers, offering new insights into the
neurobiological mechanisms underlying AD progression. Our findings highlight
the potential of spatio-temporal graph-based learning for early, individualized
prediction of AD progression, even in the context of irregularly-sampled
longitudinal imaging data.

</details>


### [112] [POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization](https://arxiv.org/abs/2509.21737)
*Ziqing Wang,Yibo Wen,William Pattie,Xiao Luo,Weimin Wu,Jerry Yao-Chieh Hu,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: POLO is a novel LLM-based method for lead optimization that uses multi-turn trajectory learning with preference-guided policy optimization to achieve superior sample efficiency in drug discovery.


<details>
  <summary>Details</summary>
Motivation: Traditional lead optimization methods struggle with sample efficiency, and existing LLM approaches fail to leverage iterative optimization trajectories, treating each step independently.

Method: POLO introduces Preference-Guided Policy Optimization (PGPO) with dual-level learning: trajectory-level optimization reinforces successful strategies, and turn-level preference learning provides dense comparative feedback by ranking intermediate molecules.

Result: POLO achieves 84% average success rate on single-property tasks (2.3x better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations.

Conclusion: POLO significantly advances sample-efficient molecular optimization by fully exploiting costly oracle calls through trajectory-based learning.

Abstract: Lead optimization in drug discovery requires efficiently navigating vast
chemical space through iterative cycles to enhance molecular properties while
preserving structural similarity to the original lead compound. Despite recent
advances, traditional optimization methods struggle with sample
efficiency-achieving good optimization performance with limited oracle
evaluations. Large Language Models (LLMs) provide a promising approach through
their in-context learning and instruction following capabilities, which align
naturally with these iterative processes. However, existing LLM-based methods
fail to leverage this strength, treating each optimization step independently.
To address this, we present POLO (Preference-guided multi-turn Optimization for
Lead Optimization), which enables LLMs to learn from complete optimization
trajectories rather than isolated steps. At its core, POLO introduces
Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning
algorithm that extracts learning signals at two complementary levels:
trajectory-level optimization reinforces successful strategies, while
turn-level preference learning provides dense comparative feedback by ranking
intermediate molecules within each trajectory. Through this dual-level learning
from intermediate evaluation, POLO achieves superior sample efficiency by fully
exploiting each costly oracle call. Extensive experiments demonstrate that POLO
achieves 84% average success rate on single-property tasks (2.3x better than
baselines) and 50% on multi-property tasks using only 500 oracle evaluations,
significantly advancing the state-of-the-art in sample-efficient molecular
optimization.

</details>


### [113] [Brain PathoGraph Learning](https://arxiv.org/abs/2509.21742)
*Ciyuan Peng,Nguyen Linh Dan Le,Shan Jin,Dexuan Ding,Shuo Yu,Feng Xia*

Main category: cs.LG

TL;DR: BrainPoG is a lightweight brain graph learning model that uses pathological pattern filtering and feature distillation to efficiently learn disease-related knowledge while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing brain graph learning methods struggle to selectively learn disease-related knowledge, leading to heavy parameters and computational costs that limit practical clinical applications.

Method: BrainPoG first uses a filter to extract pathological patterns (highly disease-relevant subgraphs) for graph pruning and lesion localization, constructing a PathoGraph. Then a pathological feature distillation module reduces disease-irrelevant noise and enhances pathological features.

Result: Extensive experiments on four benchmark datasets show BrainPoG exhibits superiority in both model performance and computational efficiency across various brain disease detection tasks.

Conclusion: BrainPoG enables efficient brain graph learning by exclusively learning informative disease-related knowledge while avoiding less relevant information, making it practical for real-world clinical applications.

Abstract: Brain graph learning has demonstrated significant achievements in the fields
of neuroscience and artificial intelligence. However, existing methods struggle
to selectively learn disease-related knowledge, leading to heavy parameters and
computational costs. This challenge diminishes their efficiency, as well as
limits their practicality for real-world clinical applications. To this end, we
propose a lightweight Brain PathoGraph Learning (BrainPoG) model that enables
efficient brain graph learning by pathological pattern filtering and
pathological feature distillation. Specifically, BrainPoG first contains a
filter to extract the pathological pattern formulated by highly
disease-relevant subgraphs, achieving graph pruning and lesion localization. A
PathoGraph is therefore constructed by dropping less disease-relevant subgraphs
from the whole brain graph. Afterwards, a pathological feature distillation
module is designed to reduce disease-irrelevant noise features and enhance
pathological features of each node in the PathoGraph. BrainPoG can exclusively
learn informative disease-related knowledge while avoiding less relevant
information, achieving efficient brain graph learning. Extensive experiments on
four benchmark datasets demonstrate that BrainPoG exhibits superiority in both
model performance and computational efficiency across various brain disease
detection tasks.

</details>


### [114] [HyperCore: Coreset Selection under Noise via Hypersphere Models](https://arxiv.org/abs/2509.21746)
*Brian B. Moser,Arundhati S. Shanbhag,Tobias C. Nauen,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: HyperCore is a robust coreset selection framework that uses hypersphere models and Youden's J statistic to automatically prune noisy data without requiring fixed pruning ratios or hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods ignore annotation errors and require fixed pruning ratios, making them impractical for real-world noisy environments.

Method: Leverages lightweight hypersphere models per class to embed in-class samples close to hypersphere centers while segregating out-of-class samples based on distance. Uses Youden's J statistic to adaptively select pruning thresholds.

Result: Consistently surpasses state-of-the-art coreset selection methods, especially under noisy and low-data regimes. Effectively discards mislabeled and ambiguous points.

Conclusion: HyperCore yields compact yet highly informative subsets suitable for scalable and noise-free learning in practical applications.

Abstract: The goal of coreset selection methods is to identify representative subsets
of datasets for efficient model training. Yet, existing methods often ignore
the possibility of annotation errors and require fixed pruning ratios, making
them impractical in real-world settings. We present HyperCore, a robust and
adaptive coreset selection framework designed explicitly for noisy
environments. HyperCore leverages lightweight hypersphere models learned per
class, embedding in-class samples close to a hypersphere center while naturally
segregating out-of-class samples based on their distance. By using Youden's J
statistic, HyperCore can adaptively select pruning thresholds, enabling
automatic, noise-aware data pruning without hyperparameter tuning. Our
experiments reveal that HyperCore consistently surpasses state-of-the-art
coreset selection methods, especially under noisy and low-data regimes.
HyperCore effectively discards mislabeled and ambiguous points, yielding
compact yet highly informative subsets suitable for scalable and noise-free
learning.

</details>


### [115] [SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection](https://arxiv.org/abs/2509.21748)
*Brian B. Moser,Tobias C. Nauen,Arundhati S. Shanbhag,Federico Raue,Stanislav Frolov,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: SubZeroCore is a training-free coreset selection method that combines submodular coverage and density objectives to identify representative data subsets without requiring expensive training signals.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods paradoxically require expensive training-based signals computed on the entire dataset before pruning, which undermines their efficiency purpose by training on samples they aim to avoid.

Method: Integrates submodular coverage and density into a unified objective with a sampling strategy based on a closed-form solution that optimally balances these objectives using a single hyperparameter to control coverage for local density measures.

Result: Matches training-based baselines and significantly outperforms them at high pruning rates while dramatically reducing computational overhead. Also shows superior robustness to label noise.

Conclusion: SubZeroCore provides practical effectiveness and scalability for real-world scenarios through its training-free approach to coreset selection.

Abstract: The goal of coreset selection is to identify representative subsets of
datasets for efficient model training. Yet, existing approaches paradoxically
require expensive training-based signals, e.g., gradients, decision boundary
estimates or forgetting counts, computed over the entire dataset prior to
pruning, which undermines their very purpose by requiring training on samples
they aim to avoid. We introduce SubZeroCore, a novel, training-free coreset
selection method that integrates submodular coverage and density into a single,
unified objective. To achieve this, we introduce a sampling strategy based on a
closed-form solution to optimally balance these objectives, guided by a single
hyperparameter that explicitly controls the desired coverage for local density
measures. Despite no training, extensive evaluations show that SubZeroCore
matches training-based baselines and significantly outperforms them at high
pruning rates, while dramatically reducing computational overhead. SubZeroCore
also demonstrates superior robustness to label noise, highlighting its
practical effectiveness and scalability for real-world scenarios.

</details>


### [116] [Reparameterizing 4DVAR with neural fields](https://arxiv.org/abs/2509.21751)
*Jaemin Oh*

Main category: cs.LG

TL;DR: Neural field-based 4DVAR reformulation enables parallel-in-time optimization for numerical weather prediction by representing spatiotemporal state as a continuous neural network function.


<details>
  <summary>Details</summary>
Motivation: Classical 4DVAR is computationally intensive and difficult to optimize due to time-sequential dependencies.

Method: Reparameterize full spatiotemporal state as continuous neural network function with physics-informed loss constraints.

Result: More stable initial condition estimates without spurious oscillations compared to baseline 4DVAR on 2D Navier-Stokes equations.

Conclusion: Neural reparameterization enables parallel optimization without requiring ground-truth data, broadening applicability to data-limited settings.

Abstract: Four-dimensional variational data assimilation (4DVAR) is a cornerstone of
numerical weather prediction, but its cost function is difficult to optimize
and computationally intensive. We propose a neural field-based reformulation in
which the full spatiotemporal state is represented as a continuous function
parameterized by a neural network. This reparameterization removes the
time-sequential dependency of classical 4DVAR, enabling parallel-in-time
optimization in parameter space. Physical constraints are incorporated directly
through a physics-informed loss, simplifying implementation and reducing
computational cost. We evaluate the method on the two-dimensional
incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a
baseline 4DVAR implementation, the neural reparameterized variants produce more
stable initial condition estimates without spurious oscillations. Notably,
unlike most machine learning-based approaches, our framework does not require
access to ground-truth states or reanalysis data, broadening its applicability
to settings with limited reference information.

</details>


### [117] [Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis](https://arxiv.org/abs/2509.21770)
*Sadman Saumik Islam,Bruna Dalcin Baldasso,Davide Cattaneo,Xianta Jiang,Michelle Ploughman*

Main category: cs.LG

TL;DR: fNIRS with machine learning detected brain activity biomarkers in MS patients during hand dexterity tasks, achieving 75% accuracy for classification and identifying suppressed activity in sensory integration and motor regions.


<details>
  <summary>Details</summary>
Motivation: To detect subtle brain activity biomarkers that explain subjective cognitive fatigue in MS patients during dexterous tasks and provide targets for future brain stimulation treatments.

Method: Used fNIRS to measure brain hemodynamic responses during single and dual hand dexterity tasks in 15 MS patients and 12 controls, analyzed with machine learning (K-Nearest Neighbor classifier) and XAI to identify important brain regions.

Result: KNN achieved 75.0% accuracy for single tasks and 66.7% for dual tasks. Key biomarkers were suppressed activity in supramarginal/angular gyri and precentral gyrus of ipsilateral hemisphere, with deoxygenated hemoglobin being a better predictor than oxygenated hemoglobin.

Conclusion: This nonconventional fNIRS analysis revealed novel brain activity biomarkers that can help develop personalized brain stimulation targets for MS patients.

Abstract: People with Multiple Sclerosis (MS) complain of problems with hand dexterity
and cognitive fatigue. However, in many cases, impairments are subtle and
difficult to detect. Functional near-infrared spectroscopy (fNIRS) is a
non-invasive neuroimaging technique that measures brain hemodynamic responses
during cognitive or motor tasks. We aimed to detect brain activity biomarkers
that could explain subjective reports of cognitive fatigue while completing
dexterous tasks and provide targets for future brain stimulation treatments. We
recruited 15 people with MS who did not have a hand (Nine Hole Peg Test
[NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched
controls. Participants completed two types of hand dexterity tasks with their
dominant hand, single task and dual task (NHPT while holding a ball between the
fifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data
(oxygenated and deoxygenated hemoglobin levels) using a machine learning
framework to classify MS patients from controls based on their brain activation
patterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest
Neighbor classifier achieved an accuracy of 75.0% for single manual dexterity
tasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we
found that the most important brain regions contributing to the machine
learning model were the supramarginal/angular gyri and the precentral gyrus
(sensory integration and motor regions) of the ipsilateral hemisphere, with
suppressed activity and slower neurovascular response in the MS group. During
both tasks, deoxygenated hemoglobin levels were better predictors than the
conventional measure of oxygenated hemoglobin. This nonconventional method of
fNIRS data analysis revealed novel brain activity biomarkers that can help
develop personalized brain stimulation targets.

</details>


### [118] [Beyond Formula Complexity: Effective Information Criterion Improves Performance and Interpretability for Symbolic Regression](https://arxiv.org/abs/2509.21780)
*Zihan Yu,Guanren Wang,Jingtao Ding,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: The paper proposes Effective Information Criterion (EIC) to evaluate formula interpretability in symbolic regression by measuring numerical stability and information flow, addressing limitations of complexity-based metrics.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression methods use complexity metrics as proxy for interpretability, but these only consider formula size and ignore internal mathematical structure, leading to formulas that are difficult to analyze mathematically.

Method: Proposes Effective Information Criterion (EIC) that treats formulas as information processing systems and identifies unreasonable structures through loss of significant digits or amplification of rounding noise during data flow.

Result: EIC improves performance on Pareto frontier with search-based algorithms, reduces irrational structures, and improves sample efficiency by 2-4x with generative algorithms. Shows 70.2% agreement with human expert preferences for interpretability.

Conclusion: EIC effectively measures formula interpretability by detecting unreasonable mathematical structures through numerical stability analysis, bridging the gap between discovered formulas and real-world physical formulas.

Abstract: Symbolic regression discovers accurate and interpretable formulas to describe
given data, thereby providing scientific insights for domain experts and
promoting scientific discovery. However, existing symbolic regression methods
often use complexity metrics as a proxy for interoperability, which only
considers the size of the formula but ignores its internal mathematical
structure. Therefore, while they can discover formulas with compact forms, the
discovered formulas often have structures that are difficult to analyze or
interpret mathematically. In this work, inspired by the observation that
physical formulas are typically numerically stable under limited calculation
precision, we propose the Effective Information Criterion (EIC). It treats
formulas as information processing systems with specific internal structures
and identifies the unreasonable structure in them by the loss of significant
digits or the amplification of rounding noise as data flows through the system.
We find that this criterion reveals the gap between the structural rationality
of models discovered by existing symbolic regression algorithms and real-world
physical formulas. Combining EIC with various search-based symbolic regression
algorithms improves their performance on the Pareto frontier and reduces the
irrational structure in the results. Combining EIC with generative-based
algorithms reduces the number of samples required for pre-training, improving
sample efficiency by 2~4 times. Finally, for different formulas with similar
accuracy and complexity, EIC shows a 70.2% agreement with 108 human experts'
preferences for formula interpretability, demonstrating that EIC, by measuring
the unreasonable structures in formulas, actually reflects the formula's
interpretability.

</details>


### [119] [FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning](https://arxiv.org/abs/2509.21792)
*Yizhou Zhang,Ning Lv,Teng Wang,Jisheng Dang*

Main category: cs.LG

TL;DR: Proposes concurrency-aware speculative decoding and online draft learning to accelerate GRPO training, achieving 2.35x-2.72x speedup over baselines.


<details>
  <summary>Details</summary>
Motivation: GRPO training is slow due to computationally intensive autoregressive generation, and direct speculative decoding provides limited speedup under high concurrency.

Method: Concurrency-aware speculative decoding that dynamically adjusts drafting/verification strategy based on real-time concurrency, plus online draft learning to adapt draft model using target model feedback.

Result: Achieves 2.35x to 2.72x end-to-end speedups across multiple mathematical reasoning datasets and models, significantly outperforming baseline approaches.

Conclusion: The proposed framework effectively accelerates GRPO training while maintaining performance, with dynamic concurrency adaptation and continuous draft model updates.

Abstract: Group relative policy optimization (GRPO) has demonstrated significant
potential in improving the reasoning capabilities of large language models
(LLMs) via reinforcement learning. However, its practical deployment is impeded
by an excessively slow training process, primarily attributed to the
computationally intensive autoregressive generation of multiple responses per
query, which makes the generation phase the primary performance bottleneck.
Although speculative decoding presents a promising direction for acceleration,
its direct application in GRPO achieves limited speedup under high-concurrency
training conditions. To overcome this limitation, we propose a
concurrency-aware speculative decoding framework that dynamically adjusts the
drafting and verification strategy according to real-time concurrency levels,
thereby maximizing the acceleration of the generation process. Furthermore, to
address performance degradation arising from distributional drift between the
evolving target model and the fixed draft model during training, we introduce
an online draft learning mechanism that enables the draft model to continuously
adapt using feedback signals from the target model. Experimental results across
multiple mathematical reasoning datasets and models demonstrate that the
proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly
surpassing baseline approaches in efficiency. The code is available at
https://github.com/yedaotian9/GRPO_speculative.

</details>


### [120] [Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection](https://arxiv.org/abs/2509.21794)
*Kourosh Kakhi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharyab*

Main category: cs.LG

TL;DR: This paper proposes using multi-modal physiological signal fusion for fatigue detection, finding that combining EMG and EEG signals with XGBoost achieves best performance, outperforming single-signal approaches.


<details>
  <summary>Details</summary>
Motivation: Current fatigue detection studies focus on single modalities, but combining multiple physiological signals can improve classification robustness and accuracy for applications in transportation, healthcare, and performance monitoring.

Method: Used DROZY dataset with ECG, EMG, EOG, and EEG signals; extracted features across 15 signal combinations; evaluated with Decision Tree, Random Forest, Logistic Regression, and XGBoost; performed SHAP analysis for interpretability.

Result: XGBoost with EMG-EEG combination achieved best performance; multi-signal models consistently outperformed single-signal ones; SHAP analysis identified ECG-EOG correlation as key feature.

Conclusion: Feature-level fusion of physiological signals enhances accuracy, interpretability, and practical applicability of fatigue monitoring systems, demonstrating the value of multi-modal approaches.

Abstract: Fatigue detection using physiological signals is critical in domains such as
transportation, healthcare, and performance monitoring. While most studies
focus on single modalities, this work examines statistical relationships
between signal pairs to improve classification robustness. Using the DROZY
dataset, we extracted features from ECG, EMG, EOG, and EEG across 15 signal
combinations and evaluated them with Decision Tree, Random Forest, Logistic
Regression, and XGBoost. Results show that XGBoost with the EMG EEG combination
achieved the best performance. SHAP analysis highlighted ECG EOG correlation as
a key feature, and multi signal models consistently outperformed single signal
ones. These findings demonstrate that feature level fusion of physiological
signals enhances accuracy, interpretability, and practical applicability of
fatigue monitoring systems.

</details>


### [121] [ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations](https://arxiv.org/abs/2509.21802)
*Chang Liu,Bohao Zhao,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: ChaosNexus is a foundation model for chaotic systems forecasting that achieves state-of-the-art zero-shot generalization through multi-scale architecture and diverse training data.


<details>
  <summary>Details</summary>
Motivation: Traditional chaotic system models lack generalization capacity for real-world applications due to system-specific training and data scarcity, requiring robust zero-shot/few-shot forecasting on novel scenarios.

Method: Proposes ChaosNexus with multi-scale ScaleFormer architecture augmented with Mixture-of-Experts layers, pre-trained on diverse corpus of chaotic dynamics to capture universal patterns and system-specific behaviors.

Result: Achieves >40% improvement in long-term attractor statistics on 9,000+ synthetic systems; competitive zero-shot weather forecasting (<1 degree mean error) that improves with fine-tuning; demonstrates scaling benefits from training diversity rather than data volume.

Conclusion: ChaosNexus enables robust generalization for chaotic systems forecasting, with cross-system generalization driven by training diversity rather than data volume, providing principles for scientific foundation models.

Abstract: Accurately forecasting chaotic systems, prevalent in domains such as weather
prediction and fluid dynamics, remains a significant scientific challenge. The
inherent sensitivity of these systems to initial conditions, coupled with a
scarcity of observational data, severely constrains traditional modeling
approaches. Since these models are typically trained for a specific system,
they lack the generalization capacity necessary for real-world applications,
which demand robust zero-shot or few-shot forecasting on novel or data-limited
scenarios. To overcome this generalization barrier, we propose ChaosNexus, a
foundation model pre-trained on a diverse corpus of chaotic dynamics.
ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented
with Mixture-of-Experts layers, to capture both universal patterns and
system-specific behaviors. The model demonstrates state-of-the-art zero-shot
generalization across both synthetic and real-world benchmarks. On a
large-scale testbed comprising over 9,000 synthetic chaotic systems, it
improves the fidelity of long-term attractor statistics by more than 40%
compared to the leading baseline. This robust performance extends to real-world
applications with exceptional data efficiency. For instance, in 5-day global
weather forecasting, ChaosNexus achieves a competitive zero-shot mean error
below 1 degree, a result that further improves with few-shot fine-tuning.
Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding
principle for scientific foundation models: cross-system generalization stems
from the diversity of training systems, rather than sheer data volume.

</details>


### [122] [Scaling Laws for Neural Material Models](https://arxiv.org/abs/2509.21811)
*Akshay Trikha,Kyle Chu,Advait Gosai,Parker Szachta,Eric Weiner*

Main category: cs.LG

TL;DR: The paper analyzes how scaling training data, model size, and compute affects neural network performance for material property prediction, establishing empirical scaling laws.


<details>
  <summary>Details</summary>
Motivation: Predicting material properties is crucial for designing better batteries, semiconductors, and medical devices, and deep learning helps scientists quickly find promising materials.

Method: Trained transformer and EquiformerV2 neural networks to predict material properties and analyzed scaling effects of training data, model size, and compute using power law relationships.

Result: Found empirical scaling laws showing loss follows power law relationship L = α·N^(-β), where N is the relevant hyperparameter (data, model size, or compute).

Conclusion: Future work could investigate scaling laws for other neural network models like GemNet and fully connected networks to compare with the trained models.

Abstract: Predicting material properties is crucial for designing better batteries,
semiconductors, and medical devices. Deep learning helps scientists quickly
find promising materials by predicting their energy, forces, and stresses.
Companies scale capacities of deep learning models in multiple domains, such as
language modeling, and invest many millions of dollars into such models. Our
team analyzes how scaling training data (giving models more information to
learn from), model sizes (giving models more capacity to learn patterns), and
compute (giving models more computational resources) for neural networks
affects their performance for material property prediction. In particular, we
trained both transformer and EquiformerV2 neural networks to predict material
properties. We find empirical scaling laws for these models: we can predict how
increasing each of the three hyperparameters (training data, model size, and
compute) affects predictive performance. In particular, the loss $L$ can be
measured with a power law relationship $L = \alpha \cdot N^{-\beta}$, where
$\alpha$ and $\beta$ are constants while $N$ is the relevant hyperparameter. We
also incorporate command-line arguments for changing training settings such as
the amount of epochs, maximum learning rate, and whether mixed precision is
enabled. Future work could entail further investigating scaling laws for other
neural network models in this domain, such as GemNet and fully connected
networks, to assess how they compare to the models we trained.

</details>


### [123] [Sharpness-Aware Minimization Can Hallucinate Minimizers](https://arxiv.org/abs/2509.21818)
*Chanwoong Park,Uijeong Jang,Ernest K. Ryu,Insoon Yang*

Main category: cs.LG

TL;DR: SAM can converge to hallucinated minimizers that are not actual minimizers of the original objective, and a simple remedy is proposed to avoid them.


<details>
  <summary>Details</summary>
Motivation: To investigate and demonstrate that Sharpness-Aware Minimization (SAM), despite its popularity for finding flatter minimizers that generalize better, can converge to hallucinated minimizers that don't minimize the original objective.

Method: Theoretical proof of hallucinated minimizers' existence, establishment of local convergence conditions, empirical validation of convergence to these points, and proposal of a simple remedy.

Result: Theoretical and empirical evidence confirms SAM can converge to hallucinated minimizers, and the proposed remedy effectively avoids them.

Conclusion: SAM has a previously unrecognized limitation of potentially converging to hallucinated minimizers, but this can be addressed with the proposed simple remedy.

Abstract: Sharpness-Aware Minimization (SAM) is a widely used method that steers
training toward flatter minimizers, which typically generalize better. In this
work, however, we show that SAM can converge to hallucinated minimizers --
points that are not minimizers of the original objective. We theoretically
prove the existence of such hallucinated minimizers and establish conditions
for local convergence to them. We further provide empirical evidence
demonstrating that SAM can indeed converge to these points in practice.
Finally, we propose a simple yet effective remedy for avoiding hallucinated
minimizers.

</details>


### [124] [On the Complexity Theory of Masked Discrete Diffusion: From $\mathrm{poly}(1/ε)$ to Nearly $ε$-Free](https://arxiv.org/abs/2509.21835)
*Xunpeng Huang,Yingyu Lin,Nishant Jain,Kaibo Wang,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: This paper provides the first rigorous theoretical analysis of masked discrete diffusion for text generation, showing it outperforms uniform discrete diffusion with better complexity bounds and practical advantages.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical analyses of masked discrete diffusion are insufficient - they overlook Euler samplers, impose restrictive assumptions, or fail to demonstrate advantages over uniform diffusion, creating a gap in understanding this widely used paradigm.

Method: The paper analyzes Euler samplers for masked discrete diffusion and proposes Mask-Aware Truncated Uniformization (MATU), which removes bounded-score assumptions and exploits the property that each token can be unmasked at most once.

Result: Euler samplers achieve ε-accuracy with Õ(d²ε^{-3/2}) score evaluations. MATU achieves nearly ε-free complexity of O(d ln d · (1-ε²)), eliminating the ln(1/ε) factor and substantially speeding up convergence compared to uniform diffusion methods.

Conclusion: The findings provide rigorous theoretical foundation for masked discrete diffusion, demonstrate its practical advantages over uniform diffusion for text generation, and pave the way for analyzing diffusion-based language models using masking paradigms.

Abstract: We study masked discrete diffusion -- a flexible paradigm for text generation
in which tokens are progressively corrupted by special mask symbols before
being denoised. Although this approach has demonstrated strong empirical
performance, its theoretical complexity in high-dimensional settings remains
insufficiently understood. Existing analyses largely focus on uniform discrete
diffusion, and more recent attempts addressing masked diffusion either (1)
overlook widely used Euler samplers, (2) impose restrictive bounded-score
assumptions, or (3) fail to showcase the advantages of masked discrete
diffusion over its uniform counterpart. To address this gap, we show that Euler
samplers can achieve $\epsilon$-accuracy in total variation (TV) with
$\tilde{O}(d^{2}\epsilon^{-3/2})$ discrete score evaluations, thereby providing
the first rigorous analysis of typical Euler sampler in masked discrete
diffusion. We then propose a Mask-Aware Truncated Uniformization (MATU)
approach that both removes bounded-score assumptions and preserves unbiased
discrete score approximation. By exploiting the property that each token can be
unmasked at most once, MATU attains a nearly $\epsilon$-free complexity of
$O(d\,\ln d\cdot (1-\epsilon^2))$. This result surpasses existing
uniformization methods under uniform discrete diffusion, eliminating the
$\ln(1/\epsilon)$ factor and substantially speeding up convergence. Our
findings not only provide a rigorous theoretical foundation for masked discrete
diffusion, showcasing its practical advantages over uniform diffusion for text
generation, but also pave the way for future efforts to analyze diffusion-based
language models developed under masking paradigm.

</details>


### [125] [Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms](https://arxiv.org/abs/2509.21847)
*Rohan Deb,Qiaobo Li,Mayank Shrivastava,Arindam Banerjee*

Main category: cs.LG

TL;DR: Developed a general framework for uniform bounds on sketched bilinear forms using geometric complexities and generic chaining, extending to multiple sketching matrices and applications in federated learning and bandit algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing uniform bounds for sketched bilinear forms are either inapplicable or not sharp on general sets, while such forms are crucial for modern machine learning analyses including JL lemma, RIP, randomized sketching, and approximate linear algebra.

Method: Used generic chaining and introduced new techniques for handling suprema over pairs of sets to analyze sketched bilinear forms, extending to sums of T independent sketching matrices.

Result: Derived uniform bounds in terms of geometric complexities, showing deviation scales as √T for multiple sketching matrices, recovering JL lemma and extending RIP guarantees while improving convergence for sketched federated learning and bandit algorithms.

Conclusion: The framework provides unified analysis for sketched bilinear forms with applications across machine learning, offering sharper bounds based on geometric complexities rather than ambient dimensions.

Abstract: Uniform bounds on sketched inner products of vectors or matrices underpin
several important computational and statistical results in machine learning and
randomized algorithms, including the Johnson-Lindenstrauss (J-L) lemma, the
Restricted Isometry Property (RIP), randomized sketching, and approximate
linear algebra. However, many modern analyses involve *sketched bilinear
forms*, for which existing uniform bounds either do not apply or are not sharp
on general sets. In this work, we develop a general framework to analyze such
sketched bilinear forms and derive uniform bounds in terms of geometric
complexities of the associated sets. Our approach relies on generic chaining
and introduces new techniques for handling suprema over pairs of sets. We
further extend these results to the setting where the bilinear form involves a
sum of $T$ independent sketching matrices and show that the deviation scales as
$\sqrt{T}$. This unified analysis recovers known results such as the J-L lemma
as special cases, while extending RIP-type guarantees. Additionally, we obtain
improved convergence bounds for sketched Federated Learning algorithms where
such cross terms arise naturally due to sketched gradient compression, and
design sketched variants of bandit algorithms with sharper regret bounds that
depend on the geometric complexity of the action and parameter sets, rather
than the ambient dimension.

</details>


### [126] [Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration](https://arxiv.org/abs/2509.21848)
*Taejong Joo,Shu Ishida,Ivan Sosnovik,Bryan Lim,Sahand Rezaei-Shoshtari,Adam Gaier,Robert Giaquinto*

Main category: cs.LG

TL;DR: Graph of Agents (GoA) is a multi-agent framework that dynamically constructs input-dependent collaboration structures to maximize information compression, enabling large language models to process long contexts beyond their window limits without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems for long context modeling rely heavily on hand-crafted collaboration strategies and prompt engineering, which limits their generalizability and performance.

Method: Formalizes long context modeling as a compression problem with an information-theoretic objective, then proposes Graph of Agents (GoA) that dynamically constructs input-dependent collaboration structures to maximize this objective.

Result: For Llama 3.1 8B and Qwen3 8B across six document QA benchmarks, GoA improves average F1 score by 5.7% over retrieval-augmented generation and 16.35% over fixed-structure multi-agent baselines. With only 2K context window, GoA surpasses 128K context window Llama 3.1 8B on LongBench.

Conclusion: GoA dramatically increases effective context length and outperforms existing approaches, demonstrating the effectiveness of dynamic, input-dependent collaboration structures in multi-agent systems for long context modeling.

Abstract: As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.

</details>


### [127] [Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2509.21882)
*Aaron Tu,Weihao Xuan,Heli Qi,Xu Huang,Qingcheng Zeng,Shayan Talaei,Yijia Xiao,Peng Xia,Xiangru Tang,Yuchen Zhuang,Bing Hu,Hanqun Cao,Wenqi Shi,Tianang Leng,Rui Yang,Yingjian Chen,Ziqi Wang,Irene Li,Nan Liu,Huaxiu Yao,Li Erran Li,Ge Liu,Amin Saberi,Naoto Yokoya,Jure Leskovec,Yejin Choi,Fang Wu*

Main category: cs.LG

TL;DR: RLVR (Reinforcement Learning with Verifiable Rewards) shows real but often overstated gains in LLM enhancement; proper evaluation reveals smaller benefits due to RLVR tax, evaluation issues, and data contamination.


<details>
  <summary>Details</summary>
Motivation: To assess the true effectiveness of RLVR by examining how much reported gains survive under strict parity-controlled evaluation and whether RLVR imposes measurable costs.

Method: Used partial-prompt contamination audit and matched-budget reproductions across base and RL models, plus proposed tax-aware training/evaluation protocol for co-optimizing accuracy, grounding, and calibrated abstention.

Result: Several headline performance gaps shrink or disappear under clean evaluation; the proposed protocol provides more reliable gain estimates and revises some prior conclusions.

Conclusion: RLVR is valuable and industry-ready, but practical benefits should be maintained while prioritizing reliability, safety, and proper measurement to avoid overstatement of gains.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a practical and
scalable approach to enhancing large language models in areas such as math,
code, and other structured tasks. Two questions motivate this paper: how much
of the reported gains survive under strictly parity-controlled evaluation, and
whether RLVR is cost-free or exacts a measurable tax. We argue that progress is
real, but gains are often overstated due to three forces - an RLVR tax,
evaluation pitfalls, and data contamination. Using a partial-prompt
contamination audit and matched-budget reproductions across base and RL models,
we show that several headline gaps shrink or vanish under clean,
parity-controlled evaluation. We then propose a tax-aware training and
evaluation protocol that co-optimizes accuracy, grounding, and calibrated
abstention and standardizes budgeting and provenance checks. Applied to recent
RLVR setups, this protocol yields more reliable estimates of reasoning gains
and, in several cases, revises prior conclusions. Our position is constructive:
RLVR is valuable and industry-ready; we advocate keeping its practical benefits
while prioritizing reliability, safety, and measurement.

</details>


### [128] [MolSpectLLM: A Molecular Foundation Model Bridging Spectroscopy, Molecule Elucidation, and 3D Structure Generation](https://arxiv.org/abs/2509.21861)
*Shuaike Shen,Jiaqing Xie,Zhuo Yang,Antong Zhang,Shuzhou Sun,Ben Gao,Tianfan Fu,Biqing Qi,Yuqiang Li*

Main category: cs.LG

TL;DR: MolSpectLLM is a molecular foundation model that integrates experimental spectroscopy with 3D molecular structure, achieving state-of-the-art performance on spectrum-related tasks and enabling accurate 3D structure generation from SMILES or spectral inputs.


<details>
  <summary>Details</summary>
Motivation: Most existing molecular foundation models rely only on SMILES representations and ignore experimental spectra and 3D structural information, limiting their effectiveness in tasks requiring stereochemistry, spatial conformation, and experimental validation.

Method: Proposed MolSpectLLM, a molecular foundation model pretrained on Qwen2.5-7B that unifies experimental spectroscopy with molecular 3D structure by explicitly modeling molecular spectra.

Result: Achieved state-of-the-art performance on spectrum-related tasks with 0.53 average accuracy across NMR, IR, and MS benchmarks. Obtained 15.5% sequence accuracy and 41.7% token accuracy on Spectra-to-SMILES, substantially outperforming large general-purpose LLMs. Generated accurate 3D molecular structures directly from SMILES or spectral inputs.

Conclusion: MolSpectLLM bridges spectral analysis, molecular elucidation, and molecular design by integrating spectroscopy with 3D structural information, overcoming limitations of SMILES-only approaches.

Abstract: Recent advances in molecular foundation models have shown impressive
performance in molecular property prediction and de novo molecular design, with
promising applications in areas such as drug discovery and reaction prediction.
Nevertheless, most existing approaches rely exclusively on SMILES
representations and overlook both experimental spectra and 3D structural
information-two indispensable sources for capturing molecular behavior in
real-world scenarios. This limitation reduces their effectiveness in tasks
where stereochemistry, spatial conformation, and experimental validation are
critical. To overcome these challenges, we propose MolSpectLLM, a molecular
foundation model pretrained on Qwen2.5-7B that unifies experimental
spectroscopy with molecular 3D structure. By explicitly modeling molecular
spectra, MolSpectLLM achieves state-of-the-art performance on spectrum-related
tasks, with an average accuracy of 0.53 across NMR, IR, and MS benchmarks.
MolSpectLLM also shows strong performance on the spectra analysis task,
obtaining 15.5% sequence accuracy and 41.7% token accuracy on
Spectra-to-SMILES, substantially outperforming large general-purpose LLMs. More
importantly, MolSpectLLM not only achieves strong performance on molecular
elucidation tasks, but also generates accurate 3D molecular structures directly
from SMILES or spectral inputs, bridging spectral analysis, molecular
elucidation, and molecular design.

</details>


### [129] [Generation Properties of Stochastic Interpolation under Finite Training Set](https://arxiv.org/abs/2509.21925)
*Yunchen Li,Shaohui Lin,Zhou Yu*

Main category: cs.LG

TL;DR: The paper analyzes generative models with finite training data, deriving closed-form expressions for optimal velocity/score functions and showing deterministic processes recover training samples while stochastic processes add Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical behavior of generative models when trained on limited datasets, addressing how they perform under finite sample conditions.

Method: Uses stochastic interpolation generative framework to derive analytical expressions, considers model estimation errors, and introduces formal definitions of underfitting/overfitting for generative models.

Result: Deterministic processes exactly recover training samples; stochastic processes produce training samples with Gaussian noise. With estimation errors, generation becomes convex combinations of training samples corrupted by mixed uniform/Gaussian noise.

Conclusion: The theoretical framework explains generative model behavior under finite training, with experimental validation on generation and classification tasks supporting the findings.

Abstract: This paper investigates the theoretical behavior of generative models under
finite training populations. Within the stochastic interpolation generative
framework, we derive closed-form expressions for the optimal velocity field and
score function when only a finite number of training samples are available. We
demonstrate that, under some regularity conditions, the deterministic
generative process exactly recovers the training samples, while the stochastic
generative process manifests as training samples with added Gaussian noise.
Beyond the idealized setting, we consider model estimation errors and introduce
formal definitions of underfitting and overfitting specific to generative
models. Our theoretical analysis reveals that, in the presence of estimation
errors, the stochastic generation process effectively produces convex
combinations of training samples corrupted by a mixture of uniform and Gaussian
noise. Experiments on generation tasks and downstream tasks such as
classification support our theory.

</details>


### [130] [Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding](https://arxiv.org/abs/2509.21865)
*Seong-Woong Shim,Myunsoo Kim,Jae Hyeon Cho,Byung-Jun Lee*

Main category: cs.LG

TL;DR: LDAR is an adaptive retriever that learns to retrieve contexts while mitigating interference from distracting passages, achieving better performance with reduced token usage compared to long-context approaches.


<details>
  <summary>Details</summary>
Motivation: Recent LLMs with large context windows (128K+ tokens) can process full documents directly, but this approach has limitations: token inefficiency, 'lost in the middle' phenomenon, and amplified distraction that degrades output quality.

Method: Propose LDAR (Learning Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve contexts in a distraction-aware manner to mitigate interference from distracting passages.

Result: Extensive experiments across diverse LLM architectures and six knowledge-intensive benchmarks demonstrate LDAR's effectiveness and robustness, achieving significantly higher performance with reduced token usage.

Conclusion: The approach highlights the importance of balancing the trade-off between information coverage and distraction in retrieval-augmented generation systems.

Abstract: Retrieval-Augmented Generation (RAG) is a framework for grounding Large
Language Models (LLMs) in external, up-to-date information. However, recent
advancements in context window size allow LLMs to process inputs of up to 128K
tokens or more, offering an alternative strategy: supplying the full document
context directly to the model, rather than relying on RAG to retrieve a subset
of contexts. Nevertheless, this emerging alternative strategy has notable
limitations: (i) it is token-inefficient to handle large and potentially
redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon;
and (iii) under limited model capacity, it amplifies distraction, ultimately
degrading LLM output quality. In this paper, we propose LDAR (Learning
Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve
contexts in a way that mitigates interference from distracting passages,
thereby achieving significantly higher performance with reduced token usage
compared to long-context approaches. Extensive experiments across diverse LLM
architectures and six knowledge-intensive benchmarks demonstrate the
effectiveness and robustness of our approach, highlighting the importance of
balancing the trade-off between information coverage and distraction.

</details>


### [131] [Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947)
*Taeyoung Yun,Pierre-Luc St-Charles,Jinkyoo Park,Yoshua Bengio,Minsu Kim*

Main category: cs.LG

TL;DR: Active Attacks is a novel RL-based red-teaming algorithm that adapts attacks as the victim LLM evolves through safety fine-tuning, forcing the attacker to seek new vulnerabilities and achieving 400x improvement in cross-attack success rates.


<details>
  <summary>Details</summary>
Motivation: To automatically generate diverse attack prompts for LLMs that elicit harmful behaviors, overcoming the limitation of existing diversity-seeking RL methods that collapse to limited modes once high-reward prompts are found.

Method: Uses reinforcement learning with a toxicity classifier as reward, periodically safety fine-tunes the victim LLM with collected attack prompts to diminish rewards in exploited regions, forcing exploration of new vulnerabilities.

Result: Outperformed prior RL-based methods (GFlowNets, PPO, REINFORCE) by improving cross-attack success rates from 0.07% to 31.28% (400x relative gain) with only 6% increase in computation.

Conclusion: Active Attacks effectively induces an easy-to-hard exploration curriculum, uncovers wide range of local attack modes, and achieves broad coverage of multi-mode distribution through adaptive exploration.

Abstract: We address the challenge of generating diverse attack prompts for large
language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual
content) and are used for safety fine-tuning. Rather than relying on manual
prompt engineering, attacker LLMs can be trained with reinforcement learning
(RL) to automatically generate such prompts using only a toxicity classifier as
a reward. However, capturing a wide range of harmful behaviors is a significant
challenge that requires explicit diversity objectives. Existing
diversity-seeking RL methods often collapse to limited modes: once high-reward
prompts are found, exploration of new regions is discouraged. Inspired by the
active learning paradigm that encourages adaptive exploration, we introduce
\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its
attacks as the victim evolves. By periodically safety fine-tuning the victim
LLM with collected attack prompts, rewards in exploited regions diminish, which
forces the attacker to seek unexplored vulnerabilities. This process naturally
induces an easy-to-hard exploration curriculum, where the attacker progresses
beyond easy modes toward increasingly difficult ones. As a result, Active
Attacks uncovers a wide range of local attack modes step by step, and their
combination achieves wide coverage of the multi-mode distribution. Active
Attacks, a simple plug-and-play module that seamlessly integrates into existing
RL objectives, unexpectedly outperformed prior RL-based methods -- including
GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates
against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a
relative gain greater than $400\ \times$) with only a 6% increase in
computation. Our code is publicly available
\href{https://github.com/dbsxodud-11/active_attacks}{here}.

</details>


### [132] [Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models](https://arxiv.org/abs/2509.21874)
*Yifei Peng,Yaoli Liu,Enbo Xia,Yu Jin,Wang-Zhou Dai,Zhong Ren,Yao-Xiang Ding,Kun Zhou*

Main category: cs.LG

TL;DR: ILP-CoT bridges Inductive Logic Programming and Multimodal Large Language Models for abductive logical rule induction, using MLLMs to propose rule structures and ILP to refine them with formal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with logical rule induction from unstructured inputs: ILP requires specified background knowledge and has high computational cost, while MLLMs suffer from perceptual hallucinations.

Method: Automatically builds ILP tasks with pruned search spaces based on rule structure proposals from MLLMs, then uses ILP systems to output rules built upon rectified logical facts and formal inductive reasoning.

Result: Verified effectiveness through challenging logical induction benchmarks and demonstrated application in text-to-image customized generation with rule induction.

Conclusion: ILP-CoT successfully combines the strengths of MLLMs (rule structure proposals) and ILP (formal reasoning) to overcome limitations of both approaches for logical rule induction.

Abstract: We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP)
and Multimodal Large Language Models (MLLMs) for abductive logical rule
induction. The task involves both discovering logical facts and inducing
logical rules from a small number of unstructured textual or visual inputs,
which still remain challenging when solely relying on ILP, due to the
requirement of specified background knowledge and high computational cost, or
MLLMs, due to the appearance of perceptual hallucinations. Based on the key
observation that MLLMs could propose structure-correct rules even under
hallucinations, our approach automatically builds ILP tasks with pruned search
spaces based on the rule structure proposals from MLLMs, and utilizes ILP
system to output rules built upon rectified logical facts and formal inductive
reasoning. Its effectiveness is verified through challenging logical induction
benchmarks, as well as a potential application of our approach, namely
text-to-image customized generation with rule induction. Our code and data are
released at https://github.com/future-item/ILP-CoT.

</details>


### [133] [Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer](https://arxiv.org/abs/2509.22038)
*Zhihua Zhong,Xuanyang Huang*

Main category: cs.LG

TL;DR: Introduces a framework for customizable latent space operations in diffusion models to enable intuitive vector manipulation like in GANs, expanding creative possibilities for generative art.


<details>
  <summary>Details</summary>
Motivation: Diffusion models lack the intuitive latent vector control found in GANs, limiting their flexibility for artistic expression and creative exploration.

Method: A framework that integrates customizable latent space operations into the diffusion process, enabling direct manipulation of conceptual and spatial representations.

Result: Demonstrated through two artworks (Infinitepedia and Latent Motion) showing successful conceptual blending and dynamic motion generation. Revealed latent space structures with semantic and meaningless regions.

Conclusion: The approach expands creative possibilities in generative art and provides insights into the geometry of diffusion models, paving the way for further latent space explorations.

Abstract: Latent space is one of the key concepts in generative AI, offering powerful
means for creative exploration through vector manipulation. However, diffusion
models like Stable Diffusion lack the intuitive latent vector control found in
GANs, limiting their flexibility for artistic expression. This paper introduces
\workname, a framework for integrating customizable latent space operations
into the diffusion process. By enabling direct manipulation of conceptual and
spatial representations, this approach expands creative possibilities in
generative art. We demonstrate the potential of this framework through two
artworks, \textit{Infinitepedia} and \textit{Latent Motion}, highlighting its
use in conceptual blending and dynamic motion generation. Our findings reveal
latent space structures with semantic and meaningless regions, offering
insights into the geometry of diffusion models and paving the way for further
explorations of latent space.

</details>


### [134] [Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness](https://arxiv.org/abs/2509.21879)
*Chaoyang Luo,Yan Zou,Nanjing Huang*

Main category: cs.LG

TL;DR: Zubov-Net is an adaptive stable learning framework that reformulates Zubov's equation to reconcile accuracy and robustness in Neural ODEs by actively controlling regions of attraction geometry through tripartite losses and parallel boundary sampling.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental challenge of tension between robustness and accuracy in Neural ODEs, which stems from difficulty in imposing appropriate stability conditions for formal robustness guarantees.

Method: Reformulates Zubov's equation into consistency characterization between RoAs and prescribed RoAs; uses tripartite losses (consistency, classification, separation) and parallel boundary sampling; designs input-attention-based convex neural network with softmax attention.

Result: Maintains high classification accuracy while significantly improving robustness against various stochastic noises and adversarial attacks; provides theoretical guarantees for consistent PRoAs-RoAs alignment, trajectory stability, and non-overlapping PRoAs.

Conclusion: Zubov-Net successfully reconciles accuracy and robustness in Neural ODEs through adaptive stable learning framework with theoretical guarantees and experimental validation.

Abstract: Despite neural ordinary differential equations (Neural ODEs) exhibiting
intrinsic robustness under input perturbations due to their dynamical systems
nature, recent approaches often involve imposing Lyapunov-based stability
conditions to provide formal robustness guarantees. However, a fundamental
challenge remains: the tension between robustness and accuracy, primarily
stemming from the difficulty in imposing appropriate stability conditions. To
address this, we propose an adaptive stable learning framework named Zubov-Net,
which innovatively reformulates Zubov's equation into a consistency
characterization between regions of attraction (RoAs) and prescribed RoAs
(PRoAs). Building on this consistency, we introduce a new paradigm for actively
controlling the geometry of RoAs by directly optimizing PRoAs to reconcile
accuracy and robustness. Our approach is realized through tripartite losses
(consistency, classification, and separation losses) and a parallel boundary
sampling algorithm that co-optimizes the Neural ODE and the Lyapunov function.
To enhance the discriminativity of Lyapunov functions, we design an
input-attention-based convex neural network via a softmax attention mechanism
that focuses on equilibrium-relevant features and also serves as weight
normalization to maintain training stability in deep architectures.
Theoretically, we prove that minimizing the tripartite loss guarantees
consistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping
PRoAs. Moreover, we establish stochastic convex separability with tighter
probability bounds and fewer dimensionality requirements to justify the convex
design in Lyapunov functions. Experimentally, Zubov-Net maintains high
classification accuracy while significantly improving robustness against
various stochastic noises and adversarial attacks.

</details>


### [135] [The Rogue Scalpel: Activation Steering Compromises LLM Safety](https://arxiv.org/abs/2509.22067)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Y. Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: Activation steering, often seen as a safe alternative to fine-tuning, actually systematically breaks LLM alignment safeguards and increases harmful compliance, even with random or interpretable steering directions.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that activation steering is a precise, interpretable, and safer alternative to fine-tuning for controlling LLM behavior.

Method: Conducted extensive experiments on different model families using random steering directions and interpretable directions from sparse autoencoders (SAEs), and tested combining multiple steering vectors.

Result: Random steering increased harmful compliance from 0% to 2-27%, SAE-based steering further increased rates by 2-4%, and combining 20 random vectors created a universal attack that significantly increased harmful compliance on unseen requests.

Conclusion: Precise control over model internals does not guarantee precise control over model behavior, challenging the paradigm of safety through interpretability.

Abstract: Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model's hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.

</details>


### [136] [Reinforcement Learning for Durable Algorithmic Recourse](https://arxiv.org/abs/2509.22102)
*Marina Ceccon,Alessandro Fabris,Goran Radanović,Asia J. Biega,Gian Antonio Susto*

Main category: cs.LG

TL;DR: This paper introduces a time-aware framework for algorithmic recourse that models population adaptation dynamics and uses reinforcement learning to generate durable recommendations that remain valid over time.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on robustness to model updates but neglected temporal dynamics in competitive, resource-constrained settings where recommendations shape future applicant pools.

Method: Novel reinforcement learning-based recourse algorithm that captures evolving environmental dynamics and generates recommendations designed to be durable over predefined time horizon T.

Result: Extensive experiments show the approach substantially outperforms existing baselines, offering superior balance between feasibility and long-term validity.

Conclusion: The results underscore the importance of incorporating temporal and behavioral dynamics into practical recourse system design.

Abstract: Algorithmic recourse seeks to provide individuals with actionable
recommendations that increase their chances of receiving favorable outcomes
from automated decision systems (e.g., loan approvals). While prior research
has emphasized robustness to model updates, considerably less attention has
been given to the temporal dynamics of recourse--particularly in competitive,
resource-constrained settings where recommendations shape future applicant
pools. In this work, we present a novel time-aware framework for algorithmic
recourse, explicitly modeling how candidate populations adapt in response to
recommendations. Additionally, we introduce a novel reinforcement learning
(RL)-based recourse algorithm that captures the evolving dynamics of the
environment to generate recommendations that are both feasible and valid. We
design our recommendations to be durable, supporting validity over a predefined
time horizon T. This durability allows individuals to confidently reapply after
taking time to implement the suggested changes. Through extensive experiments
in complex simulation environments, we show that our approach substantially
outperforms existing baselines, offering a superior balance between feasibility
and long-term validity. Together, these results underscore the importance of
incorporating temporal and behavioral dynamics into the design of practical
recourse systems.

</details>


### [137] [Why High-rank Neural Networks Generalize?: An Algebraic Framework with RKHSs](https://arxiv.org/abs/2509.21895)
*Yuka Hashimoto,Sho Sonoda,Isao Ishikawa,Masahiro Ikeda*

Main category: cs.LG

TL;DR: A new Rademacher complexity bound for deep neural networks using Koopman operators and RKHS theory that explains why models with high-rank weight matrices generalize well, applicable to a wider range of realistic models than existing bounds.


<details>
  <summary>Details</summary>
Motivation: Existing Rademacher complexity bounds for neural networks have limited applicability to realistic models and fail to adequately explain why models with high-rank weight matrices generalize well.

Method: Introduce an algebraic representation of neural networks using Koopman operators, group representations, and construct a reproducing kernel Hilbert space (RKHS) with a specific kernel function to derive the bound.

Result: Derived a new Rademacher complexity bound that explains the generalization behavior of models with high-rank weight matrices and is applicable to a wider range of practical neural network architectures.

Conclusion: This work extends Koopman-based theory for Rademacher complexity bounds to more practical situations, providing better theoretical understanding of neural network generalization.

Abstract: We derive a new Rademacher complexity bound for deep neural networks using
Koopman operators, group representations, and reproducing kernel Hilbert spaces
(RKHSs). The proposed bound describes why the models with high-rank weight
matrices generalize well. Although there are existing bounds that attempt to
describe this phenomenon, these existing bounds can be applied to limited types
of models. We introduce an algebraic representation of neural networks and a
kernel function to construct an RKHS to derive a bound for a wider range of
realistic models. This work paves the way for the Koopman-based theory for
Rademacher complexity bounds to be valid for more practical situations.

</details>


### [138] [Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization](https://arxiv.org/abs/2509.22115)
*Chao Wang,Tao Yang,Hongtao Tian,Yunsheng Shi,Qiyao Ma,Xiaotao Liu,Ting Yao,Wenbo Ding*

Main category: cs.LG

TL;DR: D$^3$S is a dynamic dual-level down-sampling framework that improves policy optimization efficiency by prioritizing informative samples and tokens, achieving state-of-the-art performance with fewer samples and tokens.


<details>
  <summary>Details</summary>
Motivation: Critic-free methods like GRPO reduce memory demands but converge slowly due to diluted learning signals from uninformative samples and tokens.

Method: D$^3$S operates at two levels: sample-level selects rollouts to maximize advantage variance, and token-level prioritizes tokens with high advantage magnitude and policy entropy. It uses a dynamic down-sampling schedule inspired by curriculum learning.

Result: Extensive experiments on Qwen2.5 and Llama3.1 show state-of-the-art performance and generalization while requiring fewer samples and tokens across diverse reasoning benchmarks.

Conclusion: D$^3$S effectively addresses the slow convergence of critic-free methods by dynamically prioritizing informative data, achieving superior efficiency and performance in policy optimization.

Abstract: Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.

</details>


### [139] [Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning](https://arxiv.org/abs/2509.21898)
*Zihuan Qiu,Yi Xu,Fanman Meng,Runtong Zhang,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: IVT is a plug-and-play framework that uses Linear Mode Connectivity to mitigate catastrophic forgetting in Class Incremental Learning by periodically transforming model parameters to maintain linear connections to previous task optima.


<details>
  <summary>Details</summary>
Motivation: Current CIL methods have significant performance gaps compared to oracle models trained with full historical data. The paper is motivated by the observation that oracle solutions maintain low-loss linear connections to previous task optima.

Method: Proposes Increment Vector Transformation (IVT) which periodically teleports model parameters to transformed solutions that preserve linear connectivity to previous task optimum, using diagonal Fisher Information Matrices for efficient approximation.

Result: IVT consistently enhances CIL baselines: +5.12% last accuracy on CIFAR-100 with PASS baseline, +14.93% average accuracy and +21.95% last accuracy on FGVCAircraft with CLIP-pre-trained SLCA baseline.

Conclusion: IVT effectively mitigates catastrophic forgetting by maintaining linear connectivity to previous task optima, demonstrating significant performance improvements across multiple datasets and compatible with various CIL scenarios.

Abstract: Class Incremental Learning (CIL) aims to sequentially acquire knowledge of
new classes without forgetting previously learned ones. Despite recent
progress, current CIL methods still exhibit significant performance gaps
compared to their oracle counterparts-models trained with full access to
historical data. Inspired by recent insights on Linear Mode Connectivity (LMC),
we revisit the geometric properties of oracle solutions in CIL and uncover a
fundamental observation: these oracle solutions typically maintain low-loss
linear connections to the optimum of previous tasks. Motivated by this finding,
we propose Increment Vector Transformation (IVT), a novel plug-and-play
framework designed to mitigate catastrophic forgetting during training. Rather
than directly following CIL updates, IVT periodically teleports the model
parameters to transformed solutions that preserve linear connectivity to
previous task optimum. By maintaining low-loss along these connecting paths,
IVT effectively ensures stable performance on previously learned tasks. The
transformation is efficiently approximated using diagonal Fisher Information
Matrices, making IVT suitable for both exemplar-free and exemplar-based
scenarios, and compatible with various initialization strategies. Extensive
experiments on CIFAR-100, FGVCAircraft, ImageNet-Subset, and ImageNet-Full
demonstrate that IVT consistently enhances the performance of strong CIL
baselines. Specifically, on CIFAR-100, IVT improves the last accuracy of the
PASS baseline by +5.12% and reduces forgetting by 2.54%. For the
CLIP-pre-trained SLCA baseline on FGVCAircraft, IVT yields gains of +14.93% in
average accuracy and +21.95% in last accuracy. The code will be released.

</details>


### [140] [Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching](https://arxiv.org/abs/2509.21912)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: cs.LG

TL;DR: A novel guidance framework for discrete data that provides exact transition rates for posterior sampling, improving efficiency over existing Taylor approximation methods.


<details>
  <summary>Details</summary>
Motivation: Existing guidance approaches for discrete data use first-order Taylor approximations which can have large errors in discrete state spaces, limiting their effectiveness.

Method: Derives exact transition rates for desired distributions using learned discrete flow matching models, enabling single forward pass sampling that works with masked diffusion models.

Result: Demonstrated effectiveness on energy-guided simulations and preference alignment tasks for text-to-image generation and multimodal understanding.

Conclusion: The proposed framework provides efficient and accurate guidance for discrete data, unifying existing methods and enabling seamless application to various discrete generative models.

Abstract: Guidance provides a simple and effective framework for posterior sampling by
steering the generation process towards the desired distribution. When modeling
discrete data, existing approaches mostly focus on guidance with the
first-order Taylor approximation to improve the sampling efficiency. However,
such an approximation is inappropriate in discrete state spaces since the
approximation error could be large. A novel guidance framework for discrete
data is proposed to address this problem: We derive the exact transition rate
for the desired distribution given a learned discrete flow matching model,
leading to guidance that only requires a single forward pass in each sampling
step, significantly improving efficiency. This unified novel framework is
general enough, encompassing existing guidance methods as special cases, and it
can also be seamlessly applied to the masked diffusion model. We demonstrate
the effectiveness of our proposed guidance on energy-guided simulations and
preference alignment on text-to-image generation and multimodal understanding
tasks. The code is available through
https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main.

</details>


### [141] [Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization](https://arxiv.org/abs/2509.22161)
*Takashi Morita*

Main category: cs.LG

TL;DR: The paper introduces a novel regularization method for smoothed vector quantization that simultaneously ensures tight approximation to onehot vectors and prevents code collapse by minimizing distances between simplex vertices and their K-nearest smoothed quantizers.


<details>
  <summary>Details</summary>
Motivation: Vector quantization is widely used but faces the fundamental challenge of non-differentiable quantization blocking gradient backpropagation. Existing smoothed quantization methods address approximation tightness and code utilization separately, but lack a unified approach.

Method: Proposes a simple regularization that minimizes the distance between each simplex vertex and its K-nearest smoothed quantizers, promoting both tight approximation (close to onehot vectors) and full codebook utilization simultaneously.

Result: Experiments on discrete image autoencoding and contrastive speech representation learning show improved codebook utilization and better performance compared to prior methods.

Conclusion: The proposed regularization effectively addresses both key requirements of smoothed vector quantization - tight approximation and code utilization - in a unified manner, leading to more reliable performance across different applications.

Abstract: Vector quantization, which discretizes a continuous vector space into a
finite set of representative vectors (a codebook), has been widely adopted in
modern machine learning. Despite its effectiveness, vector quantization poses a
fundamental challenge: the non-differentiable quantization step blocks gradient
backpropagation. Smoothed vector quantization addresses this issue by relaxing
the hard assignment of a codebook vector into a weighted combination of
codebook entries, represented as the matrix product of a simplex vector and the
codebook. Effective smoothing requires two properties: (1) smoothed quantizers
should remain close to a onehot vector, ensuring tight approximation, and (2)
all codebook entries should be utilized, preventing code collapse. Existing
methods typically address these desiderata separately. By contrast, the present
study introduces a simple and intuitive regularization that promotes both
simultaneously by minimizing the distance between each simplex vertex and its
$K$-nearest smoothed quantizers. Experiments on representative benchmarks,
including discrete image autoencoding and contrastive speech representation
learning, demonstrate that the proposed method achieves more reliable codebook
utilization and improves performance compared to prior approaches.

</details>


### [142] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: MACMs combine multiplicative and additive components to improve both interpretability and predictive performance over GAMs and CESR, enabling visualization of shape functions while incorporating all feature interactions.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between interpretability and predictive performance in machine learning for high-stakes fields like healthcare, where GAMs sacrifice interaction effects for interpretability and CESR fails to outperform GAMs despite incorporating interactions.

Method: Introduce Multiplicative-Additive Constrained Models (MACMs) that augment CESR with an additive component to disentangle interactive and independent feature coefficients, effectively expanding the hypothesis space while maintaining visualizable shape functions.

Result: Neural network-based MACMs significantly outperform both CESR and state-of-the-art GAMs in predictive performance while maintaining interpretability through visualizable shape functions.

Conclusion: MACMs successfully bridge the gap between interpretability and performance by combining multiplicative and additive components, offering superior predictive capability over existing interpretable models while preserving visualization capabilities for feature effect interpretation.

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [143] [Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs](https://arxiv.org/abs/2509.22166)
*Shirin Alanova,Kristina Kazistova,Ekaterina Galaeva,Alina Kostromina,Vladimir Smirnov,Redko Dmitry,Alexey Dontsov,Maxim Zhelnin,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.LG

TL;DR: This paper presents a comprehensive analysis of post-training N:M activation pruning for LLMs, showing it preserves generative capabilities better than weight pruning at equivalent sparsity levels, with 8:16 pattern emerging as the optimal hardware-friendly approach.


<details>
  <summary>Details</summary>
Motivation: The demand for efficient LLM inference has intensified focus on sparsification techniques. While semi-structured (N:M) pruning is established for weights, activation pruning remains underexplored despite its potential for dynamic, input-adaptive compression and I/O overhead reduction.

Method: Comprehensive analysis of post-training N:M activation pruning methods across multiple LLMs, evaluating lightweight plug-and-play error mitigation techniques and pruning criteria with minimal calibration requirements.

Result: Activation pruning enables superior preservation of generative capabilities compared to weight pruning at equivalent sparsity levels. The 16:32 pattern achieves performance nearly on par with unstructured sparsity, but 8:16 pattern is identified as the superior candidate considering flexibility vs hardware complexity trade-off.

Conclusion: The findings provide effective practical methods for activation pruning and motivate future hardware to support more flexible sparsity patterns. The 8:16 pattern is recommended as the optimal balance between performance and hardware implementation complexity.

Abstract: The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-established for weights, its application to activation pruning remains
underexplored despite its potential for dynamic, input-adaptive compression and
reductions in I/O overhead. This work presents a comprehensive analysis of
methods for post-training N:M activation pruning in LLMs. Across multiple LLMs,
we demonstrate that pruning activations enables superior preservation of
generative capabilities compared to weight pruning at equivalent sparsity
levels. We evaluate lightweight, plug-and-play error mitigation techniques and
pruning criteria, establishing strong hardware-friendly baselines that require
minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's
standard 2:4, showing that the 16:32 pattern achieves performance nearly on par
with unstructured sparsity. However, considering the trade-off between
flexibility and hardware implementation complexity, we focus on the 8:16
pattern as a superior candidate. Our findings provide both effective practical
methods for activation pruning and a motivation for future hardware to support
more flexible sparsity patterns. Our code is available
https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .

</details>


### [144] [Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead](https://arxiv.org/abs/2509.22174)
*Durgesh Kalwar,Mayank Baranwal,Harshad Khadilkar*

Main category: cs.LG

TL;DR: DYNAWEIGHT is a novel dynamic weight aggregation framework for decentralized learning that accelerates training by assigning weights based on relative losses, favoring servers with diverse information.


<details>
  <summary>Details</summary>
Motivation: Distributed learning is crucial for privacy and computational efficiency in decentralized infrastructures where local processing is needed due to lack of centralized aggregation.

Method: DYNAWEIGHT dynamically allocates weights to neighboring servers based on their relative losses on local datasets, unlike traditional static weight assignments like Metropolis weights.

Result: Experiments on MNIST, CIFAR10, and CIFAR100 datasets show notable training speed improvements across various server counts and graph topologies, with minimal communication and memory overhead.

Conclusion: DYNAWEIGHT serves as a versatile aggregation scheme compatible with any server-level optimization algorithm, showing potential for widespread integration in decentralized learning systems.

Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital
tool, not only fortifying privacy measures but also streamlining computational
operations. This becomes especially crucial within fully decentralized
infrastructures where local processing is imperative due to the absence of
centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to
information aggregation in multi-agent networks. DYNAWEIGHT offers substantial
acceleration in decentralized learning with minimal additional communication
and memory overhead. Unlike traditional static weight assignments, such as
Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring
servers based on their relative losses on local datasets. Consequently, it
favors servers possessing diverse information, particularly in scenarios of
substantial data heterogeneity. Our experiments on various datasets MNIST,
CIFAR10, and CIFAR100 incorporating various server counts and graph topologies,
demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT
functions as an aggregation scheme compatible with any underlying server-level
optimization algorithm, underscoring its versatility and potential for
widespread integration.

</details>


### [145] [Extracting Actionable Insights from Building Energy Data using Vision LLMs on Wavelet and 3D Recurrence Representations](https://arxiv.org/abs/2509.21934)
*Amine Bechar,Adel Oulefki,Abbes Amira,Fatih Kurogollu,Yassine Himeur*

Main category: cs.LG

TL;DR: A framework that fine-tunes visual language large models (VLLMs) on 3D graphical representations of building energy time-series data for anomaly detection and energy efficiency recommendations.


<details>
  <summary>Details</summary>
Motivation: Complex building time-series analysis is challenging due to nonlinear and multi-scale characteristics of energy data, requiring better methods for actionable insights.

Method: Convert 1D time-series into 3D representations using continuous wavelet transforms (CWTs) and recurrence plots (RPs), then fine-tune VLLMs on these visual encodings to interpret energy patterns.

Result: Fine-tuned VLLMs successfully monitor building states, identify anomalies, and generate optimization recommendations. Idefics-7B achieves validation losses of 0.0952 (CWTs) and 0.1064 (RPs), outperforming raw time-series fine-tuning (0.1176).

Conclusion: This work bridges time-series analysis and visualization, providing a scalable and interpretable framework for energy analytics using visual representations.

Abstract: The analysis of complex building time-series for actionable insights and
recommendations remains challenging due to the nonlinear and multi-scale
characteristics of energy data. To address this, we propose a framework that
fine-tunes visual language large models (VLLMs) on 3D graphical representations
of the data. The approach converts 1D time-series into 3D representations using
continuous wavelet transforms (CWTs) and recurrence plots (RPs), which capture
temporal dynamics and localize frequency anomalies. These 3D encodings enable
VLLMs to visually interpret energy-consumption patterns, detect anomalies, and
provide recommendations for energy efficiency. We demonstrate the framework on
real-world building-energy datasets, where fine-tuned VLLMs successfully
monitor building states, identify recurring anomalies, and generate
optimization recommendations. Quantitatively, the Idefics-7B VLLM achieves
validation losses of 0.0952 with CWTs and 0.1064 with RPs on the University of
Sharjah energy dataset, outperforming direct fine-tuning on raw time-series
data (0.1176) for anomaly detection. This work bridges time-series analysis and
visualization, providing a scalable and interpretable framework for energy
analytics.

</details>


### [146] [Learning Equivariant Functions via Quadratic Forms](https://arxiv.org/abs/2509.22184)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: A method for learning group equivariant functions by discovering the underlying quadratic form from data, leveraging orthogonal group properties to build efficient neural networks with appropriate inductive biases.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that can automatically discover underlying symmetry groups in data and build equivariant models that are both simplified and efficient, addressing the challenge of learning group equivariant functions when the symmetry group is unknown.

Method: Learn the quadratic form x^T A x corresponding to the group from data, leverage orthogonal group properties to uncover symmetry, use the unique symmetric matrix and its diagonal form to incorporate inductive biases, and decompose equivariant functions into norm-invariant and scale-invariant components.

Result: The framework successfully discovers underlying symmetries and learns equivariant functions across multiple tasks including polynomial regression, top quark tagging, and moment of inertia matrix prediction, outperforming baseline methods.

Conclusion: The proposed approach provides an effective framework for automatically discovering symmetry groups and learning corresponding equivariant functions, demonstrating consistent superiority over baseline methods in both symmetry discovery and function learning efficiency.

Abstract: In this study, we introduce a method for learning group (known or unknown)
equivariant functions by learning the associated quadratic form $x^T A x$
corresponding to the group from the data. Certain groups, known as orthogonal
groups, preserve a specific quadratic form, and we leverage this property to
uncover the underlying symmetry group under the assumption that it is
orthogonal. By utilizing the corresponding unique symmetric matrix and its
inherent diagonal form, we incorporate suitable inductive biases into the
neural network architecture, leading to models that are both simplified and
efficient. Our approach results in an invariant model that preserves norms,
while the equivariant model is represented as a product of a norm-invariant
model and a scale-invariant model, where the ``product'' refers to the group
action.
  Moreover, we extend our framework to a more general setting where the
function acts on tuples of input vectors via a diagonal (or product) group
action. In this extension, the equivariant function is decomposed into an
angular component extracted solely from the normalized first vector and a
scale-invariant component that depends on the full Gram matrix of the tuple.
This decomposition captures the inter-dependencies between multiple inputs
while preserving the underlying group symmetry.
  We assess the effectiveness of our framework across multiple tasks, including
polynomial regression, top quark tagging, and moment of inertia matrix
prediction. Comparative analysis with baseline methods demonstrates that our
model consistently excels in both discovering the underlying symmetry and
efficiently learning the corresponding equivariant function.

</details>


### [147] [Statistical Advantage of Softmax Attention: Insights from Single-Location Regression](https://arxiv.org/abs/2509.21936)
*O. Duranthon,P. Marion,C. Boyer,B. Loureiro,L. Zdeborová*

Main category: cs.LG

TL;DR: This paper analyzes why softmax attention outperforms linear attention in large language models through theoretical analysis of single-location regression tasks, showing softmax achieves Bayes risk at population level while linear attention fundamentally fails.


<details>
  <summary>Details</summary>
Motivation: To understand why softmax dominates over alternative attention mechanisms in LLMs, as current theoretical works focus on easier-to-analyze linearized attention but the superiority of softmax remains poorly understood.

Method: Developed theoretical analysis using statistical physics approaches to study attention-based predictors in high-dimensional limit, examining single-location regression task where output depends on linear transformation of single input token at random location.

Result: At population level, softmax achieves Bayes risk while linear attention fundamentally falls short. In finite-sample regime, softmax consistently outperforms linear attention though no longer Bayes-optimal. Identified key properties needed for optimal performance.

Conclusion: Softmax attention's superiority is theoretically justified - it achieves optimal performance at population level and consistently outperforms linear attention in practical finite-sample settings, with connections to gradient-based optimization.

Abstract: Large language models rely on attention mechanisms with a softmax activation.
Yet the dominance of softmax over alternatives (e.g., component-wise or linear)
remains poorly understood, and many theoretical works have focused on the
easier-to-analyze linearized attention. In this work, we address this gap
through a principled study of the single-location regression task, where the
output depends on a linear transformation of a single input token at a random
location. Building on ideas from statistical physics, we develop an analysis of
attention-based predictors in the high-dimensional limit, where generalization
performance is captured by a small set of order parameters. At the population
level, we show that softmax achieves the Bayes risk, whereas linear attention
fundamentally falls short. We then examine other activation functions to
identify which properties are necessary for optimal performance. Finally, we
analyze the finite-sample regime: we provide an asymptotic characterization of
the test error and show that, while softmax is no longer Bayes-optimal, it
consistently outperforms linear attention. We discuss the connection with
optimization by gradient-based algorithms.

</details>


### [148] [Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics](https://arxiv.org/abs/2509.22207)
*Mu Huang,Linning Xu,Mingyue Dai,Yidi Shao,Bo Dai*

Main category: cs.LG

TL;DR: R-GNS is a reversible graph network simulator that unifies forward and inverse simulation for dissipative fluid systems, achieving high accuracy and efficiency without iterative optimization.


<details>
  <summary>Details</summary>
Motivation: Inverse inference in dissipative fluid systems is challenging due to irreversible dynamics, slow optimization-based solvers, and convergence issues. Existing neural simulators struggle with accurate backward dynamics approximation.

Method: Proposes a mathematically invertible design using residual reversible message passing with shared parameters in a single graph architecture, coupling forward dynamics with inverse inference without reversing the underlying physics.

Result: Achieves higher accuracy and consistency with 1/4 parameters, performs inverse inference 100x faster than optimization baselines, matches GNS speed in forward simulation, and eliminates iterative optimization in goal-conditioned tasks with orders-of-magnitude speedups.

Conclusion: R-GNS is the first reversible framework that successfully unifies forward and inverse simulation for dissipative fluid systems, demonstrating efficient and physically consistent trajectory generation toward complex target shapes.

Abstract: Simulating physically plausible trajectories toward user-defined goals is a
fundamental yet challenging task in fluid dynamics. While particle-based
simulators can efficiently reproduce forward dynamics, inverse inference
remains difficult, especially in dissipative systems where dynamics are
irreversible and optimization-based solvers are slow, unstable, and often fail
to converge. In this work, we introduce the Reversible Graph Network Simulator
(R-GNS), a unified framework that enforces bidirectional consistency within a
single graph architecture. Unlike prior neural simulators that approximate
inverse dynamics by fitting backward data, R-GNS does not attempt to reverse
the underlying physics. Instead, we propose a mathematically invertible design
based on residual reversible message passing with shared parameters, coupling
forward dynamics with inverse inference to deliver accurate predictions and
efficient recovery of plausible initial states. Experiments on three
dissipative benchmarks (Water-3D, WaterRamps, and WaterDrop) show that R-GNS
achieves higher accuracy and consistency with only one quarter of the
parameters, and performs inverse inference more than 100 times faster than
optimization-based baselines. For forward simulation, R-GNS matches the speed
of strong GNS baselines, while in goal-conditioned tasks it eliminates
iterative optimization and achieves orders-of-magnitude speedups. On
goal-conditioned tasks, R-GNS further demonstrates its ability to complex
target shapes (e.g., characters "L" and "N") through vivid, physically
consistent trajectories. To our knowledge, this is the first reversible
framework that unifies forward and inverse simulation for dissipative fluid
systems.

</details>


### [149] [Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning](https://arxiv.org/abs/2509.21942)
*Xianghua Zeng,Hao Peng,Angsheng Li,Yicheng Pan*

Main category: cs.LG

TL;DR: SIHD is a structural information-based hierarchical diffusion framework for offline RL that adaptively constructs diffusion hierarchies across multiple temporal scales, uses structural information gain as conditioning, and includes entropy regularization to improve exploration and avoid distributional shift issues.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical diffusion methods for offline RL assume fixed two-layer hierarchies with single predefined temporal scales, limiting adaptability to diverse tasks and reducing decision-making flexibility.

Method: Analyzes structural information in offline trajectories to construct adaptive diffusion hierarchies across multiple temporal scales, uses structural information gain as conditioning signal instead of reward predictions, and introduces structural entropy regularizer to encourage exploration while avoiding extrapolation errors.

Result: Extensive evaluations show SIHD significantly outperforms state-of-the-art baselines in decision-making performance and demonstrates superior generalization across diverse scenarios.

Conclusion: SIHD provides an effective and stable framework for offline policy learning in long-horizon environments with sparse rewards by leveraging structural information for adaptive hierarchical diffusion and improved exploration.

Abstract: Diffusion-based generative methods have shown promising potential for
modeling trajectories from offline reinforcement learning (RL) datasets, and
hierarchical diffusion has been introduced to mitigate variance accumulation
and computational challenges in long-horizon planning tasks. However, existing
approaches typically assume a fixed two-layer diffusion hierarchy with a single
predefined temporal scale, which limits adaptability to diverse downstream
tasks and reduces flexibility in decision making. In this work, we propose
SIHD, a novel Structural Information-based Hierarchical Diffusion framework for
effective and stable offline policy learning in long-horizon environments with
sparse rewards. Specifically, we analyze structural information embedded in
offline trajectories to construct the diffusion hierarchy adaptively, enabling
flexible trajectory modeling across multiple temporal scales. Rather than
relying on reward predictions from localized sub-trajectories, we quantify the
structural information gain of each state community and use it as a
conditioning signal within the corresponding diffusion layer. To reduce
overreliance on offline datasets, we introduce a structural entropy regularizer
that encourages exploration of underrepresented states while avoiding
extrapolation errors from distributional shifts. Extensive evaluations on
challenging offline RL tasks show that SIHD significantly outperforms
state-of-the-art baselines in decision-making performance and demonstrates
superior generalization across diverse scenarios.

</details>


### [150] [Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models](https://arxiv.org/abs/2509.21960)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: cs.LG

TL;DR: Proposes a difficulty-adaptive reasoning method for Large Audio Language Models that dynamically adjusts reasoning depth based on problem complexity, improving both performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LALMs use a "one-size-fits-all" reasoning depth, causing redundant overthinking for simple questions and insufficient reasoning for complex ones. The paper aims to enable smart reasoning by adapting depth to problem complexity.

Method: Proposes a reward function that dynamically links reasoning length to the model's perceived problem difficulty, encouraging shorter reasoning for easy tasks and more elaborate reasoning for complex ones.

Result: Extensive experiments show the method is both effective and efficient, simultaneously improving task performance and significantly reducing average reasoning length.

Conclusion: The difficulty-adaptive reasoning approach successfully enables LALMs to reason smartly by adapting to problem complexity, with analysis providing valuable insights for future work.

Abstract: Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT)
paradigm, have shown remarkable reasoning capabilities. Intuitively, different
problems often require varying depths of reasoning. While some methods can
determine whether to reason for a given problem, they typically lack a
fine-grained mechanism to modulate how much to reason. This often results in a
``one-size-fits-all'' reasoning depth, which generates redundant overthinking
for simple questions while failing to allocate sufficient thought to complex
ones. In this paper, we conduct an in-depth analysis of LALMs and find that an
effective and efficient LALM should reason smartly by adapting its reasoning
depth to the problem's complexity. To achieve this, we propose a
difficulty-adaptive reasoning method for LALMs. Specifically, we propose a
reward function that dynamically links reasoning length to the model's
perceived problem difficulty. This reward encourages shorter, concise reasoning
for easy tasks and more elaborate, in-depth reasoning for complex ones.
Extensive experiments demonstrate that our method is both effective and
efficient, simultaneously improving task performance and significantly reducing
the average reasoning length. Further analysis on reasoning structure paradigm
offers valuable insights for future work.

</details>


### [151] [Automatic Discovery of One Parameter Subgroups of $SO(n)$](https://arxiv.org/abs/2509.22219)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: A framework for automatic discovery of one-parameter subgroups of SO(n) using Jordan forms of skew-symmetric matrices, enabling standardized representations of invariant functions and recovering meaningful symmetry structures.


<details>
  <summary>Details</summary>
Motivation: One-parameter subgroups of SO(n) are crucial in robotics, quantum mechanics, and molecular structure analysis, but their automatic discovery and representation remain challenging.

Method: Utilizes standard Jordan form of skew-symmetric matrices (Lie algebra of SO(n)) to establish canonical form for orbits under subgroup action, then derives standardized representation for invariant functions to learn subgroup parameters.

Result: Successfully recovers meaningful subgroup structure in applications including double pendulum modeling, moment of inertia prediction, top quark tagging, and invariant polynomial regression.

Conclusion: The framework produces interpretable, symmetry-aware representations and effectively discovers underlying one-parameter subgroups across various domains.

Abstract: We introduce a novel framework for the automatic discovery of one-parameter
subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter
subgroups of $SO(n)$ are crucial in a wide range of applications, including
robotics, quantum mechanics, and molecular structure analysis. Our method
utilizes the standard Jordan form of skew-symmetric matrices, which define the
Lie algebra of $SO(n)$, to establish a canonical form for orbits under the
action of $H_{\gamma}$. This canonical form is then employed to derive a
standardized representation for $H_{\gamma}$-invariant functions. By learning
the appropriate parameters, the framework uncovers the underlying one-parameter
subgroup $H_{\gamma}$. The effectiveness of the proposed approach is
demonstrated through tasks such as double pendulum modeling, moment of inertia
prediction, top quark tagging and invariant polynomial regression, where it
successfully recovers meaningful subgroup structure and produces interpretable,
symmetry-aware representations.

</details>


### [152] [GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction](https://arxiv.org/abs/2509.21971)
*Feng Jiang,Amina Mollaysa,Hehuan Ma,Tommaso Mansi,Junzhou Huang,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: GRAMDTI is a pretraining framework for drug-target interaction prediction that integrates multimodal molecular and protein data using volume-based contrastive learning across four modalities, with adaptive modality dropout and IC50 weak supervision.


<details>
  <summary>Details</summary>
Motivation: Existing DTI prediction approaches primarily use SMILES-protein pairs and fail to leverage rich multimodal information available for small molecules and proteins, limiting their predictive power.

Method: GRAMDTI integrates multimodal molecular and protein inputs using volume-based contrastive learning extended to four modalities, with adaptive modality dropout to handle modality informativeness and IC50 activity measurements as weak supervision.

Result: Experiments on four publicly available datasets show GRAMDTI consistently outperforms state-of-the-art baselines in DTI prediction.

Conclusion: The results demonstrate the benefits of higher-order multimodal alignment, adaptive modality utilization, and auxiliary supervision for robust and generalizable DTI prediction.

Abstract: Drug target interaction (DTI) prediction is a cornerstone of computational
drug discovery, enabling rational design, repurposing, and mechanistic
insights. While deep learning has advanced DTI modeling, existing approaches
primarily rely on SMILES protein pairs and fail to exploit the rich multimodal
information available for small molecules and proteins. We introduce GRAMDTI, a
pretraining framework that integrates multimodal molecular and protein inputs
into unified representations. GRAMDTI extends volume based contrastive learning
to four modalities, capturing higher-order semantic alignment beyond
conventional pairwise approaches. To handle modality informativeness, we
propose adaptive modality dropout, dynamically regulating each modality's
contribution during pre-training. Additionally, IC50 activity measurements,
when available, are incorporated as weak supervision to ground representations
in biologically meaningful interaction strengths. Experiments on four publicly
available datasets demonstrate that GRAMDTI consistently outperforms state of
the art baselines. Our results highlight the benefits of higher order
multimodal alignment, adaptive modality utilization, and auxiliary supervision
for robust and generalizable DTI prediction.

</details>


### [153] [Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making](https://arxiv.org/abs/2509.22232)
*Alexandra Cimpean,Nicole Orzan,Catholijn Jonker,Pieter Libin,Ann Nowé*

Main category: cs.LG

TL;DR: A framework for exploring performance-fairness trade-offs in sequential decision problems using extended Markov decision processes (fMDP) that explicitly encode individuals and groups.


<details>
  <summary>Details</summary>
Motivation: Need for algorithms that can make transparent trade-offs between performance and fairness in real-world sequential decision problems, where the desired trade-off is hard to specify beforehand.

Method: Proposed fMDP (extended Markov decision process) that explicitly encodes individuals and groups, formalizes fairness notions in sequential contexts, and computes fairness measures over time. Evaluated in job hiring and fraud detection scenarios.

Result: Framework learns policies that are more fair across multiple scenarios with only minor performance loss. Shows that group and individual fairness notions don't necessarily imply each other.

Conclusion: Provides guidelines for applying the framework across different problem settings, highlighting its benefit when both group and individual fairness are desired.

Abstract: Equity in real-world sequential decision problems can be enforced using
fairness-aware methods. Therefore, we require algorithms that can make suitable
and transparent trade-offs between performance and the desired fairness
notions. As the desired performance-fairness trade-off is hard to specify a
priori, we propose a framework where multiple trade-offs can be explored.
Insights provided by the reinforcement learning algorithm regarding the
obtainable performance-fairness trade-offs can then guide stakeholders in
selecting the most appropriate policy. To capture fairness, we propose an
extended Markov decision process, $f$MDP, that explicitly encodes individuals
and groups. Given this $f$MDP, we formalise fairness notions in the context of
sequential decision problems and formulate a fairness framework that computes
fairness measures over time. We evaluate our framework in two scenarios with
distinct fairness requirements: job hiring, where strong teams must be composed
while treating applicants equally, and fraud detection, where fraudulent
transactions must be detected while ensuring the burden on customers is fairly
distributed. We show that our framework learns policies that are more fair
across multiple scenarios, with only minor loss in performance reward.
Moreover, we observe that group and individual fairness notions do not
necessarily imply one another, highlighting the benefit of our framework in
settings where both fairness types are desired. Finally, we provide guidelines
on how to apply this framework across different problem settings.

</details>


### [154] [Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models](https://arxiv.org/abs/2509.22007)
*Cheng Jin,Qitan Shi,Yuantao Gu*

Main category: cs.LG

TL;DR: CFG improves conditional fidelity in diffusion models but reduces diversity. The sampling process has three stages: Direction Shift (accelerates toward weighted mean), Mode Separation (suppresses weaker modes), and Concentration (diminishes fine-grained variability). Stronger guidance improves alignment but reduces diversity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Classifier-Free Guidance (CFG) on sampling dynamics in diffusion models, particularly under multimodal conditional distributions, as prior studies provided only partial understanding.

Method: Analyzed CFG under multimodal conditionals and identified three successive sampling stages: Direction Shift, Mode Separation, and Concentration. Proposed a time-varying guidance schedule based on theoretical insights.

Result: Experiments confirmed that early strong guidance erodes global diversity while late strong guidance suppresses fine-grained variation. The proposed time-varying guidance schedule consistently improved both quality and diversity.

Conclusion: CFG's sampling process unfolds in three stages that explain the trade-off between semantic alignment and diversity. A time-varying guidance schedule can mitigate this trade-off and improve both quality and diversity.

Abstract: Classifier-Free Guidance (CFG) is widely used to improve conditional fidelity
in diffusion models, but its impact on sampling dynamics remains poorly
understood. Prior studies, often restricted to unimodal conditional
distributions or simplified cases, provide only a partial picture. We analyze
CFG under multimodal conditionals and show that the sampling process unfolds in
three successive stages. In the Direction Shift stage, guidance accelerates
movement toward the weighted mean, introducing initialization bias and norm
growth. In the Mode Separation stage, local dynamics remain largely neutral,
but the inherited bias suppresses weaker modes, reducing global diversity. In
the Concentration stage, guidance amplifies within-mode contraction,
diminishing fine-grained variability. This unified view explains a widely
observed phenomenon: stronger guidance improves semantic alignment but
inevitably reduces diversity. Experiments support these predictions, showing
that early strong guidance erodes global diversity, while late strong guidance
suppresses fine-grained variation. Moreover, our theory naturally suggests a
time-varying guidance schedule, and empirical results confirm that it
consistently improves both quality and diversity.

</details>


### [155] [ASSESS: A Semantic and Structural Evaluation Framework for Statement Similarity](https://arxiv.org/abs/2509.22246)
*Xiaoyang Liu,Tao Zhu,Zineng Dong,Yuntian Liu,Qingfeng Guo,Zhaoxuan Liu,Yu Chen,Tao Luo*

Main category: cs.LG

TL;DR: ASSESS is a new framework that combines semantic and structural information to evaluate formal statement similarity, addressing limitations of existing metrics that either focus only on syntax or semantic equivalence without graded scores.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for formal statement similarity fail to balance semantic and structural information - string-based methods ignore semantics while proof-based methods lack graded similarity scores when proofs fail.

Method: The framework transforms formal statements into Operator Trees and computes similarity using TransTED (Transformation Tree Edit Distance) Similarity, which enhances traditional Tree Edit Distance with semantic awareness through transformations.

Result: Experiments on the new EPLA benchmark (524 expert-annotated formal statement pairs) show that TransTED Similarity outperforms existing methods, achieving state-of-the-art accuracy and highest Kappa coefficient.

Conclusion: ASSESS provides a comprehensive evaluation framework that successfully integrates semantic and structural information for formal statement similarity assessment, demonstrating superior performance over existing approaches.

Abstract: Statement autoformalization, the automated translation of statements from
natural language into formal languages, has seen significant advancements, yet
the development of automated evaluation metrics remains limited. Existing
metrics for formal statement similarity often fail to balance semantic and
structural information. String-based approaches capture syntactic structure but
ignore semantic meaning, whereas proof-based methods validate semantic
equivalence but disregard structural nuances and, critically, provide no graded
similarity score in the event of proof failure. To address these issues, we
introduce ASSESS (A Semantic and Structural Evaluation Framework for Statement
Similarity), which comprehensively integrates semantic and structural
information to provide a continuous similarity score. Our framework first
transforms formal statements into Operator Trees to capture their syntactic
structure and then computes a similarity score using our novel TransTED
(Transformation Tree Edit Distance) Similarity metric, which enhances
traditional Tree Edit Distance by incorporating semantic awareness through
transformations. For rigorous validation, we present EPLA (Evaluating
Provability and Likeness for Autoformalization), a new benchmark of 524
expert-annotated formal statement pairs derived from miniF2F and ProofNet, with
labels for both semantic provability and structural likeness. Experiments on
EPLA demonstrate that TransTED Similarity outperforms existing methods,
achieving state-of-the-art accuracy and the highest Kappa coefficient. The
benchmark, and implementation code will be made public soon.

</details>


### [156] [Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning](https://arxiv.org/abs/2509.22008)
*Yajie Qi,Wei Wei,Lin Li,Lijun Zhang,Zhidong Gao,Da Wang,Huizhong Song*

Main category: cs.LG

TL;DR: SGRL is a structured goal-guided RL method that uses LLMs to generate prioritized goals and prune actions, achieving superior performance on Crafter and Craftax-Classic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Real-world decision-making in complex environments challenges RL agents' exploration and planning. Existing LLM-enhanced RL methods suffer from frequent costly LLM calls and semantic mismatch issues.

Method: Uses structured goal planner with LLMs to generate reusable prioritized goal functions, and goal-conditioned action pruner with action masking to filter misaligned actions.

Result: Experimental results on Crafter and Craftax-Classic show SGRL achieves superior performance compared to state-of-the-art methods.

Conclusion: SGRL effectively integrates LLM guidance with RL through structured goal planning and action pruning, enabling efficient exploration in complex environments.

Abstract: Real-world decision-making tasks typically occur in complex and open
environments, posing significant challenges to reinforcement learning (RL)
agents' exploration efficiency and long-horizon planning capabilities. A
promising approach is LLM-enhanced RL, which leverages the rich prior knowledge
and strong planning capabilities of LLMs to guide RL agents in efficient
exploration. However, existing methods mostly rely on frequent and costly LLM
invocations and suffer from limited performance due to the semantic mismatch.
In this paper, we introduce a Structured Goal-guided Reinforcement Learning
(SGRL) method that integrates a structured goal planner and a goal-conditioned
action pruner to guide RL agents toward efficient exploration. Specifically,
the structured goal planner utilizes LLMs to generate a reusable, structured
function for goal generation, in which goals are prioritized. Furthermore, by
utilizing LLMs to determine goals' priority weights, it dynamically generates
forward-looking goals to guide the agent's policy toward more promising
decision-making trajectories. The goal-conditioned action pruner employs an
action masking mechanism that filters out actions misaligned with the current
goal, thereby constraining the RL agent to select goal-consistent policies. We
evaluate the proposed method on Crafter and Craftax-Classic, and experimental
results demonstrate that SGRL achieves superior performance compared to
existing state-of-the-art methods.

</details>


### [157] [Wavelet-Induced Rotary Encodings: RoPE Meets Graphs](https://arxiv.org/abs/2509.22259)
*Isaac Reid,Arijit Sehanobish,Cedrik Höfs,Bruno Mlodozeniec,Leonhard Vulpius,Federico Barbero,Adrian Weller,Krzysztof Choromanski,Richard E. Turner,Petar Veličković*

Main category: cs.LG

TL;DR: WIRE extends Rotary Position Encodings (RoPE) to graph-structured data, offering theoretical advantages like equivariance and compatibility with linear attention, and shows effectiveness in graph-related tasks.


<details>
  <summary>Details</summary>
Motivation: To generalize Rotary Position Encodings (RoPE), commonly used in LLMs and ViTs, to graph-structured data, enabling better handling of graph dependencies and structures.

Method: WIRE uses wavelet-induced rotary encodings to extend RoPE to graphs, ensuring properties like permutation equivariance and compatibility with linear attention, and is tested on synthetic and real-world graph tasks.

Result: WIRE effectively handles graph structures, outperforming in tasks where graph topology is crucial, such as identifying subgraphs and semantic segmentation of point clouds.

Conclusion: WIRE is a versatile and theoretically sound extension of RoPE for graphs, proving useful in graph-based applications and recovering RoPE in grid graph cases.

Abstract: We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary
Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to
graph-structured data. We demonstrate that WIRE is more general than RoPE,
recovering the latter in the special case of grid graphs. WIRE also enjoys a
host of desirable theoretical properties, including equivariance under node
ordering permutation, compatibility with linear attention, and (under select
assumptions) asymptotic dependence on graph resistive distance. We test WIRE on
a range of synthetic and real-world tasks, including identifying monochromatic
subgraphs, semantic segmentation of point clouds, and more standard graph
benchmarks. We find it to be effective in settings where the underlying graph
structure is important.

</details>


### [158] [Concept-SAE: Active Causal Probing of Visual Model Behavior](https://arxiv.org/abs/2509.22015)
*Jianrong Ding,Muxi Chen,Chenchen Zhao,Qiang Xu*

Main category: cs.LG

TL;DR: Concept-SAE introduces a framework that creates semantically grounded concept tokens using hybrid disentanglement, enabling causal probing of model behavior through direct intervention and vulnerability localization.


<details>
  <summary>Details</summary>
Motivation: Standard Sparse Autoencoders (SAEs) produce ambiguous, ungrounded features that are unreliable for active causal probing of model behavior, limiting interpretability.

Method: A novel hybrid disentanglement strategy that forges semantically grounded concept tokens through dual-supervision approach, producing faithful and spatially localized tokens.

Result: The approach outperforms alternative methods in disentanglement, enabling causal probing of internal concepts and systematic localization of adversarial vulnerabilities to specific layers.

Conclusion: Concept-SAE provides a validated blueprint for moving beyond correlational interpretation to mechanistic, causal probing of model behavior.

Abstract: Standard Sparse Autoencoders (SAEs) excel at discovering a dictionary of a
model's learned features, offering a powerful observational lens. However, the
ambiguous and ungrounded nature of these features makes them unreliable
instruments for the active, causal probing of model behavior. To solve this, we
introduce Concept-SAE, a framework that forges semantically grounded concept
tokens through a novel hybrid disentanglement strategy. We first quantitatively
demonstrate that our dual-supervision approach produces tokens that are
remarkably faithful and spatially localized, outperforming alternative methods
in disentanglement. This validated fidelity enables two critical applications:
(1) we probe the causal link between internal concepts and predictions via
direct intervention, and (2) we probe the model's failure modes by
systematically localizing adversarial vulnerabilities to specific layers.
Concept-SAE provides a validated blueprint for moving beyond correlational
interpretation to the mechanistic, causal probing of model behavior.

</details>


### [159] [HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space](https://arxiv.org/abs/2509.22299)
*Ke Li,Zheng Yang,Zhongbin Zhou,Feng Xue,Zhonglin Jiang,Wenxiao Wang*

Main category: cs.LG

TL;DR: HEAPr is a novel pruning algorithm for Mixture-of-Experts LLMs that decomposes experts into atomic experts for fine-grained pruning, using second-order information to achieve nearly lossless compression at 20-25% ratios while reducing FLOPs by 20%.


<details>
  <summary>Details</summary>
Motivation: MoE architectures have high memory requirements due to large parameter counts, and existing expert-level pruning methods cause substantial accuracy degradation due to coarse granularity.

Method: Decomposes experts into atomic experts and uses second-order information (similar to OBS theory) to measure importance, transforming second-order information from expert parameters to atomic expert outputs to reduce space complexity from O(d^4) to O(d^2).

Result: Outperforms existing expert-level pruning methods across various compression ratios and benchmarks, achieving nearly lossless compression at 20-25% ratios while reducing FLOPs by nearly 20% on models like DeepSeek MoE and Qwen MoE.

Conclusion: HEAPr enables more precise and flexible pruning of MoE models with minimal accuracy loss, making them more practical for deployment while maintaining performance.

Abstract: Mixture-of-Experts (MoE) architectures in large language models (LLMs)
deliver exceptional performance and reduced inference costs compared to dense
LLMs. However, their large parameter counts result in prohibitive memory
requirements, limiting practical deployment. While existing pruning methods
primarily focus on expert-level pruning, this coarse granularity often leads to
substantial accuracy degradation. In this work, we introduce HEAPr, a novel
pruning algorithm that decomposes experts into smaller, indivisible atomic
experts, enabling more precise and flexible atomic expert pruning. To measure
the importance of each atomic expert, we leverage second-order information
based on principles similar to Optimal Brain Surgeon (OBS) theory. To address
the computational and storage challenges posed by second-order information,
HEAPr exploits the inherent properties of atomic experts to transform the
second-order information from expert parameters into that of atomic expert
parameters, and further simplifies it to the second-order information of atomic
expert outputs. This approach reduces the space complexity from $O(d^4)$, where
d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward
passes and one backward pass on a small calibration set to compute the
importance of atomic experts. Extensive experiments on MoE models, including
DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing
expert-level pruning methods across a wide range of compression ratios and
benchmarks. Specifically, HEAPr achieves nearly lossless compression at
compression ratios of 20% ~ 25% in most models, while also reducing FLOPs
nearly by 20%. The code can be found at
\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.

</details>


### [160] [AEGIS: Authentic Edge Growth In Sparsity for Link Prediction in Edge-Sparse Bipartite Knowledge Graphs](https://arxiv.org/abs/2509.22017)
*Hugh Xuechen Liu,Kıvanç Tatar*

Main category: cs.LG

TL;DR: AEGIS is an edge-only augmentation framework for sparse bipartite knowledge graphs that resamples existing training edges to improve link prediction without introducing fabricated endpoints.


<details>
  <summary>Details</summary>
Motivation: Bipartite knowledge graphs in niche domains are typically data-poor and edge-sparse, which hinders link prediction performance.

Method: AEGIS resamples existing training edges either uniformly or with inverse-degree bias, preserving original nodes. It uses semantic KNN augmentation and evaluates on naturally sparse graphs and induced-sparsity benchmarks.

Result: On Amazon and MovieLens, semantic KNN augmentation restores AUC and calibration while other methods remain detrimental. On GDP graph, semantic KNN achieves largest AUC improvement and Brier score reduction.

Conclusion: Authenticity-constrained resampling is a data-efficient strategy for sparse bipartite link prediction, with semantic augmentation providing additional boost when informative node descriptions are available.

Abstract: Bipartite knowledge graphs in niche domains are typically data-poor and
edge-sparse, which hinders link prediction. We introduce AEGIS (Authentic Edge
Growth In Sparsity), an edge-only augmentation framework that resamples
existing training edges -either uniformly simple or with inverse-degree bias
degree-aware -thereby preserving the original node set and sidestepping
fabricated endpoints. To probe authenticity across regimes, we consider
naturally sparse graphs (game design pattern's game-pattern network) and induce
sparsity in denser benchmarks (Amazon, MovieLens) via high-rate bond
percolation. We evaluate augmentations on two complementary metrics: AUC-ROC
(higher is better) and the Brier score (lower is better), using two-tailed
paired t-tests against sparse baselines. On Amazon and MovieLens, copy-based
AEGIS variants match the baseline while the semantic KNN augmentation is the
only method that restores AUC and calibration; random and synthetic edges
remain detrimental. On the text-rich GDP graph, semantic KNN achieves the
largest AUC improvement and Brier score reduction, and simple also lowers the
Brier score relative to the sparse control. These findings position
authenticity-constrained resampling as a data-efficient strategy for sparse
bipartite link prediction, with semantic augmentation providing an additional
boost when informative node descriptions are available.

</details>


### [161] [Adaptive Policy Backbone via Shared Network](https://arxiv.org/abs/2509.22310)
*Bumgeun Park,Donghwan Lee*

Main category: cs.LG

TL;DR: APB is a meta-transfer RL method that uses lightweight linear layers around a shared backbone for parameter-efficient fine-tuning, enabling adaptation to out-of-distribution tasks while preserving prior knowledge.


<details>
  <summary>Details</summary>
Motivation: RL typically requires extensive interaction data, and existing methods using priors (datasets or reference policies) degrade under task mismatch, especially in out-of-distribution settings where current meta-RL methods fail.

Method: Insert lightweight linear layers before and after a shared backbone network, enabling parameter-efficient fine-tuning while preserving the backbone's prior knowledge during adaptation to new tasks.

Result: APB improves sample efficiency over standard RL and successfully adapts to out-of-distribution tasks where existing meta-RL baselines typically fail.

Conclusion: The proposed APB framework effectively addresses the challenge of task mismatch in RL by enabling efficient adaptation to out-of-distribution tasks while maintaining sample efficiency.

Abstract: Reinforcement learning (RL) has achieved impressive results across domains,
yet learning an optimal policy typically requires extensive interaction data,
limiting practical deployment. A common remedy is to leverage priors, such as
pre-collected datasets or reference policies, but their utility degrades under
task mismatch between training and deployment. While prior work has sought to
address this mismatch, it has largely been restricted to in-distribution
settings. To address this challenge, we propose Adaptive Policy Backbone (APB),
a meta-transfer RL method that inserts lightweight linear layers before and
after a shared backbone, thereby enabling parameter-efficient fine-tuning
(PEFT) while preserving prior knowledge during adaptation. Our results show
that APB improves sample efficiency over standard RL and adapts to
out-of-distribution (OOD) tasks where existing meta-RL baselines typically
fail.

</details>


### [162] [Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models](https://arxiv.org/abs/2509.22020)
*Shilei Cao,Hehai Lin,Jiashun Cheng,Yang Liu,Guowen Li,Xuehe Wang,Juepeng Zheng,Haoyuan Liang,Meng Jin,Chengwei Qin,Hong Cheng,Haohuan Fu*

Main category: cs.LG

TL;DR: WeatherPEFT is a parameter-efficient fine-tuning framework for Weather Foundation Models that addresses unique weather task challenges through dynamic prompting and adaptive parameter selection, achieving full-tuning performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current PEFT methods designed for vision/language tasks fail to handle weather-specific challenges like variable heterogeneity, resolution diversity, and spatiotemporal coverage variations, leading to suboptimal performance when applied to weather foundation models.

Method: WeatherPEFT introduces two innovations: Task-Adaptive Dynamic Prompting (TADP) for context-aware feature recalibration during forward pass, and Stochastic Fisher-Guided Adaptive Selection (SFAS) for identifying and updating critical parameters while preserving pre-trained knowledge during backpropagation.

Result: WeatherPEFT achieves performance parity with Full-Tuning on three downstream weather tasks while using fewer trainable parameters, overcoming the significant performance gaps of existing PEFT methods.

Conclusion: WeatherPEFT provides an effective and efficient fine-tuning solution for Weather Foundation Models that addresses domain-specific challenges while maintaining performance comparable to full fine-tuning with reduced computational requirements.

Abstract: While recent advances in machine learning have equipped Weather Foundation
Models (WFMs) with substantial generalization capabilities across diverse
downstream tasks, the escalating computational requirements associated with
their expanding scale increasingly hinder practical deployment. Current
Parameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or language
tasks, fail to address the unique challenges of weather downstream tasks, such
as variable heterogeneity, resolution diversity, and spatiotemporal coverage
variations, leading to suboptimal performance when applied to WFMs. To bridge
this gap, we introduce WeatherPEFT, a novel PEFT framework for WFMs
incorporating two synergistic innovations. First, during the forward pass,
Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embedding
weights within the encoder to the input tokens of the pre-trained backbone via
internal and external pattern extraction, enabling context-aware feature
recalibration for specific downstream tasks. Furthermore, during
backpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not only
leverages Fisher information to identify and update the most task-critical
parameters, thereby preserving invariant pre-trained knowledge, but also
introduces randomness to stabilize the selection. We demonstrate the
effectiveness and efficiency of WeatherPEFT on three downstream tasks, where
existing PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFT
achieves performance parity with Full-Tuning using fewer trainable parameters.
The code of this work will be released.

</details>


### [163] [Progressive Weight Loading: Accelerating Initial Inference and Gradually Boosting Performance on Resource-Constrained Environments](https://arxiv.org/abs/2509.22319)
*Hyunwoo Kim,Junha Lee,Mincheol Choi,Jeonghwan Lee,Jaeshin Cho*

Main category: cs.LG

TL;DR: Progressive Weight Loading (PWL) enables fast initial inference using lightweight student models, then incrementally replaces layers with teacher model weights to achieve full accuracy without compromising speed.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between model compression speed and performance in latency-sensitive environments where frequent model loading/unloading impacts user experience.

Method: Progressive Weight Loading technique that deploys lightweight student model first, then incrementally replaces layers with pre-trained teacher model layers. Includes training method to align intermediate feature representations between student and teacher layers.

Result: Experiments on VGG, ResNet, and ViT architectures show competitive distillation performance with gradual accuracy improvement as teacher layers are loaded, matching full teacher model accuracy without compromising initial inference speed.

Conclusion: PWL is particularly suited for dynamic, resource-constrained deployments where both responsiveness and performance are critical, solving the speed-performance trade-off in model compression.

Abstract: Deep learning models have become increasingly large and complex, resulting in
higher memory consumption and computational demands. Consequently, model
loading times and initial inference latency have increased, posing significant
challenges in mobile and latency-sensitive environments where frequent model
loading and unloading are required, which directly impacts user experience.
While Knowledge Distillation (KD) offers a solution by compressing large
teacher models into smaller student ones, it often comes at the cost of reduced
performance. To address this trade-off, we propose Progressive Weight Loading
(PWL), a novel technique that enables fast initial inference by first deploying
a lightweight student model, then incrementally replacing its layers with those
of a pre-trained teacher model. To support seamless layer substitution, we
introduce a training method that not only aligns intermediate feature
representations between student and teacher layers, but also improves the
overall output performance of the student model. Our experiments on VGG,
ResNet, and ViT architectures demonstrate that models trained with PWL maintain
competitive distillation performance and gradually improve accuracy as teacher
layers are loaded-matching the final accuracy of the full teacher model without
compromising initial inference speed. This makes PWL particularly suited for
dynamic, resource-constrained deployments where both responsiveness and
performance are critical.

</details>


### [164] [Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error](https://arxiv.org/abs/2509.22023)
*Panagiotis Giannoulis,Yorgos Pantis,Christos Tzamos*

Main category: cs.LG

TL;DR: The paper presents a novel approach using GPT-2 to solve Sudoku puzzles with 99% accuracy by combining imitation learning of Sudoku rules with explicit Depth-First Search exploration involving informed guessing and backtracking.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with combinatorial problems like Sudoku despite their proficiency in language tasks. The paper aims to address this gap in solving NP-class problems.

Method: Uses vanilla decoder-only Transformer (GPT-2) without custom architectures or external tools. Integrates imitation learning of Sudoku rules with explicit Depth-First Search exploration strategy involving informed guessing and backtracking.

Result: Achieves state-of-the-art accuracy of 99% on Sudoku puzzles, outperforming prior neuro-symbolic approaches.

Conclusion: The method successfully bridges the gap in LLMs' combinatorial reasoning capabilities and provides rigorous analysis connecting the approach to Min-Sum Set Cover problem.

Abstract: Despite their proficiency in various language tasks, Large Language Models
(LLMs) struggle with combinatorial problems like Satisfiability, Traveling
Salesman Problem, or even basic arithmetic. We address this gap through a novel
approach for solving problems in the class NP. We focus on the paradigmatic
task of Sudoku and achieve state-of-the-art accuracy (99\%) compared to prior
neuro-symbolic approaches. Unlike prior work that used custom architectures,
our method employs a vanilla decoder-only Transformer (GPT-2) without external
tools or function calling. Our method integrates imitation learning of simple
Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy
involving informed guessing and backtracking. Moving beyond imitation learning,
we seek to minimize the number of guesses until reaching a solution. We provide
a rigorous analysis of this setup formalizing its connection to a contextual
variant of Min-Sum Set Cover, a well-studied problem in algorithms and
stochastic optimization.

</details>


### [165] [Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning](https://arxiv.org/abs/2509.22335)
*Naicheng He,Kaicheng Guo,Arjun Prakash,Saket Tiwari,Ruo Yu Tao,Tyrone Serapio,Amy Greenwald,George Konidaris*

Main category: cs.LG

TL;DR: The paper investigates loss of plasticity in deep neural networks during continual learning, identifies Hessian spectral collapse as the cause, and proposes regularization methods to maintain plasticity.


<details>
  <summary>Details</summary>
Motivation: To understand why deep neural networks lose plasticity in continual learning and fail to learn new tasks without parameter reinitialization.

Method: Introduces τ-trainability framework, uses Kronecker factored Hessian approximation, and proposes two regularization enhancements: maintaining high effective feature rank and applying L2 penalties.

Result: Experiments on continual supervised and reinforcement learning tasks show that combining the two regularizers effectively preserves plasticity.

Conclusion: Hessian spectral collapse causes loss of plasticity, and the proposed regularization methods successfully maintain network plasticity in continual learning scenarios.

Abstract: We investigate why deep neural networks suffer from \emph{loss of plasticity}
in deep continual learning, failing to learn new tasks without reinitializing
parameters. We show that this failure is preceded by Hessian spectral collapse
at new-task initialization, where meaningful curvature directions vanish and
gradient descent becomes ineffective. To characterize the necessary condition
for successful training, we introduce the notion of $\tau$-trainability and
show that current plasticity preserving algorithms can be unified under this
framework. Targeting spectral collapse directly, we then discuss the Kronecker
factored approximation of the Hessian, which motivates two regularization
enhancements: maintaining high effective feature rank and applying $L2$
penalties. Experiments on continual supervised and reinforcement learning tasks
confirm that combining these two regularizers effectively preserves plasticity.

</details>


### [166] [MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules](https://arxiv.org/abs/2509.22028)
*Haodong Pan,Yusong Wang,Nanning Zheng,Caijui Jiang*

Main category: cs.LG

TL;DR: MCGM is a plug-and-play module that enhances geometric GNNs with hierarchical global context through efficient clustering, overcoming limitations of local message passing for modeling long-range interactions.


<details>
  <summary>Details</summary>
Motivation: Geometric GNNs struggle with long-range interactions due to locality-biased message passing. Current solutions have limitations including cubic computational scaling, system-specific physics kernels, and parameter-heavy Fourier methods.

Method: Multi-stage Clustered Global Modeling (MCGM) builds multi-resolution atomic clusters, distills global information via dynamic hierarchical clustering, and propagates context back through learned transformations with residual connections.

Result: MCGM reduces OE62 energy prediction error by 26.2% on average. On AQM, it achieves state-of-the-art accuracy (17.0 meV energy, 4.9 meV/Å forces) with 20% fewer parameters than Neural P3M.

Conclusion: MCGM provides an efficient, generalizable solution for incorporating global context into geometric GNNs, significantly improving performance on molecular property prediction tasks.

Abstract: Geometric graph neural networks (GNNs) excel at capturing molecular geometry,
yet their locality-biased message passing hampers the modeling of long-range
interactions. Current solutions have fundamental limitations: extending cutoff
radii causes computational costs to scale cubically with distance;
physics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific
and lack generality; Fourier-space methods require careful tuning of multiple
parameters (e.g., mesh size, k-space cutoff) with added computational overhead.
We introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,
plug-and-play module that endows geometric GNNs with hierarchical global
context through efficient clustering operations. MCGM builds a multi-resolution
hierarchy of atomic clusters, distills global information via dynamic
hierarchical clustering, and propagates this context back through learned
transformations, ultimately reinforcing atomic features via residual
connections. Seamlessly integrated into four diverse backbone architectures,
MCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM
achieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\AA} for
forces) while using 20% fewer parameters than Neural P3M. Code will be made
available upon acceptance.

</details>


### [167] [SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis](https://arxiv.org/abs/2509.22352)
*Marie Brockschmidt,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: SurvDiff is a diffusion model for generating synthetic survival data that jointly generates covariates, event times, and censoring while preserving event-time distributions and censoring mechanisms.


<details>
  <summary>Details</summary>
Motivation: Survival data has incomplete event information due to censoring, posing challenges for synthetic data generation that must faithfully reproduce both event-time distributions and censoring mechanisms for clinical research.

Method: End-to-end diffusion model with survival-tailored loss function that jointly generates mixed-type covariates, event times, and right-censoring, directly optimizing for downstream survival tasks.

Result: SurvDiff consistently outperforms state-of-the-art generative baselines in both distributional fidelity and downstream evaluation metrics across multiple medical datasets.

Conclusion: SurvDiff is the first diffusion model explicitly designed for synthetic survival data generation and effectively captures the data-generating mechanism while preserving key survival analysis properties.

Abstract: Survival analysis is a cornerstone of clinical research by modeling
time-to-event outcomes such as metastasis, disease relapse, or patient death.
Unlike standard tabular data, survival data often come with incomplete event
information due to dropout, or loss to follow-up. This poses unique challenges
for synthetic data generation, where it is crucial for clinical research to
faithfully reproduce both the event-time distribution and the censoring
mechanism. In this paper, we propose SurvDiff, an end-to-end diffusion model
specifically designed for generating synthetic data in survival analysis.
SurvDiff is tailored to capture the data-generating mechanism by jointly
generating mixed-type covariates, event times, and right-censoring, guided by a
survival-tailored loss function. The loss encodes the time-to-event structure
and directly optimizes for downstream survival tasks, which ensures that
SurvDiff (i) reproduces realistic event-time distributions and (ii) preserves
the censoring mechanism. Across multiple datasets, we show that \survdiff
consistently outperforms state-of-the-art generative baselines in both
distributional fidelity and downstream evaluation metrics across multiple
medical datasets. To the best of our knowledge, SurvDiff is the first diffusion
model explicitly designed for generating synthetic survival data.

</details>


### [168] [OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features](https://arxiv.org/abs/2509.22033)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Elena Tutubalina,Ivan Oseledets*

Main category: cs.LG

TL;DR: OrtSAE introduces orthogonality constraints to sparse autoencoders to reduce feature absorption and composition issues, achieving better feature disentanglement and performance.


<details>
  <summary>Details</summary>
Motivation: Current sparse autoencoders suffer from feature absorption (specialized features capturing general ones) and feature composition (independent features merging), creating representation problems.

Method: Orthogonal SAE (OrtSAE) enforces orthogonality between learned features by penalizing high pairwise cosine similarity, using a training procedure that scales linearly with SAE size.

Result: OrtSAE discovers 9% more distinct features, reduces feature absorption by 65% and composition by 15%, improves spurious correlation removal by 6%, and maintains comparable performance on other downstream tasks.

Conclusion: Orthogonality constraints effectively mitigate feature absorption and composition in sparse autoencoders, leading to more disentangled and interpretable features without significant computational overhead.

Abstract: Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural
network activations into human-interpretable features. However, current SAEs
suffer from feature absorption, where specialized features capture instances of
general features creating representation holes, and feature composition, where
independent features merge into composite representations. In this work, we
introduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these
issues by enforcing orthogonality between the learned features. By implementing
a new training procedure that penalizes high pairwise cosine similarity between
SAE features, OrtSAE promotes the development of disentangled features while
scaling linearly with the SAE size, avoiding significant computational
overhead. We train OrtSAE across different models and layers and compare it
with other methods. We find that OrtSAE discovers 9% more distinct features,
reduces feature absorption (by 65%) and composition (by 15%), improves
performance on spurious correlation removal (+6%), and achieves on-par
performance for other downstream tasks compared to traditional SAEs.

</details>


### [169] [Context and Diversity Matter: The Emergence of In-Context Learning in World Models](https://arxiv.org/abs/2509.22353)
*Fan Wang,Zhiyuan Chen,Yuxuan Zhong,Sunjian Zheng,Pengtao Shao,Bo Yu,Shaoshan Liu,Jianan Wang,Ning Ding,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: The paper formalizes in-context environment learning (ICEL) for world models, identifying two core mechanisms (environment recognition and learning), deriving error bounds, and empirically validating how data distribution and model architecture affect ICEL emergence.


<details>
  <summary>Details</summary>
Motivation: Current static world models fail with novel or rare configurations, so the authors investigate how world models can self-adapt through in-context learning rather than relying on zero-shot performance.

Method: Formalized in-context learning of world models with two mechanisms, derived theoretical error upper-bounds, and conducted empirical validation of how data distribution and model architecture affect ICEL.

Result: Confirmed distinct in-context learning mechanisms exist in world models, with findings showing ICEL emergence depends on long context and diverse environments.

Conclusion: Demonstrates potential of self-adapting world models and identifies key factors (long context, diverse environments) necessary for in-context environment learning to emerge.

Abstract: The capability of predicting environmental dynamics underpins both biological
neural systems and general embodied AI in adapting to their surroundings. Yet
prevailing approaches rest on static world models that falter when confronted
with novel or rare configurations. We investigate in-context environment
learning (ICEL), shifting attention from zero-shot performance to the growth
and asymptotic limits of the world model. Our contributions are three-fold: (1)
we formalize in-context learning of a world model and identify two core
mechanisms: environment recognition and environment learning; (2) we derive
error upper-bounds for both mechanisms that expose how the mechanisms emerge;
and (3) we empirically confirm that distinct ICL mechanisms exist in the world
model, and we further investigate how data distribution and model architecture
affect ICL in a manner consistent with theory. These findings demonstrate the
potential of self-adapting world models and highlight the key factors behind
the emergence of ICEL, most notably the necessity of long context and diverse
environments.

</details>


### [170] [Stochastic activations](https://arxiv.org/abs/2509.22358)
*Maria Lomeli,Matthijs Douze,Gergely Szilvasy,Loic Cabannes,Jade Copet,Sainbayar Sukhbaatar,Jason Weston,Gabriel Synnaeve,Pierre-Emmanuel Mazaré,Hervé Jégou*

Main category: cs.LG

TL;DR: Stochastic activations randomly select between SILU and RELU functions during training, improving optimization and enabling sparse inference with RELU or controlled diversity in text generation.


<details>
  <summary>Details</summary>
Motivation: To address RELU's optimization problems (constant shape for negative inputs that blocks gradient flow) while leveraging its sparsity benefits for efficient inference.

Method: Random selection between SILU and RELU activations via Bernoulli draws during training, with options to fine-tune with RELU for sparse inference or use stochastic activations directly for generation.

Result: Stochastic activations outperform training from scratch with RELU, provide significant CPU speedup via sparse inference, and offer reasonable generation diversity slightly inferior to SILU with temperature scaling.

Conclusion: Stochastic activations effectively circumvent RELU's optimization issues while enabling efficient sparse inference and controlled diversity in text generation as a viable alternative to existing methods.

Abstract: We introduce stochastic activations. This novel strategy randomly selects
between several non-linear functions in the feed-forward layer of a large
language model. In particular, we choose between SILU or RELU depending on a
Bernoulli draw. This strategy circumvents the optimization problem associated
with RELU, namely, the constant shape for negative inputs that prevents the
gradient flow. We leverage this strategy in two ways:
  (1) We use stochastic activations during pre-training and fine-tune the model
with RELU, which is used at inference time to provide sparse latent vectors.
This reduces the inference FLOPs and translates into a significant speedup in
the CPU. Interestingly, this leads to much better results than training from
scratch with the RELU activation function.
  (2) We evaluate stochastic activations for generation. This strategy performs
reasonably well: it is only slightly inferior to the best deterministic
non-linearity, namely SILU combined with temperature scaling. This offers an
alternative to existing strategies by providing a controlled way to increase
the diversity of the generated text.

</details>


### [171] [Convexity-Driven Projection for Point Cloud Dimensionality Reduction](https://arxiv.org/abs/2509.22043)
*Suman Sanyal*

Main category: cs.LG

TL;DR: CDP is a boundary-free linear dimensionality reduction method that preserves local non-convexity by using k-NN graphs and shortest-path ratios to create a non-convexity structure matrix.


<details>
  <summary>Details</summary>
Motivation: To develop a dimensionality reduction method that preserves detour-induced local non-convexity without requiring boundary information, addressing limitations of existing methods.

Method: Builds k-NN graph, identifies admissible pairs based on Euclidean-to-shortest-path ratios, aggregates normalized directions to form positive semidefinite non-convexity structure matrix, and projects using top-k eigenvectors.

Result: Provides two verifiable guarantees: pairwise a-posteriori certificate for bounding post-projection distortion per admissible pair, and average-case spectral bound linking captured direction energy to structure matrix spectrum.

Conclusion: CDP offers a practical linear dimensionality reduction approach with verifiable guarantees on preserving local non-convexity, enabling practitioners to validate performance on their data.

Abstract: We propose Convexity-Driven Projection (CDP), a boundary-free linear method
for dimensionality reduction of point clouds that targets preserving
detour-induced local non-convexity. CDP builds a $k$-NN graph, identifies
admissible pairs whose Euclidean-to-shortest-path ratios are below a threshold,
and aggregates their normalized directions to form a positive semidefinite
non-convexity structure matrix. The projection uses the top-$k$ eigenvectors of
the structure matrix. We give two verifiable guarantees. A pairwise
a-posteriori certificate that bounds the post-projection distortion for each
admissible pair, and an average-case spectral bound that links expected
captured direction energy to the spectrum of the structure matrix, yielding
quantile statements for typical distortion. Our evaluation protocol reports
fixed- and reselected-pairs detour errors and certificate quantiles, enabling
practitioners to check guarantees on their data.

</details>


### [172] [SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly](https://arxiv.org/abs/2509.22387)
*Narada Maugin,Tristan Cazenave*

Main category: cs.LG

TL;DR: SpinGPT is the first LLM designed for 3-player Spin & Go poker, trained via supervised fine-tuning on expert data and reinforcement learning on solver data, achieving competitive performance against existing bots.


<details>
  <summary>Details</summary>
Motivation: CFR algorithms struggle with multiplayer games due to exponential complexity and Nash equilibrium limitations in 3+ player settings, restricting their use in popular tournament formats like Spin & Go.

Method: Two-stage training: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands.

Result: Matches solver actions in 78% of decisions (tolerant accuracy) and achieves 13.4 ± 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands.

Conclusion: LLMs represent a promising new approach for handling multiplayer imperfect-information games like poker, overcoming CFR's limitations.

Abstract: The Counterfactual Regret Minimization (CFR) algorithm and its variants have
enabled the development of pokerbots capable of beating the best human players
in heads-up (1v1) cash games and competing with them in six-player formats.
However, CFR's computational complexity rises exponentially with the number of
players. Furthermore, in games with three or more players, following Nash
equilibrium no longer guarantees a non-losing outcome. These limitations, along
with others, significantly restrict the applicability of CFR to the most
popular formats: tournaments. Motivated by the recent success of Large Language
Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored
to Spin & Go, a popular three-player online poker format. SpinGPT is trained in
two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;
(2) Reinforcement Learning on 270k solver-generated hands. Our results show
that SpinGPT matches the solver's actions in 78% of decisions (tolerant
accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100
versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest
that LLMs could be a new way to deal with multi-player imperfect-information
games like poker.

</details>


### [173] [MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems](https://arxiv.org/abs/2509.22047)
*Yuki Ichihara,Yuu Jinnai,Tetsuro Morimura,Mitsuki Sakamoto,Ryota Mitsuhashi,Eiji Uchibe*

Main category: cs.LG

TL;DR: MO-GRPO extends GRPO with automatic reward normalization to address reward hacking in multi-objective settings, ensuring balanced optimization across all objectives without manual tuning.


<details>
  <summary>Details</summary>
Motivation: GRPO is vulnerable to reward hacking in multi-objective settings where it may optimize only one objective at the expense of others, especially when reliable reward models are unavailable in real-world tasks.

Method: Extends GRPO with a simple normalization method that automatically reweights reward functions according to their value variances, ensuring all rewards contribute evenly to the loss while preserving preference order.

Result: MO-GRPO achieved stable learning with balanced reward correlations across four domains: multi-armed bandits, simulated control, machine translation (WMT En-Ja, En-Zh), and instruction following tasks, outperforming standard GRPO.

Conclusion: MO-GRPO is a promising algorithm for multi-objective reinforcement learning that eliminates manual reward scaling and prevents reward hacking by ensuring balanced optimization across all objectives.

Abstract: Group Relative Policy Optimization (GRPO) has been shown to be an effective
algorithm when an accurate reward model is available. However, such a highly
reliable reward model is not available in many real-world tasks. In this paper,
we particularly focus on multi-objective settings, in which we identify that
GRPO is vulnerable to reward hacking, optimizing only one of the objectives at
the cost of the others. To address this issue, we propose MO-GRPO, an extension
of GRPO with a simple normalization method to reweight the reward functions
automatically according to the variances of their values. We first show
analytically that MO-GRPO ensures that all reward functions contribute evenly
to the loss function while preserving the order of preferences, eliminating the
need for manual tuning of the reward functions' scales. Then, we evaluate
MO-GRPO experimentally in four domains: (i) the multi-armed bandits problem,
(ii) simulated control task (Mo-Gymnasium), (iii) machine translation tasks on
the WMT benchmark (En-Ja, En-Zh), and (iv) instruction following task. MO-GRPO
achieves stable learning by evenly distributing correlations among the
components of rewards, outperforming GRPO, showing MO-GRPO to be a promising
algorithm for multi-objective reinforcement learning problems.

</details>


### [174] [Partial Parameter Updates for Efficient Distributed Training](https://arxiv.org/abs/2509.22418)
*Anastasiia Filippova,Angelos Katharopoulos,David Grangier,Ronan Collobert*

Main category: cs.LG

TL;DR: A memory- and compute-efficient distributed training method that reduces communication by restricting backpropagation to update only a subset of parameters during local steps, while maintaining full forward passes.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency of existing low-communication distributed training methods that perform multiple local updates between global synchronizations.

Method: Restrict backpropagation so each node updates only a fixed subset of parameters while keeping others frozen during local steps, with full forward passes over all parameters to avoid cross-node activation exchange.

Result: Method matches perplexity of prior approaches under identical token and bandwidth budgets while reducing training FLOPs and peak memory on a 1.3B-parameter language model across 32 nodes.

Conclusion: The constrained parameter update approach significantly improves memory and compute efficiency in distributed training while maintaining model quality.

Abstract: We introduce a memory- and compute-efficient method for low-communication
distributed training. Existing methods reduce communication by performing
multiple local updates between infrequent global synchronizations. We
demonstrate that their efficiency can be significantly improved by restricting
backpropagation: instead of updating all the parameters, each node updates only
a fixed subset while keeping the remainder frozen during local steps. This
constraint substantially reduces peak memory usage and training FLOPs, while a
full forward pass over all parameters eliminates the need for cross-node
activation exchange. Experiments on a $1.3$B-parameter language model trained
across $32$ nodes show that our method matches the perplexity of prior
low-communication approaches under identical token and bandwidth budgets while
reducing training FLOPs and peak memory.

</details>


### [175] [BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning](https://arxiv.org/abs/2509.22050)
*Yi Ding,Muyun Jiang,Weibang Jiang,Shuailei Zhang,Xinliang Zhou,Chenyu Liu,Shanglin Li,Yong Li,Cuntai Guan*

Main category: cs.LG

TL;DR: BrainPro is a large EEG foundation model that addresses limitations in existing EEG models by introducing retrieval-based spatial learning for flexible channel/region interactions and brain state-decoupling for state-aware representation learning, achieving SOTA performance across multiple BCI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing EEG foundation models fail to explicitly capture channel-to-channel and region-to-region interactions critical for EEG signals, and lack state-aware representation learning during pre-training, limiting their flexibility and effectiveness across diverse datasets and brain states.

Method: Proposes BrainPro with: 1) retrieval-based spatial learning block to flexibly capture channel- and region-level interactions across varying electrode layouts, 2) brain state-decoupling block with parallel encoders and decoupling/region-aware reconstruction losses for state-aware representation learning.

Result: Pre-trained on extensive EEG corpus, BrainPro achieves state-of-the-art performance and robust generalization across nine public BCI datasets, demonstrating superior adaptability to diverse tasks and hardware settings.

Conclusion: BrainPro effectively addresses key limitations in EEG modeling through its novel spatial learning and state-decoupling approaches, enabling seamless adaptation to diverse EEG tasks and configurations while maintaining high performance and generalization.

Abstract: Electroencephalography (EEG) is a non-invasive technique for recording brain
electrical activity, widely used in brain-computer interface (BCI) and
healthcare. Recent EEG foundation models trained on large-scale datasets have
shown improved performance and generalizability over traditional decoding
methods, yet significant challenges remain. Existing models often fail to
explicitly capture channel-to-channel and region-to-region interactions, which
are critical sources of information inherently encoded in EEG signals. Due to
varying channel configurations across datasets, they either approximate spatial
structure with self-attention or restrict training to a limited set of common
channels, sacrificing flexibility and effectiveness. Moreover, although EEG
datasets reflect diverse brain states such as emotion, motor, and others,
current models rarely learn state-aware representations during self-supervised
pre-training. To address these gaps, we propose BrainPro, a large EEG model
that introduces a retrieval-based spatial learning block to flexibly capture
channel- and region-level interactions across varying electrode layouts, and a
brain state-decoupling block that enables state-aware representation learning
through parallel encoders with decoupling and region-aware reconstruction
losses. This design allows BrainPro to adapt seamlessly to diverse tasks and
hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves
state-of-the-art performance and robust generalization across nine public BCI
datasets. Our codes and the pre-trained weights will be released.

</details>


### [176] [Global Convergence in Neural ODEs: Impact of Activation Functions](https://arxiv.org/abs/2509.22436)
*Tianxiang Gao,Siyuan Sun,Hailiang Liu,Hongyang Gao*

Main category: cs.LG

TL;DR: The paper analyzes how activation functions impact Neural ODE training, showing that smoothness ensures unique solutions while nonlinearity preserves NTK properties, enabling global convergence in overparameterized regimes.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs face training challenges due to their continuous nature and parameter-sharing efficiency, particularly with gradient computation accuracy and convergence analysis.

Method: Theoretical investigation of activation function properties (smoothness and nonlinearity) and their impact on training dynamics, validated by numerical experiments.

Result: Smooth activation functions guarantee globally unique solutions for forward/backward ODEs, while sufficient nonlinearity maintains NTK spectral properties, enabling global convergence under gradient descent.

Conclusion: The findings provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications.

Abstract: Neural Ordinary Differential Equations (ODEs) have been successful in various
applications due to their continuous nature and parameter-sharing efficiency.
However, these unique characteristics also introduce challenges in training,
particularly with respect to gradient computation accuracy and convergence
analysis. In this paper, we address these challenges by investigating the
impact of activation functions. We demonstrate that the properties of
activation functions, specifically smoothness and nonlinearity, are critical to
the training dynamics. Smooth activation functions guarantee globally unique
solutions for both forward and backward ODEs, while sufficient nonlinearity is
essential for maintaining the spectral properties of the Neural Tangent Kernel
(NTK) during training. Together, these properties enable us to establish the
global convergence of Neural ODEs under gradient descent in overparameterized
regimes. Our theoretical findings are validated by numerical experiments, which
not only support our analysis but also provide practical guidelines for scaling
Neural ODEs, potentially leading to faster training and improved performance in
real-world applications.

</details>


### [177] [Enriching Knowledge Distillation with Intra-Class Contrastive Learning](https://arxiv.org/abs/2509.22053)
*Hua Yuan,Ning Xu,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: The paper proposes enhancing knowledge distillation by incorporating intra-class contrastive loss with margin during teacher training to enrich intra-class diversity in soft labels, improving student model generalization.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods use teacher models that follow ground-truth labels without considering diverse representations within the same class, limiting the effectiveness of soft labels for student learning.

Method: Incorporates intra-class contrastive loss with margin during teacher training to enrich intra-class information in soft labels, addressing training instability and slow convergence issues.

Result: Theoretical analysis shows the method reduces intra-class distances while maintaining inter-class distances, and experiments demonstrate improved effectiveness over existing distillation approaches.

Conclusion: The proposed intra-class contrastive loss with margin enhances knowledge distillation by enriching intra-class diversity in teacher-generated soft labels, leading to better student model generalization.

Abstract: Since the advent of knowledge distillation, much research has focused on how
the soft labels generated by the teacher model can be utilized effectively.
Existing studies points out that the implicit knowledge within soft labels
originates from the multi-view structure present in the data. Feature
variations within samples of the same class allow the student model to
generalize better by learning diverse representations. However, in existing
distillation methods, teacher models predominantly adhere to ground-truth
labels as targets, without considering the diverse representations within the
same class. Therefore, we propose incorporating an intra-class contrastive loss
during teacher training to enrich the intra-class information contained in soft
labels. In practice, we find that intra-class loss causes instability in
training and slows convergence. To mitigate these issues, margin loss is
integrated into intra-class contrastive learning to improve the training
stability and convergence speed. Simultaneously, we theoretically analyze the
impact of this loss on the intra-class distances and inter-class distances. It
has been proved that the intra-class contrastive loss can enrich the
intra-class diversity. Experimental results demonstrate the effectiveness of
the proposed method.

</details>


### [178] [Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers](https://arxiv.org/abs/2509.22445)
*Peter Shaw,James Cohan,Jacob Eisenstein,Kristina Toutanova*

Main category: cs.LG

TL;DR: This paper introduces asymptotically optimal description length objectives for neural networks like Transformers, grounded in Kolmogorov complexity theory, which achieve optimal compression up to an additive constant in the limit.


<details>
  <summary>Details</summary>
Motivation: The MDL principle lacks principled measures for neural network complexity, making its application to Transformers challenging. The paper aims to provide theoretical foundations for identifying description length objectives with strong asymptotic guarantees.

Method: The authors establish asymptotically optimal objectives for Transformers by proving their computational universality and construct a tractable variational objective using an adaptive Gaussian mixture prior.

Result: Empirical analysis shows the variational objective selects low-complexity solutions with strong generalization on algorithmic tasks, though standard optimizers struggle to find such solutions from random initialization.

Conclusion: The paper outlines a path towards training neural networks that achieve greater compression and generalization through theoretically-grounded description length objectives with asymptotic optimality guarantees.

Abstract: The Minimum Description Length (MDL) principle offers a formal framework for
applying Occam's razor in machine learning. However, its application to neural
networks such as Transformers is challenging due to the lack of a principled,
universal measure for model complexity. This paper introduces the theoretical
notion of asymptotically optimal description length objectives, grounded in the
theory of Kolmogorov complexity. We establish that a minimizer of such an
objective achieves optimal compression, for any dataset, up to an additive
constant, in the limit as model resource bounds increase. We prove that
asymptotically optimal objectives exist for Transformers, building on a new
demonstration of their computational universality. We further show that such
objectives can be tractable and differentiable by constructing and analyzing a
variational objective based on an adaptive Gaussian mixture prior. Our
empirical analysis shows that this variational objective selects for a
low-complexity solution with strong generalization on an algorithmic task, but
standard optimizers fail to find such solutions from a random initialization,
highlighting key optimization challenges. More broadly, by providing a
theoretical framework for identifying description length objectives with strong
asymptotic guarantees, we outline a potential path towards training neural
networks that achieve greater compression and generalization.

</details>


### [179] [Towards Understanding Feature Learning in Parameter Transfer](https://arxiv.org/abs/2509.22056)
*Hua Yuan,Xuran Meng,Qiufeng Wang,Shiyu Xia,Ning Xu,Xu Yang,Jing Wang,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: This paper provides theoretical analysis of partial parameter transfer in ReLU CNNs, identifying conditions where parameter reuse is beneficial and explaining when it can hurt performance compared to training from scratch.


<details>
  <summary>Details</summary>
Motivation: There's a lack of theoretical understanding about when partial parameter transfer is beneficial and what factors govern its effectiveness in transfer learning.

Method: Theoretical analysis of ReLU convolutional neural networks, with numerical experiments and real-world data validation.

Result: Characterized how inherited parameters carry universal knowledge and identified key factors that amplify their beneficial impact on target tasks.

Conclusion: The analysis explains why parameter transfer can sometimes lead to lower test accuracy than training from scratch, providing insights into effective parameter reuse conditions.

Abstract: Parameter transfer is a central paradigm in transfer learning, enabling
knowledge reuse across tasks and domains by sharing model parameters between
upstream and downstream models. However, when only a subset of parameters from
the upstream model is transferred to the downstream model, there remains a lack
of theoretical understanding of the conditions under which such partial
parameter reuse is beneficial and of the factors that govern its effectiveness.
To address this gap, we analyze a setting in which both the upstream and
downstream models are ReLU convolutional neural networks (CNNs). Within this
theoretical framework, we characterize how the inherited parameters act as
carriers of universal knowledge and identify key factors that amplify their
beneficial impact on the target task. Furthermore, our analysis provides
insight into why, in certain cases, transferring parameters can lead to lower
test accuracy on the target task than training a new model from scratch.
Numerical experiments and real-world data experiments are conducted to
empirically validate our theoretical findings.

</details>


### [180] [Physics-informed GNN for medium-high voltage AC power flow with edge-aware attention and line search correction operator](https://arxiv.org/abs/2509.22458)
*Changhun Kim,Timon Conrad,Redwanul Karim,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: PIGNN-Attn-LS is a physics-informed graph neural network that combines edge-aware attention with backtracking line-search to improve AC power-flow solving accuracy and speed compared to Newton-Raphson methods.


<details>
  <summary>Details</summary>
Motivation: Current physics-informed graph neural networks (PIGNNs) need accuracy improvements and lack operative physics loss at inference, which limits their operational adoption as AC power-flow solvers.

Method: Combines edge-aware attention mechanism that encodes line physics via per-edge biases with backtracking line-search-based globalized correction operator to restore operative decrease criterion at inference.

Result: Achieves test RMSE of 0.00033 p.u. in voltage and 0.08° in angle, outperforming PIGNN-MLP baseline by 99.5% and 87.1% respectively. Delivers 2-5× faster batched inference than NR on 4-1024-bus grids.

Conclusion: PIGNN-Attn-LS significantly improves accuracy and speed for AC power-flow solving, making it a viable replacement for Newton-Raphson solvers in operational scenarios requiring thousands of evaluations.

Abstract: Physics-informed graph neural networks (PIGNNs) have emerged as fast AC
power-flow solvers that can replace classic Newton--Raphson (NR) solvers,
especially when thousands of scenarios must be evaluated. However, current
PIGNNs still need accuracy improvements at parity speed; in particular, the
physics loss is inoperative at inference, which can deter operational adoption.
We address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism
that explicitly encodes line physics via per-edge biases, capturing the grid's
anisotropy, with a backtracking line-search-based globalized correction
operator that restores an operative decrease criterion at inference. Training
and testing use a realistic High-/Medium-Voltage scenario generator, with NR
used only to construct reference states. On held-out HV cases consisting of
4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage
and 0.08$^\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\% and
87.1\%, respectively. With streaming micro-batches, it delivers 2--5$\times$
faster batched inference than NR on 4--1024-bus grids.

</details>


### [181] [Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining](https://arxiv.org/abs/2509.22468)
*Boshra Ariguib,Mathias Niepert,Andrei Manolache*

Main category: cs.LG

TL;DR: C-FREE is a self-supervised learning framework that integrates 2D molecular graphs with 3D conformer ensembles to learn molecular representations without contrastive learning, achieving state-of-the-art results on MoleculeNet.


<details>
  <summary>Details</summary>
Motivation: Existing molecular representation methods often rely on hand-crafted augmentations, complex generative objectives, and primarily use 2D topology while underutilizing valuable 3D structural information. There is a need for simpler approaches that effectively integrate both 2D and 3D molecular information.

Method: C-FREE predicts subgraph embeddings from their complementary neighborhoods in latent space using fixed-radius ego-nets across different conformers. It integrates geometric and topological information through a hybrid GNN-Transformer backbone without negatives, positional encodings, or expensive pre-processing.

Result: Pretrained on GEOM dataset, C-FREE achieves state-of-the-art results on MoleculeNet, outperforming contrastive, generative, and other multimodal self-supervised methods. Fine-tuning across diverse datasets shows effective transfer to new chemical domains.

Conclusion: The framework demonstrates the importance of 3D-informed molecular representations and provides an effective approach for learning molecular representations that transfer well across different chemical domains.

Abstract: High-quality molecular representations are essential for property prediction
and molecular design, yet large labeled datasets remain scarce. While
self-supervised pretraining on molecular graphs has shown promise, many
existing approaches either depend on hand-crafted augmentations or complex
generative objectives, and often rely solely on 2D topology, leaving valuable
3D structural information underutilized. To address this gap, we introduce
C-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework
that integrates 2D graphs with ensembles of 3D conformers. C-FREE learns
molecular representations by predicting subgraph embeddings from their
complementary neighborhoods in the latent space, using fixed-radius ego-nets as
modeling units across different conformers. This design allows us to integrate
both geometric and topological information within a hybrid Graph Neural Network
(GNN)-Transformer backbone, without negatives, positional encodings, or
expensive pre-processing. Pretraining on the GEOM dataset, which provides rich
3D conformational diversity, C-FREE achieves state-of-the-art results on
MoleculeNet, surpassing contrastive, generative, and other multimodal
self-supervised methods. Fine-tuning across datasets with diverse sizes and
molecule types further demonstrates that pretraining transfers effectively to
new chemical domains, highlighting the importance of 3D-informed molecular
representations.

</details>


### [182] [Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2509.22082)
*Li Xia,Zheng Liu,Sili Huang,Wei Tang,Xuan Liu*

Main category: cs.LG

TL;DR: NL-SME introduces nonlinear parametric trajectory modeling for Gradient Inversion Attacks in Federated Learning, replacing linear interpolation with learnable quadratic Bézier curves to better capture SGD's nonlinear complexity, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate model methods for Gradient Inversion Attacks in Federated Learning assume linear parameter trajectories, which severely underestimates SGD's nonlinear complexity and fundamentally limits attack effectiveness.

Method: Proposed Non-Linear Surrogate Model Extension (NL-SME) that replaces linear interpolation with learnable quadratic Bézier curves to capture SGD's curved characteristics through control points, combined with regularization and dvec scaling mechanisms for enhanced expressiveness.

Result: Extensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME significantly outperforms baselines across all metrics, achieving order-of-magnitude improvements in cosine similarity loss while maintaining computational efficiency.

Conclusion: This work exposes heightened privacy vulnerabilities in FL's multi-step update paradigm and offers novel perspectives for developing robust defense strategies.

Abstract: Federated Learning (FL) preserves privacy by keeping raw data local, yet
Gradient Inversion Attacks (GIAs) pose significant threats. In FedAVG
multi-step scenarios, attackers observe only aggregated gradients, making data
reconstruction challenging. Existing surrogate model methods like SME assume
linear parameter trajectories, but we demonstrate this severely underestimates
SGD's nonlinear complexity, fundamentally limiting attack effectiveness. We
propose Non-Linear Surrogate Model Extension (NL-SME), the first method to
introduce nonlinear parametric trajectory modeling for GIAs. Our approach
replaces linear interpolation with learnable quadratic B\'ezier curves that
capture SGD's curved characteristics through control points, combined with
regularization and dvec scaling mechanisms for enhanced expressiveness.
Extensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME
significantly outperforms baselines across all metrics, achieving
order-of-magnitude improvements in cosine similarity loss while maintaining
computational efficiency.This work exposes heightened privacy vulnerabilities
in FL's multi-step update paradigm and offers novel perspectives for developing
robust defense strategies.

</details>


### [183] [OFMU: Optimization-Driven Framework for Machine Unlearning](https://arxiv.org/abs/2509.22483)
*Sadia Asif,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: OFMU is a penalty-based bi-level optimization framework for machine unlearning that prioritizes forgetting while preserving retention through hierarchical optimization with provable convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Large language models need to unlearn specific knowledge for regulatory compliance, privacy, and safety without retraining from scratch. Current scalarization approaches lead to unstable training and degraded utility due to conflicting gradient directions.

Method: Proposes OFMU - a penalty-based bi-level optimization framework with inner maximization for forgetting (using similarity-aware penalty to decorrelate gradients) and outer minimization for retention restoration. Uses a two-loop algorithm with convergence guarantees.

Result: Extensive experiments show OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility across vision and language benchmarks. Achieves better trade-offs between forgetting and model utility.

Conclusion: OFMU provides an effective and scalable solution for machine unlearning with theoretical guarantees, addressing the limitations of prior methods through its hierarchical optimization structure and gradient decorrelation approach.

Abstract: Large language models deployed in sensitive applications increasingly require
the ability to unlearn specific knowledge, such as user requests, copyrighted
materials, or outdated information, without retraining from scratch to ensure
regulatory compliance, user privacy, and safety. This task, known as machine
unlearning, aims to remove the influence of targeted data (forgetting) while
maintaining performance on the remaining data (retention). A common approach is
to formulate this as a multi-objective problem and reduce it to a
single-objective problem via scalarization, where forgetting and retention
losses are combined using a weighted sum. However, this often results in
unstable training dynamics and degraded model utility due to conflicting
gradient directions. To address these challenges, we propose OFMU, a
penalty-based bi-level optimization framework that explicitly prioritizes
forgetting while preserving retention through a hierarchical structure. Our
method enforces forgetting via an inner maximization step that incorporates a
similarity-aware penalty to decorrelate the gradients of the forget and
retention objectives, and restores utility through an outer minimization step.
To ensure scalability, we develop a two-loop algorithm with provable
convergence guarantees under both convex and non-convex regimes. We further
provide a rigorous theoretical analysis of convergence rates and show that our
approach achieves better trade-offs between forgetting efficacy and model
utility compared to prior methods. Extensive experiments across vision and
language benchmarks demonstrate that OFMU consistently outperforms existing
unlearning methods in both forgetting efficacy and retained utility.

</details>


### [184] [SHAKE-GNN: Scalable Hierarchical Kirchhoff-Forest Graph Neural Network](https://arxiv.org/abs/2509.22100)
*Zhipu Cui,Johannes Lutzeyer*

Main category: cs.LG

TL;DR: SHAKE-GNN is a scalable graph-level GNN framework using Kirchhoff Forests for multi-resolution graph decomposition, enabling flexible efficiency-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Scaling GNNs to large graphs is challenging, especially for graph-level tasks, requiring more efficient approaches.

Method: Uses hierarchy of Kirchhoff Forests to create stochastic multi-resolution graph decompositions, producing multi-scale representations with data-driven parameter selection.

Result: Achieves competitive performance on large-scale graph classification benchmarks with improved scalability and analyzed time-complexity.

Conclusion: SHAKE-GNN provides an effective scalable solution for graph-level GNN tasks with flexible efficiency-performance trade-offs.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across a range
of learning tasks. However, scaling GNNs to large graphs remains a significant
challenge, especially for graph-level tasks. In this work, we introduce
SHAKE-GNN, a novel scalable graph-level GNN framework based on a hierarchy of
Kirchhoff Forests, a class of random spanning forests used to construct
stochastic multi-resolution decompositions of graphs. SHAKE-GNN produces
multi-scale representations, enabling flexible trade-offs between efficiency
and performance. We introduce an improved, data-driven strategy for selecting
the trade-off parameter and analyse the time-complexity of SHAKE-GNN.
Experimental results on multiple large-scale graph classification benchmarks
demonstrate that SHAKE-GNN achieves competitive performance while offering
improved scalability.

</details>


### [185] [A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches](https://arxiv.org/abs/2509.22484)
*Samuele Punzo,Silvia Giulia Galfrè,Francesco Massafra,Alessandro Maglione,Corrado Priami,Alina Sîrbu*

Main category: cs.LG

TL;DR: Machine learning pipeline using XGBoost and SHAP for MS biomarker discovery from PBMC data, revealing complementary biomarkers to traditional DEA methods.


<details>
  <summary>Details</summary>
Motivation: To discover biomarkers for Multiple Sclerosis by integrating multiple microarray datasets and comparing machine learning approaches with traditional statistical methods.

Method: Integrated 8 PBMC microarray datasets, preprocessed data, trained XGBoost classifier with Bayesian optimization, used SHAP for feature importance, compared with Differential Expression Analysis.

Result: Found both overlapping and unique biomarkers between SHAP and DEA methods. SHAP-selected genes were biologically relevant to MS pathways including sphingolipid signaling, Th1/Th2/Th17 differentiation, and Epstein-Barr virus infection.

Conclusion: Combining explainable AI with traditional statistical methods provides deeper insights into MS disease mechanisms and complementary biomarker discovery.

Abstract: We present a machine learning pipeline for biomarker discovery in Multiple
Sclerosis (MS), integrating eight publicly available microarray datasets from
Peripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we
trained an XGBoost classifier optimized via Bayesian search. SHapley Additive
exPlanations (SHAP) were used to identify key features for model prediction,
indicating thus possible biomarkers. These were compared with genes identified
through classical Differential Expression Analysis (DEA). Our comparison
revealed both overlapping and unique biomarkers between SHAP and DEA,
suggesting complementary strengths. Enrichment analysis confirmed the
biological relevance of SHAP-selected genes, linking them to pathways such as
sphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr
virus infection all known to be associated with MS. This study highlights the
value of combining explainable AI (xAI) with traditional statistical methods to
gain deeper insights into disease mechanism.

</details>


### [186] [Activation Function Design Sustains Plasticity in Continual Learning](https://arxiv.org/abs/2509.22562)
*Lute Lillo,Nick Cheney*

Main category: cs.LG

TL;DR: Activation functions play a crucial role in mitigating plasticity loss in continual learning, with specific nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) showing effectiveness across supervised and reinforcement learning settings.


<details>
  <summary>Details</summary>
Motivation: In continual learning, models can progressively lose adaptation ability (plasticity loss), and the role of activation functions in this failure mode is underexplored compared to i.i.d. training regimes.

Method: Introduce two drop-in nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) based on analysis of negative-branch shape and saturation behavior. Evaluate in supervised class-incremental benchmarks and reinforcement learning with non-stationary MuJoCo environments.

Result: Activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. The proposed nonlinearities sustain plasticity without extra capacity or task-specific tuning.

Conclusion: Thoughtful activation design provides a lightweight, domain-general approach to maintain plasticity in continual learning across different settings.

Abstract: In independent, identically distributed (i.i.d.) training regimes, activation
functions have been benchmarked extensively, and their differences often shrink
once model size and optimization are tuned. In continual learning, however, the
picture is different: beyond catastrophic forgetting, models can progressively
lose the ability to adapt (referred to as loss of plasticity) and the role of
the non-linearity in this failure mode remains underexplored. We show that
activation choice is a primary, architecture-agnostic lever for mitigating
plasticity loss. Building on a property-level analysis of negative-branch shape
and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky
and Randomized Smooth-Leaky) and evaluate them in two complementary settings:
(i) supervised class-incremental benchmarks and (ii) reinforcement learning
with non-stationary MuJoCo environments designed to induce controlled
distribution and dynamics shifts. We also provide a simple stress protocol and
diagnostics that link the shape of the activation to the adaptation under
change. The takeaway is straightforward: thoughtful activation design offers a
lightweight, domain-general way to sustain plasticity in continual learning
without extra capacity or task-specific tuning.

</details>


### [187] [Modeling Psychological Profiles in Volleyball via Mixed-Type Bayesian Networks](https://arxiv.org/abs/2509.22111)
*Maria Iannario,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.LG

TL;DR: The paper introduces latent MMHC, a hybrid structure learning method for discovering directed relationships among mixed-type psychological variables in volleyball players, showing better performance than existing methods and providing interpretable networks for athlete development.


<details>
  <summary>Details</summary>
Motivation: Psychological attributes operate in networks rather than isolation, and there's a need to understand how different traits relate to each other in sports psychology, particularly for athlete development decision support.

Method: Latent MMHC - a hybrid structure learner combining latent Gaussian copula with constraint-based skeleton and constrained score-based refinement to learn directed acyclic graphs (DAGs) from mixed-type variables (ordinal, categorical, continuous). Also includes bootstrap-aggregated variant for stability.

Result: In simulations, latent MMHC achieves lower structural Hamming distance and higher edge recall than recent copula-based learners while maintaining high specificity. Applied to volleyball data, it reveals mental skills organized around goal setting and self-confidence, with emotional arousal linking motivation and anxiety, and Big-Five traits (neuroticism, extraversion) upstream of skill clusters.

Conclusion: The approach provides an interpretable, data-driven framework for profiling psychological traits in sport and supporting decisions in athlete development through scenario analyses that quantify how improvements in specific skills propagate through the network.

Abstract: Psychological attributes rarely operate in isolation: coaches reason about
networks of related traits. We analyze a new dataset of 164 female volleyball
players from Italy's C and D leagues that combines standardized psychological
profiling with background information. To learn directed relationships among
mixed-type variables (ordinal questionnaire scores, categorical demographics,
continuous indicators), we introduce latent MMHC, a hybrid structure learner
that couples a latent Gaussian copula and a constraint-based skeleton with a
constrained score-based refinement to return a single DAG. We also study a
bootstrap-aggregated variant for stability. In simulations spanning sample
size, sparsity, and dimension, latent Max-Min Hill-Climbing (MMHC) attains
lower structural Hamming distance and higher edge recall than recent
copula-based learners while maintaining high specificity. Applied to
volleyball, the learned network organizes mental skills around goal setting and
self-confidence, with emotional arousal linking motivation and anxiety, and
locates Big-Five traits (notably neuroticism and extraversion) upstream of
skill clusters. Scenario analyses quantify how improvements in specific skills
propagate through the network to shift preparation, confidence, and
self-esteem. The approach provides an interpretable, data-driven framework for
profiling psychological traits in sport and for decision support in athlete
development.

</details>


### [188] [From Parameters to Behavior: Unsupervised Compression of the Policy Space](https://arxiv.org/abs/2509.22566)
*Davide Tenedini,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised method to compress DRL policy parameter space into a low-dimensional latent space organized by functional similarity, enabling more efficient learning and task adaptation.


<details>
  <summary>Details</summary>
Motivation: DRL is notoriously sample-inefficient due to optimizing policies directly in high-dimensional parameter spaces, especially problematic in multi-task settings.

Method: Train a generative model that compresses policy parameter space into a low-dimensional latent space using behavioral reconstruction loss, organizing by functional similarity rather than parameter proximity.

Result: Policy network parameterization can be compressed up to five orders of magnitude while retaining most expressivity, and the learned manifold enables task-specific adaptation via Policy Gradient in latent space.

Conclusion: Compressing policy parameter space into functionally organized latent spaces significantly improves DRL efficiency and enables better multi-task adaptation.

Abstract: Despite its recent successes, Deep Reinforcement Learning (DRL) is
notoriously sample-inefficient. We argue that this inefficiency stems from the
standard practice of optimizing policies directly in the high-dimensional and
highly redundant parameter space $\Theta$. This challenge is greatly compounded
in multi-task settings. In this work, we develop a novel, unsupervised approach
that compresses the policy parameter space $\Theta$ into a low-dimensional
latent space $\mathcal{Z}$. We train a generative model
$g:\mathcal{Z}\to\Theta$ by optimizing a behavioral reconstruction loss, which
ensures that the latent space is organized by functional similarity rather than
proximity in parameterization. We conjecture that the inherent dimensionality
of this manifold is a function of the environment's complexity, rather than the
size of the policy network. We validate our approach in continuous control
domains, showing that the parameterization of standard policy networks can be
compressed up to five orders of magnitude while retaining most of its
expressivity. As a byproduct, we show that the learned manifold enables
task-specific adaptation via Policy Gradient operating in the latent space
$\mathcal{Z}$.

</details>


### [189] [Countering adversarial evasion in regression analysis](https://arxiv.org/abs/2509.22113)
*David Benfield,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: Proposes a pessimistic bilevel optimization framework for adversarial regression scenarios, extending adversarial machine learning from classification to regression problems.


<details>
  <summary>Details</summary>
Motivation: Adversarial evasion attacks challenge prediction models by manipulating data, but existing game-theoretic defenses using pessimistic bilevel optimization have only been developed for classification, not regression scenarios.

Method: Develops a pessimistic bilevel optimization program specifically for regression that removes assumptions about convexity and uniqueness of the adversary's optimal strategy.

Result: The proposed framework extends adversarial defense capabilities to regression problems, capturing the antagonistic nature of adversaries without restrictive assumptions.

Conclusion: This work successfully adapts pessimistic bilevel optimization to regression scenarios, providing a foundation for building resilient regression models against adversarial attacks.

Abstract: Adversarial machine learning challenges the assumption that the underlying
distribution remains consistent throughout the training and implementation of a
prediction model. In particular, adversarial evasion considers scenarios where
adversaries adapt their data to influence particular outcomes from established
prediction models, such scenarios arise in applications such as spam email
filtering, malware detection and fake-image generation, where security methods
must be actively updated to keep up with the ever-improving generation of
malicious data. Game theoretic models have been shown to be effective at
modelling these scenarios and hence training resilient predictors against such
adversaries. Recent advancements in the use of pessimistic bilevel optimsiation
which remove assumptions about the convexity and uniqueness of the adversary's
optimal strategy have proved to be particularly effective at mitigating threats
to classifiers due to its ability to capture the antagonistic nature of the
adversary. However, this formulation has not yet been adapted to regression
scenarios. This article serves to propose a pessimistic bilevel optimisation
program for regression scenarios which makes no assumptions on the convexity or
uniqueness of the adversary's solutions.

</details>


### [190] [Quantile Advantage Estimation for Entropy-Safe Reasoning](https://arxiv.org/abs/2509.22611)
*Junkang Wu,Kexin Huang,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: RLVR improves LLM reasoning but suffers from entropy collapse/explosion due to mean baseline issues. QAE replaces mean with quantile baseline, enabling two-regime training that stabilizes entropy and improves performance.


<details>
  <summary>Details</summary>
Motivation: Training in RLVR oscillates between entropy collapse and entropy explosion, traced to the mean baseline improperly penalizing negative-advantage samples under reward outliers.

Method: Propose Quantile Advantage Estimation (QAE) that replaces mean baseline with group-wise K-quantile baseline, creating a response-level two-regime gate: reinforces rare successes on hard queries and targets failures on easy queries.

Result: QAE stabilizes entropy, sparsifies credit assignment (80% responses get zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023 benchmarks.

Conclusion: Baseline design, rather than token-level heuristics, is the primary mechanism for scaling RLVR, with QAE providing two-sided entropy safety.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM
reasoning, but training often oscillates between {entropy collapse} and
{entropy explosion}. We trace both hazards to the mean baseline used in
value-free RL (e.g., GRPO and DAPO), which improperly penalizes
negative-advantage samples under reward outliers. We propose {Quantile
Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile
baseline. QAE induces a response-level, two-regime gate: on hard queries (p <=
1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it
targets remaining failures. Under first-order softmax updates, we prove
{two-sided entropy safety}, giving lower and upper bounds on one-step entropy
change that curb explosion and prevent collapse. Empirically, this minimal
modification stabilizes entropy, sparsifies credit assignment (with tuned K,
roughly 80% of responses receive zero advantage), and yields sustained pass@1
gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results
identify {baseline design} -- rather than token-level heuristics -- as the
primary mechanism for scaling RLVR.

</details>


### [191] [Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models](https://arxiv.org/abs/2509.22121)
*Jeong Eul Kwon,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: VITAL is a variable-aware LLM framework that handles irregular EHR time series by differentiating between frequently recorded vital signs and sporadic lab tests, using language space reprogramming for temporal context and robust missing value handling.


<details>
  <summary>Details</summary>
Motivation: EHR time series have irregular sampling and high missingness due to clinical workflow variability, making traditional time series modeling challenging.

Method: Differentiates vital signs (frequent, temporal) and lab tests (sporadic, non-temporal); reprograms vital signs into language space for temporal context; embeds lab variables using summary values or [Not measured] token.

Result: Outperforms state-of-the-art methods on PhysioNet benchmark datasets and maintains robust performance under high missingness levels.

Conclusion: VITAL effectively addresses EHR time series challenges through variable-aware design and LLM-based temporal reasoning, showing promise for real-world clinical applications.

Abstract: Irregular sampling and high missingness are intrinsic challenges in modeling
time series derived from electronic health records (EHRs),where clinical
variables are measured at uneven intervals depending on workflow and
intervention timing. To address this, we propose VITAL, a variable-aware, large
language model (LLM) based framework tailored for learning from irregularly
sampled physiological time series. VITAL differentiates between two distinct
types of clinical variables: vital signs, which are frequently recorded and
exhibit temporal patterns, and laboratory tests, which are measured
sporadically and lack temporal structure. It reprograms vital signs into the
language space, enabling the LLM to capture temporal context and reason over
missing values through explicit encoding. In contrast, laboratory variables are
embedded either using representative summary values or a learnable [Not
measured] token, depending on their availability. Extensive evaluations on the
benchmark datasets from the PhysioNet demonstrate that VITAL outperforms state
of the art methods designed for irregular time series. Furthermore, it
maintains robust performance under high levels of missingness, which is
prevalent in real world clinical scenarios where key variables are often
unavailable.

</details>


### [192] [IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning](https://arxiv.org/abs/2509.22621)
*Aayush Mishra,Daniel Khashabi,Anqi Liu*

Main category: cs.LG

TL;DR: ICL Activation Alignment (IA2) is a self-distillation technique that uses In-Context Learning's activation patterns to improve Supervised Fine-Tuning, resulting in better accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: ICL offers better generalizability and calibrated responses than SFT in data-scarce settings, but requires more inference compute. The goal is to leverage ICL's internal computations to enhance SFT quality.

Method: IA2 is introduced as a priming step before SFT that replicates ICL's activation patterns in SFT models, incentivizing ICL-like internal reasoning through self-distillation.

Result: Extensive empirical results on 12 benchmarks and 2 model families show IA2 significantly improves accuracy and calibration of model outputs compared to standard SFT.

Conclusion: IA2 not only provides practical improvements but also offers insights into the inner mechanics of model adaptation by bridging ICL and SFT approaches.

Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.

</details>


### [193] [Slicing Wasserstein Over Wasserstein Via Functional Optimal Transport](https://arxiv.org/abs/2509.22138)
*Moritz Piening,Robert Beinert*

Main category: cs.LG

TL;DR: The paper proposes Double-Sliced Wasserstein (DSW) metric as a computationally efficient and numerically stable alternative to Wasserstein over Wasserstein (WoW) distance for comparing meta-measures (distributions over distributions).


<details>
  <summary>Details</summary>
Motivation: Existing sliced WoW accelerations suffer from numerical instability due to reliance on parametric meta-measures or high-order moments, making them computationally costly and unstable.

Method: Leverages the isometry between 1d Wasserstein space and quantile functions in L2([0,1]) space. Introduces a general sliced Wasserstein framework for Banach spaces, using infinite-dimensional L2-projections parametrized by Gaussian processes, combined with classical integration over the Euclidean unit sphere.

Result: DSW minimization is equivalent to WoW minimization for discretized meta-measures while avoiding unstable higher-order moments and achieving computational savings. Numerical experiments on datasets, shapes, and images validate DSW as a scalable substitute.

Conclusion: DSW provides a numerically stable and computationally efficient alternative to WoW distance for comparing meta-measures across various applications including datasets, shapes, and images.

Abstract: Wasserstein distances define a metric between probability measures on
arbitrary metric spaces, including meta-measures (measures over measures). The
resulting Wasserstein over Wasserstein (WoW) distance is a powerful, but
computationally costly tool for comparing datasets or distributions over images
and shapes. Existing sliced WoW accelerations rely on parametric meta-measures
or the existence of high-order moments, leading to numerical instability. As an
alternative, we propose to leverage the isometry between the 1d Wasserstein
space and the quantile functions in the function space $L_2([0,1])$. For this
purpose, we introduce a general sliced Wasserstein framework for arbitrary
Banach spaces. Due to the 1d Wasserstein isometry, this framework defines a
sliced distance between 1d meta-measures via infinite-dimensional
$L_2$-projections, parametrized by Gaussian processes. Combining this 1d
construction with classical integration over the Euclidean unit sphere yields
the double-sliced Wasserstein (DSW) metric for general meta-measures. We show
that DSW minimization is equivalent to WoW minimization for discretized
meta-measures, while avoiding unstable higher-order moments and computational
savings. Numerical experiments on datasets, shapes, and images validate DSW as
a scalable substitute for the WoW distance.

</details>


### [194] [A Theoretical Analysis of Discrete Flow Matching Generative Models](https://arxiv.org/abs/2509.22623)
*Maojiang Su,Mingcheng Lu,Jerry Yao-Chieh Hu,Shang Wu,Zhao Song,Alex Reneau,Han Liu*

Main category: cs.LG

TL;DR: Theoretical analysis of Discrete Flow Matching (DFM) generative models, proving that the generated distribution converges to the true data distribution with increasing training data.


<details>
  <summary>Details</summary>
Motivation: To provide formal guarantees for DFM models by analyzing the distribution estimation error and establishing convergence properties.

Method: Decomposed the final distribution error into velocity field risk, then analyzed approximation error (Transformer capacity) and estimation error (statistical convergence from finite datasets).

Result: Proved that total variation distance between generated and target distributions is controlled by learned velocity field risk, and established convergence rates for both approximation and estimation errors.

Conclusion: First formal proof that DFM-generated distribution provably converges to true data distribution as training set size increases.

Abstract: We provide a theoretical analysis for end-to-end training Discrete Flow
Matching (DFM) generative models. DFM is a promising discrete generative
modeling framework that learns the underlying generative dynamics by training a
neural network to approximate the transformative velocity field. Our analysis
establishes a clear chain of guarantees by decomposing the final distribution
estimation error. We first prove that the total variation distance between the
generated and target distributions is controlled by the risk of the learned
velocity field. We then bound this risk by analyzing its two primary sources:
(i) Approximation Error, where we quantify the capacity of the Transformer
architecture to represent the true velocity, and (ii) Estimation Error, where
we derive statistical convergence rates that bound the error from training on a
finite dataset. By composing these results, we provide the first formal proof
that the distribution generated by a trained DFM model provably converges to
the true data distribution as the training set size increases.

</details>


### [195] [Learning Admissible Heuristics for A*: Theory and Practice](https://arxiv.org/abs/2509.22626)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.LG

TL;DR: This paper introduces Cross-Entropy Admissibility (CEA) to enforce admissibility in learned heuristics, achieving near-admissible heuristics with stronger guidance than compressed PDB heuristics on Rubik's Cube, and provides theoretical generalization guarantees for A-star with ReLU networks.


<details>
  <summary>Details</summary>
Motivation: Recent deep learning approaches for heuristic functions often disregard admissibility (which guarantees solution optimality) and provide limited generalization guarantees beyond training data.

Method: Pose heuristic learning as constrained optimization and introduce Cross-Entropy Admissibility (CEA) loss function. Leverage PDB abstractions and graph structural properties to study sample complexity, and use ReLU neural networks for generalization bounds.

Result: On Rubik's Cube domain, CEA yields near-admissible heuristics with significantly stronger guidance than compressed pattern database heuristics. Theoretical bounds show generalization depends on network width/depth rather than graph size.

Conclusion: The approach addresses both admissibility and generalization limitations, providing the first generalization guarantees for goal-dependent heuristics while maintaining near-admissibility.

Abstract: Heuristic functions are central to the performance of search algorithms such
as A-star, where admissibility - the property of never overestimating the true
shortest-path cost - guarantees solution optimality. Recent deep learning
approaches often disregard admissibility and provide limited guarantees on
generalization beyond the training data. This paper addresses both of these
limitations. First, we pose heuristic learning as a constrained optimization
problem and introduce Cross-Entropy Admissibility (CEA), a loss function that
enforces admissibility during training. On the Rubik's Cube domain, this method
yields near-admissible heuristics with significantly stronger guidance than
compressed pattern database (PDB) heuristics. Theoretically, we study the
sample complexity of learning heuristics. By leveraging PDB abstractions and
the structural properties of graphs such as the Rubik's Cube, we tighten the
bound on the number of training samples needed for A-star to generalize.
Replacing a general hypothesis class with a ReLU neural network gives bounds
that depend primarily on the network's width and depth, rather than on graph
size. Using the same network, we also provide the first generalization
guarantees for goal-dependent heuristics.

</details>


### [196] [Mechanistic Independence: A Principle for Identifiable Disentangled Representations](https://arxiv.org/abs/2509.22196)
*Stefan Matthes,Zhiwei Han,Hao Shen*

Main category: cs.LG

TL;DR: A unified framework for disentangled representations using mechanistic independence criteria that achieves identifiability without relying on statistical assumptions about latent distributions.


<details>
  <summary>Details</summary>
Motivation: Current disentangled representation methods lack clear identifiability guarantees and often rely on statistical assumptions about latent factors, which may not hold in practice.

Method: Proposes mechanistic independence criteria that characterize latent factors by how they act on observed variables rather than their distribution. Includes support-based, sparsity-based, and higher-order conditions with graph-theoretic characterization of latent subspaces.

Result: Shows that each independence criterion yields identifiability of latent subspaces even under nonlinear, non-invertible mixing. Establishes a hierarchy among criteria and provides connected components characterization.

Conclusion: Mechanistic independence provides a robust framework for disentangled representation identifiability that is invariant to changes in latent density and doesn't require statistical assumptions about factor dependencies.

Abstract: Disentangled representations seek to recover latent factors of variation
underlying observed data, yet their identifiability is still not fully
understood. We introduce a unified framework in which disentanglement is
achieved through mechanistic independence, which characterizes latent factors
by how they act on observed variables rather than by their latent distribution.
This perspective is invariant to changes of the latent density, even when such
changes induce statistical dependencies among factors. Within this framework,
we propose several related independence criteria -- ranging from support-based
and sparsity-based to higher-order conditions -- and show that each yields
identifiability of latent subspaces, even under nonlinear, non-invertible
mixing. We further establish a hierarchy among these criteria and provide a
graph-theoretic characterization of latent subspaces as connected components.
Together, these results clarify the conditions under which disentangled
representations can be identified without relying on statistical assumptions.

</details>


### [197] [Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard Overparametrization: The Dynamic Graph Flow Case](https://arxiv.org/abs/2509.22197)
*Duc Thien Nguyen,Konstantinos Slavakis,Eleftherios Kofidis,Dimitris Pados*

Main category: cs.LG

TL;DR: KReTTaH is a regression-based framework for interpretable multi-way data imputation that uses kernel regression with tensor-train parameterization and Hadamard overparametrization for efficient and sparse modeling.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable and efficient method for multi-way data imputation that can handle missing data in complex structures like dynamic graph flows while incorporating domain-specific priors.

Method: Uses kernel regression via reproducing kernel Hilbert spaces with parameter efficiency achieved through fixed tensor-train rank tensors on Riemannian manifolds, enhanced by Hadamard overparametrization for sparsity. Learning is done by solving smooth inverse problems on the Riemannian manifold.

Result: KReTTaH consistently outperforms state-of-the-art alternatives including nonparametric tensor-based and neural-network-based methods for imputing missing, time-varying edge flows in real-world graph datasets.

Conclusion: The proposed KReTTaH framework provides an effective and interpretable solution for multi-way data imputation, particularly for dynamic graph flow estimation, demonstrating superior performance over existing methods while maintaining parameter efficiency and flexibility.

Abstract: A regression-based framework for interpretable multi-way data imputation,
termed Kernel Regression via Tensor Trains with Hadamard overparametrization
(KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting
imputation as regression via reproducing kernel Hilbert spaces. Parameter
efficiency is achieved through tensors of fixed tensor-train (TT) rank, which
reside on low-dimensional Riemannian manifolds, and is further enhanced via
Hadamard overparametrization, which promotes sparsity within the TT parameter
space. Learning is accomplished by solving a smooth inverse problem posed on
the Riemannian manifold of fixed TT-rank tensors. As a representative
application, the estimation of dynamic graph flows is considered. In this
setting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based
(topological) priors via its inverse problem formulation. Numerical tests on
real-world graph datasets demonstrate that KReTTaH consistently outperforms
state-of-the-art alternatives-including a nonparametric tensor- and a
neural-network-based methods-for imputing missing, time-varying edge flows.

</details>


### [198] [A Law of Data Reconstruction for Random Features (and Beyond)](https://arxiv.org/abs/2509.22214)
*Leonardo Iurada,Simone Bombari,Tatiana Tommasi,Marco Mondelli*

Main category: cs.LG

TL;DR: The paper shows that when the number of parameters p exceeds dn (data dimensionality × number of samples), entire training datasets can be reconstructed from model parameters, revealing a threshold for data memorization.


<details>
  <summary>Details</summary>
Motivation: To understand memorization in deep learning from a data reconstruction perspective, going beyond classical interpolation theory that only requires p > n.

Method: Analysis of random features model showing that when p >> dn, the training data subspace in feature space contains enough information to identify individual samples. Proposes an optimization method for dataset reconstruction.

Result: Demonstrated successful reconstruction of entire training datasets from model parameters when p exceeds dn threshold, across various architectures including random features, two-layer networks, and deep ResNets.

Conclusion: Reveals a law of data reconstruction where complete training dataset recovery becomes possible when model parameters exceed the threshold p > dn, providing new insights into memorization in deep learning.

Abstract: Large-scale deep learning models are known to memorize parts of the training
set. In machine learning theory, memorization is often framed as interpolation
or label fitting, and classical results show that this can be achieved when the
number of parameters $p$ in the model is larger than the number of training
samples $n$. In this work, we consider memorization from the perspective of
data reconstruction, demonstrating that this can be achieved when $p$ is larger
than $dn$, where $d$ is the dimensionality of the data. More specifically, we
show that, in the random features model, when $p \gg dn$, the subspace spanned
by the training samples in feature space gives sufficient information to
identify the individual samples in input space. Our analysis suggests an
optimization method to reconstruct the dataset from the model parameters, and
we demonstrate that this method performs well on various architectures (random
features, two-layer fully-connected and deep residual networks). Our results
reveal a law of data reconstruction, according to which the entire training
dataset can be recovered as $p$ exceeds the threshold $dn$.

</details>


### [199] [Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning](https://arxiv.org/abs/2509.22263)
*Nakyeong Yang,Dong-Kyum Kim,Jea Kwon,Minsung Kim,Kyomin Jung,Meeyoung Cha*

Main category: cs.LG

TL;DR: Ssiuu is a new unlearning method that uses attribution-guided regularization to prevent spurious negative influence and faithfully remove target knowledge from language models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods are vulnerable to "relearning" during subsequent training, allowing forgotten knowledge to resurface, due to shallow alignment that creates spurious unlearning neurons instead of truly erasing knowledge.

Method: Ssiuu employs attribution-guided regularization to prevent spurious negative influence and faithfully remove target knowledge from language models.

Result: Experimental results show Ssiuu reliably erases target knowledge and outperforms strong baselines across adversarial injection of private data and benign attack scenarios using instruction-following benchmarks.

Conclusion: The findings highlight the necessity of robust and faithful unlearning methods for safe deployment of language models to address privacy risks from memorized private or sensitive knowledge.

Abstract: Large language models trained on web-scale data can memorize private or
sensitive knowledge, raising significant privacy risks. Although some
unlearning methods mitigate these risks, they remain vulnerable to "relearning"
during subsequent training, allowing a substantial portion of forgotten
knowledge to resurface. In this paper, we show that widely used unlearning
methods cause shallow alignment: instead of faithfully erasing target
knowledge, they generate spurious unlearning neurons that amplify negative
influence to hide it. To overcome this limitation, we introduce Ssiuu, a new
class of unlearning methods that employs attribution-guided regularization to
prevent spurious negative influence and faithfully remove target knowledge.
Experimental results confirm that our method reliably erases target knowledge
and outperforms strong baselines across two practical retraining scenarios: (1)
adversarial injection of private data, and (2) benign attack using an
instruction-following benchmark. Our findings highlight the necessity of robust
and faithful unlearning methods for safe deployment of language models.

</details>


### [200] [Towards a more realistic evaluation of machine learning models for bearing fault diagnosis](https://arxiv.org/abs/2509.22267)
*João Paulo Vieira,Victor Afonso Bauler,Rodrigo Kobashikawa Rosa,Danilo Silva*

Main category: cs.LG

TL;DR: This paper addresses data leakage issues in bearing fault diagnosis using vibration data, proposing a leakage-free evaluation methodology with bearing-wise data partitioning and multi-label classification to improve generalization and reliability.


<details>
  <summary>Details</summary>
Motivation: Many machine learning studies in bearing fault diagnosis fail to generalize to real-world applications due to methodological flaws, particularly data leakage from improper dataset partitioning strategies that create spurious correlations and inflate performance metrics.

Method: Proposes a rigorous leakage-free evaluation methodology using bearing-wise data partitioning to ensure no overlap between training and testing physical components. Also reformulates classification as a multi-label problem for detecting co-occurring faults and uses prevalence-independent metrics like Macro AUROC.

Result: Demonstrates that common dataset partitioning strategies (segment-wise and condition-wise splits) introduce spurious correlations that inflate performance. Shows that bearing-wise partitioning prevents leakage and that dataset diversity (number of unique training bearings) is crucial for robust generalization.

Conclusion: Highlights the importance of leakage-aware evaluation protocols and provides practical guidelines for dataset partitioning, model selection, and validation to develop more trustworthy ML systems for industrial fault diagnosis applications.

Abstract: Reliable detection of bearing faults is essential for maintaining the safety
and operational efficiency of rotating machinery. While recent advances in
machine learning (ML), particularly deep learning, have shown strong
performance in controlled settings, many studies fail to generalize to
real-world applications due to methodological flaws, most notably data leakage.
This paper investigates the issue of data leakage in vibration-based bearing
fault diagnosis and its impact on model evaluation. We demonstrate that common
dataset partitioning strategies, such as segment-wise and condition-wise
splits, introduce spurious correlations that inflate performance metrics. To
address this, we propose a rigorous, leakage-free evaluation methodology
centered on bearing-wise data partitioning, ensuring no overlap between the
physical components used for training and testing. Additionally, we reformulate
the classification task as a multi-label problem, enabling the detection of
co-occurring fault types and the use of prevalence-independent metrics such as
Macro AUROC. Beyond preventing leakage, we also examine the effect of dataset
diversity on generalization, showing that the number of unique training
bearings is a decisive factor for achieving robust performance. We evaluate our
methodology on three widely adopted datasets: CWRU, Paderborn University (PU),
and University of Ottawa (UORED-VAFCLS). This study highlights the importance
of leakage-aware evaluation protocols and provides practical guidelines for
dataset partitioning, model selection, and validation, fostering the
development of more trustworthy ML systems for industrial fault diagnosis
applications.

</details>


### [201] [Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach](https://arxiv.org/abs/2509.22272)
*Nassim Walha,Sebastian G. Gruber,Thomas Decker,Yinchong Yang,Alireza Javanmardi,Eyke Hüllermeier,Florian Buettner*

Main category: cs.LG

TL;DR: Spectral Uncertainty is a novel method using Von Neumann entropy to quantify and decompose uncertainties in LLMs into aleatoric and epistemic components, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in diverse applications, obtaining reliable measures of their predictive uncertainty is critically important, requiring precise distinction between aleatoric uncertainty (from data ambiguities) and epistemic uncertainty (from model limitations).

Method: Leverages Von Neumann entropy from quantum information theory to separate total uncertainty into distinct aleatoric and epistemic components, incorporating fine-grained representation of semantic similarity for nuanced differentiation among semantic interpretations.

Result: Empirical evaluations demonstrate that Spectral Uncertainty outperforms state-of-the-art methods in estimating both aleatoric and total uncertainty across diverse models and benchmark datasets.

Conclusion: The proposed Spectral Uncertainty approach provides a rigorous theoretical foundation for uncertainty quantification in LLMs and shows superior performance compared to existing methods.

Abstract: As Large Language Models (LLMs) are increasingly integrated in diverse
applications, obtaining reliable measures of their predictive uncertainty has
become critically important. A precise distinction between aleatoric
uncertainty, arising from inherent ambiguities within input data, and epistemic
uncertainty, originating exclusively from model limitations, is essential to
effectively address each uncertainty source. In this paper, we introduce
Spectral Uncertainty, a novel approach to quantifying and decomposing
uncertainties in LLMs. Leveraging the Von Neumann entropy from quantum
information theory, Spectral Uncertainty provides a rigorous theoretical
foundation for separating total uncertainty into distinct aleatoric and
epistemic components. Unlike existing baseline methods, our approach
incorporates a fine-grained representation of semantic similarity, enabling
nuanced differentiation among various semantic interpretations in model
responses. Empirical evaluations demonstrate that Spectral Uncertainty
outperforms state-of-the-art methods in estimating both aleatoric and total
uncertainty across diverse models and benchmark datasets.

</details>


### [202] [Unlocking the Power of Mixture-of-Experts for Task-Aware Time Series Analytics](https://arxiv.org/abs/2509.22279)
*Xingjian Wu,Zhengyu Li,Hanyin Cheng,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: PatchMoE is a novel Mixture-of-Experts framework for time series analysis that introduces task-aware routing and channel correlation modeling to overcome limitations of traditional MoE approaches in time series tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional Mixture-of-Experts architectures are task-agnostic and lack capability in modeling channel correlations, making them suboptimal for versatile time series analytics tasks like forecasting, classification, and imputation.

Method: Proposes Recurrent Noisy Gating to utilize hierarchical information for task-specific routing, operates routing on time series tokens in temporal and channel dimensions, and uses Temporal & Channel Load Balancing Loss to model correlations.

Result: Comprehensive experiments on five downstream tasks demonstrate state-of-the-art performance, showing effectiveness across various time series applications.

Conclusion: PatchMoE successfully adapts MoE architecture for time series analytics by incorporating task-awareness and channel correlation modeling, achieving superior performance on multiple tasks.

Abstract: Time Series Analysis is widely used in various real-world applications such
as weather forecasting, financial fraud detection, imputation for missing data
in IoT systems, and classification for action recognization. Mixture-of-Experts
(MoE), as a powerful architecture, though demonstrating effectiveness in NLP,
still falls short in adapting to versatile tasks in time series analytics due
to its task-agnostic router and the lack of capability in modeling channel
correlations. In this study, we propose a novel, general MoE-based time series
framework called PatchMoE to support the intricate ``knowledge'' utilization
for distinct tasks, thus task-aware. Based on the observation that hierarchical
representations often vary across tasks, e.g., forecasting vs. classification,
we propose a Recurrent Noisy Gating to utilize the hierarchical information in
routing, thus obtaining task-sepcific capability. And the routing strategy is
operated on time series tokens in both temporal and channel dimensions, and
encouraged by a meticulously designed Temporal \& Channel Load Balancing Loss
to model the intricate temporal and channel correlations. Comprehensive
experiments on five downstream tasks demonstrate the state-of-the-art
performance of PatchMoE.

</details>


### [203] [Conditional Denoising Diffusion Autoencoders for Wireless Semantic Communications](https://arxiv.org/abs/2509.22282)
*Mehdi Letafati,Samad Ali,Matti Latva-aho*

Main category: cs.LG

TL;DR: The paper proposes diffusion autoencoder models for wireless semantic communication to address limitations of existing autoencoder-based approaches, enabling better semantic-to-clean mapping and improved scalability.


<details>
  <summary>Details</summary>
Motivation: Existing semantic communication systems use autoencoder architectures that tightly couple encoding with matched decoders, causing scalability issues and insufficient exploration of signal distribution. They treat the problem more like channel-adaptive neural encoding rather than proper domain translation.

Method: Proposes diffusion autoencoder models with a neural encoder at the transmitter to extract high-level semantics, and a conditional diffusion model (CDiff) at the receiver that uses source distribution for signal-space denoising. Received semantic latents condition the decoding process to steer it toward intended semantics.

Result: The proposed decoder model is analytically proven to be a consistent estimator of ground-truth data. Extensive simulations on CIFAR-10 and MNIST datasets show performance improvements over legacy autoencoders and variational autoencoders. Multi-user semantic communication simulations identify key factors in realistic setups.

Conclusion: Diffusion autoencoder models effectively address scalability and signal distribution exploration issues in semantic communication, providing a more robust framework for semantic-to-clean mapping that outperforms traditional autoencoder approaches.

Abstract: Semantic communication (SemCom) systems aim to learn the mapping from
low-dimensional semantics to high-dimensional ground-truth. While this is more
akin to a "domain translation" problem, existing frameworks typically emphasize
on channel-adaptive neural encoding-decoding schemes, lacking full exploration
of signal distribution. Moreover, such methods so far have employed
autoencoder-based architectures, where the encoding is tightly coupled to a
matched decoder, causing scalability issues in practice. To address these gaps,
diffusion autoencoder models are proposed for wireless SemCom. The goal is to
learn a "semantic-to-clean" mapping, from the semantic space to the
ground-truth probability distribution. A neural encoder at semantic transmitter
extracts the high-level semantics, and a conditional diffusion model (CDiff) at
the semantic receiver exploits the source distribution for signal-space
denoising, while the received semantic latents are incorporated as the
conditioning input to "steer" the decoding process towards the semantics
intended by the transmitter. It is analytically proved that the proposed
decoder model is a consistent estimator of the ground-truth data. Furthermore,
extensive simulations over CIFAR-10 and MNIST datasets are provided along with
design insights, highlighting the performance compared to legacy autoencoders
and variational autoencoders (VAE). Simulations are further extended to the
multi-user SemCom, identifying the dominating factors in a more realistic
setup.

</details>


### [204] [A Multi-Level Framework for Multi-Objective Hypergraph Partitioning: Combining Minimum Spanning Tree and Proximal Gradient](https://arxiv.org/abs/2509.22294)
*Yingying Li,Mingxuan Xie,Hailong You,Yongqiang Yao,Hongwei Liu*

Main category: cs.LG

TL;DR: A hypergraph partitioning framework using multi-objective non-convex constrained relaxation with MST-based strategies for different data scales, achieving 2-5% average cut size reduction and up to 35% improvement on specific instances compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient hypergraph partitioning method that avoids local optima and enhances partition quality through diverse vertex features and scalable MST-based strategies.

Method: Uses multi-objective non-convex constrained relaxation with accelerated proximal gradient algorithm for vertex features. Employs two MST-based strategies: Prim algorithm with pruning for small data, and representative node selection for large data. Includes refinement strategies like greedy migration, swapping, and recursive MST clustering.

Result: Achieves 2-5% average cut size reduction vs KaHyPar in 2,3,4-way partitioning, with up to 35% improvement on specific instances. Outperforms KaHyPar, hMetis, Mt-KaHyPar, and K-SpecPart on weighted vertex sets. Refinement strategy improves hMetis partitions by up to 16%.

Conclusion: The proposed framework demonstrates superior partitioning quality and competitiveness, with comprehensive evaluation validating its performance trade-offs and effectiveness across different data scales and problem instances.

Abstract: This paper proposes an efficient hypergraph partitioning framework based on a
novel multi-objective non-convex constrained relaxation model. A modified
accelerated proximal gradient algorithm is employed to generate diverse
$k$-dimensional vertex features to avoid local optima and enhance partition
quality. Two MST-based strategies are designed for different data scales: for
small-scale data, the Prim algorithm constructs a minimum spanning tree
followed by pruning and clustering; for large-scale data, a subset of
representative nodes is selected to build a smaller MST, while the remaining
nodes are assigned accordingly to reduce complexity. To further improve
partitioning results, refinement strategies including greedy migration,
swapping, and recursive MST-based clustering are introduced for partitions.
  Experimental results on public benchmark sets demonstrate that the proposed
algorithm achieves reductions in cut size of approximately 2\%--5\% on average
compared to KaHyPar in 2, 3, and 4-way partitioning, with improvements of up to
35\% on specific instances. Particularly on weighted vertex sets, our algorithm
outperforms state-of-the-art partitioners including KaHyPar, hMetis,
Mt-KaHyPar, and K-SpecPart, highlighting its superior partitioning quality and
competitiveness. Furthermore, the proposed refinement strategy improves hMetis
partitions by up to 16\%. A comprehensive evaluation based on virtual instance
methodology and parameter sensitivity analysis validates the algorithm's
competitiveness and characterizes its performance trade-offs.

</details>


### [205] [Aurora: Towards Universal Generative Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.22295)
*Xingjian Wu,Jianxin Jin,Wanghui Qiu,Peng Chen,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: Aurora is a multimodal time series foundation model that supports zero-shot inference and cross-domain generalization by extracting domain knowledge from text and image modalities to guide temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing unimodal time series models lack explicit utilization of domain-specific knowledge from other modalities, while multimodal supervised models don't support zero-shot inference for cross-domain scenarios.

Method: Uses tokenization, encoding, and distillation to extract multimodal domain knowledge, employs Modality-Guided Multi-head Self-Attention to inject knowledge into temporal modeling, and uses Prototype-Guided Flow Matching for generative probabilistic forecasting.

Result: Achieves state-of-the-art performance on TimeMMD, TSFM-Bench and ProbTS benchmarks in both unimodal and multimodal scenarios.

Conclusion: Aurora demonstrates strong cross-domain generalization capability through adaptive extraction of multimodal domain knowledge and supports zero-shot inference.

Abstract: Cross-domain generalization is very important in Time Series Forecasting
because similar historical information may lead to distinct future trends due
to the domain-specific characteristics. Recent works focus on building unimodal
time series foundation models and end-to-end multimodal supervised models.
Since domain-specific knowledge is often contained in modalities like texts,
the former lacks the explicit utilization of them, thus hindering the
performance. The latter is tailored for end-to-end scenarios and does not
support zero-shot inference for cross-domain scenarios. In this work, we
introduce Aurora, a Multimodal Time Series Foundation Model, which supports
multimodal inputs and zero-shot inference. Pretrained on Corss-domain
Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key
domain knowledge contained in corrsponding text or image modalities, thus
possessing strong Cross-domain generalization capability. Through tokenization,
encoding, and distillation, Aurora can extract multimodal domain knowledge as
guidance and then utilizes a Modality-Guided Multi-head Self-Attention to
inject them into the modeling of temporal representations. In the decoding
phase, the multimodal representations are used to generate the conditions and
prototypes of future tokens, contributing to a novel Prototype-Guided Flow
Matching for generative probabilistic forecasting. Comprehensive experiments on
well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS,
demonstrate the consistent state-of-the-art performance of Aurora on both
unimodal and multimodal scenarios.

</details>


### [206] [SoDaDE: Solvent Data-Driven Embeddings with Small Transformer Models](https://arxiv.org/abs/2509.22302)
*Gabriel Kitso Gibberd,Jose Pablo Folch,Antonio Del Rio Chanona*

Main category: cs.LG

TL;DR: SoDaDE is a new solvent representation scheme that uses a small transformer model and solvent property dataset to create data-driven fingerprints for solvents, outperforming previous representations in predicting yields.


<details>
  <summary>Details</summary>
Motivation: Generic chemical representations lack physical context specific to solvents, while harmful solvent use is a major climate issue in the chemical industry. There's growing interest in green solvent replacement that requires better solvent-specific representations.

Method: Developed Solvent Data Driven Embeddings (SoDaDE) using a small transformer model trained on solvent property datasets to create specialized solvent fingerprints.

Result: SoDaDE outperformed previous representations when used to predict yields on a recently published dataset, demonstrating effectiveness of the approach.

Conclusion: Data-driven fingerprints can be successfully created with small datasets, and the workflow can be adapted for other chemical applications beyond solvents.

Abstract: Computational representations have become crucial in unlocking the recent
growth of machine learning algorithms for chemistry. Initially hand-designed,
machine learning has shown that meaningful representations can be learnt from
data. Chemical datasets are limited and so the representations learnt from data
are generic, being trained on broad datasets which contain shallow information
on many different molecule types. For example, generic fingerprints lack
physical context specific to solvents. However, the use of harmful solvents is
a leading climate-related issue in the chemical industry, and there is a surge
of interest in green solvent replacement. To empower this research, we propose
a new solvent representation scheme by developing Solvent Data Driven
Embeddings (SoDaDE). SoDaDE uses a small transformer model and solvent property
dataset to create a fingerprint for solvents. To showcase their effectiveness,
we use SoDaDE to predict yields on a recently published dataset, outperforming
previous representations. We demonstrate through this paper that data-driven
fingerprints can be made with small datasets and set-up a workflow that can be
explored for other applications.

</details>


### [207] [Distributed Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2509.22321)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: A distributed associative memory framework where agents optimize local memory through communication over routing trees, achieving sublinear regret.


<details>
  <summary>Details</summary>
Motivation: To enable distributed associative memory recall across multiple agents while maintaining local associations and selective information sharing.

Method: Distributed online gradient descent method with communication over routing trees to optimize local associative memories.

Result: The proposed protocol consistently outperforms existing online optimization baselines in experiments.

Conclusion: The distributed approach effectively enables associative memory operations across multiple agents with theoretical guarantees.

Abstract: An associative memory (AM) enables cue-response recall, and associative
memorization has recently been noted to underlie the operation of modern neural
architectures such as Transformers. This work addresses a distributed setting
where agents maintain a local AM to recall their own associations as well as
selective information from others. Specifically, we introduce a distributed
online gradient descent method that optimizes local AMs at different agents
through communication over routing trees. Our theoretical analysis establishes
sublinear regret guarantees, and experiments demonstrate that the proposed
protocol consistently outperforms existing online optimization baselines.

</details>


### [208] [Neural Feature Geometry Evolves as Discrete Ricci Flow](https://arxiv.org/abs/2509.22362)
*Moritz Hehl,Max von Renesse,Melanie Weber*

Main category: cs.LG

TL;DR: This paper investigates neural feature geometry through discrete geometry, showing that feature representations evolve analogous to discrete Ricci flow during training, with nonlinear activations playing a crucial role in shaping geometry.


<details>
  <summary>Details</summary>
Motivation: Despite neural networks' empirical success, understanding of neural feature representations remains incomplete. The paper aims to investigate neural feature geometry through the lens of discrete geometry to better understand how networks transform input data manifolds.

Method: Approximates input data manifold using geometric graphs encoding local similarity structure. Provides theoretical analysis of graph evolution during training and introduces framework for evaluating geometric transformations by comparison with discrete Ricci flow dynamics.

Result: Shows that nonlinear activations are crucial for shaping feature geometry in feedforward networks. Geometric transformations resemble discrete Ricci flow, with class separability corresponding to emergence of community structure in graph representations. Supported by experiments on 20,000+ networks across synthetic and real-world datasets.

Conclusion: The connection to discrete Ricci flow suggests practical design principles including geometry-informed early-stopping heuristic and criterion for selecting network depth, providing new insights into neural feature geometry evolution.

Abstract: Deep neural networks learn feature representations via complex geometric
transformations of the input data manifold. Despite the models' empirical
success across domains, our understanding of neural feature representations is
still incomplete. In this work we investigate neural feature geometry through
the lens of discrete geometry. Since the input data manifold is typically
unobserved, we approximate it using geometric graphs that encode local
similarity structure. We provide theoretical results on the evolution of these
graphs during training, showing that nonlinear activations play a crucial role
in shaping feature geometry in feedforward neural networks. Moreover, we
discover that the geometric transformations resemble a discrete Ricci flow on
these graphs, suggesting that neural feature geometry evolves analogous to
Ricci flow. This connection is supported by experiments on over 20,000
feedforward neural networks trained on binary classification tasks across both
synthetic and real-world datasets. We observe that the emergence of class
separability corresponds to the emergence of community structure in the
associated graph representations, which is known to relate to discrete Ricci
flow dynamics. Building on these insights, we introduce a novel framework for
locally evaluating geometric transformations through comparison with discrete
Ricci flow dynamics. Our results suggest practical design principles, including
a geometry-informed early-stopping heuristic and a criterion for selecting
network depth.

</details>


### [209] [Investigating Faithfulness in Large Audio Language Models](https://arxiv.org/abs/2509.22363)
*Lovenya Jain,Pooneh Mousavi,Mirco Ravanelli,Cem Subakan*

Main category: cs.LG

TL;DR: This paper investigates the faithfulness of chain-of-thought (CoT) reasoning in large audio-language models (LALMs) using targeted interventions on reasoning datasets.


<details>
  <summary>Details</summary>
Motivation: Faithfulness of CoT representations is critical for safety-sensitive applications in audio-language models, where reasoning is more challenging as models must extract clues from audio before reasoning.

Method: Applied targeted interventions including paraphrasing, filler token injection, early answering, and introducing mistakes on two reasoning datasets: SAKURA and MMAR.

Result: Experiments suggest that LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.

Conclusion: Unlike text-based LLMs where CoTs are often unfaithful, LALMs show promising faithfulness in their reasoning processes.

Abstract: Faithfulness measures whether chain-of-thought (CoT) representations
accurately reflect a model's decision process and can be used as reliable
explanations. Prior work has shown that CoTs from text-based LLMs are often
unfaithful. This question has not been explored for large audio-language models
(LALMs), where faithfulness is critical for safety-sensitive applications.
Reasoning in LALMs is also more challenging, as models must first extract
relevant clues from audio before reasoning over them. In this paper, we
investigate the faithfulness of CoTs produced by several LALMs by applying
targeted interventions, including paraphrasing, filler token injection, early
answering, and introducing mistakes, on two challenging reasoning datasets:
SAKURA and MMAR. After going through the aforementioned interventions across
several datasets and tasks, our experiments suggest that, LALMs generally
produce CoTs that appear to be faithful to their underlying decision processes.

</details>


### [210] [Role-Aware Multi-modal federated learning system for detecting phishing webpages](https://arxiv.org/abs/2509.22369)
*Bo Wang,Imran Khan,Martin White,Natalia Beloff*

Main category: cs.LG

TL;DR: A federated multi-modal phishing detector supporting URL, HTML, and IMAGE inputs with flexible modality usage at inference, using role-aware bucket aggregation on FedProx with hard-gated experts for stable training.


<details>
  <summary>Details</summary>
Motivation: To create a phishing detection system that supports multiple modalities (URL, HTML, IMAGE) without binding clients to fixed modalities, enabling flexible usage while maintaining privacy through federated learning.

Method: Proposed role-aware bucket aggregation on FedProx with hard gating (selecting IMAGE/HTML/URL expert by sample modality), separate aggregation of modality-specific parameters, using GraphCodeBERT for URLs and early three-way embedding for HTML.

Result: Fusion head achieved 97.5% accuracy with 2.4% FPR across two data types; image subset: 95.5% accuracy with 5.9% FPR; WebPhish (HTML): 96.5% accuracy with 1.8% FPR; TR-OP (raw HTML): 95.1% accuracy with 4.6% FPR.

Conclusion: Bucket aggregation with hard-gated experts enables stable federated training under privacy constraints while improving usability and flexibility of multi-modal phishing detection.

Abstract: We present a federated, multi-modal phishing website detector that supports
URL, HTML, and IMAGE inputs without binding clients to a fixed modality at
inference: any client can invoke any modality head trained elsewhere.
Methodologically, we propose role-aware bucket aggregation on top of FedProx,
inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use
hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling
separate aggregation of modality-specific parameters to isolate cross-embedding
conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc
97.5% with FPR 2.4% across two data types; on the image subset (ablation) it
attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an
early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc
96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results
indicate that bucket aggregation with hard-gated experts enables stable
federated training under strict privacy, while improving the usability and
flexibility of multi-modal phishing detection.

</details>


### [211] [Enhancing Credit Risk Prediction: A Meta-Learning Framework Integrating Baseline Models, LASSO, and ECOC for Superior Accuracy](https://arxiv.org/abs/2509.22381)
*Haibo Wang,Lutfu S. Sua,Jun Huang,Figen Balo,Burak Dolar*

Main category: cs.LG

TL;DR: A meta-learning framework combining multiple ML models for credit risk assessment that improves classification accuracy and default probability prediction while addressing high-dimensional data and class imbalance issues.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning faces challenges with high-dimensional data, limited interpretability, rare event detection, and multi-class imbalance in credit risk assessment, requiring a more robust approach.

Method: Comprehensive meta-learning framework combining supervised learning (XGBoost, Random Forest, SVM, Decision Tree), unsupervised methods (KNN), deep learning (MLP), LASSO for feature selection, and Error-Correcting Output Codes as meta-classifier for imbalanced multi-class problems, with Permutation Feature Importance for transparency.

Result: The framework significantly enhances accuracy of financial entity classification for credit rating migrations (upgrades/downgrades) and default probability estimation, validated on Corporate Credit Ratings dataset with 2,029 US companies.

Conclusion: The meta-learning approach provides a more holistic and accurate computational model for credit risk assessment, addressing fundamental challenges in financial decision support.

Abstract: Effective credit risk management is fundamental to financial decision-making,
necessitating robust models for default probability prediction and financial
entity classification. Traditional machine learning approaches face significant
challenges when confronted with high-dimensional data, limited
interpretability, rare event detection, and multi-class imbalance problems in
risk assessment. This research proposes a comprehensive meta-learning framework
that synthesizes multiple complementary models: supervised learning algorithms,
including XGBoost, Random Forest, Support Vector Machine, and Decision Tree;
unsupervised methods such as K-Nearest Neighbors; deep learning architectures
like Multilayer Perceptron; alongside LASSO regularization for feature
selection and dimensionality reduction; and Error-Correcting Output Codes as a
meta-classifier for handling imbalanced multi-class problems. We implement
Permutation Feature Importance analysis for each prediction class across all
constituent models to enhance model transparency. Our framework aims to
optimize predictive performance while providing a more holistic approach to
credit risk assessment. This research contributes to the development of more
accurate and reliable computational models for strategic financial decision
support by addressing three fundamental challenges in credit risk modeling. The
empirical validation of our approach involves an analysis of the Corporate
Credit Ratings dataset with credit ratings for 2,029 publicly listed US
companies. Results demonstrate that our meta-learning framework significantly
enhances the accuracy of financial entity classification regarding credit
rating migrations (upgrades and downgrades) and default probability estimation.

</details>


### [212] [(Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation for Interpretable Classification](https://arxiv.org/abs/2509.22384)
*Luca Bergamin,Roberto Confalonieri,Fabio Aiolli*

Main category: cs.LG

TL;DR: A differentiable L0 regularization method is applied to Multi-layer Logical Perceptron (MLLP) networks to reduce complexity of interpretable Concept Rule Sets (CRS) while maintaining performance, comparing against random binarization heuristics.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack interpretability, which is crucial for many AI applications where high performance alone is insufficient for adoption. Model transparency and interpretability are key requirements.

Method: Adapted differentiable approximation of L0 regularization into Multi-layer Logical Perceptron (MLLP) networks, comparing with Random Binarization heuristics for network weight sparsification.

Result: The study evaluates the trade-off between CRS complexity and performance, determining if L0 regularization provides better results than random-based sparsification techniques.

Conclusion: The work demonstrates that using differentiable L0 regularization can effectively reduce interpretable model complexity while preserving performance, offering a less-noisy alternative to random binarization methods.

Abstract: Deep neural networks are widely used in practical applications of AI,
however, their inner structure and complexity made them generally not easily
interpretable. Model transparency and interpretability are key requirements for
multiple scenarios where high performance is not enough to adopt the proposed
solution. In this work, a differentiable approximation of $L_0$ regularization
is adapted into a logic-based neural network, the Multi-layer Logical
Perceptron (MLLP), to study its efficacy in reducing the complexity of its
discrete interpretable version, the Concept Rule Set (CRS), while retaining its
performance. The results are compared to alternative heuristics like Random
Binarization of the network weights, to determine if better results can be
achieved when using a less-noisy technique that sparsifies the network based on
the loss function instead of a random distribution. The trade-off between the
CRS complexity and its performance is discussed.

</details>


### [213] [Improving accuracy in short mortality rate series: Exploring Multi-step Forecasting Approaches in Hybrid Systems](https://arxiv.org/abs/2509.22395)
*Filipe C. L. Duarte,Paulo S. G. de Mattos Neto,Paulo R. A. Firmino*

Main category: cs.LG

TL;DR: Hybrid ARIMA-LSTM model with recursive multi-step forecasting approach outperforms other models for mortality rate prediction.


<details>
  <summary>Details</summary>
Motivation: Accurate mortality forecasting is crucial for insurance and pension markets, especially with declining interest rates and economic stabilization. Multi-step predictions are essential but challenging with limited data.

Method: Evaluated hybrid systems combining statistical and ML models using three multi-step forecasting approaches (Recursive, Direct, MIMO) across 12 datasets and 21 models.

Result: ARIMA-LSTM hybrid with recursive approach showed superior performance compared to other models in most cases.

Conclusion: Both the multi-step forecasting approach and ML model selection are critical for improving mortality forecasting accuracy in hybrid systems.

Abstract: The decline in interest rates and economic stabilization has heightened the
importance of accurate mortality rate forecasting, particularly in insurance
and pension markets. Multi-step-ahead predictions are crucial for public
health, demographic planning, and insurance risk assessments; however, they
face challenges when data are limited. Hybrid systems that combine statistical
and Machine Learning (ML) models offer a promising solution for handling both
linear and nonlinear patterns. This study evaluated the impact of different
multi-step forecasting approaches (Recursive, Direct, and Multi-Input
Multi-Output) and ML models on the accuracy of hybrid systems. Results from 12
datasets and 21 models show that the selection of both the multi-step approach
and the ML model is essential for improving performance, with the ARIMA-LSTM
hybrid using a recursive approach outperforming other models in most cases.

</details>


### [214] [ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation](https://arxiv.org/abs/2509.22402)
*Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu*

Main category: cs.LG

TL;DR: ReLAM is a framework that automatically generates dense rewards from video demonstrations for visual RL in robotic manipulation by learning an anticipation model that proposes keypoint-based subgoals.


<details>
  <summary>Details</summary>
Motivation: Reward design is a bottleneck in visual RL for robotics, as precise positional information is often unavailable in real-world settings due to sensory limitations.

Method: Uses keypoints to infer spatial distances, learns an anticipation model as a planner to propose intermediate subgoals, and provides continuous rewards to train a goal-conditioned policy under HRL framework.

Result: Significantly accelerates learning and achieves superior performance on complex, long-horizon manipulation tasks compared to state-of-the-art methods.

Conclusion: ReLAM effectively addresses reward design challenges in visual RL by automatically generating structured rewards from demonstrations, enabling efficient learning of complex manipulation tasks.

Abstract: Reward design remains a critical bottleneck in visual reinforcement learning
(RL) for robotic manipulation. In simulated environments, rewards are
conventionally designed based on the distance to a target position. However,
such precise positional information is often unavailable in real-world visual
settings due to sensory and perceptual limitations. In this study, we propose a
method that implicitly infers spatial distances through keypoints extracted
from images. Building on this, we introduce Reward Learning with Anticipation
Model (ReLAM), a novel framework that automatically generates dense, structured
rewards from action-free video demonstrations. ReLAM first learns an
anticipation model that serves as a planner and proposes intermediate
keypoint-based subgoals on the optimal path to the final goal, creating a
structured learning curriculum directly aligned with the task's geometric
objectives. Based on the anticipated subgoals, a continuous reward signal is
provided to train a low-level, goal-conditioned policy under the hierarchical
reinforcement learning (HRL) framework with provable sub-optimality bound.
Extensive experiments on complex, long-horizon manipulation tasks show that
ReLAM significantly accelerates learning and achieves superior performance
compared to state-of-the-art methods.

</details>


### [215] [MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning](https://arxiv.org/abs/2509.22403)
*Fanjin Meng,Yuan Yuan,Jingtao Ding,Jie Feng,Chonghua Han,Yong Li*

Main category: cs.LG

TL;DR: MoveFM-R is a framework that combines mobility foundation models with large language models to improve human mobility modeling by bridging geographic and semantic understanding gaps.


<details>
  <summary>Details</summary>
Motivation: Current mobility foundation models have limitations in data scale and semantic understanding, while LLMs lack spatio-temporal statistical knowledge needed for realistic mobility trajectory generation.

Method: Uses three innovations: semantically enhanced location encoding, progressive curriculum for LLM-mobility alignment, and interactive self-reflection mechanism for conditional trajectory generation.

Result: Significantly outperforms existing MFM-based and LLM-based baselines, shows robust zero-shot generalization, and excels at generating realistic trajectories from natural language instructions.

Conclusion: MoveFM-R pioneers a new paradigm that synthesizes statistical power of MFMs with semantic understanding of LLMs for more comprehensive, interpretable, and powerful human mobility modeling.

Abstract: Mobility Foundation Models (MFMs) have advanced the modeling of human
movement patterns, yet they face a ceiling due to limitations in data scale and
semantic understanding. While Large Language Models (LLMs) offer powerful
semantic reasoning, they lack the innate understanding of spatio-temporal
statistics required for generating physically plausible mobility trajectories.
To address these gaps, we propose MoveFM-R, a novel framework that unlocks the
full potential of mobility foundation models by leveraging language-driven
semantic reasoning capabilities. It tackles two key challenges: the vocabulary
mismatch between continuous geographic coordinates and discrete language
tokens, and the representation gap between the latent vectors of MFMs and the
semantic world of LLMs. MoveFM-R is built on three core innovations: a
semantically enhanced location encoding to bridge the geography-language gap, a
progressive curriculum to align the LLM's reasoning with mobility patterns, and
an interactive self-reflection mechanism for conditional trajectory generation.
Extensive experiments demonstrate that MoveFM-R significantly outperforms
existing MFM-based and LLM-based baselines. It also shows robust generalization
in zero-shot settings and excels at generating realistic trajectories from
natural language instructions. By synthesizing the statistical power of MFMs
with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm
that enables a more comprehensive, interpretable, and powerful modeling of
human mobility. The implementation of MoveFM-R is available online at
https://anonymous.4open.science/r/MoveFM-R-CDE7/.

</details>


### [216] [Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators](https://arxiv.org/abs/2509.22411)
*Xiao Xue,Marco F. P. ten Eikelder,Mingyang Gao,Xiaoyuan Cheng,Yiming Yang,Yi He,Shuo Wang,Sibo Cheng,Yukun Hu,Peter V. Coveney*

Main category: cs.LG

TL;DR: A physics-informed neural operator framework for the lattice Boltzmann equation that enables large-time prediction without step-by-step integration, bypassing collision kernel computations.


<details>
  <summary>Details</summary>
Motivation: Traditional LBE numerical solutions are computationally intensive due to strict time-step restrictions from collision kernels.

Method: Physics-informed neural operator framework incorporating moment-matching constraints and global equivariance, enabling discretization-invariant and collision-model-agnostic predictions.

Result: Demonstrated robustness across complex flow scenarios including von Karman vortex shedding, ligament breakup, and bubble adhesion.

Conclusion: Establishes a new data-driven pathway for modeling kinetic systems with improved computational efficiency and generalization capabilities.

Abstract: The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a
powerful framework for capturing complex flow behaviour by describing the
evolution of single-particle distribution functions (PDFs). Despite its
success, solving the LBE numerically remains computationally intensive due to
strict time-step restrictions imposed by collision kernels. Here, we introduce
a physics-informed neural operator framework for the LBE that enables
prediction over large time horizons without step-by-step integration,
effectively bypassing the need to explicitly solve the collision kernel. We
incorporate intrinsic moment-matching constraints of the LBE, along with global
equivariance of the full distribution field, enabling the model to capture the
complex dynamics of the underlying kinetic system. Our framework is
discretization-invariant, enabling models trained on coarse lattices to
generalise to finer ones (kinetic super-resolution). In addition, it is
agnostic to the specific form of the underlying collision model, which makes it
naturally applicable across different kinetic datasets regardless of the
governing dynamics. Our results demonstrate robustness across complex flow
scenarios, including von Karman vortex shedding, ligament breakup, and bubble
adhesion. This establishes a new data-driven pathway for modelling kinetic
systems.

</details>


### [217] [One Prompt Fits All: Universal Graph Adaptation for Pretrained Models](https://arxiv.org/abs/2509.22416)
*Yongqi Huang,Jitao Zhao,Dongxiao He,Xiaobao Wang,Yawen Li,Yuxiao Huang,Di Jin,Zhiyong Feng*

Main category: cs.LG

TL;DR: This paper analyzes Graph Prompt Learning (GPL) limitations and proposes UniPrompt, a method that adapts pretrained models to downstream tasks while preserving graph structure.


<details>
  <summary>Details</summary>
Motivation: Current GPL methods lack consensus on underlying mechanisms and have limited adaptability across diverse downstream scenarios, especially under data distribution shifts.

Method: The authors theoretically analyze existing GPL approaches and propose UniPrompt, which focuses on unleashing pretrained model capabilities while adapting classifiers to downstream scenarios.

Result: Extensive experiments show UniPrompt effectively integrates with various pretrained models and achieves strong performance across in-domain and cross-domain scenarios.

Conclusion: Graph prompt learning should focus on unleashing pretrained model capabilities rather than fine-tuning, and UniPrompt provides an effective solution that adapts to diverse downstream scenarios.

Abstract: Graph Prompt Learning (GPL) has emerged as a promising paradigm that bridges
graph pretraining models and downstream scenarios, mitigating label dependency
and the misalignment between upstream pretraining and downstream tasks.
Although existing GPL studies explore various prompt strategies, their
effectiveness and underlying principles remain unclear. We identify two
critical limitations: (1) Lack of consensus on underlying mechanisms: Despite
current GPLs have advanced the field, there is no consensus on how prompts
interact with pretrained models, as different strategies intervene at varying
spaces within the model, i.e., input-level, layer-wise, and
representation-level prompts. (2) Limited scenario adaptability: Most methods
fail to generalize across diverse downstream scenarios, especially under data
distribution shifts (e.g., homophilic-to-heterophilic graphs). To address these
issues, we theoretically analyze existing GPL approaches and reveal that
representation-level prompts essentially function as fine-tuning a simple
downstream classifier, proposing that graph prompt learning should focus on
unleashing the capability of pretrained models, and the classifier adapts to
downstream scenarios. Based on our findings, we propose UniPrompt, a novel GPL
method that adapts any pretrained models, unleashing the capability of
pretrained models while preserving the structure of the input graph. Extensive
experiments demonstrate that our method can effectively integrate with various
pretrained models and achieve strong performance across in-domain and
cross-domain scenarios.

</details>


### [218] [The Flood Complex: Large-Scale Persistent Homology on Millions of Points](https://arxiv.org/abs/2509.22432)
*Florian Graf,Paolo Pellizzoni,Martin Uray,Stefan Huber,Roland Kwitt*

Main category: cs.LG

TL;DR: The paper introduces the Flood complex, a scalable alternative to Vietoris-Rips complexes for persistent homology computation on large Euclidean point clouds, enabling PH computation on millions of points with improved classification performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Vietoris-Rips complexes face computational limitations due to exponential growth in simplex count, preventing their use on large-scale point clouds for downstream machine learning tasks.

Method: The Flood complex construction uses Delaunay triangulation of a small subset of points and includes simplices fully covered by balls of radius r emanating from the full point cloud (flooding process), allowing efficient PH computation and GPU parallelization.

Result: Scaling experiments on 3D point clouds demonstrate PH computation up to dimension 2 on several million points, with superior object classification performance on real-world and synthetic data compared to other PH-based methods and neural networks.

Conclusion: The Flood complex provides a scalable solution for persistent homology computation on large point clouds, with evidence that this scaling capability is necessary for geometrically or topologically complex objects, outperforming existing methods.

Abstract: We consider the problem of computing persistent homology (PH) for large-scale
Euclidean point cloud data, aimed at downstream machine learning tasks, where
the exponential growth of the most widely-used Vietoris-Rips complex imposes
serious computational limitations. Although more scalable alternatives such as
the Alpha complex or sparse Rips approximations exist, they often still result
in a prohibitively large number of simplices. This poses challenges in the
complex construction and in the subsequent PH computation, prohibiting their
use on large-scale point clouds. To mitigate these issues, we introduce the
Flood complex, inspired by the advantages of the Alpha and Witness complex
constructions. Informally, at a given filtration value $r\geq 0$, the Flood
complex contains all simplices from a Delaunay triangulation of a small subset
of the point cloud $X$ that are fully covered by balls of radius $r$ emanating
from $X$, a process we call flooding. Our construction allows for efficient PH
computation, possesses several desirable theoretical properties, and is
amenable to GPU parallelization. Scaling experiments on 3D point cloud data
show that we can compute PH of up to dimension 2 on several millions of points.
Importantly, when evaluating object classification performance on real-world
and synthetic data, we provide evidence that this scaling capability is needed,
especially if objects are geometrically or topologically complex, yielding
performance superior to other PH-based methods and neural networks for point
cloud data.

</details>


### [219] [Overclocking Electrostatic Generative Models](https://arxiv.org/abs/2509.22454)
*Daniil Shlenskii,Alexander Korotin*

Main category: cs.LG

TL;DR: IPFM is a novel distillation framework that accelerates electrostatic generative models (PFGM++) by learning a generator whose induced electrostatic field matches the teacher's, achieving near-teacher quality with few function evaluations.


<details>
  <summary>Details</summary>
Motivation: Electrostatic generative models like PFGM++ achieve state-of-the-art performance but are computationally expensive due to ODE simulations, requiring acceleration methods.

Method: Propose Inverse Poisson Flow Matching (IPFM) that formulates distillation as an inverse problem - learning a generator whose electrostatic field matches the teacher's, with a tractable training objective.

Result: IPFM produces distilled generators achieving near-teacher or superior sample quality using only a few function evaluations, with faster convergence for finite D than diffusion models.

Conclusion: IPFM effectively accelerates electrostatic generative models across all D values, with finite-D models showing more favorable optimization properties than diffusion models.

Abstract: Electrostatic generative models such as PFGM++ have recently emerged as a
powerful framework, achieving state-of-the-art performance in image synthesis.
PFGM++ operates in an extended data space with auxiliary dimensionality $D$,
recovering the diffusion model framework as $D\to\infty$, while yielding
superior empirical results for finite $D$. Like diffusion models, PFGM++ relies
on expensive ODE simulations to generate samples, making it computationally
costly. To address this, we propose Inverse Poisson Flow Matching (IPFM), a
novel distillation framework that accelerates electrostatic generative models
across all values of $D$. Our IPFM reformulates distillation as an inverse
problem: learning a generator whose induced electrostatic field matches that of
the teacher. We derive a tractable training objective for this problem and show
that, as $D \to \infty$, our IPFM closely recovers Score Identity Distillation
(SiD), a recent method for distilling diffusion models. Empirically, our IPFM
produces distilled generators that achieve near-teacher or even superior sample
quality using only a few function evaluations. Moreover, we observe that
distillation converges faster for finite $D$ than in the $D \to \infty$
(diffusion) limit, which is consistent with prior findings that finite-$D$
PFGM++ models exhibit more favorable optimization and sampling properties.

</details>


### [220] [Nonlinear Optimization with GPU-Accelerated Neural Network Constraints](https://arxiv.org/abs/2509.22462)
*Robert Parker,Oscar Dowson,Nicole LoGiudice,Manuel Garcia,Russell Bent*

Main category: cs.LG

TL;DR: A reduced-space formulation for optimizing over trained neural networks that treats networks as gray boxes, leading to faster optimization solves and fewer iterations compared to full-space approaches.


<details>
  <summary>Details</summary>
Motivation: To improve optimization performance when working with trained neural networks by avoiding exposure of intermediate variables and constraints to the solver, particularly beneficial for GPU-accelerated computations.

Method: Treat neural networks as gray boxes where only inputs and outputs are exposed to the optimization solver, rather than exposing all intermediate variables and constraints as in full-space formulations.

Result: The reduced-space formulation achieves faster solves and fewer iterations in interior point methods compared to full-space approaches, demonstrated on adversarial generation for MNIST classifiers and security-constrained optimal power flow problems.

Conclusion: The gray box approach provides computational advantages for optimization over trained neural networks, making it particularly suitable for GPU-accelerated applications where network outputs and derivatives are evaluated on GPU.

Abstract: We propose a reduced-space formulation for optimizing over trained neural
networks where the network's outputs and derivatives are evaluated on a GPU. To
do this, we treat the neural network as a "gray box" where intermediate
variables and constraints are not exposed to the optimization solver. Compared
to the full-space formulation, in which intermediate variables and constraints
are exposed to the optimization solver, the reduced-space formulation leads to
faster solves and fewer iterations in an interior point method. We demonstrate
the benefits of this method on two optimization problems: Adversarial
generation for a classifier trained on MNIST images and security-constrained
optimal power flow with transient feasibility enforced using a neural network
surrogate.

</details>


### [221] [IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method](https://arxiv.org/abs/2509.22463)
*Xinyu Liu,Bei Li,Jiahao Liu,Junhao Ruan,Kechen Jiao,Hongyin Tang,Jingang Wang,Xiao Tong,Jingbo Zhu*

Main category: cs.LG

TL;DR: IIET Transformer improves performance over vanilla Transformers and PCformer using iterative implicit Euler method, with IIAD distillation enabling efficient variants that maintain accuracy while reducing inference overhead.


<details>
  <summary>Details</summary>
Motivation: High-order numerical methods in Transformers create performance-efficiency trade-offs, and conventional efficiency techniques like distillation can harm performance of models like PCformer.

Method: Propose IIET (Iterative Implicit Euler Transformer) that simplifies high-order methods using iterative implicit Euler approach, and IIAD (Iteration Influence-Aware Distillation) for flexible efficiency-performance balance.

Result: IIET boosts average accuracy by 2.65% over vanilla Transformers and 0.8% over PCformer. E-IIET variant cuts inference overhead by 55% while retaining 99.4% accuracy. Most efficient variant achieves >1.6% gain over vanilla Transformer with comparable speed.

Conclusion: IIET provides superior performance and facilitates model compression compared to PCformer, with IIAD enabling effective balance of performance-efficiency trade-off through flexible thresholding.

Abstract: High-order numerical methods enhance Transformer performance in tasks like
NLP and CV, but introduce a performance-efficiency trade-off due to increased
computational overhead. Our analysis reveals that conventional efficiency
techniques, such as distillation, can be detrimental to the performance of
these models, exemplified by PCformer. To explore more optimizable ODE-based
Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit
\textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies
high-order methods using an iterative implicit Euler approach. This
simplification not only leads to superior performance but also facilitates
model compression compared to PCformer. To enhance inference efficiency, we
introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware
\textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD
allows users to effectively balance the performance-efficiency trade-off. On
lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla
Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET,
significantly cuts inference overhead by 55\% while retaining 99.4\% of the
original task accuracy. Moreover, the most efficient IIET variant achieves an
average performance gain exceeding 1.6\% over vanilla Transformer with
comparable speed.

</details>


### [222] [Bayesian Transfer Operators in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2509.22482)
*Septimus Boshoff,Sebastian Peitz,Stefan Klus*

Main category: cs.LG

TL;DR: This paper unifies Gaussian process regression with dynamic mode decomposition to improve kernel-based Koopman operator methods, addressing scalability and hyperparameter optimization challenges.


<details>
  <summary>Details</summary>
Motivation: To address two key problems with kernel-based Koopman algorithms: computational scalability issues (sparsity) and difficulties in hyperparameter optimization/dictionary learning for adapting models to dynamical systems.

Method: Combines Koopman operator theory with reproducing kernel Hilbert spaces and Gaussian process methods, integrating Gaussian process regression with dynamic mode decomposition.

Result: Shows that computational demands can be reduced while improving resilience against sensor noise, providing more practical kernel-based Koopman algorithms.

Conclusion: The main contribution is the successful unification of Gaussian process regression and dynamic mode decomposition, offering improved performance for Koopman operator applications in nonlinear dynamical systems.

Abstract: The Koopman operator, as a linear representation of a nonlinear dynamical
system, has been attracting attention in many fields of science. Recently,
Koopman operator theory has been combined with another concept that is popular
in data science: reproducing kernel Hilbert spaces. We follow this thread into
Gaussian process methods, and illustrate how these methods can alleviate two
pervasive problems with kernel-based Koopman algorithms. The first being
sparsity: most kernel methods do not scale well and require an approximation to
become practical. We show that not only can the computational demands be
reduced, but also demonstrate improved resilience against sensor noise. The
second problem involves hyperparameter optimization and dictionary learning to
adapt the model to the dynamical system. In summary, the main contribution of
this work is the unification of Gaussian process regression and dynamic mode
decomposition.

</details>


### [223] [Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise](https://arxiv.org/abs/2509.22500)
*Juan Ramirez,Simon Lacoste-Julien*

Main category: cs.LG

TL;DR: Dual optimistic ascent on Lagrangian is equivalent to gradient descent-ascent on Augmented Lagrangian, transferring ALM's theoretical guarantees to dual optimistic methods.


<details>
  <summary>Details</summary>
Motivation: Constrained deep learning problems often use first-order methods on min-max Lagrangian formulation, but suffer from oscillations and failure to find all local solutions. While ALM addresses these issues, practitioners prefer dual optimistic ascent schemes which perform well empirically but lack formal guarantees.

Method: Established equivalence between dual optimistic ascent on Lagrangian and gradient descent-ascent on Augmented Lagrangian, allowing transfer of ALM's theoretical guarantees.

Result: Proved dual optimistic ascent converges linearly to all local solutions, and provided principled guidance for tuning the optimism hyper-parameter.

Conclusion: The work closes the critical gap between empirical success of dual optimistic methods and their theoretical foundation by establishing their equivalence with ALM.

Abstract: Constrained optimization is a powerful framework for enforcing requirements
on neural networks. These constrained deep learning problems are typically
solved using first-order methods on their min-max Lagrangian formulation, but
such approaches often suffer from oscillations and can fail to find all local
solutions. While the Augmented Lagrangian method (ALM) addresses these issues,
practitioners often favor dual optimistic ascent schemes (PI control) on the
standard Lagrangian, which perform well empirically but lack formal guarantees.
In this paper, we establish a previously unknown equivalence between these
approaches: dual optimistic ascent on the Lagrangian is equivalent to gradient
descent-ascent on the Augmented Lagrangian. This finding allows us to transfer
the robust theoretical guarantees of the ALM to the dual optimistic setting,
proving it converges linearly to all local solutions. Furthermore, the
equivalence provides principled guidance for tuning the optimism
hyper-parameter. Our work closes a critical gap between the empirical success
of dual optimistic methods and their theoretical foundation.

</details>


### [224] [Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data](https://arxiv.org/abs/2509.22507)
*Zahid Iqbal*

Main category: cs.LG

TL;DR: Proposes three federated learning methods (DL-SH, DL-MH, I-DL-MH) to address statistical heterogeneity, model heterogeneity, and client incentive challenges, achieving significant accuracy improvements and reduced communication costs.


<details>
  <summary>Details</summary>
Motivation: Address key challenges in Federated Learning: statistical heterogeneity (non-IID data), model heterogeneity (different client capabilities), and lack of cost-effective incentive mechanisms for client participation.

Method: Three proposed approaches: DL-SH for statistical heterogeneity with privacy and communication efficiency; DL-MH for fully heterogeneous models with statistical disparities; I-DL-MH as incentive-based extension to promote client engagement.

Result: Significant performance improvements: DL-SH improved global model accuracy by 153%, I-DL-MH achieved 225% improvement under non-IID conditions. Enhanced accuracy and decreased communication costs compared to state-of-the-art approaches.

Conclusion: The proposed approaches effectively address statistical and model heterogeneity in federated learning while providing incentives for client participation, demonstrating superior performance across various datasets and data distributions.

Abstract: Federated Learning (FL) has emerged as a promising decentralized learning
(DL) approach that enables the use of distributed data without compromising
user privacy. However, FL poses several key challenges. First, it is frequently
assumed that every client can train the same machine learning models, however,
not all clients are able to meet this assumption because of differences in
their business needs and computational resources. Second, statistical
heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can
lead to lower global model performance. Third, while addressing these
challenges, there is a need for a cost-effective incentive mechanism to
encourage clients to participate in FL training. In response to these
challenges, we propose several methodologies: DL-SH, which facilitates
efficient, privacy-preserving, and communication-efficient learning in the
context of statistical heterogeneity; DL-MH, designed to manage fully
heterogeneous models while tackling statistical disparities; and I-DL-MH, an
incentive-based extension of DL-MH that promotes client engagement in federated
learning training by providing incentives within this complex federated
learning framework. Comprehensive experiments were carried out to assess the
performance and scalability of the proposed approaches across a range of
complex experimental settings. This involved utilizing various model
architectures, in diverse data distributions, including IID and several non-IID
scenarios, as well as multiple datasets. Experimental results demonstrate that
the proposed approaches significantly enhance accuracy and decrease
communication costs while effectively addressing statistical heterogeneity and
model heterogeneity in comparison to existing state-of-the-art approaches and
baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH
achieving a 225% improvement under non-IID conditions.

</details>


### [225] [JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation](https://arxiv.org/abs/2509.22522)
*Guillem Capellera,Luis Ferraz,Antonio Rubio,Alexandre Alahi,Antonio Agudo*

Main category: cs.LG

TL;DR: JointDiff is a diffusion framework that unifies continuous spatio-temporal data and discrete events generation, achieving SOTA performance in sports domain with novel controllable generation scenarios.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between modeling continuous data and discrete events as separate processes in generative models, especially for complex interactive systems where they interact synchronously.

Method: JointDiff diffusion framework with CrossGuid conditioning operation for multi-agent domains, enabling simultaneous generation of continuous trajectories and discrete events with controllable generation through weak-possessor-guidance and text-guidance.

Result: Achieves state-of-the-art performance in sports domain, validated with non-controllable generation and two novel controllable generation scenarios, using a new unified sports benchmark with textual descriptions.

Conclusion: Joint modeling of continuous and discrete processes is crucial for building realistic and controllable generative models for interactive systems.

Abstract: Generative models often treat continuous data and discrete events as separate
processes, creating a gap in modeling complex systems where they interact
synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion
framework designed to unify these two processes by simultaneously generating
continuous spatio-temporal data and synchronous discrete events. We demonstrate
its efficacy in the sports domain by simultaneously modeling multi-agent
trajectories and key possession events. This joint modeling is validated with
non-controllable generation and two novel controllable generation scenarios:
weak-possessor-guidance, which offers flexible semantic control over game
dynamics through a simple list of intended ball possessors, and text-guidance,
which enables fine-grained, language-driven generation. To enable the
conditioning with these guidance signals, we introduce CrossGuid, an effective
conditioning operation for multi-agent domains. We also share a new unified
sports benchmark enhanced with textual descriptions for soccer and football
datasets. JointDiff achieves state-of-the-art performance, demonstrating that
joint modeling is crucial for building realistic and controllable generative
models for interactive systems.

</details>


### [226] [ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models](https://arxiv.org/abs/2509.22556)
*Chenyu Liu,Yuqiu Deng,Tianyu Liu,Jinan Zhou,Xinliang Zhou,Ziyu Jia,Yi Ding*

Main category: cs.LG

TL;DR: ECHO is a decoder-centric Large EEG Model that reformulates EEG modeling as sequence-to-sequence learning, enabling in-context learning and superior multi-task performance without parameter updates.


<details>
  <summary>Details</summary>
Motivation: Current encoder-centric Large EEG Models lack decoders of comparable capacity, limiting full utilization of learned features and generalization across diverse EEG tasks and datasets.

Method: ECHO captures layered relationships among signals, labels, and tasks within sequence space, incorporating discrete support samples to construct contextual cues for in-context learning.

Result: Extensive experiments show ECHO consistently outperforms state-of-the-art single-task LEMs in multi-task settings, demonstrating superior generalization and adaptability across multiple datasets.

Conclusion: The decoder-centric paradigm of ECHO effectively addresses limitations of encoder-centric approaches, enabling dynamic adaptation to heterogeneous EEG tasks without parameter updates.

Abstract: Electroencephalography (EEG), with its broad range of applications,
necessitates models that can generalize effectively across various tasks and
datasets. Large EEG Models (LEMs) address this by pretraining encoder-centric
architectures on large-scale unlabeled data to extract universal
representations. While effective, these models lack decoders of comparable
capacity, limiting the full utilization of the learned features. To address
this issue, we introduce ECHO, a novel decoder-centric LEM paradigm that
reformulates EEG modeling as sequence-to-sequence learning. ECHO captures
layered relationships among signals, labels, and tasks within sequence space,
while incorporating discrete support samples to construct contextual cues. This
design equips ECHO with in-context learning, enabling dynamic adaptation to
heterogeneous tasks without parameter updates. Extensive experiments across
multiple datasets demonstrate that, even with basic model components, ECHO
consistently outperforms state-of-the-art single-task LEMs in multi-task
settings, showing superior generalization and adaptability.

</details>


### [227] [Learning to Price Bundles: A GCN Approach for Mixed Bundling](https://arxiv.org/abs/2509.22557)
*Liangyu Ding,Chenghan Wu,Guokai Li,Zizhuo Wang*

Main category: cs.LG

TL;DR: A GCN-based framework for solving the bundle pricing problem, achieving near-optimal solutions with significantly reduced computational time compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Bundle pricing is a classic revenue management problem that becomes intractable due to the exponential number of candidate bundles, requiring efficient solutions.

Method: Developed a graph representation of mixed bundling model, trained GCN to learn optimal bundle patterns, proposed two inference strategies with local-search improvement.

Result: Achieved near-optimal solutions (better than 97%) with fraction of computational time for small-medium problems, superior performance for larger problems compared to BSP, effective for non-additive utilities up to 30+ products.

Conclusion: GCN-based framework provides an effective and efficient approach for bundle pricing problems, demonstrating strong performance across various problem sizes and challenging scenarios.

Abstract: Bundle pricing refers to designing several product combinations (i.e.,
bundles) and determining their prices in order to maximize the expected profit.
It is a classic problem in revenue management and arises in many industries,
such as e-commerce, tourism, and video games. However, the problem is typically
intractable due to the exponential number of candidate bundles. In this paper,
we explore the usage of graph convolutional networks (GCNs) in solving the
bundle pricing problem. Specifically, we first develop a graph representation
of the mixed bundling model (where every possible bundle is assigned with a
specific price) and then train a GCN to learn the latent patterns of optimal
bundles. Based on the trained GCN, we propose two inference strategies to
derive high-quality feasible solutions. A local-search technique is further
proposed to improve the solution quality. Numerical experiments validate the
effectiveness and efficiency of our proposed GCN-based framework. Using a GCN
trained on instances with 5 products, our methods consistently achieve
near-optimal solutions (better than 97%) with only a fraction of computational
time for problems of small to medium size. It also achieves superior solutions
for larger size of problems compared with other heuristic methods such as
bundle size pricing (BSP). The method can also provide high quality solutions
for instances with more than 30 products even for the challenging cases where
product utilities are non-additive.

</details>


### [228] [Machine learning approaches to seismic event classification in the Ostrava region](https://arxiv.org/abs/2509.22574)
*Marek Pecha,Michael Skotnica,Jana Rušajová,Bohdan Rieznikov,Vít Wandrol,Markéta Rösnerová,Jaromír Knejzlík*

Main category: cs.LG

TL;DR: Machine learning methods (LSTM and XGBoost) were applied to classify seismic events in northeastern Czech Republic, achieving high F1-scores of 0.94-0.95 for distinguishing between tectonic and mining-induced events.


<details>
  <summary>Details</summary>
Motivation: The northeastern Czech Republic is seismically active with both natural tectonic events and anthropogenic events (mining-induced and quarry explosions). Rapid differentiation between these event types is important despite mining cessation, as mine-induced events still occur.

Method: Applied and compared machine learning methods (Long Short-Term Memory recurrent neural network and XGBoost) to the Seismic Polygon Frenštát dataset containing labeled records of tectonic and mining-induced events from 1992-2002.

Result: Both LSTM and XGBoost achieved high performance with F1-scores of 0.94-0.95 for binary classification of tectonic vs mining-induced seismic events.

Conclusion: Modern machine learning techniques show strong potential for rapid characterization and differentiation of seismic event types in this seismically active region.

Abstract: The northeastern region of the Czech Republic is among the most seismically
active areas in the country. The most frequent seismic events are
mining-induced since there used to be strong mining activity in the past.
However, natural tectonic events may also occur. In addition, seismic stations
often record explosions in quarries in the region. Despite the cessation of
mining activities, mine-induced seismic events still occur. Therefore, a rapid
differentiation between tectonic and anthropogenic events is still important.
  The region is currently monitored by the OKC seismic station in
Ostrava-Kr\'{a}sn\'{e} Pole built in 1983 which is a part of the Czech Regional
Seismic Network. The station has been providing digital continuous waveform
data at 100 Hz since 2007. In the years 1992--2002, the region was co-monitored
by the Seismic Polygon Fren\v{s}t\'{a}t (SPF) which consisted of five seismic
stations using a triggered STA/LTA system.
  In this study, we apply and compare machine learning methods to the SPF
dataset, which contains labeled records of tectonic and mining-induced events.
For binary classification, a Long Short-Term Memory recurrent neural network
and XGBoost achieved an F1-score of 0.94 -- 0.95, demonstrating the potential
of modern machine learning techniques for rapid event characterization.

</details>


### [229] [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
*Xu Wujiang,Wentian Zhao,Zhenting Wang,Li Yu-Jhe,Jin Can,Jin Mingyu,Mei Kai,Wan Kun,Metaxas Dimitris*

Main category: cs.LG

TL;DR: EPO addresses exploration-exploitation cascade failure in multi-turn sparse-reward environments through entropy regularization, smoothing, and adaptive phase-based weighting, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Training LLM agents in multi-turn environments with sparse rewards (30+ turns per task) presents fundamental RL challenges, particularly exploration-exploitation cascade failure where agents prematurely converge to flawed strategies and then collapse into chaotic exploration.

Method: Entropy-regularized Policy Optimization (EPO) with three mechanisms: (1) entropy regularization for exploration, (2) entropy smoothing regularizer to bound policy entropy within historical averages, (3) adaptive phase-based weighting to balance exploration and exploitation across training phases.

Result: EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld, while guaranteeing monotonically decreasing entropy variance while maintaining convergence.

Conclusion: Multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with EPO providing an effective framework for LLM agent training in such environments.

Abstract: Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

</details>


### [230] [The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?](https://arxiv.org/abs/2509.22580)
*Guannan Lai,Da-Wei Zhou,Xin Yang,Han-Jia Ye*

Main category: cs.LG

TL;DR: Current CIL evaluation protocols using random sequence sampling fail to capture true performance distribution. EDGE protocol uses inter-task similarity to identify extreme sequences for more accurate evaluation.


<details>
  <summary>Details</summary>
Motivation: Current CIL evaluation methods use small random samples of class sequences, which fail to capture the full performance range and underestimate true variance in model performance across different class orders.

Method: Propose EDGE protocol that identifies extreme sequences using inter-task similarity correlation, adaptively sampling sequences to better approximate the ground-truth performance distribution.

Result: EDGE effectively captures performance extremes and provides more accurate estimates of distributional boundaries compared to random sampling approaches.

Conclusion: EDGE offers a more robust evaluation protocol for CIL that provides actionable insights for model selection and robustness checking by better characterizing the true performance distribution.

Abstract: Class Incremental Learning (CIL) requires models to continuously learn new
classes without forgetting previously learned ones, while maintaining stable
performance across all possible class sequences. In real-world settings, the
order in which classes arrive is diverse and unpredictable, and model
performance can vary substantially across different sequences. Yet mainstream
evaluation protocols calculate mean and variance from only a small set of
randomly sampled sequences. Our theoretical analysis and empirical results
demonstrate that this sampling strategy fails to capture the full performance
range, resulting in biased mean estimates and a severe underestimation of the
true variance in the performance distribution. We therefore contend that a
robust CIL evaluation protocol should accurately characterize and estimate the
entire performance distribution. To this end, we introduce the concept of
extreme sequences and provide theoretical justification for their crucial role
in the reliable evaluation of CIL. Moreover, we observe a consistent positive
correlation between inter-task similarity and model performance, a relation
that can be leveraged to guide the search for extreme sequences. Building on
these insights, we propose EDGE (Extreme case-based Distribution and
Generalization Evaluation), an evaluation protocol that adaptively identifies
and samples extreme class sequences using inter-task similarity, offering a
closer approximation of the ground-truth performance distribution. Extensive
experiments demonstrate that EDGE effectively captures performance extremes and
yields more accurate estimates of distributional boundaries, providing
actionable insights for model selection and robustness checking. Our code is
available at https://github.com/AIGNLAI/EDGE.

</details>


### [231] [Transport Based Mean Flows for Generative Modeling](https://arxiv.org/abs/2509.22592)
*Elaheh Akbari,Ping He,Ahmadreza Moradipari,Yikun Bai,Soheil Kolouri*

Main category: cs.LG

TL;DR: The paper proposes an improved one-step generation method for flow-matching models by incorporating optimal transport-based sampling strategies into the Mean Flow framework, achieving better fidelity and diversity while maintaining fast inference.


<details>
  <summary>Details</summary>
Motivation: Flow-matching models suffer from slow inference due to multiple sequential sampling steps. While Mean Flows offer one-step generation with speedups, they often fail to faithfully approximate the original multi-step process in continuous domains.

Method: Incorporated optimal transport-based sampling strategies into the Mean Flow framework to enable one-step generators that better preserve fidelity and diversity of the original multi-step flow process.

Result: Experiments on low-dimensional settings and high-dimensional tasks (image generation, image-to-image translation, point cloud generation) demonstrate superior inference accuracy in one-step generative modeling compared to previous approaches.

Conclusion: The proposed method successfully addresses the limitations of Mean Flows by combining optimal transport strategies, enabling efficient one-step generation while maintaining high fidelity and diversity comparable to multi-step flow-matching processes.

Abstract: Flow-matching generative models have emerged as a powerful paradigm for
continuous data generation, achieving state-of-the-art results across domains
such as images, 3D shapes, and point clouds. Despite their success, these
models suffer from slow inference due to the requirement of numerous sequential
sampling steps. Recent work has sought to accelerate inference by reducing the
number of sampling steps. In particular, Mean Flows offer a one-step generation
approach that delivers substantial speedups while retaining strong generative
performance. Yet, in many continuous domains, Mean Flows fail to faithfully
approximate the behavior of the original multi-step flow-matching process. In
this work, we address this limitation by incorporating optimal transport-based
sampling strategies into the Mean Flow framework, enabling one-step generators
that better preserve the fidelity and diversity of the original multi-step flow
process. Experiments on controlled low-dimensional settings and on
high-dimensional tasks such as image generation, image-to-image translation,
and point cloud generation demonstrate that our approach achieves superior
inference accuracy in one-step generative modeling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [232] [Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow](https://arxiv.org/abs/2509.21789)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Yongbo He,Zhangquan Chen,Zhucun Xue,Jiangning Zhang,Yue Liao,Xiaobin Hu,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.MA

TL;DR: ViF addresses multi-agent visual hallucination snowballing in VLM-powered systems by relaying visual tokens and reallocating attention to preserve visual evidence.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems using VLMs suffer from hallucination snowballing where visual errors propagate and amplify through textual communication, reducing visual attention.

Method: Proposes ViF paradigm that relays inter-agent messages with visual flow using selected visual relay tokens and applies attention reallocation to amplify visual patterns.

Result: Significantly reduces hallucination snowballing and improves performance across 8 benchmarks using 4 MAS structures and 10 base models.

Conclusion: ViF effectively mitigates visual hallucination snowballing in multi-agent VLM systems through visual token relay and attention reallocation.

Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code will
be available at: https://github.com/YU-deep/ViF.git.

</details>


### [233] [RobustFlow: Towards Robust Agentic Workflow Generation](https://arxiv.org/abs/2509.21834)
*Shengxiang Xu,Jiayi Zhang,Shimin Di,Yuyu Luo,Liang Yao,Hanmo Liu,Jia Zhu,Fan Liu,Min-Ling Zhang*

Main category: cs.MA

TL;DR: The paper identifies robustness issues in automated agentic workflow generation and proposes RobustFlow, a training framework that improves workflow consistency across semantically equivalent but differently phrased instructions.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agentic workflow generation methods produce inconsistent workflows for semantically identical but differently phrased instructions, undermining reliability for real-world applications.

Method: Proposed metrics for workflow consistency evaluation and developed RobustFlow - a training framework using preference optimization to teach models invariance to instruction variations by training on sets of synonymous task descriptions.

Result: RobustFlow boosts workflow robustness scores to 70-90%, representing substantial improvement over existing approaches.

Conclusion: The proposed RobustFlow framework effectively addresses workflow brittleness and significantly improves consistency across instruction variations, enhancing reliability for practical applications.

Abstract: The automated generation of agentic workflows is a promising frontier for
enabling large language models (LLMs) to solve complex tasks. However, our
investigation reveals that the robustness of agentic workflow remains a
critical, unaddressed challenge. Current methods often generate wildly
inconsistent workflows when provided with instructions that are semantically
identical but differently phrased. This brittleness severely undermines their
reliability and trustworthiness for real-world applications. To quantitatively
diagnose this instability, we propose metrics based on nodal and topological
similarity to evaluate workflow consistency against common semantic variations
such as paraphrasing and noise injection. Subsequently, we further propose a
novel training framework, RobustFlow, that leverages preference optimization to
teach models invariance to instruction variations. By training on sets of
synonymous task descriptions, RobustFlow boosts workflow robustness scores to
70\% - 90\%, which is a substantial improvement over existing approaches. The
code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.

</details>


### [234] [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)
*Merve Atasever,Matthew Hong,Mihir Nitin Kulkarni,Qingpei Li,Jyotirmoy V. Deshmukh*

Main category: cs.MA

TL;DR: A decentralized MAPF framework using Decision Transformer and GPT-4o that reduces training time from weeks to hours while improving collision avoidance and adaptability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in MAPF including combinatorial complexity, partial observability, self-centered behaviors causing collisions, and long training times in decentralized reinforcement learning methods.

Method: Propose an efficient decentralized planning framework based on Decision Transformer with offline RL, integrated with GPT-4o for dynamic policy guidance in changing environments.

Result: Significantly reduces training duration from weeks to hours, effectively handles long-horizon credit assignment, improves performance in sparse/delayed reward scenarios, and enhances adaptability in dynamic environments.

Conclusion: The DT-based approach augmented by GPT-4o provides an efficient solution for MAPF that overcomes limitations of traditional RL methods and demonstrates superior performance in both static and dynamic environments.

Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.

</details>


### [235] [Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.22216)
*Ahmet Onur Akman,Anastasia Psarou,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.MA

TL;DR: Study examines RL-enabled autonomous vehicles' impact on urban traffic flow in mixed environments, showing AVs can optimize travel times by up to 5% with varying effects on human drivers depending on behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how different behavioral strategies of RL-enabled autonomous vehicles affect traffic flow and travel times in mixed traffic environments with human drivers.

Method: Used multi-agent RL framework (PARCOUR) with Deep Q-learning, converting 1/3 of population to AVs with defined behaviors (selfish, collaborative, competitive, social, altruistic, malicious) imposed through reward functions.

Result: AVs optimized travel times by up to 5%, with self-serving behaviors achieving shorter travel times than human drivers. Impact on human drivers varied significantly based on AV behavior adopted.

Conclusion: Multi-agent RL is applicable for collective routing on traffic networks, but the impact on coexisting parties varies greatly with the behaviors adopted, highlighting complexity differences in learning tasks for each behavior.

Abstract: This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.

</details>


### [236] [VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture](https://arxiv.org/abs/2509.22218)
*Sandaru Fernando,Imasha Jayarathne,Sithumini Abeysekara,Shanuja Sithamparanthan,Thushari Silva,Deshan Jayawardana*

Main category: cs.MA

TL;DR: VizGen is an AI-powered system that enables users to create data visualizations using natural language, eliminating the need for technical expertise.


<details>
  <summary>Details</summary>
Motivation: Traditional data visualization tools require technical skills, limiting accessibility for non-technical users who want to analyze and visualize complex datasets.

Method: Uses advanced NLP and LLMs (Claude 3.7 Sonnet, Gemini 2.0 Flash) to translate natural language queries into SQL, with a multi-agent architecture handling SQL generation, graph creation, customization, and insight extraction.

Result: The system successfully bridges the technical complexity gap by allowing conversational graph refinement, real-time SQL database interaction, and providing enriched explanations with contextual internet information.

Conclusion: VizGen democratizes data visualization by making it intuitive and accessible through natural language interaction, empowering users without technical backgrounds to create meaningful visualizations.

Abstract: Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.

</details>


### [237] [Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives](https://arxiv.org/abs/2509.22596)
*Qixin Zhang,Yan Sun,Can Jin,Xikun Zhang,Yao Shu,Puning Zhao,Li Shen,Dacheng Tao*

Main category: cs.MA

TL;DR: Two policy learning algorithms (MA-SPL and MA-MPL) for multi-agent online coordination with submodular and weakly submodular objectives, achieving optimal approximation guarantees with parameter-free capability.


<details>
  <summary>Details</summary>
Motivation: To address multi-agent online coordination problems with submodular objectives, particularly handling challenging weakly submodular scenarios and reducing reliance on unknown parameters.

Method: Proposed MA-SPL algorithm for submodular and weakly submodular objectives, and parameter-free MA-MPL algorithm using novel policy-based continuous extension technique instead of multi-linear extension.

Result: MA-SPL achieves optimal (1-c/e)-approximation for submodular objectives and handles weakly submodular scenarios. MA-MPL maintains same approximation ratio without requiring unknown parameters. Extensive simulations validate effectiveness.

Conclusion: The proposed algorithms effectively solve multi-agent online coordination problems with various submodular objectives, with MA-MPL providing parameter-free solution while maintaining performance guarantees.

Abstract: In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.

</details>


### [238] [Voting-Bloc Entropy: A New Metric for DAO Decentralization](https://arxiv.org/abs/2509.22620)
*Andrés Fábrega,Amy Zhao,Jay Yu,James Austgen,Sarah Allen,Kushal Babel,Mahimna Kelkar,Ari Juels*

Main category: cs.MA

TL;DR: Proposes Voting-Bloc Entropy (VBE), a new framework for measuring DAO decentralization based on voter utility function similarity, derived from reinforcement learning principles.


<details>
  <summary>Details</summary>
Motivation: Existing definitions of decentralization in DAOs fail to capture key properties for diverse and equitable participation, requiring a more principled measurement approach.

Method: Develops VBE framework that measures similarity of participants' utility functions across voting rounds, based on a reinforcement learning conceptual model for voting.

Result: Proves theoretical results about centralizing effects of vote delegation, proposal bundling, and bribery; provides practical suggestions for enhancing DAO decentralization; conducts empirical measurement studies and governance experiments.

Conclusion: VBE provides both theoretical and practical tools for measuring and improving DAO decentralization, with open-source artifacts made available for community use.

Abstract: Decentralized Autonomous Organizations (DAOs) use smart contracts to foster
communities working toward common goals. Existing definitions of
decentralization, however -- the 'D' in DAO -- fall short of capturing the key
properties characteristic of diverse and equitable participation. This work
proposes a new framework for measuring DAO decentralization called Voting-Bloc
Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with
closely aligned interests act as a centralizing force and should be modeled as
such. VBE formalizes this notion by measuring the similarity of participants'
utility functions across a set of voting rounds. Unlike prior, ad hoc
definitions of decentralization, VBE derives from first principles: We
introduce a simple (yet powerful) reinforcement learning-based conceptual model
for voting, that in turn implies VBE. We first show VBE's utility as a
theoretical tool. We prove a number of results about the (de)centralizing
effects of vote delegation, proposal bundling, bribery, etc. that are
overlooked in previous notions of DAO decentralization. Our results lead to
practical suggestions for enhancing DAO decentralization. We also show how VBE
can be used empirically by presenting measurement studies and VBE-based
governance experiments. We make the tools we developed for these results
available to the community in the form of open-source artifacts in order to
facilitate future study of DAO decentralization.

</details>
