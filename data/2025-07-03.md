<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*IÃ±aki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: The paper clarifies debates about LRMs' reasoning capabilities by refining benchmarks (Towers of Hanoi, River Crossing), showing failures stem from complexity and unsolvable setups, not just output constraints.


<details>
  <summary>Details</summary>
Motivation: To address controversy over whether LRMs lack genuine reasoning, sparked by Apple's 'The Illusion of Thinking' and subsequent debates.

Method: Replicated and refined contentious benchmarks, introducing incremental stepwise prompting and agentic collaborative dialogue.

Result: LRMs struggle with moderate complexity (e.g., 8-disk Towers of Hanoi) but solve large solvable problems (e.g., 100+ agent pairs in River Crossing).

Conclusion: LRMs are stochastic searchers in poorly understood state spaces; progress requires fine-grained analysis of their reasoning limits.

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [2] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Agent-as-tool, a hierarchical framework separating tool calling and reasoning processes in LLM-based agents, improving performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with simultaneous tool calling and reasoning, burdening models with redundant information.

Method: Proposes Agent-as-tool, decoupling tool calling (handled by another agent) from reasoning, allowing focus on verbal reasoning.

Result: Achieved comparable results with slight fine-tuning (180 samples) and outperformed Search-R1 in Bamboogle (63.2% exact match, 75.2% cover exact match).

Conclusion: Agent-as-tool effectively addresses the challenge of integrating tool calling and reasoning, enhancing LLM-based agent performance.

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [3] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: The paper discusses the limitations of LLMs in clinical settings, particularly for dementia diagnosis, and suggests hybrid AI approaches combining statistical learning with expert knowledge for better interpretability and workflow integration.


<details>
  <summary>Details</summary>
Motivation: To address the gap between high benchmark scores of LLMs and their lack of measurable clinical impact, especially in dementia diagnosis and care.

Method: A scoping review highlighting limitations of data-driven AI (e.g., lack of transparency, hallucinations) and proposing hybrid approaches (e.g., neuro-symbolic AI) involving clinicians.

Result: Current LLMs fail to improve diagnostic accuracy or speed in clinical settings. Hybrid methods like PEIRS and ATHENA-CDS show promise by integrating expert knowledge and improving interpretability.

Conclusion: Future AI should prioritize explanatory coherence, clinician understanding, and workflow fit, moving beyond accuracy metrics to measure real-world impact on patient outcomes.

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [4] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: Agent Ideate, a framework using LLMs and autonomous agents, generates business ideas from patents, outperforming standalone LLMs in quality, relevance, and novelty.


<details>
  <summary>Details</summary>
Motivation: Patents hold valuable technical knowledge, but accessing and interpreting them for innovative ideas is challenging.

Method: Agent Ideate combines LLMs and agent-based architectures to mine and generate product concepts from patents, tested in Computer Science, NLP, and Material Chemistry.

Result: The agentic approach consistently outperformed standalone LLMs in idea quality, relevance, and novelty.

Conclusion: Combining LLMs with agentic workflows enhances innovation by unlocking patent data's potential for business idea generation.

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [5] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: The paper reviews AI agents, LLM-Agents, MLLM-Agents, and Agentic AI, exploring their capabilities, applications in smart manufacturing, and challenges.


<details>
  <summary>Details</summary>
Motivation: To clarify the definitions, boundaries, and practical uses of emerging AI paradigms in smart manufacturing.

Method: Systematic review of AI and AI agent evolution, focusing on LLM-Agents, MLLM-Agents, and Agentic AI.

Result: Identifies advancements in AI agents' capabilities and potential applications in manufacturing, alongside challenges.

Conclusion: The study highlights the promise of AI agents in smart manufacturing but notes the need for clearer definitions and addressing integration challenges.

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [6] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: A formal method for Ethical Decision Making models using fuzzy rules and Petri nets, validated with a medical case study.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating moral machine performance due to ontological and epistemic complexities in ethics.

Method: Develops Ethical Decision Making models via ethical risk assessment, represented as fuzzy rules, and validates them using fuzzy Petri nets.

Result: Demonstrates the approach's feasibility through a medical case study.

Conclusion: Proposes a viable method for verifying and validating ethical decision models in machines.

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [7] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve is an AI-assisted grading platform using LLMs to transcribe and evaluate handwritten STEM responses, reducing grading time by 65% with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Handwritten grading is a bottleneck in large STEM courses; Pensieve aims to streamline the process with AI assistance.

Method: Pensieve leverages LLMs for transcription and evaluation, supporting the entire grading pipeline with a human-in-the-loop interface.

Result: Deployed in 20+ institutions, graded 300,000+ responses, reducing grading time by 65% with 95.4% agreement for high-confidence predictions.

Conclusion: Pensieve effectively reduces grading workload while maintaining high accuracy, proving viable for large-scale STEM courses.

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [8] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: A multi-agent system using LLMs and fuzzy logic to improve customer service quality by reducing hallucination risks in SMS-based requests.


<details>
  <summary>Details</summary>
Motivation: Enhancing customer service quality and response time is vital for loyalty and market share, but LLMs' hallucination risks pose challenges.

Method: Proposes a multi-agent system combining LLMs with fuzzy logic to handle SMS customer requests.

Result: The system aims to mitigate hallucination risks while improving service efficiency.

Conclusion: Integrating fuzzy logic with LLMs can effectively address hallucination challenges in customer service.

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [9] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: The paper proposes T3DM, a method for Temporal Knowledge Graph Reasoning (TKGR) that addresses distribution shift and improves negative sampling, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing TKGR methods struggle with event distribution shifts between training and test samples and rely on low-quality random negative sampling.

Method: T3DM models distribution shifts and uses adversarial training for high-quality negative sampling.

Result: T3DM achieves better and more robust performance than state-of-the-art baselines.

Conclusion: T3DM effectively addresses key challenges in TKGR, enhancing model consistency and sampling quality.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [10] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: The paper proposes a crowd-shipping system using in-store customers as couriers, optimized via MDP, NeurADP, and DDQN for cost-efficient last-mile delivery.


<details>
  <summary>Details</summary>
Motivation: Address the growing need for efficient last-mile delivery in urban areas by leveraging in-store shoppers as delivery couriers.

Method: Uses a Markov Decision Process (MDP) model with NeurADP for adaptive order-to-shopper assignment and DDQN for dynamic pricing.

Result: Achieves 6.7% cost savings over NeurADP with fixed pricing and 18% over baselines; flexible delays and multi-destination routing reduce costs further.

Conclusion: Dynamic, forward-looking policies enhance crowd-shipping efficiency, offering practical benefits for urban logistics.

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [11] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: The paper questions mandatory conditions for answer set semantics in non-monotonic logic programming, proposes refined principles, and defines new semantics.


<details>
  <summary>Details</summary>
Motivation: To evaluate if existing conditions (minimal model property, constraint monotonicity, foundedness) are too restrictive and to propose alternative principles for answer set semantics.

Method: Illustrates limitations of current conditions, refines Gelfond's principles (well-supportedness, minimality), extends well-supportedness, defines new semantics, and analyzes computational complexity.

Result: Existing conditions may exclude expected answer sets; refined principles (well-supportedness, minimality) offer a better baseline for semantics.

Conclusion: Refined GAS principles provide a more flexible and intuitive framework for answer set semantics, validated through new definitions and complexity analysis.

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ is a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis by dynamically re-scoring class attributes and using group-level prototypes.


<details>
  <summary>Details</summary>
Motivation: Traditional generative ZSL methods require extensive synthetic data and computational resources, relaxing ZSL assumptions. FSIGenZ addresses this by leveraging instance-level attribute variability.

Method: Introduces Model-Specific Attribute Scoring (MSAS) to dynamically adjust class attributes and group-level prototypes. Uses Dual-Purpose Semantic Regularization (DPSR) and a semantic-aware contrastive classifier (SCC).

Result: Achieves competitive performance on SUN, AwA2, and CUB benchmarks with fewer synthetic features.

Conclusion: FSIGenZ effectively reduces computational demands while maintaining performance in ZSL tasks.

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [13] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant is a PTQ framework for LLMs, achieving 1-bit weight and 6-bit activation quantization with minimal performance loss using LTDB.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory challenges in LLMs by improving quantization effectiveness, reducing errors from weight distributions and activation outliers.

Method: Uses LTDB to transform single-bell weight distributions into dual-bell forms, reducing binarization errors and smoothing activations.

Result: Achieves perplexity of 14.39 on LLaMA2-13B (Wikitext2), outperforming BiLLM's 21.35.

Conclusion: DBellQuant sets a new SOTA for LLM compression, enabling practical deployment with minimal performance degradation.

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [14] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: The paper analyzes non-contrastive self-supervised learning methods, focusing on stop gradient and exponential moving average procedures to prevent representation collapse. It provides theoretical insights into their optimization and dynamical system behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand why stop gradient and exponential moving average procedures avoid representation collapse in non-contrastive self-supervised learning, despite not optimizing the original objective.

Method: The study uses optimization and dynamical system perspectives to analyze these procedures, particularly in the linear case, without extra assumptions.

Result: Stop gradient and exponential moving average avoid collapse, while minimizing the original objective without them leads to collapse. Their dynamical systems have stable equilibria without trivial solutions.

Conclusion: The procedures are theoretically justified for preventing collapse, offering stable solutions in non-contrastive self-supervised learning.

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [15] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: PathCoT is a zero-shot CoT prompting method integrating pathology expert knowledge and self-evaluation to improve MLLMs' performance in pathology visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs underperform in pathology tasks due to lack of domain knowledge and errors introduced by additional CoT steps.

Method: PathCoT integrates expert knowledge into MLLMs' reasoning and adds self-evaluation to mitigate errors.

Result: Experiments on PathMMU show PathCoT enhances pathology visual understanding and reasoning.

Conclusion: PathCoT effectively addresses domain-specific challenges in MLLMs for pathology tasks.

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [16] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: The paper explores using machine learning (ML) to regenerate Flamelet libraries for methane combustion, comparing four ML algorithms. MLP was selected as the best method, achieving 99.81% accuracy after hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: The Flamelet Generated Manifold (FGM) is precise but memory-intensive. This research aims to optimize FGM libraries for methane combustion using ML.

Method: Four ML algorithms (MLP, Random Forest, Linear Regression, SVM) were tested to regenerate Flamelet libraries. Hyperparameter tuning was applied to MLP.

Result: MLP outperformed others, achieving 99.81% accuracy with four hidden layers (10, 15, 20, 25 neurons). Error rate was 2.30%.

Conclusion: MLP is optimal for regenerating FGM libraries, offering high accuracy for methane combustion simulations.

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [17] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: The paper discusses porting PyTorch-based geometric learning frameworks to Intel's Gaudi-v2 HPUs, addressing challenges and providing utilities, tutorials, and examples to ease adoption.


<details>
  <summary>Details</summary>
Motivation: Geometric learning is powerful for non-Euclidean data, but non-CUDA hardware like Intel's Gaudi-v2 HPUs requires significant adaptation. The work aims to lower barriers for researchers using such hardware.

Method: The authors port PyTorch-based frameworks to Gaudi-v2 HPUs, introducing core utilities for essential operations and providing tutorials and real-world examples with diagnostic analyses.

Result: A collection of utilities, tutorials, and examples is created, consolidated into a GitHub repository, enabling easier experimentation with geometric learning on non-CUDA hardware.

Conclusion: The work facilitates geometric learning on non-CUDA hardware, laying groundwork for further optimizations and cross-platform portability.

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [18] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: Proposes an uncertainty-aware, multi-view dynamic decision framework for multi-omics classification to reduce costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High costs and resource waste in multi-omics profiling necessitate a cost-effective, accurate diagnostic method.

Method: Uses neural networks with refined activation functions for Dirichlet distribution parameters, subjective logic for belief/uncertainty quantification, and Dempster-Shafer theory for multi-omics fusion. A dynamic decision mechanism incrementally uses omics data.

Result: Achieves accurate classification with fewer omics modalities in 50%+ cases, reducing redundant testing while matching full-omics model performance.

Conclusion: The framework balances cost and accuracy, preserving biological insights and reducing unnecessary testing.

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [19] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: The study proposes a data-driven approach using LSTM to forecast electricity load, generation, and deficits in Benghazi, Libya, outperforming other models and aiding grid stability.


<details>
  <summary>Details</summary>
Motivation: Accurate electricity forecasting is vital for grid stability and planning in Benghazi, Libya, due to frequent load shedding and infrastructure challenges.

Method: Multiple time series models (ARIMA, SARIMA, dynamic regression ARIMA, exponential smoothing, XGBoost, LSTM) were applied to historical data (2019 and 2023), enhanced with preprocessing techniques.

Result: LSTM outperformed other models, effectively handling non-stationary and seasonal data, and integrating exogenous factors like temperature and humidity.

Conclusion: The optimized LSTM framework provides actionable insights for policymakers and grid operators in volatile, data-scarce regions.

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [20] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: The paper proposes a hybrid GNN-LLM recommender system optimized for speed and efficiency using quantization, LoRA, distillation, FPGA, and DeepSpeed, achieving higher accuracy and reduced training time.


<details>
  <summary>Details</summary>
Motivation: Address computational bottlenecks in hybrid GNN-LLM recommender systems to improve inference latency and training efficiency.

Method: Hybrid GNN-LLM architecture with optimization strategies (quantization, LoRA, distillation) and hardware acceleration (FPGA, DeepSpeed).

Result: Optimal configuration improved accuracy by 13.6% (NDCG@10: 0.75) at 40-60ms latency, and LoRA reduced training time by 66% (3.8 hours).

Conclusion: Hardware-software co-design and parameter-efficient tuning enable hybrid models to outperform standalone GNN or LLM approaches, recommending FPGA and LoRA for real-time deployment. Future work includes federated learning and advanced fusion architectures.

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [21] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: The paper introduces the FSTA decomposition technique and L2Seg neural framework to accelerate iterative solvers for VRPs by focusing on unstable portions of solutions, achieving up to 7x speedup.


<details>
  <summary>Details</summary>
Motivation: Redundant computations in iterative solvers due to stable solution segments in large-scale VRPs.

Method: FSTA preserves stable segments and aggregates them into hypernodes, while L2Seg intelligently identifies stable/unstable portions using three variants (non-autoregressive, autoregressive, and their synergy).

Result: Empirical results show up to 7x acceleration in state-of-the-art solvers for CVRP and VRPTW.

Conclusion: L2Seg is a flexible framework compatible with various solvers and VRPs, with the NAR and AR synergy offering the best performance.

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [22] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: A reinforcement learning approach using PPO for training neuro-fuzzy controllers shows improved stability and faster convergence compared to DQN-based methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the training of explainable neuro-fuzzy controllers by replacing off-policy DQN with stable on-policy PPO.

Method: PPO is applied to train neuro-fuzzy controllers, evaluated in CartPole-v1 with multiple random seeds and compared to ANFIS-DQN baselines.

Result: PPO-trained agents achieved a mean return of 500 +/- 0, with less variance and faster convergence than DQN methods.

Conclusion: PPO is a promising method for training explainable neuro-fuzzy controllers in RL tasks.

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [23] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers enhance PDE modeling using Clifford Algebra, optimizing 2/3D convolutional and multivector activation layers for CPU performance, achieving 30% speedup over PyTorch.


<details>
  <summary>Details</summary>
Motivation: To improve PDE modeling efficiency by integrating Clifford Algebra into neural networks, focusing on CPU performance optimization.

Method: Optimizing inference of 2/3D Clifford convolutional layers and multivector activation layers for single-core CPU performance.

Result: 30% faster than standard PyTorch implementation for large data and network sizes (>L2 cache).

Conclusion: The optimized Clifford Neural Layers show significant performance gains, with open-sourced code available for further use.

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [24] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: The paper introduces a DAG-based algorithm for optimal model splitting in split learning, reducing computational complexity and training delay.


<details>
  <summary>Details</summary>
Motivation: Complex AI model architectures increase computational complexity for optimal model splitting in split learning, necessitating efficient solutions.

Method: Represent AI models as DAGs, reformulate splitting as a minimum s-t cut problem, and propose fast DAG-based and block-wise splitting algorithms.

Result: The algorithms achieve optimal splitting in milliseconds and reduce training delay by 24.62%-38.95% compared to benchmarks.

Conclusion: The proposed methods efficiently solve model splitting in split learning, offering significant performance improvements.

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [25] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Åwiderski,Agnieszka JastrzÄbska*

Main category: cs.LG

TL;DR: A method for dynamically adjusting neural network architecture during training using Monte Carlo tree search, validated on visual and time series datasets.


<details>
  <summary>Details</summary>
Motivation: To optimize neural network performance by dynamically adjusting architecture during training, beyond just tuning weights.

Method: Uses Monte Carlo tree search to simulate and compare candidate architecture changes, enabling dynamic growing and shrinking of the model.

Result: Demonstrated effectiveness, especially in multivariate time series classification, due to dynamic adaptability.

Conclusion: The method is robust and adaptable, with promising performance in diverse tasks, supported by reproducible Python code.

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [26] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: A cardiac sensing foundation model (CSFM) using transformers and masked pretraining achieves superior performance across diverse cardiac biosignal tasks, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional deep learning approaches in cardiac biosignal analysis, which lack robustness and generalizability due to reliance on homogeneous datasets and static models.

Method: Developed CSFM with transformer architectures and generative masked pretraining, trained on multi-modal data (ECG, PPG, and text reports) from 1.7 million individuals.

Result: CSFM embeddings excel in feature extraction and transfer learning, outperforming traditional methods in diagnostic tasks, demographic recognition, vital sign measurement, and more.

Conclusion: CSFM is a versatile, scalable solution for comprehensive cardiac monitoring, robust across various sensor modalities and lead configurations.

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [27] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: The paper proposes a Variational Digital Twin (VDT) framework to address gaps in current digital twin literature, offering real-time updates, uncertainty calibration, and efficiency. It demonstrates success in energy-sector applications.


<details>
  <summary>Details</summary>
Motivation: Current digital twin literature lacks clear frameworks for real-time implementation, information exchange, and model uncertainty. The VDT aims to solve these issues.

Method: The VDT augments standard neural architectures with a Bayesian output layer and a novel updating algorithm, enabling fast updates and calibrated uncertainty bounds.

Result: The VDT achieves high accuracy (R2 > 0.95) in energy-sector problems, reduces experiments by 47%, and adapts to sensor loss or end-of-life conditions.

Conclusion: The VDT framework transforms conventional models into uncertainty-aware, efficient digital twins, suitable for industrial and scientific energy systems.

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [28] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,AfrÃ¢nio JosÃ© de Melo Junior,Celso JosÃ© Munaro,ClÃ¡udio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,FlÃ¡vio Miguel VarejÃ£o,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime AndrÃ©s Lozano Cadena,Jean Carlos Dias de AraÃºjo,JoÃ£o Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,RogÃ©rio Leite Alves Pinto*

Main category: cs.LG

TL;DR: The paper describes the updated 3W Dataset, a public resource for detecting undesirable events in oil wells using AI/ML, aimed at improving early detection and mitigation.


<details>
  <summary>Details</summary>
Motivation: Undesirable events in oil wells cause economic, environmental, and human harm. Public datasets for AI/ML solutions were lacking, prompting the creation and collaborative development of the 3W Dataset.

Method: The 3W Dataset is a labeled multivariate time series dataset, developed and refined collaboratively since 2019. The current version includes structural updates and additional labeled data.

Result: The dataset serves as a foundational reference for research, enabling improved methodologies and digital tools for early event detection.

Conclusion: The updated 3W Dataset supports the community in advancing detection capabilities, fostering corrective actions and new solutions.

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [29] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: A two-stage training framework for detoxifying social media text achieves strong detoxification, semantic preservation, and generalization with reduced reliance on annotated data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing detoxification methods, which struggle with performance, semantic preservation, and data efficiency.

Method: A two-stage approach: supervised fine-tuning on filtered parallel data, followed by training with unlabeled toxic inputs and a custom reward model using Group Relative Policy Optimization.

Result: State-of-the-art performance in detoxification, improved generalization, and reduced dependence on annotated data.

Conclusion: The proposed framework effectively mitigates trade-offs in detoxification, offering a robust and efficient solution.

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [30] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: A novel energy functional for long-sequence memory is introduced, using dense Hopfield networks with higher-order interactions and a temporal kernel for efficient sequential retrieval. Demonstrated with movie frames, it has applications in transformers for long-context tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of transformers in long-context tasks by improving storage capacity and sequential retrieval in memory models.

Method: Proposes a temporal kernel $K(m, k)$ within dense Hopfield networks to incorporate temporal dependencies for sequential pattern retrieval.

Result: Successful storage and sequential retrieval of movie frames, showcasing the model's capability with high-dimensional data.

Conclusion: The model enhances transformers for long-sequence tasks, with potential applications in NLP, forecasting, and time-series data.

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [31] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: A scalable multimodal framework is proposed for materials discovery, using elemental composition and XRD without crystal structure input, achieving faster convergence and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Current structure-based models are impractical for real-world applications where atomic structures are unknown or hard to obtain.

Method: The framework integrates modality-specific encoders with a cross-attention fusion module, trained on the Alexandria dataset, and uses self-supervised pretraining (MXM and contrastive alignment).

Result: Pretraining yields faster convergence (4.2x speedup), improved accuracy, and better representation quality. Multimodal performance scales better with dataset size than unimodal baselines.

Conclusion: The work establishes a path toward structure-free, experimentally grounded foundation models for materials science.

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [32] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: The study examines how flooding accelerates pavement roughness (measured by IRI) using 20 years of TxDOT data and XAI techniques like SHAP and LIME. Flooded pavements degrade faster, highlighting the need for mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Flooding causes immediate and long-term damage to pavements, but its specific impact on roughness (IRI) is not well quantified.

Method: Used 20 years of TxDOT PMIS data combined with flood event details. Applied statistical analysis and XAI (SHAP, LIME) to assess flood impact on IRI.

Result: Flood-exposed pavements show faster roughness increase than non-flooded ones.

Conclusion: Proactive flood mitigation (better drainage, resistant materials, maintenance) is crucial for pavement resilience in flood-prone areas.

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [33] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: An intelligent optimization system using deep CNN for mesh quality, featuring Loop2Net generator and loss functions to predict and optimize mesh generation.


<details>
  <summary>Details</summary>
Motivation: To improve mesh generation and optimization for given wing coordinates using deep learning techniques.

Method: Utilizes a deep convolutional neural network (CNN) with Loop2Net generator and two key loss functions for training and optimization.

Result: The system successfully predicts and optimizes mesh generation, achieving the goal through disciplined penalties.

Conclusion: The proposed system effectively enhances mesh quality and generation for specified coordinates.

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [34] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: The paper explores optimizing a foundational model for forecasting rare, spiky events in high-performance ML services, comparing it with classical stochastic models and achieving <6% error in outage predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forecasting rare, spiky events (e.g., production outages) where foundational models haven't been applied, despite their strengths in other time series tasks.

Method: Optimize a state-of-the-art foundational model for sporadic events and compare its forecasting errors with classical stochastic models (e.g., moving average, autoregressive).

Result: Foundational model outperforms classical models for spiky events, with <6% error in year-long outage predictions for specific root causes.

Conclusion: Foundational models can be effectively adapted for rare, spiky event forecasting, offering better accuracy than traditional methods.

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [35] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: The paper develops explainable AI methods using IMU data to predict Freezing of Gait (FOG) in Parkinson's disease, achieving 99% accuracy with a Stacking Ensemble model and incorporating federated learning for decentralized training.


<details>
  <summary>Details</summary>
Motivation: To improve early detection and prediction of FOG in Parkinson's disease using explainable AI and federated learning.

Method: Uses CatBoost, XGBoost, and Extra Trees classifiers, with a Stacking Ensemble model outperforming a hybrid bidirectional GRU. SHAP analysis identifies key features, and federated learning with a hybrid Conv1D + LSTM architecture is employed.

Result: The Stacking Ensemble model achieves nearly 99% classification accuracy, with time (seconds) as the most influential feature.

Conclusion: The proposed framework effectively predicts FOG with high accuracy and interpretability, while federated learning enhances scalability and privacy.

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [36] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: A novel 3D encoding module for GNNs using rotational sampling achieves rotational invariance and improves molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with 3D molecular structures due to variability in orientations, limiting generalization and robustness. Existing methods are either inflexible or computationally expensive.

Method: Proposes a plug-and-play 3D encoding module using rotational sampling and SO(3) group expectation, with a post-alignment strategy for strict invariance.

Result: Outperforms existing methods on QM9 and C10 datasets in accuracy, robustness, and generalization, with low computational cost.

Conclusion: The method offers an efficient, interpretable solution for 3D molecular data in drug discovery and material design.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [37] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a library for collecting provenance data in AI model training to optimize efficiency, accuracy, and energy use.


<details>
  <summary>Details</summary>
Motivation: The growing demand for large-scale AI models necessitates tools to balance computational efficiency, execution time, accuracy, and energy consumption, requiring insights into training processes.

Method: The yProv4ML library collects provenance data in JSON format, adhering to W3C PROV and ProvML standards, and supports extensibility via plugins.

Result: The library enables monitoring and analysis of resource usage, inefficiencies, and ensures reproducibility in AI workflows.

Conclusion: yProv4ML provides a flexible and extensible solution for optimizing distributed resource usage in large AI model training.

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [38] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: LBMs show marginal gains over traditional methods in BCI tasks but with higher computational costs. LoRA helps reduce parameters without losing performance, indicating inefficiencies in current LBM designs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of Large Brainwave Foundation Models (LBMs) in brainwave modeling and BCI tasks, given their unclear capabilities in this domain.

Method: Systematic fine-tuning experiments on BCI benchmarks, including memory tasks and sleep stage classification, using full model fine-tuning and LoRA for parameter-efficient adaptation.

Result: LBMs achieve only slight improvements (0.9%-1.2%) over traditional methods but require significantly more parameters. LoRA reduces parameters without performance loss.

Conclusion: Current LBMs are inefficient for BCI tasks and may need redesign. Domain-specific strategies are crucial to unlock their potential in brainwave analysis.

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [39] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: A decoder-only LLM is proposed for anomaly detection in ECU communication logs, addressing challenges like lack of tailored LLMs and inconsistent ground truth data. It uses entropy regularization and generative capabilities for scalable, accurate detection.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like automotive communication systems lack scalable anomaly detection solutions, and existing methods struggle with inconsistent labeling and tailored models.

Method: A decoder-only LLM learns from UDP communication logs, using entropy regularization to handle inconsistent labels and detect anomalies as deviations from normal behavior.

Result: The approach introduces a novel architecture, handles inconsistent labeling, and adapts to various ECU use cases, improving accuracy and scalability.

Conclusion: The proposed LLM offers a scalable, accurate solution for anomaly detection in ECU communication, reducing reliance on manual labeling and addressing domain-specific challenges.

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [40] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: A novel stochastic conjugate subgradient method with adaptive sampling is proposed for training large language models (LLMs), outperforming traditional SGD in convergence and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional SGD methods show performance limitations in large-scale LLM training, prompting the need for more efficient optimization techniques.

Method: The proposed method combines stochastic conjugate subgradients, adaptive sampling, and AdamW-like step size adjustment to handle nonconvexity and non-smoothness in LLMs.

Result: The method achieves faster convergence, improved scalability, and better optimization speed and accuracy compared to SGD.

Conclusion: The new approach effectively addresses SGD's limitations, offering superior performance for LLM training.

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [41] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a framework for capturing provenance data in machine learning processes using PROV-JSON format to address transparency and rigor issues in LLM development.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and rigor in large language model (LLM) development, especially in tracking hyperparameters and epochs, necessitates better provenance tracking tools.

Method: The paper proposes yProv4ML, a framework that captures provenance information during ML processes in PROV-JSON format with minimal code changes.

Result: yProv4ML provides a standardized way to track ML process data, addressing the limitations of proprietary formats and lineage neglect in existing tools like MLFlow.

Conclusion: yProv4ML enhances transparency and rigor in ML development by enabling provenance tracking in a standardized format.

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [42] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: The paper introduces PULSE, a protocol for evaluating unlearning in large multimodal models (LMMs), addressing gaps in existing benchmarks by focusing on pre-trained knowledge unlearning and long-term sustainability.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning benchmarks for LMMs lack realism, focusing only on single-operation fine-tuned knowledge unlearning. This study aims to address this gap.

Method: The PULSE protocol evaluates unlearning in LMMs by considering pre-trained knowledge unlearning and sequential unlearning requests.

Result: Current unlearning methods struggle with pre-trained knowledge and sequential unlearning, showing performance degradation.

Conclusion: The study highlights the need for more robust unlearning techniques in LMMs, particularly for pre-trained knowledge and sequential scenarios.

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [43] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,AmÃ©lie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: The study compares three AI models (NLP, LLM, JEPA) for ED triage, finding LLM (URGENTIAPARSE) most accurate, outperforming nurses and other models.


<details>
  <summary>Details</summary>
Motivation: To address triage errors (undertriage/overtriage) in EDs by evaluating AI models' performance against traditional methods.

Method: Retrospective analysis of patient data using three AI models (NLP, LLM, JEPA) trained on demographic, complaint, and vital sign data, validated against the FRENCH scale.

Result: LLM (URGENTIAPARSE) achieved the highest accuracy (composite score: 2.514), outperforming JEPA (0.438), NLP (-3.511), and nurse triage (-4.343).

Conclusion: LLMs show promise for improving ED triage accuracy, but integration requires addressing limitations and ethical concerns.

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [44] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper introduces a Neural Hamiltonian Operator (NHO) to solve high-dimensional stochastic control problems using deep learning, framed as learning an operator from simulated data.


<details>
  <summary>Details</summary>
Motivation: Addressing the curse of dimensionality in stochastic control problems by leveraging Pontryagin's Maximum Principle (PMP) and deep learning.

Method: Defines NHO to parameterize FBSDE dynamics via neural networks, training them to enforce PMP consistency conditions.

Result: Demonstrates universal approximation capabilities of NHOs under general martingale drivers.

Conclusion: The NHO framework provides a rigorous, operator-theoretic approach to solving high-dimensional stochastic control problems with deep learning.

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [45] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: The note explains Ziyin et al.'s proof of the 'perfect' Platonic Representation Hypothesis (PRH) for embedded deep linear networks (EDLN), showing SGD leads to identical layer representations up to rotation, despite most global minima not being Platonic. It also links PRH to progressive sharpening and identifies six ways PRH can fail.


<details>
  <summary>Details</summary>
Motivation: To clarify and detail Ziyin et al.'s proof of PRH in EDLNs, emphasizing the surprising role of SGD in achieving Platonic representations and exploring connections to other deep learning phenomena.

Method: Analyzes the proof for PRH in EDLNs trained with SGD, demonstrating how layers converge to identical representations up to rotation, and identifies conditions under which PRH fails.

Result: SGD uniquely finds perfectly Platonic solutions in EDLNs, despite most global minima not being Platonic. PRH's emergence is linked to progressive sharpening, suggesting a common cause. Six ways to break PRH are identified.

Conclusion: The note underscores the significance of emergent 'entropic forces' in SGD-driven representation learning, revealing unexpected connections between seemingly unrelated deep learning phenomena.

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [46] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces the dual-learning hypothesis to explain LLMs' vulnerability to backdoor attacks in ICL and proposes ICLShield, a defense mechanism that dynamically adjusts concept preference ratios to mitigate attacks.


<details>
  <summary>Details</summary>
Motivation: ICL's adaptability in LLMs makes it vulnerable to backdoor attacks, where adversaries manipulate model behavior via poisoned demonstrations.

Method: Proposes the dual-learning hypothesis and ICLShield, which adjusts concept preference ratios using confidence and similarity scores to select clean demonstrations.

Result: ICLShield outperforms existing methods by 26.02% on average and shows strong adaptability, even with closed-source models like GPT-4.

Conclusion: The dual-learning hypothesis and ICLShield provide a robust defense against ICL backdoor attacks, enhancing LLM security.

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [47] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: A neural operator combining DMD and DL for efficient spatiotemporal modeling, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Balancing lightweight and accurate computations in scientific AI, especially for solving PDEs with high resource demands.

Method: Uses dynamic mode decomposition (DMD) and deep learning to map functional spaces, extracting key modes and dynamics for predictions.

Result: Outperforms DeepONet and FNO in accuracy for heat, Laplace, and Burgers equations with lower computational costs.

Conclusion: The DMD-based neural operator is efficient for spatiotemporal modeling, offering a lightweight yet accurate alternative to traditional methods.

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [48] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: The paper introduces APARL, a framework for detecting abnormal events in customer service dialogues, enhancing adaptability and OOD generalization with a dual-loop dynamic curriculum learning approach.


<details>
  <summary>Details</summary>
Motivation: Detecting abnormal events in customer service dialogues is complex due to dynamic interactions and diverse business scenarios, requiring strong OOD generalization for commercial value.

Method: Proposes APARL, leveraging large language models with a dual-loop dynamic curriculum learning architecture to progressively tackle challenging samples.

Result: Achieves significant improvements: 17.19% higher F1 score and 9.59% better OOD transfer performance in food delivery dialogue tasks.

Conclusion: APARL offers a superior solution for industrial anomaly detection, boosting operational efficiency and commercial benefits.

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [49] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: The paper critiques DP training methods, finding that unbiased second-moment estimates in adaptive optimizers are less effective than the scale-then-privatize technique, which performs better empirically.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap in DP training caused by spherical noise in adaptive optimizers like AdaGrad and Adam.

Method: Survey and compare variants of DP training methods, focusing on theoretical intuition and empirical studies.

Result: Scale-then-privatize outperforms other methods, despite not providing unbiased second-moment estimates, and aligns better with correlated noise mechanisms.

Conclusion: The scale-then-privatize technique is more effective and theoretically sound for DP training, challenging common intuitions about unbiased estimates.

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [50] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: TDNs replace CG tensor products with low-rank decompositions for faster, approximately equivariant MLIPs, achieving competitive performance with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of CG tensor products in SO(3)-equivariant networks for MLIPs.

Method: Develop TDNs using low-rank tensor decompositions (e.g., CP) and path-weight sharing to reduce complexity.

Result: TDNs achieve competitive performance on PubChemQCR, OC20, and OC22 datasets with significant speedup.

Conclusion: TDNs offer a practical, efficient alternative to traditional SO(3)-equivariant networks for MLIPs.

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [51] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: DSAC-D, a distributional reinforcement learning algorithm, addresses bias in value function estimation and achieves multimodal policy representations, outperforming existing methods in control tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal distributions in reinforcement learning cause bias in value function estimation, leading to poor performance.

Method: DSAC-D introduces policy entropy and a value distribution function, using a diffusion model for reverse sampling to create a multimodal distributional policy iteration framework.

Result: The algorithm achieves SOTA performance in 9 control tasks, reducing bias and improving returns by over 10%. Real-world tests confirm its ability to model multimodal driving styles.

Conclusion: DSAC-D effectively addresses bias and enables multimodal policy learning, demonstrating superior performance in both simulated and real-world tasks.

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [52] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: SMH addresses imbalanced regression on graph-structured data by generating synthetic samples preserving topology and focusing on underrepresented target ranges, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: Imbalanced regression in graph-structured data lacks research, especially for scientifically valuable target ranges. Conventional methods ignore topology or fail to target specific ranges, leading to biased models.

Method: Spectral Manifold Harmonization (SMH) generates synthetic graph samples preserving topological properties and targeting underrepresented target distribution regions.

Result: SMH shows consistent improvements in predictive performance for target domain ranges on chemistry and drug discovery benchmarks.

Conclusion: SMH effectively addresses imbalanced regression in graph-structured data by harmonizing topology and target distribution focus, outperforming conventional methods.

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [53] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: The paper introduces a TVM compiler-based workflow to optimize AI workloads for RISC-V Vector Extension (RVV), outperforming autovectorization and hand-crafted libraries in efficiency and code size.


<details>
  <summary>Details</summary>
Motivation: Efficiently deploying AI workloads on RISC-V RVV without expert knowledge is challenging due to lack of autotuning frameworks.

Method: Integrated RVV into TVM's MetaSchedule framework, tested on FPGA-implemented RISC-V SoCs and a commercial SoC.

Result: 46% faster than GCC autovectorization, 29% faster than muRISCV-NN, smaller code footprint, and 35% faster than LLVM on a commercial SoC.

Conclusion: The proposed workflow effectively optimizes AI workloads for RISC-V RVV, with open-sourced code for community expansion.

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [54] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP introduces a cache-friendly per-layer DP-SGD method to reduce memory and computational overhead in differentially private training of LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies and high memory demands of existing DP-SGD methods like Opacus and GhostClip.

Method: FlashDP consolidates operations into a single task, calculating gradients once in a fused manner, reducing memory movement and redundant computations.

Result: FlashDP reduces memory movement by 50% and redundant computations by 20%, achieving 90% throughput of Non-DP methods while maintaining accuracy.

Conclusion: FlashDP is a significant advancement for efficient and privacy-preserving LLM training, with open-sourced code available.

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [55] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: The paper challenges the assumption that reward frequency measures task difficulty in reinforcement learning, identifying zero-incentive dynamics as a key issue where critical subgoals lack rewards.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current methods that fail when essential subgoals don't yield direct rewards.

Method: Analyzes zero-incentive dynamics and evaluates state-of-the-art deep subgoal-based algorithms.

Result: Shows these algorithms struggle with unrewarded critical transitions and are sensitive to reward timing.

Conclusion: Highlights the need for new mechanisms to infer latent task structure beyond immediate rewards.

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [56] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: Diffusion Explorer is an interactive tool designed to explain the geometric properties of diffusion models through 2D training and real-time observation, making the concept accessible without advanced theory.


<details>
  <summary>Details</summary>
Motivation: Existing explanations of diffusion models are either too theoretical or overly focused on neural architectures, neglecting their geometric properties.

Method: Developed Diffusion Explorer, a browser-based interactive tool with animations to visualize the temporal dynamics of 2D diffusion models.

Result: Users can train and observe diffusion models in real-time, enhancing understanding of their stochastic and geometric nature.

Conclusion: Diffusion Explorer successfully bridges the gap in accessible explanations of diffusion models' geometric properties, with an open-source demo available.

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [57] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax is a JAX-based environment for simulating electric vehicle charging stations, offering 100x-1000x faster training for RL agents compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Addressing slow reinforcement learning training and high sample complexity in sustainable energy applications, particularly grid congestion and operational efficiency.

Method: Developed Chargax, a JAX-based environment for realistic EV charging station simulations, validated with real data and compared RL agents to baselines.

Result: Chargax achieves 100x-1000x computational performance improvements and supports diverse real-world charging station configurations.

Conclusion: Chargax provides a scalable, efficient solution for training RL agents in sustainable energy applications, bridging the gap between toy problems and real-world scenarios.

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [58] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO is a novel framework for process-aware reinforcement learning in LLMs, eliminating the need for external reward models and improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead and lack of theoretical framework for process-level advantage estimation in PRL.

Method: SPRO derives process rewards intrinsically from the policy model and introduces cumulative process rewards and Masked Step Advantage (MSA).

Result: SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% test accuracy improvement, and reduced response length.

Conclusion: SPRO offers a computationally efficient, stable, and effective solution for process-aware RL in LLMs.

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [59] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: The paper proposes the Joint Autoencoder Modulator (JAM) framework to align disjoint vision and language representations, optimizing for mutual coherence while preserving modality-specific structures.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests that independently trained vision and language models may converge toward a shared statistical model of reality, prompting the need for explicit alignment optimization.

Method: The JAM framework jointly trains modality-specific autoencoders on pre-trained single-modality models, using reconstruction and cross-modal objectives to encourage alignment.

Result: The framework reliably induces alignment across frozen, independently trained representations, with evaluations on alignment objectives, layer depth, and foundation model scale.

Conclusion: JAM offers a lightweight, Pareto-efficient solution for transforming unimodal foundations into specialist multimodal models, bridging disjoint representations.

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [60] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: The paper introduces GradMetaNet, a novel architecture for processing gradients in neural networks, guided by principles of equivariant design, multi-point gradient processing, and efficient representation. It outperforms previous methods on tasks like optimization and model editing.


<details>
  <summary>Details</summary>
Motivation: Gradients in neural networks are valuable for tasks like optimization and model analysis, but existing architectures for gradient processing are limited. This work aims to design a principled architecture for efficient and effective gradient processing.

Method: The authors propose GradMetaNet, built on equivariant blocks, to process gradients while preserving symmetries and capturing curvature. It uses rank-1 decomposition for efficient representation.

Result: GradMetaNet achieves universality in gradient-based functions and outperforms prior methods on tasks like learned optimization and loss landscape curvature estimation.

Conclusion: GradMetaNet provides a robust framework for gradient processing, demonstrating superior performance across diverse tasks, and offers theoretical guarantees of universality.

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [61] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: The paper proposes using fast neural network techniques (Distilling and Pruning) to deploy Intrusion Detection Systems (IDS) on low-cost hardware like Raspberry Pi 4, achieving real-time performance with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles rely on automotive Ethernet, which is vulnerable to attacks like flow injection. Existing deep learning-based IDS require expensive hardware for real-time operation, limiting practicality.

Method: The authors evaluate and apply Distilling and Pruning techniques to optimize IDS models for deployment on low-cost platforms (e.g., Raspberry Pi 4).

Result: The optimized IDS achieves intrusion detection times of 727 Âµs on Raspberry Pi 4 with an AUCROC of 0.9890.

Conclusion: Fast neural network techniques enable efficient, real-time IDS deployment on affordable hardware, enhancing automotive security.

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [62] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow is an asynchronous streaming RL framework for efficient post-training of LLMs, addressing scalability and resource issues in traditional RL frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL frameworks for LLMs face scalability bottlenecks, complex dataflows, and tight coupling with training/inference engines, limiting flexibility and efficiency.

Method: Proposes AsyncFlow with distributed data storage, fine-grained scheduling, and a producer-consumer workflow to minimize idleness and enable dynamic load balancing.

Result: Achieves a 1.59x throughput improvement over state-of-the-art baselines.

Conclusion: AsyncFlow offers modular, customizable, and efficient RL post-training, providing insights for future RL system designs.

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [63] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM is a privacy-aware, efficient method for fine-tuning large language models on mobile devices using server-assisted side-tuning, reducing communication costs and protecting data privacy.


<details>
  <summary>Details</summary>
Motivation: Address the gap between mobile device limitations and the demand for on-device LLM fine-tuning, while mitigating communication burdens and privacy risks of existing server-assisted methods.

Method: Uses server-assisted additive side-tuning with activation caching, a one-token activation shortcut, and additive adapter side-network design to enhance efficiency and privacy.

Result: Reduces communication costs, accelerates fine-tuning convergence, and ensures data, label, and model privacy.

Conclusion: PAE MobiLLM effectively balances efficiency and privacy for on-device LLM fine-tuning, making it practical for mobile applications.

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [64] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: Prefix-RFT is a hybrid approach combining SFT and RFT, outperforming both and integrating easily into existing frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs between SFT (good at mimicking but poor generalization) and RFT (performance-sensitive but prone to unexpected behaviors).

Method: Proposes Prefix-RFT, a hybrid method synergizing demonstration (SFT) and exploration (RFT), tested on mathematical reasoning problems.

Result: Prefix-RFT outperforms standalone SFT and RFT, and parallel mixed-policy RFT methods, with robustness to data variations.

Conclusion: Prefix-RFT harmonizes SFT and RFT effectively, suggesting a unified paradigm for future LLM post-training research.

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [65] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: Quantum machine learning models (QSVM and QNN) were tested for classifying pedestrian stress using SCR data. QNN outperformed QSVM and classical methods with 55% test accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for modeling complex SCR events in pedestrian stress scenarios, addressing limitations of classical methods.

Method: Developed QSVM and QNN models using an eight-qubit ZZ feature map on Pennylane, tested on SCR data with amplitude-based classes.

Result: QSVM showed overfitting (45% test accuracy), while QNN achieved better performance (55% test accuracy).

Conclusion: QNN is more reliable than QSVM and classical models for classifying pedestrian stress, though further improvements are needed.

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [66] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: SODA, a gradient-based algorithm, reconstructs exact inputs from LLM outputs for forensic analysis, outperforming existing methods but struggling with longer sequences.


<details>
  <summary>Details</summary>
Motivation: To enable post-incident analysis and detect fake outputs by reconstructing the exact input from LLM outputs.

Method: Formalizes input reconstruction as a discrete optimization problem, introduces SODA with gradient-based search, periodic restarts, and parameter decay.

Result: Recovers 79.5% of shorter out-of-distribution inputs with no false positives; struggles with longer sequences (15+ tokens).

Conclusion: Standard deployment practices may currently protect against malicious use of SODA, but it excels in forensic analysis of shorter inputs.

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [67] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: RelFCI is a causal discovery algorithm for relational data with latent confounders, addressing gaps in existing methods by combining FCI and RCD approaches.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods fail for relational data with latent confounders, either assuming i.i.d. data or causal sufficiency.

Method: RelFCI builds on FCI and RCD, introducing new graphical models for relational d-separation with latent confounders.

Result: RelFCI effectively identifies correct causal structures in relational models with latent confounders.

Conclusion: RelFCI fills a critical gap in causal discovery for relational data, offering sound and complete guarantees.

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [68] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: Dist-FedAvg is a distance-based aggregation method for graph federated recommendation systems, improving personalization and efficiency by weighting similar user embeddings higher and preserving anchor user influence.


<details>
  <summary>Details</summary>
Motivation: Traditional aggregation methods in federated learning overlook the complexity of user embeddings and the role of user similarity, limiting recommendation effectiveness.

Method: Introduces Dist-FedAvg, which assigns higher weights to users with similar embeddings and ensures anchor users retain influence in local updates.

Result: Empirical evaluations show Dist-FedAvg outperforms baseline methods, enhancing recommendation accuracy while integrating seamlessly into federated frameworks.

Conclusion: Dist-FedAvg addresses limitations of existing aggregation methods, offering improved personalization and efficiency in graph federated recommendation systems.

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [69] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER is an evolutionary black-box method for LLM post-training, addressing privacy and security concerns of gradient-based optimization while overcoming scalability and computational challenges of black-box methods.


<details>
  <summary>Details</summary>
Motivation: Gradient-based optimization in deep learning raises privacy and security concerns (e.g., data poisoning, overfitting). Black-box methods offer an alternative but face scalability and computational issues, especially in high-dimensional spaces like LLMs.

Method: BBoxER introduces an evolutionary black-box approach for LLM post-training, using implicit data compression to create an information bottleneck. It provides theoretical guarantees on generalization, privacy, and robustness.

Result: Experiments show BBoxER improves LLM performance and generalization on reasoning benchmarks with minimal iterations, offering a lightweight, modular enhancement.

Conclusion: BBoxER is a promising add-on to gradient-based optimization, suitable for privacy-sensitive environments, with strong theoretical and empirical support.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [70] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: The paper introduces Clipped Density and Clipped Coverage, two novel metrics for evaluating generative models, addressing issues of fidelity, coverage, and robustness in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current quality metrics for generative models lack reliability and interpretability due to calibration and robustness issues, hindering their use in critical applications.

Method: The authors propose Clipped Density and Clipped Coverage, which clip individual sample contributions and nearest neighbor radii to prevent bias from outliers.

Result: The metrics show linear score degradation with poor samples and outperform existing methods in robustness, sensitivity, and interpretability.

Conclusion: Clipped Density and Clipped Coverage provide reliable, interpretable evaluation of generative models, advancing their practical applicability.

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [71] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia RodrÃ­guez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet converts decision tree ensembles into sparse neural networks, combining symbolic structure with gradient-based optimization for improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic (decision trees) and neural network approaches, enabling interpretability and optimization benefits.

Method: Maps decision tree branches to hidden neurons, creating sparse neural networks without manual tuning.

Result: Outperforms XGBoost in accuracy on multi-class benchmarks, with significant gains.

Conclusion: BranchNet offers interpretable, compact models but may need adaptive calibration for binary tasks.

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [72] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz SÃ¡ez de OcÃ¡riz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: A CPU-friendly method for LoRA fine-tuning of LLMs using pre-trained adapters, offering a practical alternative to GPU-based training.


<details>
  <summary>Details</summary>
Motivation: To enable parameter-efficient fine-tuning of LLMs for users with limited computational resources, such as standard laptop CPUs.

Method: Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters, avoiding gradient-based updates.

Result: Adapters outperform the base Mistral model but fall short of GPU-trained counterparts.

Conclusion: Provides a viable, accessible alternative to GPU-based fine-tuning for resource-constrained users.

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [73] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: The paper introduces the Wavelet Diffusion Model (WDM) for downscaling precipitation data from 10 km to 1 km resolution, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Standard global precipitation data (e.g., IMERG) lacks fine resolution (10 km), limiting hydrological modeling and extreme weather analysis.

Method: WDM is a conditional diffusion model that learns precipitation structure from MRMS radar data in the wavelet domain, focusing on high-frequency coefficients.

Result: WDM achieves 10x spatial super-resolution (1 km), 9x faster inference than pixel-based models, and produces realistic, artifact-free results.

Conclusion: WDM addresses accuracy and speed challenges in geoscience super-resolution, enhancing hydrological forecasting reliability.

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [74] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: A novel method uses GNNs to solve SAT problems by mapping k-CNF formulae to MILP, encoding them as bipartite graphs, and training GNNs. Theoretical results include invariance properties, limitations for foldable formulae, and universal approximation with RNI. Experiments show promising results.


<details>
  <summary>Details</summary>
Motivation: To leverage GNNs for solving SAT problems by bridging the gap between k-CNF formulae and MILP, enabling efficient and scalable SAT solving.

Method: Map k-CNF formulae to MILP, encode as weighted bipartite graphs, and train GNNs. Theoretical analysis includes invariance, limitations, and approximation guarantees.

Result: The method achieves permutation and equivalence invariance, identifies limitations for foldable formulae, and proves universal approximation with RNI. Experiments show promising performance.

Conclusion: The proposed GNN-based method effectively solves SAT problems with theoretical guarantees and practical promise, though limitations exist for certain formula classes.

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [75] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: mGRADE is a hybrid-memory system combining temporal convolution and minimal gated recurrence for efficient multi-scale temporal processing on edge devices.


<details>
  <summary>Details</summary>
Motivation: Address the need for models that handle short- and long-range dynamics under tight memory constraints, where Transformers, RNNs, and TCNs fall short.

Method: Integrates a 1D-convolution with learnable spacings and a minimal gated recurrent unit (minGRU) for flexible delay embedding and global context.

Result: Outperforms pure convolutional and recurrent models on synthetic tasks and image classification benchmarks with 20% less memory.

Conclusion: mGRADE is an efficient solution for memory-constrained multi-scale temporal processing at the edge.

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [76] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: An enhanced surrogate model integrates slack variables into a factorization machine and Ising representation, improving performance in predicting drug combination effects.


<details>
  <summary>Details</summary>
Motivation: To unify the two-step process of surrogate modeling into a single step and improve performance by accounting for higher-order feature interactions.

Method: Incorporates slack variables into the factorization machine and Ising representation, iteratively updating them during training.

Result: Notable performance improvement in predicting drug combination effects.

Conclusion: The proposed algorithm is promising for efficient surrogate models leveraging quantum advantages.

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [77] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*GastÃ³n GarcÃ­a GonzÃ¡lez,Pedro Casas,Emilio MartÃ­nez,Alicia FernÃ¡ndez*

Main category: cs.LG

TL;DR: FAE (Foundation Auto-Encoders) is a foundation generative-AI model for anomaly detection in time-series data, leveraging VAEs and DCNNs for zero-shot applications.


<details>
  <summary>Details</summary>
Motivation: To create a pretrained model for time-series data that can detect anomalies accurately across unseen datasets.

Method: Uses Variational Auto-Encoders (VAEs) and Dilated Convolutional Neural Networks (DCNNs) for univariate time-series modeling.

Result: Preliminary results show effectiveness on multi-dimensional datasets, including a real mobile ISP dataset and KDD 2021.

Conclusion: FAE demonstrates potential for zero-shot anomaly detection in diverse time-series domains.

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [78] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: The paper introduces a toy problem combining linear-regression-style continuous in-context learning (ICL) with discrete associative recall, analyzing transformer models' ability to recall and predict sequences.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer models develop capabilities for associative recall and sequence prediction in a controlled setting.

Method: Pretraining transformer models on symbolic-labeled interleaved state observations from linear dynamical systems, analyzing training dynamics and mechanistic behaviors.

Result: Two distinct mechanisms emerge: one for associative recall using symbolic labels, and another for Bayesian-style prediction. These mechanisms have different learning dynamics.

Conclusion: The findings suggest multi-mechanism learning in transformers, supported by similar observations in a translation task, indicating broader applicability.

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [79] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: A hybrid deep learning approach combining LSTM and Transformers, with pseudo-labeling via iForest and AE, is proposed for anomaly detection in mental healthcare billing, showing high recall but lower precision.


<details>
  <summary>Details</summary>
Motivation: The complexity of mental healthcare billing and challenges like class imbalance, label scarcity, and sequential patterns motivate the exploration of hybrid models for anomaly detection.

Method: The study uses a hybrid approach with LSTM and Transformers, leveraging pseudo-labeling via iForest and AE, evaluated on two real-world mental healthcare billing datasets.

Result: The iForest LSTM baseline achieves high recall (0.963) on declaration-level data, while the hybrid iForest-based model achieves 0.744 recall on operation-level data, though with lower precision.

Conclusion: The hybrid approach with pseudo-labeling shows promise for complex, imbalanced anomaly detection in healthcare billing.

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


### [80] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,JoaquÃ­n Torres-Sospedra*

Main category: cs.LG

TL;DR: The paper introduces BAR, a novel RSS dataset for indoor positioning in museums, addressing the lack of public datasets for cultural heritage sites. It also provides a baseline classification method using proximity and k-NN algorithms.


<details>
  <summary>Details</summary>
Motivation: Enhancing visitor experiences in cultural heritage institutions through IPSs is hindered by the lack of suitable RSS datasets for museum environments.

Method: The authors collected RSS data in front of 90 artworks across 13 museum rooms using Android and iOS platforms, and proposed a baseline classification method combining proximity-based techniques and k-NN algorithms.

Result: The study presents the BAR dataset and evaluates the baseline method, discussing its performance and implications.

Conclusion: The BAR dataset fills a critical gap in IPS research for museums, and the proposed baseline method offers a foundation for future algorithm development in this context.

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [81] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda GregorovÃ¡*

Main category: cs.LG

TL;DR: This paper explores and unifies various loss functions in diffusion models under the variational lower bound framework, analyzing their relationships, performance differences, and impact on model goals like sample quality and likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: To address the key question of which loss functions diffusion models should train with, given the multiple formulations in literature with varying links and differences.

Method: A systematic overview of target objectives and loss functions, unified under the variational lower bound framework, complemented by empirical studies on performance divergence and goal impact.

Result: Provides insights into when and why objectives diverge in performance and how the choice of objective affects model goals like sample quality and likelihood estimation.

Conclusion: Offers a unified understanding of loss functions in diffusion models, aiding more efficient and goal-oriented future designs.

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [82] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS enables small vision-language models to predict any data modality accurately without training, outperforming Gemini by 16% and approaching specialized methods.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between specialized models' high performance but lack of flexibility and foundation models' versatility but underperformance, especially in non-traditional domains.

Method: Transforms latent embedding spaces into visual representations, leveraging VLMs' spatial and fine-grained reasoning skills for interpretation.

Result: Achieves competitive performance across vision, audio, biological, and tabular domains using a single 3B parameter model, outperforming Gemini by 16%.

Conclusion: MARVIS offers a training-free, flexible solution for multi-modal prediction without P.I.I. exposure or domain-specific training, with open-sourced code and datasets.

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [83] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: Zapping (resampling weights in the last layer) improves continual learning, but its mechanisms are unclear. This study explores its effects in challenging settings like few-shot transfer learning.


<details>
  <summary>Details</summary>
Motivation: To understand how zapping and optimizer choices impact learning and forgetting in continual learning scenarios.

Method: Experiments with convolutional neural networks in continual and few-shot transfer learning, using handwritten characters and natural images.

Result: Zapping helps models recover faster from domain shifts. Optimizer choice also influences learning dynamics, causing complex task interactions.

Conclusion: Zapping and optimizer selection significantly affect continual learning, revealing intricate patterns of task synergy/interference.

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [84] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: Proposes a Federated Learning (FL)-based approach for indoor localization using DNN, addressing privacy and efficiency issues of centralized methods.


<details>
  <summary>Details</summary>
Motivation: Traditional indoor localization methods have high errors and privacy concerns due to centralized data collection. ML techniques, while promising, also face privacy and reliability issues.

Method: Uses Federated Learning (FL) with a Deep Neural Network (DNN) model to enable privacy-preserving, efficient indoor localization.

Result: FL achieves performance close to centralized models while ensuring data privacy, bandwidth efficiency, and server reliability.

Conclusion: The FL approach is a viable solution for privacy-enhanced indoor localization, advancing secure and efficient systems.

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [85] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper analyzes Muon, a new optimizer for neural networks, proving convergence for its variants and showing tighter bounds with weight decay. It also derives Muon's critical batch size and validates findings experimentally.


<details>
  <summary>Details</summary>
Motivation: To theoretically analyze Muon, a novel optimizer leveraging neural network parameter structures, and explore its variants, convergence, and practical implications.

Method: Theoretical analysis of Muon's convergence for four variants (with/without Nesterov momentum and weight decay), derivation of bounds, and relationship between weight decay and learning rate. Also, derivation of critical batch size for SFO complexity.

Result: Weight decay leads to tighter bounds on parameter and gradient norms. The relationship between weight decay and learning rate is clarified. Critical batch size for minimizing SFO complexity is derived.

Conclusion: Muon's theoretical properties are validated, showing practical benefits of weight decay and optimal batch size for efficient training.

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [86] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: An efficient online dictionary learning algorithm for kernel-based sparse representations, using RLS for updates, outperforms existing methods and matches batch-trained accuracy with lower complexity.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance of online kernel dictionary learning for sparse representations in high-dimensional feature spaces.

Method: Recursive least squares (RLS)-based dictionary update mechanism, working with single samples or mini-batches, maintaining low computational complexity.

Result: Outperforms existing online kernel dictionary learning methods and achieves classification accuracy close to batch-trained models, with higher efficiency.

Conclusion: The proposed method is effective and efficient for online kernel dictionary learning, balancing performance and computational cost.

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [87] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: DDCL improves DDR chart generation using ConvLSTM, outperforming the previous CNN-LSTM method (DDC).


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of automatic DDR chart generation.

Method: Uses a ConvLSTM-based model for chart generation.

Result: Substantially increases accuracy compared to DDC.

Conclusion: DDCL is a superior method for automatic DDR chart generation.

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [88] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: PERTINENCE is an online method that dynamically selects the most suitable pre-trained DNN model for a given input, balancing accuracy and computational efficiency using a genetic algorithm. It achieves comparable accuracy with up to 36% fewer operations.


<details>
  <summary>Details</summary>
Motivation: Large DNNs are resource-intensive, but their high computational cost is often unnecessary for simpler inputs. Combining models dynamically can improve efficiency without sacrificing accuracy.

Method: PERTINENCE uses a genetic algorithm to train an ML-based dispatcher that selects models based on input complexity, optimizing for accuracy and efficiency.

Result: Tested on CNNs (CIFAR-10, CIFAR-100) and ViTs (TinyImageNet), PERTINENCE matches or exceeds accuracy while reducing operations by up to 36%.

Conclusion: PERTINENCE demonstrates that dynamic model selection can significantly enhance efficiency without compromising accuracy, offering a practical solution for resource-constrained applications.

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [89] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: Proposes Variational Neural Network versions of Graph Convolutional Networks to estimate uncertainty, improving explainability and accuracy in tasks like social trading and human action recognition.


<details>
  <summary>Details</summary>
Motivation: Enhancing model explainability and accuracy by estimating uncertainty in Graph Convolutional Networks, useful for critical applications requiring expert verification.

Method: Introduces Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks, estimating uncertainty in outputs and layer-wise attentions.

Result: Demonstrates improved model accuracy and uncertainty estimation on Finnish board membership, NTU-60, NTU-120, and Kinetics datasets.

Conclusion: The proposed method effectively enhances model explainability and accuracy through uncertainty estimation, validated on diverse datasets.

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [90] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: The paper proposes replacing PINN ensembles with Bayesian PINNs, using posterior variance for evaluation, showing improved performance over ensembles.


<details>
  <summary>Details</summary>
Motivation: Address convergence issues in PINNs for forward problems by ensuring information propagation from well-defined regions.

Method: Replace ensemble approach with Bayesian PINNs, evaluating posterior variance instead of consensus.

Result: Outperforms ensemble methods on benchmarks and competes with Adam-LBFGS-trained ensembles.

Conclusion: Bayesian PINNs offer a principled, effective alternative to ensembles for improving PINN convergence.

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [91] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: The paper evaluates learning rate control methods in deep learning, finding current approaches inconsistent across tasks and advocating for algorithm selection and new directions like meta-learning.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of various learning rate control paradigms in deep learning and identify gaps in current research.

Method: Comparison of paradigms including multi-fidelity hyperparameter optimization, fixed schedules, and hyperparameter-free learning across tasks.

Result: Current methods perform well on specific tasks but lack reliability across settings; hyperparameter optimization becomes less effective with complex models.

Conclusion: Algorithm selection and new approaches like meta-learning are needed to improve learning rate control in deep learning.

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [92] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim FlÃ¼hmann,Artemii Shlychkov,JosÃ© Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: Proposes a Simulation-Based Inference (SBI) method using Neural Posterior Estimation for efficient parameter estimation in Type 1 Diabetes models, outperforming traditional MCMC methods.


<details>
  <summary>Details</summary>
Motivation: Accurate parameter estimation for physiological models, especially in Type 1 Diabetes, is challenging due to complex glucose-insulin interactions and computational inefficiency of traditional methods.

Method: Uses Simulation-Based Inference with Neural Posterior Estimation to model relationships between meal intake, insulin, and glucose levels, enabling faster, amortized inference.

Result: SBI outperforms traditional MCMC methods in parameter estimation, generalizes better to unseen conditions, and provides real-time posterior inference with uncertainty quantification.

Conclusion: The proposed SBI approach offers a computationally efficient and reliable solution for parameter estimation in Type 1 Diabetes models, improving over traditional methods.

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [93] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: Proposes decentralized and sustainable foundation model training using edge AI devices to address environmental and centralization concerns.


<details>
  <summary>Details</summary>
Motivation: Addresses the environmental impact and centralized control risks of current foundation models by leveraging underutilized edge AI devices.

Method: Envisions decentralized training using collective compute of connected edge devices, emphasizing sustainability.

Result: Identifies challenges to realize decentralized, sustainable foundation model training.

Conclusion: Advocates for a shift to decentralized training to mitigate environmental and centralization issues.

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [94] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: A novel knowledge transfer method in model-based reinforcement learning efficiently distills a large multi-task agent into a compact model, improving performance and reducing size for resource-constrained deployment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying large world models in resource-constrained environments by enabling efficient knowledge transfer.

Method: Distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) and optimizes it with FP16 post-training quantization.

Result: Achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93, and reduces model size by ~50%.

Conclusion: The method effectively consolidates complex multi-task knowledge, offering practical deployment solutions and insights for efficient multi-task reinforcement learning systems.

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [95] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: The paper critiques current OOD detection methods, highlighting their fundamental misalignment with the task of identifying out-of-distribution data. It shows that uncertainty-based and feature-based methods fail due to inherent flaws, and even advanced interventions don't resolve these issues. Unsupervised methods also have limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expose the flaws in popular OOD detection methods, which misalign with the actual goal of identifying OOD data, leading to unreliable results.

Method: The paper critically analyzes uncertainty-based and feature-based OOD detection methods, demonstrating their limitations through theoretical and empirical evidence. It also evaluates interventions like hybrid methods and unsupervised approaches.

Result: The results reveal irreducible errors in current OOD detection methods, showing they conflate uncertainty or feature distance with OOD status. Advanced fixes fail to address the core misalignment.

Conclusion: The conclusion is that current OOD detection methods are fundamentally flawed due to misaligned objectives, and new approaches are needed to address these limitations.

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [96] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: SubLoRA introduces a second-order rank determination method for LoRA, using submodular function maximization and Hessian matrix, outperforming prior methods like AdaLoRA.


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on inaccurate first-order approximations of the loss function, especially when LoRA parameters are well-optimized, necessitating a more reliable second-order approach.

Method: SubLoRA reformulates rank determination as a combinatorial optimization problem with a quadratic objective, solved via submodular function maximization and a greedy algorithm with approximation guarantees.

Result: SubLoRA outperforms existing methods in rank determination and joint training performance, validated through experiments on physics-informed neural networks for PDEs.

Conclusion: SubLoRA combines theoretical rigor, second-order accuracy, and computational efficiency, offering a superior alternative to first-order methods for LoRA rank determination.

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [97] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model achieving OpenAI o3's performance via SPRM, integrating policy and reward models efficiently. It supports test-time scaling and open-sourced for research.


<details>
  <summary>Details</summary>
Motivation: To create an efficient, unified model combining policy and process reward without extra annotations, reducing parameters and enabling scalable reasoning.

Method: Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, enabling efficient reasoning and test-time scaling.

Result: Achieves OpenAI-o3-mini performance with 32B parameters and establishes a scaling law for thinking computation.

Conclusion: MetaStone-S1 is a scalable, efficient model open-sourced for community use, matching high-performance benchmarks with fewer parameters.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [98] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: The paper proposes using natural language for intent and reasoning communication in multi-agent collaborative driving to overcome bandwidth, interoperability, and decision-level fusion limitations.


<details>
  <summary>Details</summary>
Motivation: Existing communication methods (sensor data, neural features, perception results) lack bandwidth efficiency, completeness, and interoperability, and ignore decision-level fusion.

Method: Transition from perception-oriented data exchanges to natural language for communicating intent and reasoning.

Result: Natural language improves semantic density, bandwidth flexibility, and interoperability, enabling proactive coordination.

Conclusion: Natural language enhances safety, efficiency, and transparency in collaborative driving by enabling direct communication of intentions and decisions.

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [99] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: The paper proposes RALLY, a Role-Adaptive LLM-Driven Yoked navigation algorithm, to enhance UAV swarm control by combining LLM semantic reasoning with MARL adaptability, improving task coverage and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL lacks semantic communication and role flexibility, while LLM-based frameworks struggle with exploration and adaptability. RALLY aims to bridge these gaps.

Method: RALLY integrates an LLM-driven semantic decision framework, dynamic role-heterogeneity, and a Role-value Mixing Network (RMIX) to merge offline priors with online MARL policies.

Result: Experiments show RALLY outperforms conventional methods in task coverage, convergence speed, and generalization.

Conclusion: RALLY demonstrates strong potential for collaborative navigation in multi-UAV systems by combining semantic reasoning and adaptive learning.

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [100] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Main category: cs.MA

TL;DR: Incorporating blackboard architecture into LLM multi-agent systems improves information sharing, dynamic agent selection, and consensus-building, achieving competitive performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-agent systems by enabling dynamic information sharing and agent selection, especially in scenarios lacking predefined workflows.

Method: Proposes a blackboard architecture for LLM multi-agent systems, where agents share information via a blackboard, and actions are selected based on blackboard content iteratively until consensus.

Result: Competitive with SOTA static and dynamic MASs, achieving best average performance with fewer tokens.

Conclusion: The blackboard architecture enables complex, dynamic problem-solving in unstructured scenarios.

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


### [101] [Distance-based Relative Orbital Transition for Satellite Swarm Array Deployment Under J2 Perturbation](https://arxiv.org/abs/2507.01769)
*Yuta Takahashi,Shin-ichiro Sakai*

Main category: cs.MA

TL;DR: Autonomous guidance for satellite swarms enables scalable distributed space structures using decentralized control and fuel-free actuation.


<details>
  <summary>Details</summary>
Motivation: To enable innovative science and business opportunities by developing scalable, decentralized satellite swarm formations.

Method: Derived averaged $J_2$ orbital parameters, designed a distance-based orbital stabilizer, and used fuel-free actuation (magnetic field interaction, differential aerodynamic forces).

Result: Achieved autonomous deployment into coplanar equidistant formations and minimized drift during communication outages.

Conclusion: Decentralized control and fuel-free actuation can maintain stable satellite swarm formations, addressing challenges like drift and communication loss.

Abstract: This paper presents an autonomous guidance and control strategy for a
satellite swarm that enables scalable distributed space structures for
innovative science and business opportunities. The averaged $J_2$ orbital
parameters that describe the drift and periodic orbital motion were derived
along with their target values to achieve a distributed space structure in a
decentralized manner. This enabled the design of a distance-based orbital
stabilizer to ensure autonomous deployment into a monolithic formation of a
coplanar equidistant configuration on a user-defined orbital plane. Continuous
formation control was assumed to be achieved through fuel-free actuation, such
as satellite magnetic field interaction and differential aerodynamic forces,
thereby maintaining long-term formation stability without thruster usage. A
major challenge for such actuation systems is the potential loss of control
capability due to increasing inter-satellite distances resulting from unstable
orbital dynamics, particularly for autonomous satellite swarms. To mitigate
this risk, our decentralized deployment controller minimized drift distance
during unexpected communication outages. As a case study, we consider the
deployment of palm-sized satellites into a coplanar equidistant formation in a
$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.

</details>
