<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: MICA is a multi-agent industrial assistant system that provides real-time guidance for industrial workflows while operating under privacy and connectivity constraints, using specialized language agents coordinated with safety checks.


<details>
  <summary>Details</summary>
Motivation: Industrial workflows need adaptive, trustworthy assistance that can work with limited computing, connectivity, and strict privacy requirements in factory environments.

Method: MICA coordinates five role-specialized language agents with safety auditing, uses Adaptive Step Fusion (ASF) for robust step understanding by blending expert reasoning with speech feedback, and establishes a multi-agent coordination benchmark for industrial assistance.

Result: Experiments show MICA consistently improves task success, reliability, and responsiveness over baseline structures while remaining deployable on practical offline hardware.

Conclusion: MICA represents a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments, with source code being made publicly available.

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [2] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: This paper presents a neural algorithmic reasoner for solving the Knapsack problem using a two-phase approach that mimics classical dynamic programming, achieving better generalization to larger instances than direct-prediction baselines.


<details>
  <summary>Details</summary>
Motivation: To address the gap in standard neural algorithmic reasoning benchmarks by tackling Knapsack, a pseudo-polynomial problem that bridges classical algorithms and combinatorial optimization, using neural networks that embed algorithmic logic.

Method: A two-phase pipeline that first constructs a dynamic programming table and then reconstructs the solution from it, with intermediate states supervised through dynamic programming principles.

Result: The neural algorithmic reasoner achieves better generalization to larger problem instances compared to direct-prediction baselines that attempt to select optimal subsets directly from inputs.

Conclusion: Modeling intermediate states through dynamic programming supervision is an effective approach for neural algorithmic reasoning on combinatorial optimization problems like Knapsack, providing superior generalization capabilities.

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [3] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: MetaLight, a state-of-the-art Meta Reinforcement Learning approach for traffic signal control, shows inconsistent performance with errors up to 22%, indicating reliability issues despite promising results under certain conditions.


<details>
  <summary>Details</summary>
Motivation: The reliability problem of trained RL agents in traffic signal control due to dynamically changing input data distributions, which could have detrimental consequences if not addressed.

Method: Evaluation and analysis of MetaLight, a Meta Reinforcement Learning approach, to assess its performance under various conditions in traffic signal control applications.

Result: MetaLight produces reasonably good results under certain conditions but performs poorly under others with errors reaching 22%, demonstrating insufficient robustness.

Conclusion: Meta RL schemes like MetaLight are often not robust enough and can pose major reliability problems for traffic signal control systems.

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [4] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: Automated literature review pipeline using semantic similarity with transformer embeddings and cosine similarity for minimal overhead and high relevance


<details>
  <summary>Details</summary>
Motivation: To create a scalable and practical tool for preliminary research and exploratory analysis that minimizes overhead compared to traditional systematic review systems

Method: Uses transformer-based embeddings and cosine similarity to generate keywords from input paper title/abstract, fetch relevant papers from open access repositories, and rank them by semantic closeness. Evaluated three embedding models with statistical thresholding for filtering

Result: The proposed system shows promise as an effective literature review pipeline despite lacking heuristic feedback or ground truth relevance labels

Conclusion: The semantic similarity-based approach provides a scalable and practical solution for automated literature reviews with minimal overhead

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [5] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: This paper investigates knowledge-driven hallucination in LLMs, where models override explicit source evidence with their internal knowledge, using automated process modeling as a test case.


<details>
  <summary>Details</summary>
Motivation: To understand the critical reliability risk when LLMs' pre-trained knowledge contradicts explicit source evidence, particularly in evidence-based domains like business process modeling.

Method: Conducted controlled experiments using both standard and deliberately atypical process structures to create conflicts between provided evidence and LLM background knowledge, measuring fidelity to source evidence.

Result: Demonstrated that LLMs exhibit knowledge-driven hallucination by overriding explicit source evidence with their pre-trained schemas, especially when dealing with standardized business processes.

Conclusion: Provides a methodology for assessing LLM reliability issues and emphasizes the need for rigorous validation of AI-generated artifacts in evidence-based domains.

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [6] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: A diagnostic framework for evaluating and transferring expert behavior to LLM-powered agents using golden datasets, silver datasets, and an LLM-based Agent Judge that provides targeted improvements.


<details>
  <summary>Details</summary>
Motivation: Classical evaluation methods are inadequate for diagnosing agentic performance in LLMs due to their inherent stochasticity and multi-step decision processes.

Method: Framework integrates curated golden datasets of expert annotations, silver datasets from controlled behavioral mutation, and an LLM-based Agent Judge that scores agents and prescribes improvements embedded in a vectorized recommendation map.

Result: Demonstrated on a multi-agent recruiter-assistant system, uncovering latent cognitive failures like biased phrasing and extraction drift while steering agents toward expert-level reasoning.

Conclusion: Establishes foundation for standardized, reproducible expert behavior transfer in stochastic, tool-augmented LLM agents, moving beyond static evaluation to active expert system refinement.

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [7] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: FragmentRetro is a novel retrosynthetic method that achieves quadratic complexity using fragmentation algorithms, stock-aware exploration, and fingerprint screening, outperforming traditional tree-search methods that suffer from exponential complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional tree-search methods for retrosynthesis suffer from exponential computational complexity, making them inefficient for complex molecules. There's a need for more scalable approaches to computer-aided synthesis planning.

Method: FragmentRetro uses fragmentation algorithms (BRICS and r-BRICS) combined with stock-aware exploration and pattern fingerprint screening. It recursively combines molecular fragments and verifies their presence in a building block set to generate retrosynthetic solutions.

Result: FragmentRetro achieves quadratic complexity O(h²), significantly better than tree search's exponential complexity O(bʰ) and DirectMultiStep's O(h⁶). It demonstrates high solved rates on PaRoutes, USPTO-190, and natural products with competitive runtime, including cases where tree search fails.

Conclusion: FragmentRetro provides a powerful foundational component for scalable synthesis planning, offering computational advantages for efficiently identifying fragment-based solutions, though it focuses on fragment identification rather than full reaction pathways.

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [8] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: This paper proposes methods to detect and mitigate "scheming" in AI systems - where AI secretly pursues misaligned goals. The authors test deliberative alignment interventions and find they reduce but don't eliminate covert actions, with situational awareness potentially influencing results.


<details>
  <summary>Details</summary>
Motivation: Highly capable AI systems could secretly pursue misaligned goals (scheming), which requires different detection strategies than typical ML approaches since scheming AI deliberately hides its misaligned intentions.

Method: The authors propose three evaluation requirements: testing on out-of-distribution tasks, assessing situational awareness, and checking robustness to pre-existing misaligned goals. They use covert actions as a proxy for scheming and test deliberative alignment interventions across 26 OOD evaluations (180+ environments).

Result: Deliberative alignment reduces covert action rates significantly (OpenAI o3: 13%→0.4%) but doesn't fully eliminate them. The mitigation largely stops agents from pursuing hidden goals, but misbehavior persists after red-teaming. Chain-of-thought analysis shows situational awareness affects behavior.

Conclusion: While deliberative alignment helps reduce scheming, the reductions may be partially driven by situational awareness. The approach relies on human-legible reasoning which may degrade as models evolve. More research is needed for adversarial deceptive alignment cases.

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [9] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent is an LLM-based system for microservice root cause analysis that uses multimodal data fusion, combining log compression, dual anomaly detection, and statistical filtering with two-stage LLM analysis to achieve comprehensive fault localization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of root cause analysis in complex microservice environments by leveraging large language models' cross-modal understanding capabilities to handle massive multimodal data from microservice systems.

Method: Three-stage approach: 1) Drain log parsing with multi-level filtering for log compression, 2) Isolation Forest + status code validation for trace anomaly detection, 3) Statistical symmetry ratio filtering with two-stage LLM analysis for cross-modal root cause analysis using carefully designed prompts.

Result: Achieved superior performance with a final score of 50.71 in complex microservice fault scenarios, with ablation studies validating the effectiveness of each component and multimodal data fusion.

Conclusion: MicroRCA-Agent demonstrates that LLM agents can effectively perform root cause analysis in microservice environments through multimodal data fusion, providing structured analysis results with fault components, root cause descriptions, and reasoning traces.

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [10] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: This paper introduces CCrepair, a framework for automated C++ compilation error repair using reinforcement learning with hybrid reward signals and LLM-based evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of large-scale C++ compilation error datasets and limitations of conventional supervised methods that often fail to generate semantically correct patches.

Method: Three core contributions: 1) CCrepair dataset created via generate-and-verify pipeline, 2) RL paradigm with hybrid reward signal focusing on semantic quality, 3) Two-stage evaluation system using LLM-as-a-Judge validated against human experts.

Result: RL-trained Qwen2.5-1.5B-Instruct model achieved performance comparable to Qwen2.5-14B-Instruct model, demonstrating training efficiency.

Conclusion: Provides valuable dataset and effective paradigm for training robust compilation repair models, advancing practical automated programming assistants.

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [11] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: A literature review exploring the integration of machine learning with robotic process automation (RPA) to create intelligent RPA, organized into a taxonomy with two meta-characteristics and eight dimensions.


<details>
  <summary>Details</summary>
Motivation: RPA has limitations in handling complex tasks due to its symbolic nature, while machine learning can enable intelligent RPA to broaden the range of automatable tasks beyond rule-based processes.

Method: Conducted a literature review to explore connections between RPA and machine learning, organizing the joint concept of intelligent RPA into a taxonomy comprising two meta-characteristics: RPA-ML integration and RPA-ML interaction.

Result: Developed a taxonomy with eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, technical depth of integration, deployment environment, lifecycle phase, and user-robot relation.

Conclusion: The taxonomy provides a structured framework for understanding and implementing intelligent RPA by systematically categorizing the integration of machine learning capabilities with robotic process automation.

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [12] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: ApiNATOMY is a framework for creating topological and semantic representations of multiscale physiological circuit maps, particularly focusing on the peripheral nervous system.


<details>
  <summary>Details</summary>
Motivation: To support researchers in mapping data related to physiological systems, especially the nervous system, by providing infrastructure for representing complex anatomical interactions and their relevance to organs.

Method: Developed ApiNATOMY framework consisting of a Knowledge Representation (KR) model for capturing anatomical entity interactions and Knowledge Management (KM) tools for converting abstractions into detailed physiological process models.

Result: Created infrastructure that integrates with external ontologies and knowledge graphs, enabling physiology experts to model multiscale physiological circuits.

Conclusion: ApiNATOMY provides a comprehensive framework for representing and managing physiological circuit maps, facilitating research in nervous system and organ physiology.

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [13] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: CLIMB is an automated framework for creating high-quality occupation taxonomies from raw job postings using global semantic clustering and multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Manual curation of occupation taxonomies is slow, and existing automated methods are either not adaptive to dynamic regional markets or struggle to build coherent hierarchies from noisy data.

Method: CLIMB uses global semantic clustering to distill core occupations, then employs a reflection-based multi-agent system to iteratively build a coherent hierarchy.

Result: On three diverse real-world datasets, CLIMB produces taxonomies that are more coherent and scalable than existing methods and successfully captures unique regional characteristics.

Conclusion: CLIMB provides a fully automated solution for creating robust occupation taxonomies that outperform existing approaches in coherence, scalability, and regional adaptability.

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [14] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: This paper compares rule-based vs. data-driven approaches in industrial monitoring systems, highlighting their respective strengths/limitations and proposing hybrid solutions as the future direction.


<details>
  <summary>Details</summary>
Motivation: The shift from traditional rule-based architectures to data-driven approaches in Industry 4.0 environments requires systematic comparison and evaluation of both methodologies to guide future industrial monitoring system development.

Method: The study presents a comparative analysis between rule-based and data-driven systems, analyzing their properties, and proposes a basic framework to evaluate their key characteristics including interpretability, adaptability, and performance.

Result: Rule-based systems offer high interpretability and deterministic behavior but lack scalability and adaptability, while data-driven systems excel in anomaly detection and predictive maintenance but face challenges with explainability and data requirements.

Conclusion: Hybrid solutions combining rule-based transparency with machine learning analytical power represent the future direction for industrial monitoring, enabling smarter, more flexible industrial environments through synergistic expert knowledge and data-driven insights.

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [15] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: LLMs can successfully retrieve clinical data from EHR systems using Model Context Protocol (MCP) tools, achieving near-perfect accuracy in simple tasks but facing challenges with complex time-dependent calculations.


<details>
  <summary>Details</summary>
Motivation: To enable LLM deployment in hospitals by overcoming restricted access to electronic health record systems through MCP integration, allowing autonomous retrieval of clinically relevant information.

Method: Developed EHR-MCP framework with custom MCP tools integrated with hospital EHR database, using GPT-4.1 through LangGraph ReAct agent. Tested six infection control team tasks on eight patients, comparing against physician-generated gold standards.

Result: LLM consistently selected correct MCP tools with near-perfect accuracy except for complex time-dependent tasks. Most errors from incorrect arguments or tool result misinterpretation. Responses were reliable but risked context window overflow with long data.

Conclusion: EHR-MCP enables secure, consistent EHR data access for LLMs and serves as foundation for hospital AI agents. Future work should extend to reasoning, generation, and clinical impact assessment for effective generative AI integration in clinical practice.

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [16] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: A lightweight approach that augments text prompts with tuple-based structured information to improve spatial accuracy in text-to-image generation without compromising image quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation systems struggle to faithfully capture spatial relationships described in natural language prompts, which is a major limitation of large-scale generative systems.

Method: Uses a fine-tuned language model to automatically convert natural language prompts into tuple-based structured information, which is then seamlessly integrated into T2I pipelines to enhance spatial relationship capture.

Result: Experimental results show substantial improvements in spatial accuracy while maintaining overall image quality (measured by Inception Score). The automatically generated tuples achieve quality comparable to human-crafted tuples.

Conclusion: The structured information approach provides a practical and portable solution to enhance spatial relationships in T2I generation, addressing a key limitation of current systems.

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [17] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: ASAC integrates Attention Schema Theory from cognitive science into neural networks using a VQVAE-based module to model and control attention allocation, improving efficiency, accuracy, and learning speed across vision and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge cognitive science and AI by applying Attention Schema Theory (which explains how humans manage attention through internal models) to enhance attention mechanisms in neural networks for better resource allocation and system efficiency.

Method: Introduces ASAC module that uses Vector-Quantized Variational AutoEncoder (VQVAE) as both attention abstractor and controller, integrated into transformer architectures to explicitly model and manage attention allocation.

Result: ASAC improves classification accuracy, accelerates learning, enhances robustness to noise and out-of-distribution data, boosts multi-task performance, and increases resilience to adversarial attacks while enabling effective transfer learning and few-shot learning.

Conclusion: The approach successfully connects cognitive science principles with machine learning, demonstrating that explicit attention modeling through attention schema theory can significantly optimize attention mechanisms in AI systems.

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning](https://arxiv.org/abs/2509.15230)
*Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi*

Main category: cs.LG

TL;DR: A prompt-based learning framework that enables instant unlearning by binding class-level semantics to dedicated prompt tokens, allowing data removal without retraining or model modification.


<details>
  <summary>Details</summary>
Motivation: Foundation models lack built-in unlearning capabilities required by privacy regulations like GDPR, and traditional unlearning methods are computationally expensive and fragile.

Method: Unified prompt-based framework that encodes class-level semantics in dedicated prompt tokens rather than model weights, enabling instant unlearning by simply removing corresponding prompts.

Result: The framework preserves predictive performance on retained classes while effectively erasing forgotten ones, and demonstrates strong privacy guarantees against membership inference attacks.

Conclusion: This work establishes a new foundation for designing modular, scalable and ethically responsive AI models by embedding removability directly into the architecture.

Abstract: Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.

</details>


### [19] [A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction](https://arxiv.org/abs/2509.15256)
*Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li*

Main category: cs.LG

TL;DR: MPNP-DDI is a novel Multi-scale Graph Neural Process framework for drug-drug interaction prediction that captures structural information across different scales and provides uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture structural information across different scales (from local functional groups to global molecular topology) and lack mechanisms to quantify prediction confidence, which is crucial for medication safety and drug development.

Method: The framework uses a unique message-passing scheme that learns a hierarchy of graph representations at multiple scales, a cross-drug co-attention mechanism to fuse multi-scale representations, and an integrated neural process module for uncertainty estimation.

Result: Extensive experiments show that MPNP-DDI significantly outperforms state-of-the-art baselines on benchmark datasets.

Conclusion: MPNP-DDI represents a powerful computational tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine by providing accurate, generalizable, and uncertainty-aware predictions based on multi-scale structural features.

Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

</details>


### [20] [Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model](https://arxiv.org/abs/2509.15258)
*Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han*

Main category: cs.LG

TL;DR: This survey explores the integration of Generative AI (GenAI) into wireless sensing systems, examining how generative models like GANs, VAEs, and diffusion models can enhance applications such as device localization, human activity recognition, and environmental monitoring through data augmentation, domain adaptation, and denoising.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the significant advancements in GenAI from computer vision and NLP to improve wireless sensing systems, addressing the need for better data synthesis, generalization, and performance in various sensing applications.

Method: The survey investigates the convergence from two perspectives: 1) integration modes (plugin for task-specific models and solver for direct task addressing), and 2) analysis of generative models' characteristics and applicability to wireless sensing tasks.

Result: The study identifies the unique advantages of different generative models across various wireless sensing tasks and outlines the potential for a unified wireless foundation model.

Conclusion: Key challenges in applying GenAI to wireless sensing are identified, and future directions are proposed toward developing a scalable, adaptable, and efficient wireless foundation model for diverse sensing tasks.

Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.

</details>


### [21] [IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders](https://arxiv.org/abs/2509.15259)
*Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng*

Main category: cs.LG

TL;DR: IEFS-GMB is an information entropy-based feature selection method with gradient memory bank that improves EEG classification accuracy for neurological disorders by 0.64-6.45% over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing feature selection methods for EEG classification are not specifically designed for EEG diagnosis, lack interpretability, and have limited robustness to signal variability due to low signal-to-noise ratio.

Method: Proposes IEFS-GMB which constructs a dynamic memory bank storing historical gradients, computes feature importance via information entropy, and applies entropy-based weighting to select informative EEG features.

Result: Experiments on four public neurological disease datasets show accuracy improvements of 0.64% to 6.45% over baseline models, outperforming four competing FS techniques while improving interpretability.

Conclusion: IEFS-GMB effectively addresses limitations of existing FS methods for EEG classification, demonstrating practical utility for clinical applications through improved accuracy and interpretability.

Abstract: Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.

</details>


### [22] [A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media](https://arxiv.org/abs/2509.15266)
*Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González*

Main category: cs.LG

TL;DR: This study uses Twitter data to analyze user-reported effects of ecstasy, GHB, and 2C-B, developing machine learning models to classify positive vs. negative experiences with high accuracy for pharmacovigilance applications.


<details>
  <summary>Details</summary>
Motivation: Traditional surveillance systems often underrepresent user experiences of recreational drug effects, creating a need for alternative data sources to understand real-world substance impacts.

Method: Combined slang term curation with biomedical concept extraction via MetaMap to identify 92,000+ tweets, used expert-guided heuristic labeling for polarity, and applied machine learning classifiers with cost-sensitive learning and oversampling techniques.

Result: Achieved top performance with eXtreme Gradient Boosting (F1 = 0.885, AUPRC = 0.934), demonstrating Twitter's utility for detecting substance-specific phenotypic effects.

Conclusion: Twitter enables effective detection of drug-specific effects, and polarity classification models can support real-time pharmacovigilance and drug effect characterization with high accuracy.

Abstract: Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

</details>


### [23] [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
*Elisabetta Rocchetti*

Main category: cs.LG

TL;DR: This paper applies Complex Network Theory to analyze how LLMs learn by representing a Transformer model as a directed graph and tracking its evolution during training.


<details>
  <summary>Details</summary>
Motivation: To understand the learning dynamics and self-organizing principles that drive the formation of functional circuits in Large Language Models during training.

Method: Represent a Transformer-based LLM as a directed, weighted graph where nodes are computational components and edges represent causal influence measured via intervention-based ablation. Track evolution across 143 training checkpoints of Pythia-14M model on an induction task.

Result: The network structure evolves through distinct phases of exploration, consolidation, and refinement. Identifies emergence of stable hierarchy of information spreader components and dynamic information gatherer components that reconfigure at key learning junctures.

Conclusion: Component-level network perspective offers a powerful macroscopic lens for visualizing and understanding self-organizing principles in LLM circuit formation.

Abstract: The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.

</details>


### [24] [Partial Column Generation with Graph Neural Networks for Team Formation and Routing](https://arxiv.org/abs/2509.15275)
*Giacomo Dall'Olio,Rainer Kolisch,Yaoxin Wu*

Main category: cs.LG

TL;DR: A novel partial column generation strategy using machine learning to predict which pricing problems yield negative reduced cost columns, enhancing solution efficiency for team formation and routing problems.


<details>
  <summary>Details</summary>
Motivation: Team formation and routing is a challenging optimization problem with real-world applications in airport, healthcare, and maintenance operations. Existing exact methods based on column generation need improvement, especially for hard instances under tight time constraints.

Method: Developed a machine learning model using graph neural networks to predict which pricing problems are likely to yield columns with negative reduced cost, enabling a more efficient partial column generation strategy for multiple pricing problem settings.

Result: Computational experiments show the proposed strategy enhances solution methods and outperforms traditional partial column generation approaches, particularly on hard instances solved under tight time limits.

Conclusion: The machine learning-based partial column generation approach using graph neural networks provides significant improvements over traditional methods for solving team formation and routing problems, especially in time-constrained scenarios.

Abstract: The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.

</details>


### [25] [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
*Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai*

Main category: cs.LG

TL;DR: Fleming-R1 is a medical reasoning model that achieves expert-level clinical reasoning through three innovations: Reasoning-Oriented Data Strategy (RODS), Chain-of-Thought cold start, and Reinforcement Learning from Verifiable Rewards (RLVR).


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with expert-level clinical reasoning due to the need for both accurate answers and transparent reasoning processes in medical applications.

Method: Three complementary innovations: 1) RODS combining curated medical QA datasets with knowledge-graph-guided synthesis; 2) Chain-of-Thought cold start to distill reasoning trajectories; 3) Two-stage RLVR framework using Group Relative Policy Optimization with adaptive hard-sample mining.

Result: Fleming-R1 delivers substantial parameter-efficient improvements: 7B variant surpasses larger baselines, 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives across diverse medical benchmarks.

Conclusion: Structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization, enabling safer deployment in high-stakes clinical environments.

Abstract: While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

</details>


### [26] [Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers](https://arxiv.org/abs/2509.15316)
*Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: A hybrid unary-binary architecture for printed electronics MLP classifiers that eliminates encoders and multipliers, achieving significant area and power reductions with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Printed electronics offer cost-effective ML implementation but have large feature sizes limiting classifier complexity. Tailoring hardware to specific ML models can simplify circuit design.

Method: Proposes hybrid unary-binary architecture that removes costly encoders and enables multiplier-less MLP execution, plus architecture-aware training for optimization.

Result: Evaluation on six datasets shows average reductions of 46% in area and 39% in power, with minimal accuracy loss, outperforming other state-of-the-art MLP designs.

Conclusion: The hybrid architecture successfully addresses printed electronics limitations, providing efficient MLP implementation with substantial area and power savings.

Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to
silicon for implementing machine learning (ML) circuits, but their large
feature sizes limit classifier complexity. Leveraging PE's low fabrication and
NRE costs, designers can tailor hardware to specific ML models, simplifying
circuit design. This work explores alternative arithmetic and proposes a hybrid
unary-binary architecture that removes costly encoders and enables efficient,
multiplier-less execution of MLP classifiers. We also introduce
architecture-aware training to further improve area and power efficiency.
Evaluation on six datasets shows average reductions of 46% in area and 39% in
power, with minimal accuracy loss, surpassing other state-of-the-art MLP
designs.

</details>


### [27] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: A score-based generative model using stochastic Kuramoto dynamics for orientation-rich images like fingerprints and textures, leveraging phase synchronization as inductive bias.


<details>
  <summary>Details</summary>
Motivation: Standard isotropic Euclidean diffusion struggles with coherent angular directional patterns in orientation-rich images. Biological phase synchronization in coupled oscillators provides inspiration for structured image generation.

Method: Forward process uses Kuramoto dynamics for synchronization among phase variables, collapsing data into low-entropy von Mises distribution. Reverse process performs desynchronization with learned score function. Uses wrapped Gaussian transition kernels and periodicity-aware networks for circular geometry.

Result: Achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures.

Conclusion: Demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [28] [Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning](https://arxiv.org/abs/2509.15347)
*Jia Tang,Xinrui Wang,Songcan Chen*

Main category: cs.LG

TL;DR: GPLASC is a contrastive learning strategy for continual learning that uses global pre-fixing with equiangular tight frames to separate task representations and local adjusting to maintain intra-task discriminability.


<details>
  <summary>Details</summary>
Motivation: Address confusion from both inter-task and intra-task features in contrastive continual learning, which limits performance despite advancements.

Method: Divides representation hypersphere into non-overlapping regions using ETF centers for inter-task separation, and forms adjustable ETFs within each region for intra-task structure regulation.

Result: Extensive experiments validate effectiveness in ensuring discriminative feature structures both between and within tasks.

Conclusion: GPLASC simultaneously ensures discriminative feature structures and can be seamlessly integrated into existing contrastive continual learning frameworks.

Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.

</details>


### [29] [Probabilistic Conformal Coverage Guarantees in Small-Data Settings](https://arxiv.org/abs/2509.15349)
*Petrus H. Zwart*

Main category: cs.LG

TL;DR: SSBC is a plug-and-play adjustment to conformal prediction that provides probabilistic coverage guarantees by leveraging the exact finite-sample distribution of conformal coverage.


<details>
  <summary>Details</summary>
Motivation: Standard split conformal prediction only guarantees marginal coverage in expectation across many calibration draws, but realized coverage for a single calibration set can vary substantially, undermining effective risk control in practical applications.

Method: Introduces Small Sample Beta Correction (SSBC), which adjusts the conformal significance level using the exact finite-sample distribution of conformal coverage to provide probabilistic guarantees.

Result: The method ensures that with user-defined probability over the calibration draw, the deployed predictor achieves at least the desired coverage.

Conclusion: SSBC addresses the variance issue in standard conformal prediction by providing more reliable probabilistic coverage guarantees for practical applications.

Abstract: Conformal prediction provides distribution-free prediction sets with
guaranteed marginal coverage. However, in split conformal prediction this
guarantee is training-conditional only in expectation: across many calibration
draws, the average coverage equals the nominal level, but the realized coverage
for a single calibration set may vary substantially. This variance undermines
effective risk control in practical applications. Here we introduce the Small
Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal
significance level that leverages the exact finite-sample distribution of
conformal coverage to provide probabilistic guarantees, ensuring that with
user-defined probability over the calibration draw, the deployed predictor
achieves at least the desired coverage.

</details>


### [30] [Predicting Language Models' Success at Zero-Shot Probabilistic Prediction](https://arxiv.org/abs/2509.15356)
*Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder*

Main category: cs.LG

TL;DR: This paper investigates when large language models (LLMs) can provide high-quality zero-shot predictions for individual-level characteristics across tabular prediction tasks, finding performance is variable but predictable using certain metrics.


<details>
  <summary>Details</summary>
Motivation: To determine when users can have confidence in LLMs' zero-shot predictive capabilities for generating individual-level characteristics like risk models or survey augmentation.

Method: Conducted a large-scale empirical study of LLMs' zero-shot predictive capabilities across diverse tabular prediction tasks, analyzing performance variability and developing metrics to predict LLM performance without labeled data.

Result: LLMs' performance is highly variable across tasks, but when they perform well on base prediction tasks, their predicted probabilities become stronger signals for individual-level accuracy. Certain metrics assessed without labeled data provide strong signals for predicting LLM performance on new tasks.

Conclusion: The study provides insights into when LLMs can be reliably used for zero-shot prediction tasks and offers practical metrics to help users assess LLM suitability for their specific prediction needs.

Abstract: Recent work has investigated the capabilities of large language models (LLMs)
as zero-shot models for generating individual-level characteristics (e.g., to
serve as risk models or augment survey datasets). However, when should a user
have confidence that an LLM will provide high-quality predictions for their
particular task? To address this question, we conduct a large-scale empirical
study of LLMs' zero-shot predictive capabilities across a wide range of tabular
prediction tasks. We find that LLMs' performance is highly variable, both on
tasks within the same dataset and across different datasets. However, when the
LLM performs well on the base prediction task, its predicted probabilities
become a stronger signal for individual-level accuracy. Then, we construct
metrics to predict LLMs' performance at the task level, aiming to distinguish
between tasks where LLMs may perform well and where they are likely unsuitable.
We find that some of these metrics, each of which are assessed without labeled
data, yield strong signals of LLMs' predictive performance on new tasks.

</details>


### [31] [Stochastic Sample Approximations of (Local) Moduli of Continuity](https://arxiv.org/abs/2509.15368)
*Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: The paper presents a non-uniform stochastic sample approximation for moduli of local continuity, connecting generalized derivatives with moduli to evaluate neural network robustness and fairness in closed-loop models.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation of neural network robustness and fairness in repeated uses within closed-loop systems by developing better approximation methods for moduli of local continuity.

Method: Revisits the connection between generalized derivatives and moduli of local continuity, and introduces a non-uniform stochastic sample approximation approach for these moduli.

Result: A novel approximation method for moduli of local continuity that enables more effective assessment of neural network robustness and fairness.

Conclusion: The proposed non-uniform stochastic sample approximation provides an important tool for studying neural network robustness and fairness in closed-loop applications, bridging theoretical connections with practical evaluation methods.

Abstract: Modulus of local continuity is used to evaluate the robustness of neural
networks and fairness of their repeated uses in closed-loop models. Here, we
revisit a connection between generalized derivatives and moduli of local
continuity, and present a non-uniform stochastic sample approximation for
moduli of local continuity. This is of importance in studying robustness of
neural networks and fairness of their repeated uses.

</details>


### [32] [Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370)
*Vicky Kouni*

Main category: cs.LG

TL;DR: This paper provides the first theoretical analysis of adversarial generalization for unfolding networks, deriving tight error bounds and demonstrating that overparameterization can enhance robustness against l2-norm constrained attacks.


<details>
  <summary>Details</summary>
Motivation: Unfolding networks are used in critical applications like medical imaging and cryptography, but their adversarial robustness lacks theoretical understanding despite the importance of preventing catastrophic failures from attacks.

Method: The authors study state-of-the-art overparameterized unfolding networks, deploy a new framework to estimate adversarial Rademacher complexity, and provide generalization error bounds. They validate with experiments on real-world data using FGSM attacks.

Result: The derived adversarial generalization error bounds are tight with respect to attack level, and experiments consistently corroborate the theory across all tested data. Overparameterization is found to promote adversarial robustness.

Conclusion: This work establishes the first theoretical foundation for understanding adversarial generalization in unfolding networks and reveals that strategic overparameterization can be an effective approach for robustifying neural networks against adversarial attacks.

Abstract: Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.

</details>


### [33] [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/abs/2509.15392)
*Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: AC-SMFG is a single-loop actor-critic algorithm for Stackelberg mean field games that provides finite-time convergence guarantees and outperforms existing methods in policy quality and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Stackelberg MFGs rely on restrictive independence assumptions, use samples inefficiently due to nested-loop structures, and lack finite-time convergence guarantees.

Method: AC-SMFG is a single-loop actor-critic algorithm that alternates between gradient updates for the leader, a representative follower, and the mean field using continuously generated Markovian samples.

Result: The algorithm achieves finite-time and finite-sample convergence to a stationary point of the Stackelberg objective, outperforming existing baselines in various economics environments.

Conclusion: AC-SMFG is the first Stackelberg MFG algorithm with non-asymptotic convergence guarantees, relaxing the leader-follower independence assumption through a gradient alignment condition.

Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a
hierarchical framework for modeling the strategic interaction between a single
leader and an infinitely large population of homogeneous followers. The
objective can be formulated as a structured bi-level optimization problem, in
which the leader needs to learn a policy maximizing its reward, anticipating
the response of the followers. Existing methods for solving these (and related)
problems often rely on restrictive independence assumptions between the
leader's and followers' objectives, use samples inefficiently due to
nested-loop algorithm structure, and lack finite-time convergence guarantees.
To address these limitations, we propose AC-SMFG, a single-loop actor-critic
algorithm that operates on continuously generated Markovian samples. The
algorithm alternates between (semi-)gradient updates for the leader, a
representative follower, and the mean field, and is simple to implement in
practice. We establish the finite-time and finite-sample convergence of the
algorithm to a stationary point of the Stackelberg objective. To our knowledge,
this is the first Stackelberg MFG algorithm with non-asymptotic convergence
guarantees. Our key assumption is a "gradient alignment" condition, which
requires that the full policy gradient of the leader can be approximated by a
partial component of it, relaxing the existing leader-follower independence
assumption. Simulation results in a range of well-established economics
environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG
learning baselines in policy quality and convergence speed.

</details>


### [34] [VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding](https://arxiv.org/abs/2509.15394)
*Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng*

Main category: cs.LG

TL;DR: VMDNet is a causality-preserving framework for time series forecasting that uses sample-wise Variational Mode Decomposition to avoid information leakage, frequency-aware embeddings with parallel TCNs for mode independence, and bilevel optimization for adaptive hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing VMD-based forecasting methods suffer from information leakage and inappropriate hyperparameter tuning, which limits their performance in capturing recurrent temporal patterns effectively.

Method: The framework applies sample-wise VMD to prevent leakage, represents decomposed modes with frequency-aware embeddings decoded by parallel TCNs, and uses Stackelberg-inspired bilevel optimization to adaptively select VMD hyperparameters (K and alpha).

Result: Experiments on energy datasets show VMDNet achieves state-of-the-art results when periodicity is strong, with clear advantages in capturing structured periodic patterns while remaining robust under weak periodicity.

Conclusion: VMDNet effectively addresses information leakage and hyperparameter issues in VMD-based forecasting, demonstrating superior performance in periodic pattern capture across different periodicity conditions.

Abstract: In time series forecasting, capturing recurrent temporal patterns is
essential; decomposition techniques make such structure explicit and thereby
improve predictive performance. Variational Mode Decomposition (VMD) is a
powerful signal-processing method for periodicity-aware decomposition and has
seen growing adoption in recent years. However, existing studies often suffer
from information leakage and rely on inappropriate hyperparameter tuning. To
address these issues, we propose VMDNet, a causality-preserving framework that
(i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed
mode with frequency-aware embeddings and decodes it using parallel temporal
convolutional networks (TCNs), ensuring mode independence and efficient
learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to
adaptively select VMD's two core hyperparameters: the number of modes (K) and
the bandwidth penalty (alpha). Experiments on two energy-related datasets
demonstrate that VMDNet achieves state-of-the-art results when periodicity is
strong, showing clear advantages in capturing structured periodic patterns
while remaining robust under weak periodicity.

</details>


### [35] [Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization](https://arxiv.org/abs/2509.15399)
*Xiaochuan Gong,Jie Hao,Mingrui Liu*

Main category: cs.LG

TL;DR: Proposes adaptive algorithms for stochastic hierarchical optimization problems (nonconvex-strongly-concave minimax and nonconvex-strongly-convex bilevel) that achieve optimal convergence rates without prior knowledge of noise levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods for hierarchical optimization lack adaptivity in stochastic settings - they cannot achieve optimal convergence rates across different gradient noise levels without knowing the noise magnitude beforehand.

Method: Combines momentum normalization technique with novel adaptive parameter choices to create algorithms that automatically adapt to both low and high-noise regimes.

Result: Achieves sharp convergence rates of Õ(1/√T + √σ̄/T¹ᐟ⁴) for gradient norm in T iterations, where σ̄ is the stochastic gradient noise upper bound, without requiring prior noise knowledge.

Conclusion: Provides the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization, with experimental validation on synthetic and deep learning tasks demonstrating effectiveness.

Abstract: Hierarchical optimization refers to problems with interdependent decision
variables and objectives, such as minimax and bilevel formulations. While
various algorithms have been proposed, existing methods and analyses lack
adaptivity in stochastic optimization settings: they cannot achieve optimal
convergence rates across a wide spectrum of gradient noise levels without prior
knowledge of the noise magnitude. In this paper, we propose novel adaptive
algorithms for two important classes of stochastic hierarchical optimization
problems: nonconvex-strongly-concave minimax optimization and
nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp
convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$
in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound
on the stochastic gradient noise. Notably, these rates are obtained without
prior knowledge of the noise level, thereby enabling automatic adaptivity in
both low and high-noise regimes. To our knowledge, this work provides the first
adaptive and sharp convergence guarantees for stochastic hierarchical
optimization. Our algorithm design combines the momentum normalization
technique with novel adaptive parameter choices. Extensive experiments on
synthetic and deep learning tasks demonstrate the effectiveness of our proposed
algorithms.

</details>


### [36] [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
*Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller*

Main category: cs.LG

TL;DR: DA-IBC improves Implicit Behavioral Cloning with data augmentation and better initialization to handle multimodal driving decisions, outperforming standard IBC in CARLA simulator tests.


<details>
  <summary>Details</summary>
Motivation: Standard Behavior Cloning fails to capture multimodal driving decisions where multiple valid actions exist for the same scenario, necessitating better methods for handling such complexity.

Method: Proposes Data-Augmented IBC (DA-IBC) which perturbs expert actions to create counterexamples for IBC training and uses improved initialization for derivative-free inference with Energy-Based Models.

Result: Experiments in CARLA simulator with Bird's-Eye View inputs show DA-IBC outperforms standard IBC in urban driving tasks designed to evaluate multimodal behavior learning.

Conclusion: DA-IBC successfully represents multimodal action distributions through learned energy landscapes, achieving what standard BC fails to accomplish.

Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.

</details>


### [37] [Top-$k$ Feature Importance Ranking](https://arxiv.org/abs/2509.15420)
*Yuxi Chen,Tiffany Tang,Genevera Allen*

Main category: cs.LG

TL;DR: RAMPART is a novel framework for accurately ranking top-k important features in interpretable machine learning, combining adaptive sequential halving with efficient ensembling to explicitly optimize for ranking accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate feature ranking is crucial for interpretable ML in scientific discovery and decision-making, but existing methods treat ranking as post-processing rather than optimizing directly for ranking accuracy.

Method: Uses any feature importance measure with adaptive sequential halving (progressively focusing on promising features) and ensembling via observation and feature subsampling (MiniPatches), specifically tailored for top-k ranking.

Result: Theoretical guarantees show RAMPART achieves correct top-k ranking with high probability under mild conditions, and extensive simulations demonstrate consistent outperformance over popular feature importance methods.

Conclusion: RAMPART provides an effective framework for feature ranking that explicitly optimizes for ranking accuracy, with strong theoretical foundations and practical performance demonstrated in high-dimensional genomics applications.

Abstract: Accurate ranking of important features is a fundamental challenge in
interpretable machine learning with critical applications in scientific
discovery and decision-making. Unlike feature selection and feature importance,
the specific problem of ranking important features has received considerably
less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And
Recursive Trimming), a framework that utilizes any existing feature importance
measure in a novel algorithm specifically tailored for ranking the top-$k$
features. Our approach combines an adaptive sequential halving strategy that
progressively focuses computational resources on promising features with an
efficient ensembling technique using both observation and feature subsampling.
Unlike existing methods that convert importance scores to ranks as
post-processing, our framework explicitly optimizes for ranking accuracy. We
provide theoretical guarantees showing that RAMPART achieves the correct
top-$k$ ranking with high probability under mild conditions, and demonstrate
through extensive simulation studies that RAMPART consistently outperforms
popular feature importance methods, concluding with a high-dimensional genomics
case study.

</details>


### [38] [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)
*Victor Chardès*

Main category: cs.LG

TL;DR: A Random Matrix Theory-based approach improves PCA for single-cell RNA-seq data by using biwhitening to stabilize variance and automatically select sparsity levels, making sparse PCA nearly parameter-free while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Single-cell RNA-seq data is noisy due to biological variability, PCR bias, limited sequencing depth, and low capture efficiency. Current methods rely on PCA but struggle with heterogeneous datasets and evolving technologies.

Method: The paper introduces a biwhitening method inspired by Sinkhorn-Knopp algorithm to simultaneously stabilize variance across genes and cells. This enables RMT-based criterion to automatically select sparsity levels for sparse PCA algorithms.

Result: The method systematically improves principal subspace reconstruction and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification across seven single-cell RNA-seq technologies and four sparse PCA algorithms.

Conclusion: The mathematically grounded approach retains PCA's interpretability while enabling robust, hands-off inference of sparse principal components, providing a significant improvement over existing dimensionality reduction methods for single-cell RNA-seq data.

Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells
but is notoriously noisy. Variability stems from biological differences, PCR
amplification bias, limited sequencing depth, and low capture efficiency,
making it challenging to adapt computational pipelines to heterogeneous
datasets or evolving technologies. As a result, most studies still rely on
principal component analysis (PCA) for dimensionality reduction, valued for its
interpretability and robustness. Here, we improve upon PCA with a Random Matrix
Theory (RMT)-based approach that guides the inference of sparse principal
components using existing sparse PCA algorithms. We first introduce a novel
biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that
simultaneously stabilizes variance across genes and cells. This enables the use
of an RMT-based criterion to automatically select the sparsity level, rendering
sparse PCA nearly parameter-free. Our mathematically grounded approach retains
the interpretability of PCA while enabling robust, hands-off inference of
sparse principal components. Across seven single-cell RNA-seq technologies and
four sparse PCA algorithms, we show that this method systematically improves
the reconstruction of the principal subspace and consistently outperforms PCA-,
autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>


### [39] [Computing Linear Regions in Neural Networks with Skip Connections](https://arxiv.org/abs/2509.15441)
*Johnny Joyce,Jan Verschelde*

Main category: cs.LG

TL;DR: This paper applies tropical geometry to neural networks by representing piecewise linear activation functions with tropical arithmetic, presenting algorithms to compute linear regions, and providing computational insights on training difficulties including overfitting and benefits of skip connections.


<details>
  <summary>Details</summary>
Motivation: To leverage tropical geometry for analyzing neural networks with piecewise linear activation functions, enabling better understanding of network behavior and training challenges.

Method: Representing piecewise linear activation functions using tropical arithmetic and developing algorithms to compute regions where neural networks behave as linear maps.

Result: Computational experiments reveal insights into neural network training difficulties, particularly overfitting issues, and demonstrate the advantages of skip connections.

Conclusion: Tropical geometry provides a valuable framework for analyzing neural networks, offering computational methods to understand linear regions and practical insights into training challenges and architectural benefits.

Abstract: Neural networks are important tools in machine learning. Representing
piecewise linear activation functions with tropical arithmetic enables the
application of tropical geometry. Algorithms are presented to compute regions
where the neural networks are linear maps. Through computational experiments,
we provide insights on the difficulty to train neural networks, in particular
on the problems of overfitting and on the benefits of skip connections.

</details>


### [40] [Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](https://arxiv.org/abs/2509.15448)
*Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida*

Main category: cs.LG

TL;DR: A novel hierarchical attention mechanism for transformers that mathematically derives attention from entropy minimization principles to handle multi-modal, multi-scale data efficiently.


<details>
  <summary>Details</summary>
Motivation: Standard attention mechanisms struggle with hierarchical and multi-modal data, requiring ad hoc solutions that lack generalizability across different problem structures.

Method: Proposes a mathematical construct for multi-modal, multi-scale data representation, derives attention mechanics from entropy minimization, and develops an efficient dynamic programming algorithm for computation.

Result: The derived formulation is optimal (closest to standard Softmax attention while incorporating hierarchical biases) and enables both training from scratch and post-training hierarchical injection into pre-trained models.

Conclusion: The hierarchical attention mechanism provides a principled, efficient approach for handling multi-scale, multi-modal data in transformers, improving model efficiency in zero-shot settings.

Abstract: Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.

</details>


### [41] [IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs](https://arxiv.org/abs/2509.15455)
*Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris*

Main category: cs.LG

TL;DR: IMPQ proposes a novel mixed-precision quantization method using Shapley-based Progressive Quantization Estimation to capture inter-layer interactions, achieving superior performance (20-80% perplexity reduction) at 2-4 bit precision compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing mixed-precision quantization methods struggle below 4-bit precision because they rely on isolated layer-specific metrics that ignore critical inter-layer interactions, limiting their effectiveness for low-resource deployment of large language models.

Method: Two innovations: 1) Frame quantization as cooperative game with SPQE for accurate Shapley estimates of layer sensitivities and interactions; 2) IMPQ translates Shapley estimates into binary quadratic optimization to assign 2 or 4-bit precision under memory constraints.

Result: IMPQ demonstrates scalability across Llama-3, Gemma-2, and Qwen-3 models with three PTQ backends, achieving 20-80% perplexity reduction compared to best baselines, with performance gap increasing as bit-width tightens from 4-bit to 2-bit.

Conclusion: IMPQ's interaction-aware approach significantly outperforms isolated metric methods, proving that capturing inter-layer interactions is crucial for effective mixed-precision quantization at ultra-low precisions.

Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their
multi-billion-parameter scale makes on-device or low-resource deployment
prohibitive. Mixed-precision quantization offers a compelling solution, but
existing methods struggle when the average precision drops below four bits, as
they rely on isolated, layer-specific metrics that overlook critical
inter-layer interactions affecting overall performance. In this paper, we
propose two innovations to address these limitations. First, we frame the
mixed-precision quantization problem as a cooperative game among layers and
introduce Shapley-based Progressive Quantization Estimation (SPQE) to
efficiently obtain accurate Shapley estimates of layer sensitivities and
inter-layer interactions. Second, building upon SPQE, we propose
Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these
Shapley estimates into a binary quadratic optimization formulation, assigning
either 2 or 4-bit precision to layers under strict memory constraints.
Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models
across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's
scalability and consistently superior performance compared to methods relying
solely on isolated metrics. Across average precisions spanning 4 bit down to 2
bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline,
with the margin growing as the bit-width tightens.

</details>


### [42] [Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs](https://arxiv.org/abs/2509.15464)
*Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: EvoReasoner and EvoKG address LLMs' limitations in handling evolving knowledge by combining temporal-aware reasoning with dynamic KG updates, achieving performance comparable to much larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reasoning over temporally evolving knowledge, and existing KG-augmented approaches assume static KGs, ignoring temporal dynamics and factual inconsistencies in real-world data.

Method: Proposes EvoReasoner (temporal multi-hop reasoning with global-local entity grounding and temporally grounded scoring) and EvoKG (noise-tolerant KG evolution module with contradiction resolution and temporal trend tracking).

Result: Outperforms prompting-based and KG-enhanced baselines on temporal QA benchmarks, with an 8B-parameter model matching the performance of a 671B model prompted seven months later.

Conclusion: Combining temporal reasoning with KG evolution is crucial for robust and up-to-date LLM performance, effectively narrowing the gap between small and large models on dynamic question answering.

Abstract: Large language models (LLMs) excel at many language understanding tasks but
struggle to reason over knowledge that evolves. To address this, recent work
has explored augmenting LLMs with knowledge graphs (KGs) to provide structured,
up-to-date information. However, many existing approaches assume a static
snapshot of the KG and overlook the temporal dynamics and factual
inconsistencies inherent in real-world data. To address the challenge of
reasoning over temporally shifting knowledge, we propose EvoReasoner, a
temporal-aware multi-hop reasoning algorithm that performs global-local entity
grounding, multi-route decomposition, and temporally grounded scoring. To
ensure that the underlying KG remains accurate and up-to-date, we introduce
EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG
from unstructured documents through confidence-based contradiction resolution
and temporal trend tracking. We evaluate our approach on temporal QA benchmarks
and a novel end-to-end setting where the KG is dynamically updated from raw
documents. Our method outperforms both prompting-based and KG-enhanced
baselines, effectively narrowing the gap between small and large LLMs on
dynamic question answering. Notably, an 8B-parameter model using our approach
matches the performance of a 671B model prompted seven months later. These
results highlight the importance of combining temporal reasoning with KG
evolution for robust and up-to-date LLM performance. Our code is publicly
available at github.com/junhongmit/TREK.

</details>


### [43] [Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies](https://arxiv.org/abs/2509.15481)
*Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini*

Main category: cs.LG

TL;DR: SolarCAST is a causally informed model for solar forecasting that uses only historical global horizontal irradiance (GHI) data from target and nearby stations, outperforming commercial solutions with 25.9% error reduction.


<details>
  <summary>Details</summary>
Motivation: Accurate solar forecasting is crucial for renewable energy management, but existing methods rely on specialized hardware like sky-cameras or satellite imagery that require heavy preprocessing. SolarCAST aims to provide a lightweight, practical solution using only public sensor data.

Method: SolarCAST models three classes of confounding factors using neural components: (i) observable synchronous variables via embedding module, (ii) latent synchronous factors via spatio-temporal graph neural network, and (iii) time-lagged influences via gated transformer that learns temporal shifts.

Result: SolarCAST outperforms leading time-series and multimodal baselines across diverse geographical conditions and achieves a 25.9% error reduction over the top commercial forecaster, Solcast.

Conclusion: SolarCAST offers a lightweight, practical, and generalizable solution for localized solar forecasting that requires only public sensor data without specialized hardware or heavy preprocessing.

Abstract: Accurate solar forecasting underpins effective renewable energy management.
We present SolarCAST, a causally informed model predicting future global
horizontal irradiance (GHI) at a target site using only historical GHI from
site X and nearby stations S - unlike prior work that relies on sky-camera or
satellite imagery requiring specialized hardware and heavy preprocessing. To
deliver high accuracy with only public sensor data, SolarCAST models three
classes of confounding factors behind X-S correlations using scalable neural
components: (i) observable synchronous variables (e.g., time of day, station
identity), handled via an embedding module; (ii) latent synchronous factors
(e.g., regional weather patterns), captured by a spatio-temporal graph neural
network; and (iii) time-lagged influences (e.g., cloud movement across
stations), modeled with a gated transformer that learns temporal shifts. It
outperforms leading time-series and multimodal baselines across diverse
geographical conditions, and achieves a 25.9% error reduction over the top
commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and
generalizable solution for localized solar forecasting.

</details>


### [44] [FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data](https://arxiv.org/abs/2509.15493)
*Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos*

Main category: cs.LG

TL;DR: FRAUDGUESS is a system for detecting new types of financial fraud by identifying micro-clusters in a feature space, with visualization tools for justification and expert analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional fraud detection relies on known fraud patterns, but there's a need to discover new, unknown fraud types and provide evidence to support detection decisions for domain experts.

Method: FRAUDGUESS uses micro-cluster detection in a carefully designed feature space for fraud detection, and employs visualization, heatmaps, and interactive dashboards for justification and deep analysis.

Result: The system was tested on real million-scale financial data and discovered three new fraudulent behaviors, two of which were confirmed by experts, catching hundreds of previously undetected fraudulent transactions.

Conclusion: FRAUDGUESS is effective for detecting unknown fraud patterns and providing justification evidence, with real-world deployment potential as demonstrated by its use in an Anonymous Financial Institution.

Abstract: Given a set of financial transactions (who buys from whom, when, and for how
much), as well as prior information from buyers and sellers, how can we find
fraudulent transactions? If we have labels for some transactions for known
types of fraud, we can build a classifier. However, we also want to find new
types of fraud, still unknown to the domain experts ('Detection'). Moreover, we
also want to provide evidence to experts that supports our opinion
('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals:
(a) for 'Detection', it spots new types of fraud as micro-clusters in a
carefully designed feature space; (b) for 'Justification', it uses
visualization and heatmaps for evidence, as well as an interactive dashboard
for deep dives. FRAUDGUESS is used in real life and is currently considered for
deployment in an Anonymous Financial Institution (AFI). Thus, we also present
the three new behaviors that FRAUDGUESS discovered in a real, million-scale
financial dataset. Two of these behaviors are deemed fraudulent or suspicious
by domain experts, catching hundreds of fraudulent transactions that would
otherwise go un-noticed.

</details>


### [45] [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557)
*Mirza Farhan Bin Tarek,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: RLVR (Reinforcement Learning from Verifiable Rewards) shows LLMs can develop reasoning without supervision, but medical QA applications suffer from reward hacking during reasoning. The paper introduces penalties for two hacking behaviors to improve reasoning format and reliability.


<details>
  <summary>Details</summary>
Motivation: Medical question answering using RLVR is vulnerable to reward hacking behaviors where models bypass proper reasoning or use non-standard formats to exploit rewards, compromising reliability.

Method: Introduces a composite reward function with specific penalties for two hacking behaviors: providing answers without reasoning and using non-standard reasoning formats to exploit rewards.

Result: Experiments show that extending RLVR with the proposed reward model leads to better-formatted reasoning, less reward hacking, and good accuracy compared to baselines.

Conclusion: This approach reduces reward hacking and enhances the reliability of models using RLVR, representing progress toward more trustworthy medical QA systems.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.

</details>


### [46] [Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations](https://arxiv.org/abs/2509.15494)
*Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner*

Main category: cs.LG

TL;DR: WIEN-INR is a wavelet-informed implicit neural representation that improves multi-scale structure and fine detail preservation in compact neural networks for scientific data encoding.


<details>
  <summary>Details</summary>
Motivation: Existing implicit neural representations struggle to faithfully represent multi-scale structures, high-frequency information, and fine textures in scientific datasets when constrained to compact network sizes.

Method: Proposes a multi-scale architecture that distributes modeling across different resolution scales and employs a specialized kernel network at the finest scale to recover subtle details, allowing for smaller networks while retaining full information spectrum.

Result: WIEN-INR achieves superior reconstruction fidelity while maintaining compact model size across diverse scientific datasets spanning different scales and structural complexities.

Conclusion: WIEN-INR extends the applicability of implicit neural representations to domains where efficient preservation of fine detail is essential, demonstrating practical high-fidelity scientific data encoding.

Abstract: Implicit neural representations (INRs) have emerged as a compact and
parametric alternative to discrete array-based data representations, encoding
information directly in neural network weights to enable resolution-independent
representation and memory efficiency. However, existing INR approaches, when
constrained to compact network sizes, struggle to faithfully represent the
multi-scale structures, high-frequency information, and fine textures that
characterize the majority of scientific datasets. To address this limitation,
we propose WIEN-INR, a wavelet-informed implicit neural representation that
distributes modeling across different resolution scales and employs a
specialized kernel network at the finest scale to recover subtle details. This
multi-scale architecture allows for the use of smaller networks to retain the
full spectrum of information while preserving the training efficiency and
reducing storage cost. Through extensive experiments on diverse scientific
datasets spanning different scales and structural complexities, WIEN-INR
achieves superior reconstruction fidelity while maintaining a compact model
size. These results demonstrate WIEN-INR as a practical neural representation
framework for high-fidelity scientific data encoding, extending the
applicability of INRs to domains where efficient preservation of fine detail is
essential.

</details>


### [47] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: LZN introduces a unified framework using shared Gaussian latent space with disjoint zones for different data types, enabling generative modeling, representation learning, and classification through encoder-decoder compositions.


<details>
  <summary>Details</summary>
Motivation: To unify three core ML problems (generative modeling, representation learning, classification) that currently have disjoint state-of-the-art solutions, simplifying ML pipelines and fostering synergy across tasks.

Method: Latent Zoning Network (LZN) creates a shared Gaussian latent space where each data type has dedicated encoders mapping to disjoint zones and decoders mapping back. Tasks are expressed as encoder-decoder compositions (e.g., label encoder + image decoder for conditional generation).

Result: LZN improves FID on CIFAR10 from 2.76 to 2.59 when combined with Rectified Flow; outperforms MoCo by 9.3% and SimCLR by 0.2% on ImageNet linear classification; achieves SOTA classification accuracy on CIFAR10 while improving FID in joint tasks.

Conclusion: LZN demonstrates promise as a unified principle for multiple ML tasks, showing improvements across generation, representation learning, and classification without requiring task-specific modifications or auxiliary losses.

Abstract: Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


### [48] [Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers](https://arxiv.org/abs/2509.15498)
*Zahra Aref,Narayan B. Mandayam*

Main category: cs.LG

TL;DR: EWA-VQ-ODT enhances Online Decision Transformers by adding a lightweight memory module that tracks action effectiveness using vector quantization and cognitive-inspired attraction mechanisms, improving sample efficiency in continuous control tasks.


<details>
  <summary>Details</summary>
Motivation: Standard attention in Online Decision Transformers lacks explicit memory of action-specific outcomes, leading to inefficiencies in learning long-term action effectiveness. The paper aims to address this limitation by incorporating cognitive models of action evaluation.

Method: Proposes Experience-Weighted Attraction with Vector Quantization (EWA-VQ-ODT), which maintains per-action mental accounts using a vector-quantized codebook. Continuous actions are routed to codes storing scalar attractions updated online through decay and reward-based reinforcement, which modulate attention by biasing action token columns.

Result: On standard continuous-control benchmarks, EWA-VQ-ODT improves sample efficiency and average return over standard ODT, particularly during early training. The module is computationally efficient and interpretable.

Conclusion: The proposed EWA-VQ-ODT framework successfully enhances Online Decision Transformers by incorporating cognitive-inspired action evaluation mechanisms, providing better performance without changing the backbone architecture or training objective.

Abstract: Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.

</details>


### [49] [Information Geometry of Variational Bayes](https://arxiv.org/abs/2509.15641)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: The paper establishes a fundamental connection between information geometry and variational Bayes, showing that VB solutions require natural gradients and demonstrating practical consequences including simplified Bayes' rule, generalized quadratic surrogates, and large-scale VB implementations.


<details>
  <summary>Details</summary>
Motivation: To highlight and emphasize the fundamental connection between information geometry and variational Bayes, facilitating more interdisciplinary work between these two fields.

Method: Uses the natural-gradient descent algorithm (Bayesian Learning Rule) by Khan and Rue (2023) to demonstrate the connection and its practical consequences.

Result: Shows that VB solutions require natural gradients, leading to simplified Bayes' rule as addition of natural gradients, generalization of quadratic surrogates, and enabling large-scale VB implementations for LLMs.

Conclusion: The connection between information geometry and Bayes has common origins, and this work aims to encourage more research at the intersection of these fields, though the connection itself is not entirely new.

Abstract: We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.

</details>


### [50] [Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses](https://arxiv.org/abs/2509.15509)
*Xiaoshuang Wang,Yifan Lin,Enlu Zhou*

Main category: cs.LG

TL;DR: This paper proposes a policy gradient optimization method for Markov decision processes with general loss functions and unknown parameters, using Bayesian estimation and coherent risk measures to handle epistemic uncertainty.


<details>
  <summary>Details</summary>
Motivation: Many application problems involve MDPs with unknown parameters and general loss functions, requiring methods to handle epistemic uncertainty when standard Bellman equations don't apply due to violation of the interchangeability principle.

Method: A policy gradient optimization method leveraging dual representation of coherent risk measures and extending the envelope theorem to continuous cases, with extensions to episodic settings.

Result: The algorithm achieves stationary convergence rate of O(T^{-1/2}+r^{-1/2}) where T is policy gradient iterations and r is sample size. Episodic extension shows global convergence with O(ε) error bounds.

Conclusion: The proposed policy gradient approach effectively solves MDPs with unknown parameters and coherent risk measures where traditional dynamic programming fails, providing theoretical guarantees for convergence.

Abstract: Motivated by many application problems, we consider Markov decision processes
(MDPs) with a general loss function and unknown parameters. To mitigate the
epistemic uncertainty associated with unknown parameters, we take a Bayesian
approach to estimate the parameters from data and impose a coherent risk
functional (with respect to the Bayesian posterior distribution) on the loss.
Since this formulation usually does not satisfy the interchangeability
principle, it does not admit Bellman equations and cannot be solved by
approaches based on dynamic programming. Therefore, We propose a policy
gradient optimization method, leveraging the dual representation of coherent
risk measures and extending the envelope theorem to continuous cases. We then
show the stationary analysis of the algorithm with a convergence rate of
$O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations
and $r$ is the sample size of the gradient estimator. We further extend our
algorithm to an episodic setting, and establish the global convergence of the
extended algorithm and provide bounds on the number of iterations needed to
achieve an error bound $O(\epsilon)$ in each episode.

</details>


### [51] [Toward Efficient Influence Function: Dropout as a Compression Tool](https://arxiv.org/abs/2509.15651)
*Yuchen Zhang,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: A novel method using dropout as gradient compression for efficient influence function computation in large-scale machine learning models, reducing computational and memory costs while preserving data influence accuracy.


<details>
  <summary>Details</summary>
Motivation: Influence functions are crucial for understanding model behavior and transparency but face significant computational and memory challenges with large-scale models, even with approximation methods.

Method: Leverages dropout as a gradient compression mechanism to compute influence functions more efficiently, reducing computational and memory overhead in both influence computation and gradient compression processes.

Result: The method significantly reduces computational and memory costs while preserving critical components of data influence, enabling application to modern large-scale models.

Conclusion: The proposed dropout-based gradient compression approach provides an efficient and practical solution for influence function computation in large-scale machine learning models.

Abstract: Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.

</details>


### [52] [KoopCast: Trajectory Forecasting via Koopman Operators](https://arxiv.org/abs/2509.15513)
*Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang*

Main category: cs.LG

TL;DR: KoopCast is a lightweight trajectory forecasting model that uses Koopman operator theory to achieve linear representation of nonlinear dynamics through a two-stage design with neural goal estimation and Koopman-based refinement.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient trajectory forecasting model that can handle general dynamic environments with rich multi-agent interactions and map-constrained nonlinear motion while maintaining interpretability and low latency.

Method: Two-stage approach: 1) probabilistic neural goal estimator predicts long-term targets, 2) Koopman operator-based refinement module incorporates intention and history into nonlinear feature space for linear prediction. Leverages Koopman operator theory for linear representation of nonlinear dynamics.

Result: Competitive accuracy across ETH/UCY, Waymo Open Motion Dataset, and nuScenes benchmarks. The model delivers high predictive accuracy with mode-level interpretability and practical efficiency.

Conclusion: KoopCast successfully combines competitive accuracy, interpretability through Koopman spectral theory, and low-latency deployment, making it effective for trajectory forecasting in complex dynamic environments.

Abstract: We present KoopCast, a lightweight yet efficient model for trajectory
forecasting in general dynamic environments. Our approach leverages Koopman
operator theory, which enables a linear representation of nonlinear dynamics by
lifting trajectories into a higher-dimensional space. The framework follows a
two-stage design: first, a probabilistic neural goal estimator predicts
plausible long-term targets, specifying where to go; second, a Koopman
operator-based refinement module incorporates intention and history into a
nonlinear feature space, enabling linear prediction that dictates how to go.
This dual structure not only ensures strong predictive accuracy but also
inherits the favorable properties of linear operators while faithfully
capturing nonlinear dynamics. As a result, our model offers three key
advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman
spectral theory, and (iii) low-latency deployment. We validate these benefits
on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich
multi-agent interactions and map-constrained nonlinear motion. Across
benchmarks, KoopCast consistently delivers high predictive accuracy together
with mode-level interpretability and practical efficiency.

</details>


### [53] [Inference Offloading for Cost-Sensitive Binary Classification at the Edge](https://arxiv.org/abs/2509.15674)
*Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: This paper proposes an online learning framework called H2T2 for hierarchical inference systems in edge intelligence, where a local model is supplemented by a remote model, with optimized thresholds to balance classification accuracy and offloading costs.


<details>
  <summary>Details</summary>
Motivation: In edge intelligence systems, false negatives are more costly than false positives, and there's a need to optimize the trade-off between classification accuracy and the costs of offloading samples to a remote model when local model confidence is low.

Method: The authors propose H2T2, an online two-threshold hierarchical inference policy that continuously adapts thresholds on local model confidence scores. These thresholds determine local predictions and offloading decisions. The method includes a closed-form solution for calibrated models and an online learning approach for uncalibrated models.

Result: Simulations on real-world datasets show H2T2 consistently outperforms naive and single-threshold policies, sometimes even surpassing offline optima. The policy demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.

Conclusion: H2T2 provides an effective online learning solution for hierarchical inference systems that achieves sublinear regret, is model-agnostic, requires no training, and learns during inference with limited feedback, making it practical for real-world edge intelligence applications.

Abstract: We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.

</details>


### [54] [Manifold Dimension Estimation: An Empirical Study](https://arxiv.org/abs/2509.15517)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: A comprehensive survey of manifold dimension estimation methods, analyzing theoretical foundations, comparing eight estimators through controlled experiments, and providing practical guidance that simpler methods often perform better.


<details>
  <summary>Details</summary>
Motivation: The manifold hypothesis suggests high-dimensional data lies on low-dimensional manifolds, but existing dimension estimation work is fragmented and lacks systematic evaluation, creating a need for comprehensive analysis and practical guidance.

Method: Review theoretical foundations, present eight representative estimators, conduct controlled experiments analyzing factors like noise and curvature, compare on synthetic and real datasets with principled hyperparameter tuning.

Result: Systematic evaluation reveals how different factors affect estimator performance, with comparison results showing that simpler methods often outperform more complex ones for this general problem.

Conclusion: The survey provides practical guidance for dimension estimation and suggests that simpler methods are often more effective for this type of general problem, offering valuable insights for researchers and practitioners.

Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or
near a low-dimensional manifold. Estimating the dimension of this manifold is
essential for leveraging its structure, yet existing work on dimension
estimation is fragmented and lacks systematic evaluation. This article provides
a comprehensive survey for both researchers and practitioners. We review
often-overlooked theoretical foundations and present eight representative
estimators. Through controlled experiments, we analyze how individual factors
such as noise, curvature, and sample size affect performance. We also compare
the estimators on diverse synthetic and real-world datasets, introducing a
principled approach to dataset-specific hyperparameter tuning. Our results
offer practical guidance and suggest that, for a problem of this generality,
simpler methods often perform better.

</details>


### [55] [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
*Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: The paper proposes a principled, information theory-driven approach for selecting diverse and structure-aware examples in in-context learning (ICL) to improve performance on specific queries, addressing limitations of nearest-neighbor methods.


<details>
  <summary>Details</summary>
Motivation: Traditional nearest-neighbor methods for example selection in ICL suffer from poor generalization and lack of diversity in high-dimensional embedding spaces. The authors aim to develop a more principled approach that optimizes for accurate prediction on specific queries rather than general generalization.

Method: The authors frame example selection as a query-specific optimization problem, modeling LLMs as linear functions over input embeddings. They derive a submodular surrogate objective and use a greedy algorithm with approximation guarantees, enhanced by kernel trick for high-dimensional spaces and optimal design-based regularizer for diversity.

Result: Empirical evaluations show significant improvements over standard retrieval methods across various classification tasks, demonstrating the benefits of structure-aware, diverse example selection in label-scarce scenarios.

Conclusion: The proposed information theory-driven approach provides a principled framework for example selection in ICL that outperforms traditional methods by focusing on query-specific optimization and promoting diversity through optimal design regularization.

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

</details>


### [56] [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem](https://arxiv.org/abs/2509.15519)
*Chao Li,Bingkun Bao,Yang Gao*

Main category: cs.LG

TL;DR: DAC is a novel method for fully decentralized cooperative multi-agent reinforcement learning that addresses non-stationarity and relative overgeneralization through dynamics-aware context modeling, treating each agent's local task as a Contextual Markov Decision Process.


<details>
  <summary>Details</summary>
Motivation: In fully decentralized cooperative MARL, agents cannot access other agents' actions, leading to non-stationarity during value function updates and relative overgeneralization during estimation, which existing methods fail to address simultaneously.

Method: DAC formalizes each agent's local task as a Contextual MDP, attributes non-stationary dynamics to switches between unobserved contexts (distinct joint policies), models step-wise dynamics with latent variables, introduces context-based value functions, and uses optimistic marginal values for cooperative action selection.

Result: Experimental evaluation on matrix games, predator-prey, and SMAC tasks shows DAC achieves superior performance against multiple baselines.

Conclusion: DAC effectively addresses both non-stationarity and relative overgeneralization in fully decentralized cooperative MARL through dynamics-aware context modeling, enabling more effective cooperative policy learning.

Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement
learning, where each agent solely observes the states, its local actions, and
the shared rewards. The inability to access other agents' actions often leads
to non-stationarity during value function updates and relative
overgeneralization during value function estimation, hindering effective
cooperative policy learning. However, existing works fail to address both
issues simultaneously, due to their inability to model the joint policy of
other agents in a fully decentralized setting. To overcome this limitation, we
propose a novel method named Dynamics-Aware Context (DAC), which formalizes the
task, as locally perceived by each agent, as an Contextual Markov Decision
Process, and further addresses both non-stationarity and relative
overgeneralization through dynamics-aware context modeling. Specifically, DAC
attributes the non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using latent variables and
refers to them as contexts. For each agent, DAC introduces a context-based
value function to address the non-stationarity issue during value function
update. For value function estimation, an optimistic marginal value is derived
to promote the selection of cooperative actions, thereby addressing the
relative overgeneralization issue. Experimentally, we evaluate DAC on various
cooperative tasks (including matrix game, predator and prey, and SMAC), and its
superior performance against multiple baselines validates its effectiveness.

</details>


### [57] [On Optimal Steering to Achieve Exact Fairness](https://arxiv.org/abs/2509.15759)
*Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah*

Main category: cs.LG

TL;DR: This paper proposes optimal steering techniques to fix 'bias in, bias out' problems in machine learning by steering feature distributions or LLM representations toward ideal distributions that guarantee group-fair outcomes without fairness-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental 'bias in, bias out' problem in fair machine learning by ensuring that feature distributions and LLM representations are steered toward ideal distributions that inherently produce fair outcomes.

Method: Formulated an optimization program for optimal steering by finding the nearest ideal distribution in KL-divergence, with efficient algorithms for parametric families (normal, log-normal). Applied affine steering of LLM representations and internal representations toward desired outputs.

Result: Empirical results on synthetic and real-world datasets show improved fairness without diminishing utility (sometimes even improving utility). Demonstrated successful bias reduction in multi-class classification tasks like occupation prediction from biographies.

Conclusion: The proposed optimal steering techniques effectively mitigate bias in machine learning systems while maintaining or even enhancing utility, providing a practical solution to fairness-utility trade-offs.

Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.

</details>


### [58] [Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows](https://arxiv.org/abs/2509.15533)
*Peter Amorese,Morteza Lahijanian*

Main category: cs.LG

TL;DR: This paper proposes a novel model that combines normalizing flows and Bernstein polynomials to learn stochastic dynamics from data while enabling analytical belief propagation for nonlinear systems with non-Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of belief propagation in stochastic systems where analytical solutions are intractable due to nonlinear dynamics, especially when the system model is unknown and must be learned from data.

Method: The method combines normalizing flows for universal approximation of nonlinear stochastic dynamics with Bernstein polynomials to maintain analytical tractability for belief propagation.

Result: Empirical results demonstrate that the proposed model outperforms state-of-the-art data-driven methods for belief propagation, particularly for highly nonlinear systems with non-additive, non-Gaussian noise.

Conclusion: The paper establishes theoretical foundations for a class of models that can both universally approximate general nonlinear stochastic dynamics and support analytical belief propagation, providing an effective solution for reasoning under uncertainty in complex systems.

Abstract: Predicting the distribution of future states in a stochastic system, known as
belief propagation, is fundamental to reasoning under uncertainty. However,
nonlinear dynamics often make analytical belief propagation intractable,
requiring approximate methods. When the system model is unknown and must be
learned from data, a key question arises: can we learn a model that (i)
universally approximates general nonlinear stochastic dynamics, and (ii)
supports analytical belief propagation? This paper establishes the theoretical
foundations for a class of models that satisfy both properties. The proposed
approach combines the expressiveness of normalizing flows for density
estimation with the analytical tractability of Bernstein polynomials. Empirical
results show the efficacy of our learned model over state-of-the-art
data-driven methods for belief propagation, especially for highly non-linear
systems with non-additive, non-Gaussian noise.

</details>


### [59] [Monte Carlo Tree Diffusion with Multiple Experts for Protein Design](https://arxiv.org/abs/2509.15796)
*Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: MCTD-ME is a novel protein design method that combines masked diffusion models with Monte Carlo Tree Search and multiple experts to overcome limitations of autoregressive approaches, enabling efficient multi-token planning and scaling to large sequence spaces.


<details>
  <summary>Details</summary>
Motivation: Prior methods using autoregressive language models with MCTS struggle with long-range dependencies and face impractically large search spaces in protein design. The authors aim to develop a more efficient approach that can handle complex protein sequences better.

Method: MCTD-ME integrates masked diffusion models with tree search, using biophysical-fidelity-enhanced diffusion denoising as rollout engine. It employs multiple experts of varying capacities guided by pLDDT-based masking schedule, with a novel multi-expert selection rule (PH-UCT-ME) that extends predictive-entropy UCT to expert ensembles.

Result: On inverse folding tasks (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance.

Conclusion: MCTD-ME provides an effective framework for protein design that is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.

Abstract: The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.

</details>


### [60] [Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2509.15543)
*Xinwen Zhang,Yihan Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: A novel decentralized stochastic bilevel optimization algorithm for nonconvex problems under heavy-tailed noises, using normalized stochastic variance-reduced gradient descent without clipping operations.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized stochastic optimization methods assume strong convexity and finite variance, which are often not satisfied in real-world machine learning models with heavy-tailed noise distributions.

Method: Developed a normalized stochastic variance-reduced bilevel gradient descent algorithm that handles interdependent gradient sequences under heavy-tailed noises without relying on clipping operations.

Result: Established convergence rate for nonconvex decentralized bilevel optimization under heavy-tailed noises, representing the first algorithm with rigorous theoretical guarantees in this setting.

Conclusion: The algorithm effectively handles heavy-tailed noises in decentralized bilevel optimization, with experimental results confirming its practical effectiveness.

Abstract: Existing decentralized stochastic optimization methods assume the lower-level
loss function is strongly convex and the stochastic gradient noise has finite
variance. These strong assumptions typically are not satisfied in real-world
machine learning models. To address these limitations, we develop a novel
decentralized stochastic bilevel optimization algorithm for the nonconvex
bilevel optimization problem under heavy-tailed noises. Specifically, we
develop a normalized stochastic variance-reduced bilevel gradient descent
algorithm, which does not rely on any clipping operation. Moreover, we
establish its convergence rate by innovatively bounding interdependent gradient
sequences under heavy-tailed noises for nonconvex decentralized bilevel
optimization problems. As far as we know, this is the first decentralized
bilevel optimization algorithm with rigorous theoretical guarantees under
heavy-tailed noises. The extensive experimental results confirm the
effectiveness of our algorithm in handling heavy-tailed noises.

</details>


### [61] [Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering](https://arxiv.org/abs/2509.15810)
*Chen Wang,Zeyuan Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: LSRE is a method to generate diverse training problem instances for Meta-Black-Box Optimization (MetaBBO) to prevent overfitting and improve generalization by using autoencoders and genetic programming.


<details>
  <summary>Details</summary>
Motivation: Current MetaBBO methods use limited-diversity benchmark suites like CoCo-BBOB, which risks overfitting and poor generalization. More diverse training instances are needed.

Method: LSRE trains an autoencoder to map problem features to a 2D latent space, performs uniform-grid sampling for diversity, then uses genetic programming to reverse engineer function formulas matching these representations, creating the Diverse-BBO dataset.

Result: Experiments show MetaBBOs trained on Diverse-BBO achieve superior generalization on synthetic and realistic scenarios compared to existing training sets.

Conclusion: LSRE effectively generates diverse training instances, improving MetaBBO generalization, with ablation studies confirming the importance of instance diversity.

Abstract: To relieve intensive human-expertise required to design optimization
algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage
generalization strength of meta-learning to train neural network-based
algorithm design policies over a predefined training problem set, which
automates the adaptability of the low-level optimizers on unseen problem
instances. Currently, a common training problem set choice in existing MetaBBOs
is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the
MetaBBO's development, problem instances in CoCo-BBOB are more or less limited
in diversity, raising the risk of overfitting of MetaBBOs, which might further
results in poor generalization. In this paper, we propose an instance
generation approach, termed as \textbf{LSRE}, which could generate diverse
training problem instances for MetaBBOs to learn more generalizable policies.
LSRE first trains an autoencoder which maps high-dimensional problem features
into a 2-dimensional latent space. Uniform-grid sampling in this latent space
leads to hidden representations of problem instances with sufficient diversity.
By leveraging a genetic-programming approach to search function formulas with
minimal L2-distance to these hidden representations, LSRE reverse engineers a
diversified problem set, termed as \textbf{Diverse-BBO}. We validate the
effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe
their generalization performances on either synthetic or realistic scenarios.
Extensive experimental results underscore the superiority of Diverse-BBO to
existing training set choices in MetaBBOs. Further ablation studies not only
demonstrate the effectiveness of design choices in LSRE, but also reveal
interesting insights on instance diversity and MetaBBO's generalization.

</details>


### [62] [PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors](https://arxiv.org/abs/2509.15551)
*Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: PolyJuice is a black-box, image-agnostic red-teaming method for synthetic image detectors that identifies distribution shifts in T2I latent space to universally steer generated images toward detector failure modes.


<details>
  <summary>Details</summary>
Motivation: Existing red-teaming solutions require white-box access to detectors and generate image-specific attacks through expensive online optimization, which is infeasible for proprietary state-of-the-art detectors.

Method: PolyJuice identifies the direction of distribution shift between correctly and incorrectly classified samples through lightweight offline black-box access, then exploits this direction to universally steer all generated images toward the detector's failure modes.

Result: PolyJuice-steered T2I models deceive SIDs up to 84% more effectively than unsteered counterparts, and steering directions can be efficiently estimated at lower resolutions and transferred to higher resolutions via interpolation.

Conclusion: PolyJuice provides an effective black-box red-teaming approach that can also enhance detector performance when used for data augmentation, improving SID performance by up to 30%.

Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by
the growing realism of images from text-to-image (T2I) models. Red teaming
improves SID's effectiveness by identifying and exploiting their failure modes
via misclassified synthetic images. However, existing red-teaming solutions (i)
require white-box access to SIDs, which is infeasible for proprietary
state-of-the-art detectors, and (ii) generate image-specific attacks through
expensive online optimization. To address these limitations, we propose
PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs,
based on an observed distribution shift in the T2I latent space between samples
correctly and incorrectly classified by the SID. PolyJuice generates attacks by
(i) identifying the direction of this shift through a lightweight offline
process that only requires black-box access to the SID, and (ii) exploiting
this direction by universally steering all generated images towards the SID's
failure modes. PolyJuice-steered T2I models are significantly more effective at
deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also
show that the steering directions can be estimated efficiently at lower
resolutions and transferred to higher resolutions using simple interpolation,
reducing computational overhead. Finally, tuning SID models on
PolyJuice-augmented datasets notably enhances the performance of the detectors
(up to 30%).

</details>


### [63] [EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network](https://arxiv.org/abs/2509.15857)
*Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai*

Main category: cs.LG

TL;DR: EvoBrain is a novel seizure detection model that addresses limitations of existing dynamic GNNs by incorporating explicit dynamic graph structures and a time-then-graph approach, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current dynamic GNN methods for EEG-based seizure detection have two key limitations: they use temporally fixed static graphs that don't reflect evolving brain connectivity, and they inadequately model interactions between temporal signals and graph structures, leading to inconsistent performance.

Method: EvoBrain integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. It incorporates explicitly dynamic graph structures where both nodes and edges evolve over time, using a time-then-graph dynamic GNN approach.

Result: The model significantly improves AUROC by 23% and F1 score by 30% compared to dynamic GNN baselines. It also demonstrates effectiveness in challenging early seizure prediction tasks.

Conclusion: Explicit dynamic modeling and time-then-graph approaches are theoretically proven to be more expressive and necessary for accurate seizure detection, with EvoBrain providing a novel and efficient solution that outperforms existing methods.

Abstract: Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.

</details>


### [64] [The Multi-Query Paradox in Zeroth-Order Optimization](https://arxiv.org/abs/2509.15552)
*Wei Lin,Qingyu Song,Hong Xu*

Main category: cs.LG

TL;DR: This paper systematically analyzes the query allocation problem in zeroth-order optimization, revealing that the optimal strategy depends on the aggregation method: single-query is optimal for simple averaging, while full-subspace estimation is optimal for the new projection alignment method.


<details>
  <summary>Details</summary>
Motivation: Zeroth-order optimization faces a fundamental trade-off between queries per iteration and total iterations under fixed budget. The multi-query paradigm improves estimation accuracy but creates allocation challenges that are under-explored.

Method: The paper analyzes two aggregation methods: simple averaging (ZO-Avg) and a new Projection Alignment method (ZO-Align) derived from local surrogate minimization. It derives convergence rates across strongly convex, convex, non-convex, and stochastic settings.

Result: A stark dichotomy is uncovered: ZO-Avg is query-inefficient with more than one query per iteration (single-query optimal), while ZO-Align performs better with more queries (full-subspace estimation optimal). The choice is between two classic algorithms dictated by aggregation method.

Conclusion: The multi-query problem reduces to choosing between two algorithms based on aggregation method, not intermediate query sizes. Theoretical findings are validated through extensive experiments, clarifying the fundamental query allocation trade-off in zeroth-order optimization.

Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems
where explicit gradients are unavailable and have to be approximated using only
queries to function value. The prevalent single-query approach is simple, but
suffers from high estimation variance, motivating a multi-query paradigm to
improves estimation accuracy. This, however, creates a critical trade-off:
under a fixed budget of queries (i.e. cost), queries per iteration and the
total number of optimization iterations are inversely proportional to one
another. How to best allocate this budget is a fundamental, under-explored
question.
  This work systematically resolves this query allocation problem. We analyze
two aggregation methods: the de facto simple averaging (ZO-Avg), and a new
Projection Alignment method (ZO-Align) we derive from local surrogate
minimization. By deriving convergence rates for both methods that make the
dependence on the number of queries explicit across strongly convex, convex,
non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg,
we prove that using more than one query per iteration is always
query-inefficient, rendering the single-query approach optimal. On the
contrary, ZO-Align generally performs better with more queries per iteration,
resulting in a full-subspace estimation as the optimal approach. Thus, our work
clarifies that the multi-query problem boils down to a choice not about an
intermediate query size, but between two classic algorithms, a choice dictated
entirely by the aggregation method used. These theoretical findings are also
consistently validated by extensive experiments.

</details>


### [65] [From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction](https://arxiv.org/abs/2509.15895)
*Henning Höfener,Farina Kock,Martina Pontones,Tabita Ghete,David Pfrang,Nicholas Dickel,Meik Kunz,Daniela P. Schacherer,David A. Clunie,Andrey Fedorov,Max Westphal,Markus Metzler*

Main category: cs.LG

TL;DR: This paper presents a large public leukemia bone marrow dataset and AI methods for cell detection, classification, and diagnosis prediction to improve leukemia diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current leukemia diagnosis relies on manual microscopic analysis which is complex and time-consuming. Existing AI solutions use private datasets and only cover parts of the diagnostic pipeline.

Method: Created a comprehensive public dataset with 246 pediatric patients, over 40,000 annotated cells, and proposed AI methods for cell detection, cell classification, and diagnosis prediction using predicted cell counts.

Result: Achieved average precision of 0.96 for cell detection, AUC of 0.98 and F1-score of 0.61 for 33-class cell classification, and mean F1-score of 0.90 for diagnosis prediction.

Conclusion: The proposed approaches demonstrate usefulness for AI-assisted diagnostics, and the public dataset will foster further research to improve leukemia diagnosis and patient outcomes.

Abstract: Leukemia diagnosis primarily relies on manual microscopic analysis of bone
marrow morphology supported by additional laboratory parameters, making it
complex and time consuming. While artificial intelligence (AI) solutions have
been proposed, most utilize private datasets and only cover parts of the
diagnostic pipeline. Therefore, we present a large, high-quality, publicly
available leukemia bone marrow dataset spanning the entire diagnostic process,
from cell detection to diagnosis. Using this dataset, we further propose
methods for cell detection, cell classification, and diagnosis prediction. The
dataset comprises 246 pediatric patients with diagnostic, clinical and
laboratory information, over 40 000 cells with bounding box annotations and
more than 28 000 of these with high-quality class labels, making it the most
comprehensive dataset publicly available. Evaluation of the AI models yielded
an average precision of 0.96 for the cell detection, an area under the curve of
0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean
F1-score of 0.90 for the diagnosis prediction using predicted cell counts.
While the proposed approaches demonstrate their usefulness for AI-assisted
diagnostics, the dataset will foster further research and development in the
field, ultimately contributing to more precise diagnoses and improved patient
outcomes.

</details>


### [66] [Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](https://arxiv.org/abs/2509.15915)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: The paper evaluates two strategies for integrating foundation models into reinforcement learning: foundation world models (FWMs) for simulation and foundation agents (FAs) for decision-making, showing promising results in grid-world environments.


<details>
  <summary>Details</summary>
Motivation: Real-world applications with expensive interactions require more sample-efficient agents than traditional RL from scratch. Foundation models possess broad knowledge and reasoning capabilities that could improve sample efficiency.

Method: Empirical evaluation of two approaches: (1) Foundation World Models (FWMs) that use FM prior knowledge for simulation, and (2) Foundation Agents (FAs) that leverage FM reasoning for decision-making. Tested in grid-world environments suitable for current LLMs.

Result: Improvements in LLMs translate to better FWMs and FAs; current LLM-based FAs provide excellent policies for simple environments; FWMs coupled with RL agents show high promise for complex settings with partial observability and stochastic elements.

Conclusion: Both FWMs and FAs are promising strategies for integrating foundation models into RL, with FAs working well for simple environments and FWMs+RL combinations showing potential for more complex scenarios.

Abstract: While reinforcement learning from scratch has shown impressive results in
solving sequential decision-making tasks with efficient simulators, real-world
applications with expensive interactions require more sample-efficient agents.
Foundation models (FMs) are natural candidates to improve sample efficiency as
they possess broad knowledge and reasoning capabilities, but it is yet unclear
how to effectively integrate them into the reinforcement learning framework. In
this paper, we anticipate and, most importantly, evaluate two promising
strategies. First, we consider the use of foundation world models (FWMs) that
exploit the prior knowledge of FMs to enable training and evaluating agents
with simulated interactions. Second, we consider the use of foundation agents
(FAs) that exploit the reasoning capabilities of FMs for decision-making. We
evaluate both approaches empirically in a family of grid-world environments
that are suitable for the current generation of large language models (LLMs).
Our results suggest that improvements in LLMs already translate into better
FWMs and FAs; that FAs based on current LLMs can already provide excellent
policies for sufficiently simple environments; and that the coupling of FWMs
and reinforcement learning agents is highly promising for more complex settings
with partial observability and stochastic elements.

</details>


### [67] [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
*Om Naphade,Saksham Bansal,Parikshit Pareek*

Main category: cs.LG

TL;DR: The paper proposes an Expert Block Framework using Small LLMs for Hyper-parameter Tuning (HPT), achieving near-GPT-4 performance with significantly smaller models by using a Trajectory Context Summarizer to structure optimization data.


<details>
  <summary>Details</summary>
Motivation: Hyper-parameter tuning is computationally expensive and opaque with larger models. Current LLM-based HPT approaches rely on massive models (100B+ parameters), which are resource-intensive. The authors aim to enable effective HPT using smaller, locally-runnable LLMs.

Method: The core innovation is the Trajectory Context Summarizer (TCS), a deterministic block that transforms raw training trajectories into structured context. This allows small LLMs to analyze optimization progress effectively. The framework was tested using phi4:reasoning14B and qwen2.5-coder:32B models with a 10-trial budget.

Result: The TCS-enabled HPT pipeline achieved average performance within ~0.9 percentage points of GPT-4 across six diverse tasks, demonstrating that small LLMs can match the HPT capabilities of much larger models when provided with properly structured context.

Conclusion: Small LLMs can effectively perform hyper-parameter tuning when equipped with the right contextual framework, making HPT more accessible and computationally efficient without sacrificing performance compared to massive models like GPT-4.

Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

</details>


### [68] [Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search](https://arxiv.org/abs/2509.15927)
*Zhiyu Mou,Yiqin Lv,Miao Xu,Cheems Wang,Yixiu Mao,Qichen Ye,Chao Li,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: AIGB-Pearl is a novel auto-bidding method that combines generative planning with policy optimization, using a trajectory evaluator to improve generation quality beyond static datasets.


<details>
  <summary>Details</summary>
Motivation: Existing AI-Generated Bidding (AIGB) methods face performance bottlenecks due to neglecting fine-grained generation quality evaluation and inability to explore beyond static datasets.

Method: Proposes AIGB-Pearl which integrates generative planning and policy optimization with a non-bootstrapped trajectory evaluator. Uses LLM-based architecture, hybrid point-wise/pair-wise losses, and expert feedback integration for evaluator accuracy.

Result: Extensive experiments on simulated and real-world advertising systems demonstrate state-of-the-art performance.

Conclusion: AIGB-Pearl effectively addresses limitations of previous AIGB methods by enabling iterative optimization through interaction and enhanced evaluator accuracy.

Abstract: Auto-bidding is an essential tool for advertisers to enhance their
advertising performance. Recent progress has shown that AI-Generated Bidding
(AIGB), which formulates the auto-bidding as a trajectory generation task and
trains a conditional diffusion-based planner on offline data, achieves superior
and stable performance compared to typical offline reinforcement learning
(RL)-based auto-bidding methods. However, existing AIGB methods still encounter
a performance bottleneck due to their neglect of fine-grained generation
quality evaluation and inability to explore beyond static datasets. To address
this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel
method that integrates generative planning and policy optimization. The key to
AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to
assign rewards and guide policy search, enabling the planner to optimize its
generation quality iteratively through interaction. Furthermore, to enhance
trajectory evaluator accuracy in offline settings, we incorporate three key
techniques: (i) a Large Language Model (LLM)-based architecture for better
representational capacity, (ii) hybrid point-wise and pair-wise losses for
better score learning, and (iii) adaptive integration of expert feedback for
better generalization ability. Extensive experiments on both simulated and
real-world advertising systems demonstrate the state-of-the-art performance of
our approach.

</details>


### [69] [How many classes do we need to see for novel class discovery?](https://arxiv.org/abs/2509.15585)
*Akanksha Sarkar,Been Kim,Jennifer J. Sun*

Main category: cs.LG

TL;DR: The paper proposes a controlled experimental framework using dSprites dataset to systematically study factors influencing successful novel class discovery, particularly examining the relationship between number of known/unknown classes and discovery performance.


<details>
  <summary>Details</summary>
Motivation: Novel class discovery is essential for ML models to adapt to evolving real-world data, but current datasets contain complex entangled factors making systematic study difficult. Fundamental questions remain unanswered about why and when new class discoveries are successful.

Method: A simple controlled experimental framework using the dSprites dataset with procedurally generated modifying factors to investigate influences on successful class discovery, studying relationships between known/unknown class numbers and discovery performance.

Result: Empirical results show that the benefit of increasing known classes reaches a saturation point beyond which discovery performance plateaus. The pattern of diminishing returns provides insights for cost-benefit analysis.

Conclusion: The findings offer practical insights for practitioners and serve as a starting point for more rigorous future research on class discovery in complex real-world datasets.

Abstract: Novel class discovery is essential for ML models to adapt to evolving
real-world data, with applications ranging from scientific discovery to
robotics. However, these datasets contain complex and entangled factors of
variation, making a systematic study of class discovery difficult. As a result,
many fundamental questions are yet to be answered on why and when new class
discoveries are more likely to be successful. To address this, we propose a
simple controlled experimental framework using the dSprites dataset with
procedurally generated modifying factors. This allows us to investigate what
influences successful class discovery. In particular, we study the relationship
between the number of known/unknown classes and discovery performance, as well
as the impact of known class 'coverage' on discovering new classes. Our
empirical results indicate that the benefit of the number of known classes
reaches a saturation point beyond which discovery performance plateaus. The
pattern of diminishing return across different settings provides an insight for
cost-benefit analysis for practitioners and a starting point for more rigorous
future research of class discovery on complex real-world datasets.

</details>


### [70] [The Alignment Bottleneck](https://arxiv.org/abs/2509.15932)
*Wenjun Cao*

Main category: cs.LG

TL;DR: The paper presents a capacity-coupled alignment performance interval for large language models, showing that alignment is fundamentally limited by cognitive capacity and that adding labels alone cannot overcome these bounds without increasing capacity.


<details>
  <summary>Details</summary>
Motivation: To address systematic deviations in feedback-based alignment of large language models by viewing judgment as resource-limited and feedback as a constrained channel, drawing inspiration from bounded rationality in economics and cognitive science.

Method: Models the alignment loop as a two-stage cascade U→H→Y given S, with cognitive capacity C_cog|S and average total capacity C_tot|S. Proves a Fano lower bound and PAC-Bayes upper bound controlled by the same channel capacity.

Result: Establishes that with fixed value complexity and capacity, adding labels alone cannot cross the performance bound; achieving lower risk on more complex targets requires capacity growing with log M; and once capacity saturates, further optimization fits channel regularities (sycophancy, reward hacking).

Conclusion: Alignment should be viewed as interface engineering: measure and allocate limited capacity, manage task complexity, and strategically decide where information is spent, rather than assuming unlimited optimization capability.

Abstract: Large language models improve with scale, yet feedback-based alignment still
exhibits systematic deviations from intended behavior. Motivated by bounded
rationality in economics and cognitive science, we view judgment as
resource-limited and feedback as a constrained channel. On this basis, we model
the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive
capacity $C_{\text{cog}|S}$ and average total capacity
$\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment
Performance Interval. It pairs a data size-independent Fano lower bound proved
on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is
controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes
bound becomes an upper bound on the same true risk when the canonical
observable loss is used and the dataset is drawn from the same mixture. Under
these matched conditions, both limits are governed by a single capacity.
Consequences include that, with value complexity and capacity fixed, adding
labels alone cannot cross the bound; attaining lower risk on more complex
targets requires capacity that grows with $\log M$; and once useful signal
saturates capacity, further optimization tends to fit channel regularities,
consistent with reports of sycophancy and reward hacking. The analysis views
alignment as interface engineering: measure and allocate limited capacity,
manage task complexity, and decide where information is spent.

</details>


### [71] [RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation](https://arxiv.org/abs/2509.15965)
*Chao Yu,Yuanqing Wang,Zhen Guo,Hao Lin,Si Xu,Hongzhi Zang,Quanlu Zhang,Yongji Wu,Chunyang Zhu,Junhao Hu,Zixiao Huang,Mingjie Wei,Yuqing Xie,Ke Yang,Bo Dai,Zhexuan Xu,Xiangyuan Wang,Xu Fu,Zhihao Liu,Kang Chen,Weilin Liu,Gang Liu,Boxun Li,Jianlei Yang,Zhi Yang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: RLinf is a high-performance RL training system that addresses low hardware utilization in RL workflows through a novel macro-to-micro flow transformation (M2Flow) paradigm, achieving 1.1x-2.13x speedup over state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: The inherent heterogeneity and dynamicity of RL workflows lead to low hardware utilization and slow training on existing systems, creating a need for more flexible and efficient RL training systems.

Method: RLinf uses M2Flow paradigm to automatically break down high-level RL workflows at temporal and spatial dimensions, recomposing them into optimized execution flows with context switching, elastic pipelining, and profiling-guided scheduling.

Result: Extensive evaluations on reasoning RL and embodied RL tasks show RLinf consistently outperforms state-of-the-art systems with 1.1x-2.13x speedup in end-to-end training throughput.

Conclusion: RLinf demonstrates that addressing system flexibility through the M2Flow paradigm effectively improves RL training efficiency and hardware utilization.

Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing
artificial general intelligence, agentic intelligence, and embodied
intelligence. However, the inherent heterogeneity and dynamicity of RL
workflows often lead to low hardware utilization and slow training on existing
systems. In this paper, we present RLinf, a high-performance RL training system
based on our key observation that the major roadblock to efficient RL training
lies in system flexibility. To maximize flexibility and efficiency, RLinf is
built atop a novel RL system design paradigm called macro-to-micro flow
transformation (M2Flow), which automatically breaks down high-level,
easy-to-compose RL workflows at both the temporal and spatial dimensions, and
recomposes them into optimized execution flows. Supported by RLinf worker's
adaptive communication capability, we devise context switching and elastic
pipelining to realize M2Flow transformation, and a profiling-guided scheduling
policy to generate optimal execution plans. Extensive evaluations on both
reasoning RL and embodied RL tasks demonstrate that RLinf consistently
outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in
end-to-end training throughput.

</details>


### [72] [Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution](https://arxiv.org/abs/2509.15592)
*Jizhou Huang,Brendan Juba*

Main category: cs.LG

TL;DR: This paper proposes a Personalized Prediction scheme that learns easy-to-interpret sparse linear classifiers per query point, focusing on achieving competitive performance on sub-populations containing the query.


<details>
  <summary>Details</summary>
Motivation: Real-world machine learning applications require complex models for competitive performance, but this sacrifices interpretability. High-stakes applications like healthcare need methods for both accurate and explainable predictions.

Method: The paper develops a distribution-specific PAC-learning algorithm for learning reference classes for personalized prediction. It combines this with a list learner of sparse linear representations to create personalized sparse linear classifiers for sub-populations represented by halfspaces.

Result: The paper proves the first upper bound of O(opt^{1/4}) for personalized prediction with sparse linear classifiers and homogeneous halfspace subsets. The algorithms are evaluated on standard benchmark datasets.

Conclusion: The proposed personalized prediction scheme provides a framework for creating interpretable yet competitive predictors tailored to specific query points and their surrounding sub-populations, addressing the trade-off between accuracy and interpretability in high-stakes applications.

Abstract: In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.

</details>


### [73] [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](https://arxiv.org/abs/2509.15981)
*Yujie Zhu,Charles A. Hepburn,Matthew Thorpe,Giovanni Montana*

Main category: cs.LG

TL;DR: SPReD is a reinforcement learning framework that uses ensemble methods to determine when to imitate demonstrations vs follow the agent's own policy, applying continuous uncertainty-proportional regularization weights instead of binary decisions.


<details>
  <summary>Details</summary>
Motivation: In sparse reward RL, demonstrations accelerate learning but determining when to imitate them is challenging. Current methods make binary imitation decisions which can be suboptimal.

Method: Uses ensemble methods to model Q-value distributions for both demonstration and policy actions, quantifying uncertainty. Develops two uncertainty-aware approaches: probabilistic (estimating likelihood of demonstration superiority) and advantage-based (scaling imitation by statistical significance).

Result: Achieves up to 14x performance gains in complex robotics tasks compared to existing approaches, while maintaining robustness to demonstration quality and quantity.

Conclusion: SPReD's continuous, uncertainty-proportional regularization approach outperforms binary decision methods, providing more effective demonstration utilization in sparse reward RL settings.

Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate
learning, but determining when to imitate them remains challenging. We propose
Smooth Policy Regularisation from Demonstrations (SPReD), a framework that
addresses the fundamental question: when should an agent imitate a
demonstration versus follow its own policy? SPReD uses ensemble methods to
explicitly model Q-value distributions for both demonstration and policy
actions, quantifying uncertainty for comparisons. We develop two complementary
uncertainty-aware methods: a probabilistic approach estimating the likelihood
of demonstration superiority, and an advantage-based approach scaling imitation
by statistical significance. Unlike prevailing methods (e.g. Q-filter) that
make binary imitation decisions, SPReD applies continuous,
uncertainty-proportional regularisation weights, reducing gradient variance
during training. Despite its computational simplicity, SPReD achieves
remarkable gains in experiments across eight robotics tasks, outperforming
existing approaches by up to a factor of 14 in complex tasks while maintaining
robustness to demonstration quality and quantity. Our code is available at
https://github.com/YujieZhu7/SPReD.

</details>


### [74] [Efficient Extractive Text Summarization for Online News Articles Using Machine Learning](https://arxiv.org/abs/2509.15614)
*Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker*

Main category: cs.LG

TL;DR: This paper presents an extractive text summarization approach using machine learning techniques, particularly LSTM networks, which outperform baseline methods on the Cornell Newsroom dataset.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of efficient content management for online news articles through automated summarization to enhance accessibility and user engagement in the age of information overload.

Method: The authors used the Cornell Newsroom dataset (1.3M article-summary pairs) and developed a pipeline using BERT embeddings to transform text into numerical representations. They framed the task as binary classification and tested logistic regression, feed-forward neural networks, and LSTM networks.

Result: LSTM networks outperformed baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics, demonstrating superior ability to capture sequential dependencies in text.

Conclusion: The study highlights the potential of automated summarization, particularly LSTM-based approaches, for improving content management systems in online news platforms, enabling more efficient content organization and enhanced user experiences.

Abstract: In the age of information overload, content management for online news
articles relies on efficient summarization to enhance accessibility and user
engagement. This article addresses the challenge of extractive text
summarization by employing advanced machine learning techniques to generate
concise and coherent summaries while preserving the original meaning. Using the
Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we
developed a pipeline leveraging BERT embeddings to transform textual data into
numerical representations. By framing the task as a binary classification
problem, we explored various models, including logistic regression,
feed-forward neural networks, and long short-term memory (LSTM) networks. Our
findings demonstrate that LSTM networks, with their ability to capture
sequential dependencies, outperform baseline methods like Lede-3 and simpler
models in F1 score and ROUGE-1 metrics. This study underscores the potential of
automated summarization in improving content management systems for online news
platforms, enabling more efficient content organization and enhanced user
experiences.

</details>


### [75] [EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions](https://arxiv.org/abs/2509.15986)
*Xinchen Wan,Jinhua Liang,Huan Zhang*

Main category: cs.LG

TL;DR: EmoHeal is an AI-powered digital mental wellness system that uses fine-grained emotion detection and music therapy principles to deliver personalized supportive narratives, showing significant mood improvement in user studies.


<details>
  <summary>Details</summary>
Motivation: Existing digital mental wellness tools are static and one-size-fits-all, failing to address nuanced emotional states like pre-sleep anxiety that affects over 1.5 billion people worldwide.

Method: EmoHeal uses a fine-tuned XLM-RoBERTa model to detect 27 fine-grained emotions from user text, maps them to musical parameters via a knowledge graph based on music therapy principles (GEMS, iso-principle), and retrieves audiovisual content using CLAMP3 model to guide users through a "match-guide-target" approach.

Result: A within-subjects study (N=40) showed significant mood improvement (M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05, p<0.001), with strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p<0.001).

Conclusion: The findings establish the viability of theory-driven, emotion-aware digital wellness tools and provide a scalable AI blueprint for operationalizing music therapy principles.

Abstract: Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.

</details>


### [76] [Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning](https://arxiv.org/abs/2509.16068)
*Yuchen Ye,Hong Liang,Chaoxia Yuan,Mingyu Li,Aoqi Zhou,Chunqing Shang,Hua Cai,Peixi Liu,Kezuan Wang,Yifeng Zheng*

Main category: cs.LG

TL;DR: G-WindCast is a deep learning framework that uses 5G GNSS signal strength variations to retrieve and forecast 3D atmospheric wind fields with promising accuracy comparable to NWP models.


<details>
  <summary>Details</summary>
Motivation: Obtaining high-resolution wind data is challenging due to limitations in traditional observation methods and the computational expense of NWP models. There's a need for cost-effective, scalable solutions for atmospheric monitoring.

Method: Uses Forward Neural Networks and Transformer networks to capture complex spatiotemporal relationships between GNSS-derived features and wind dynamics from 5G signal strength variations.

Result: Achieves promising accuracy in wind retrieval and short-term forecasting (up to 30 minutes), with performance comparable to high-resolution NWP outputs. Maintains excellent performance even with reduced GNSS stations (~100), showing robustness across forecast horizons and pressure levels.

Conclusion: This interdisciplinary approach demonstrates the transformative potential of using non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.

Abstract: Accurate atmospheric wind field information is crucial for various
applications, including weather forecasting, aviation safety, and disaster risk
reduction. However, obtaining high spatiotemporal resolution wind data remains
challenging due to limitations in traditional in-situ observations and remote
sensing techniques, as well as the computational expense and biases of
numerical weather prediction (NWP) models. This paper introduces G-WindCast, a
novel deep learning framework that leverages signal strength variations from 5G
Global Navigation Satellite System (GNSS) signals to retrieve and forecast
three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward
Neural Networks (FNN) and Transformer networks to capture complex, nonlinear,
and spatiotemporal relationships between GNSS-derived features and wind
dynamics. Our preliminary results demonstrate promising accuracy in both wind
retrieval and short-term wind forecasting (up to 30 minutes lead time), with
skill scores comparable to high-resolution NWP outputs in certain scenarios.
The model exhibits robustness across different forecast horizons and pressure
levels, and its predictions for wind speed and direction show superior
agreement with observations compared to concurrent ERA5 reanalysis data.
Furthermore, we show that the system can maintain excellent performance for
localized forecasting even with a significantly reduced number of GNSS stations
(e.g., around 100), highlighting its cost-effectiveness and scalability. This
interdisciplinary approach underscores the transformative potential of
exploiting non-traditional data sources and deep learning for advanced
environmental monitoring and real-time atmospheric applications.

</details>


### [77] [DiffusionNFT: Online Diffusion Reinforcement with Forward Process](https://arxiv.org/abs/2509.16117)
*Kaiwen Zheng,Huayu Chen,Haotian Ye,Haoxiang Wang,Qinsheng Zhang,Kai Jiang,Hang Su,Stefano Ermon,Jun Zhu,Ming-Yu Liu*

Main category: cs.LG

TL;DR: DiffusionNFT is a new online RL method for diffusion models that optimizes directly on the forward process via flow matching, avoiding limitations of previous discretization approaches and achieving 25x efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for diffusion models face challenges including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance, motivating a more direct optimization approach.

Method: DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, incorporating reinforcement signals into supervised learning via flow matching on the forward process without needing likelihood estimation.

Result: DiffusionNFT is up to 25x more efficient than FlowGRPO, improves GenEval score from 0.24 to 0.98 within 1k steps, and significantly boosts SD3.5-Medium performance across benchmarks without requiring CFG.

Conclusion: The method provides an efficient, CFG-free RL paradigm for diffusion models that works with arbitrary solvers and requires only clean images rather than sampling trajectories.

Abstract: Online reinforcement learning (RL) has been central to post-training language
models, but its extension to diffusion models remains challenging due to
intractable likelihoods. Recent works discretize the reverse sampling process
to enable GRPO-style training, yet they inherit fundamental drawbacks,
including solver restrictions, forward-reverse inconsistency, and complicated
integration with classifier-free guidance (CFG). We introduce Diffusion
Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that
optimizes diffusion models directly on the forward process via flow matching.
DiffusionNFT contrasts positive and negative generations to define an implicit
policy improvement direction, naturally incorporating reinforcement signals
into the supervised learning objective. This formulation enables training with
arbitrary black-box solvers, eliminates the need for likelihood estimation, and
requires only clean images rather than sampling trajectories for policy
optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in
head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT
improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO
achieves 0.95 with over 5k steps and additional CFG employment. By leveraging
multiple reward models, DiffusionNFT significantly boosts the performance of
SD3.5-Medium in every benchmark tested.

</details>


### [78] [Nonconvex Regularization for Feature Selection in Reinforcement Learning](https://arxiv.org/abs/2509.15652)
*Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: Proposes an efficient batch algorithm for feature selection in RL with theoretical convergence guarantees, using a nonconvex PMC penalty to mitigate estimation bias in policy evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the estimation bias inherent in conventional regularization schemes for feature selection in reinforcement learning, particularly in scenarios with many noisy features.

Method: Extends policy evaluation within the LSTD framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Uses forward-reflected-backward splitting (FRBS) algorithm with novel convergence conditions.

Result: Numerical experiments on benchmark datasets show the proposed approach substantially outperforms state-of-the-art feature-selection methods, especially when dealing with many noisy features.

Conclusion: The proposed method provides an effective solution for feature selection in RL with theoretical guarantees and superior performance compared to existing methods.

Abstract: This work proposes an efficient batch algorithm for feature selection in
reinforcement learning (RL) with theoretical convergence guarantees. To
mitigate the estimation bias inherent in conventional regularization schemes,
the first contribution extends policy evaluation within the classical
least-squares temporal-difference (LSTD) framework by formulating a
Bellman-residual objective regularized with the sparsity-inducing, nonconvex
projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC
penalty, this formulation can be interpreted as a special instance of a general
nonmonotone-inclusion problem. The second contribution establishes novel
convergence conditions for the forward-reflected-backward splitting (FRBS)
algorithm to solve this class of problems. Numerical experiments on benchmark
datasets demonstrate that the proposed approach substantially outperforms
state-of-the-art feature-selection methods, particularly in scenarios with many
noisy features.

</details>


### [79] [Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers](https://arxiv.org/abs/2509.16126)
*Janayna M. Fernandes,Robinson Sabino-Silva,Murillo G. Carneiro*

Main category: cs.LG

TL;DR: GANet, a genetic algorithm-based network optimization framework using PageRank and Degree metrics, achieves superior ASD detection from salivary FTIR spectroscopy data compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Autism Spectrum Disorder lacks reliable biological markers for early diagnosis, creating a need for non-invasive, precise detection tools.

Method: Developed GANet framework that uses genetic algorithms to optimize network structure with PageRank and Degree metrics for feature characterization from 159 salivary samples analyzed by ATR-FTIR spectroscopy.

Result: GANet achieved 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and 0.74 harmonic mean, outperforming linear discriminant analysis, support vector machines, and deep learning models.

Conclusion: GANet demonstrates potential as a robust, bio-inspired, non-invasive tool for precise ASD detection and broader spectral-based health applications.

Abstract: Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying
early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,
we developed GANet, a genetic algorithm-based network optimization framework
leveraging PageRank and Degree for importance-based feature characterization.
GANet systematically optimizes network structure to extract meaningful patterns
from high-dimensional spectral data. It achieved superior performance compared
to linear discriminant analysis, support vector machines, and deep learning
models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74
harmonic mean. These results demonstrate GANet's potential as a robust,
bio-inspired, non-invasive tool for precise ASD detection and broader
spectral-based health applications.

</details>


### [80] [RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation](https://arxiv.org/abs/2509.15724)
*Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: RMT-KD is a compression method using Random Matrix Theory for knowledge distillation to reduce large deep learning models' size while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large models like BERT and ResNet are costly to deploy at the edge due to their size and compute demands, requiring efficient compression methods.

Method: Uses RMT-based causal reduction layer by layer with self-distillation, preserving only informative directions identified via spectral properties of hidden representations instead of pruning or heuristic rank selection.

Result: Achieves up to 80% parameter reduction with only 2% accuracy loss on GLUE, AG News, and CIFAR-10, delivering 2.8x faster inference and nearly halved power consumption.

Conclusion: RMT-KD establishes itself as a mathematically grounded approach to network distillation that effectively balances model compression and performance.

Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art
performance but are costly to deploy at the edge due to their size and compute
demands. We present RMT-KD, a compression method that leverages Random Matrix
Theory (RMT) for knowledge distillation to iteratively reduce network size.
Instead of pruning or heuristic rank selection, RMT-KD preserves only
informative directions identified via the spectral properties of hidden
representations. RMT-based causal reduction is applied layer by layer with
self-distillation to maintain stability and accuracy. On GLUE, AG News, and
CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy
loss, delivering 2.8x faster inference and nearly halved power consumption.
These results establish RMT-KD as a mathematically grounded approach to network
distillation.

</details>


### [81] [EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs](https://arxiv.org/abs/2509.15735)
*Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: EigenTrack is an interpretable real-time detector that uses spectral geometry of hidden activations to detect hallucinations and out-of-distribution errors in LLMs before surface errors appear.


<details>
  <summary>Details</summary>
Motivation: Large language models are prone to hallucination and out-of-distribution errors, but existing detection methods have limitations - black/grey-box methods require resampling, while white-box methods lack temporal context and global signal aggregation.

Method: Uses spectral geometry of hidden activations by streaming covariance-spectrum statistics (entropy, eigenvalue gaps, KL divergence) into a lightweight recurrent classifier to track temporal shifts in representation structure.

Result: EigenTrack can detect hallucinations and OOD drift before surface errors appear, requires only a single forward pass without resampling, preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs.

Conclusion: EigenTrack provides an effective interpretable real-time detection method for LLM hallucinations and OOD errors that overcomes limitations of existing approaches.

Abstract: Large language models (LLMs) offer broad utility but remain prone to
hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an
interpretable real-time detector that uses the spectral geometry of hidden
activations, a compact global signature of model dynamics. By streaming
covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL
divergence from random baselines into a lightweight recurrent classifier,
EigenTrack tracks temporal shifts in representation structure that signal
hallucination and OOD drift before surface errors appear. Unlike black- and
grey-box methods, it needs only a single forward pass without resampling.
Unlike existing white-box detectors, it preserves temporal context, aggregates
global signals, and offers interpretable accuracy-latency trade-offs.

</details>


### [82] [Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks](https://arxiv.org/abs/2509.15736)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun*

Main category: cs.LG

TL;DR: This paper investigates methods to incorporate engine ageing effects into fuel-flow prediction models for Airbus A320-214 aircraft, showing that age-dependent corrections and neural networks significantly improve accuracy over baseline models.


<details>
  <summary>Details</summary>
Motivation: Standard parametric fuel-flow models neglect performance deterioration that occurs as aircraft age, which is crucial for accurate operational planning and environmental impact assessment.

Method: Evaluated multiple approaches including classical physics-based models, empirical correction coefficients, and neural network architectures that incorporate age as an input feature or multiplicative bias, using a dataset of ~19,000 flights from nine airframes with varying service years.

Result: Baseline models consistently underestimate fuel consumption for older aircraft, but age-dependent correction factors and neural models substantially reduce bias and improve prediction accuracy.

Conclusion: Accounting for ageing effects is essential in fuel-flow models to improve reliability of operational and environmental assessments, though limitations exist due to small airframe sample size and lack of maintenance records, highlighting the need for more diverse datasets.

Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational
planning and environmental impact assessment, yet standard parametric models
often neglect performance deterioration that occurs as aircraft age. This paper
investigates multiple approaches to integrate engine ageing effects into
fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of
approximately nineteen thousand Quick Access Recorder flights from nine
distinct airframes with varying years in service. We systematically evaluate
classical physics-based models, empirical correction coefficients, and
data-driven neural network architectures that incorporate age either as an
input feature or as an explicit multiplicative bias. Results demonstrate that
while baseline models consistently underestimate fuel consumption for older
aircraft, the use of age-dependent correction factors and neural models
substantially reduces bias and improves prediction accuracy. Nevertheless,
limitations arise from the small number of airframes and the lack of detailed
maintenance event records, which constrain the representativeness and
generalization of age-based corrections. This study emphasizes the importance
of accounting for the effects of ageing in parametric and machine learning
frameworks to improve the reliability of operational and environmental
assessments. The study also highlights the need for more diverse datasets that
can capture the complexity of real-world engine deterioration.

</details>


### [83] [GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning](https://arxiv.org/abs/2509.15738)
*Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li*

Main category: cs.LG

TL;DR: GUI-ReWalk is a reasoning-enhanced framework for synthesizing realistic and diverse GUI trajectory data to address the scarcity of high-quality training data for GUI agents.


<details>
  <summary>Details</summary>
Motivation: Current GUI agent development is constrained by limited scalable trajectory data, with existing methods being either costly manual annotations or synthetic generation that sacrifices diversity for task coverage.

Method: GUI-ReWalk uses a multi-stage approach: stochastic exploration phase mimicking human trial-and-error, followed by reasoning-guided phase where inferred goals drive purposeful interactions. It supports multi-stride task generation for long-horizon workflows across applications.

Result: Trained Qwen2.5-VL-7B on GUI-ReWalk dataset and evaluated across multiple benchmarks, showing superior coverage of interaction flows, higher trajectory entropy, and more realistic user intent compared to existing methods.

Conclusion: GUI-ReWalk establishes a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation by combining randomness for diversity with goal-aware reasoning.

Abstract: Graphical User Interface (GUI) Agents, powered by large language and
vision-language models, hold promise for enabling end-to-end automation in
digital environments. However, their progress is fundamentally constrained by
the scarcity of scalable, high-quality trajectory data. Existing data
collection strategies either rely on costly and inconsistent manual annotations
or on synthetic generation methods that trade off between diversity and
meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a
reasoning-enhanced, multi-stage framework for synthesizing realistic and
diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase
that emulates human trial-and-error behaviors, and progressively transitions
into a reasoning-guided phase where inferred goals drive coherent and
purposeful interactions. Moreover, it supports multi-stride task generation,
enabling the construction of long-horizon workflows across multiple
applications. By combining randomness for diversity with goal-aware reasoning
for structure, GUI-ReWalk produces data that better reflects the intent-aware,
adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B
on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including
Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results
demonstrate that GUI-ReWalk enables superior coverage of diverse interaction
flows, higher trajectory entropy, and more realistic user intent. These
findings establish GUI-ReWalk as a scalable and data-efficient framework for
advancing GUI agent research and enabling robust real-world automation.

</details>


### [84] [Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets](https://arxiv.org/abs/2509.15740)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: iFSNet is an online incremental learning model for battery prognosis that uses pseudo targets to enable multistep forecasting without waiting for large streaming data batches, achieving low error rates on both smooth and irregular degradation trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing ML models for battery prognosis require offline retraining when encountering new data distributions, creating delays and inefficiencies. There's a need for online approaches that can adapt incrementally to varying distributions without extensive retraining periods.

Method: Proposed iFSNet, a modified version of FSNet that operates in single-pass mode using pseudo targets. It uses linear regression on input sequences to extrapolate pseudo future samples, calculates loss from forecast errors, and continuously updates the model while benefiting from FSNet's associative memory and adaptive structure mechanisms.

Result: The model achieved 0.00197 RMSE and 0.00154 MAE on datasets with smooth degradation trajectories, and 0.01588 RMSE and 0.01234 MAE on datasets with irregular degradation trajectories containing capacity regeneration spikes.

Conclusion: iFSNet successfully addresses the challenge of online incremental multistep forecasting for battery prognosis by using pseudo targets, enabling continuous model improvement without waiting for large data batches, and demonstrating strong performance across different degradation patterns.

Abstract: Data-driven models accurately perform early battery prognosis to prevent
equipment failure and further safety hazards. Most existing machine learning
(ML) models work in offline mode which must consider their retraining
post-deployment every time new data distribution is encountered. Hence, there
is a need for an online ML approach where the model can adapt to varying
distributions. However, existing online incremental multistep forecasts are a
great challenge as there is no way to correct the model of its forecasts at the
current instance. Also, these methods need to wait for a considerable amount of
time to acquire enough streaming data before retraining. In this study, we
propose iFSNet (incremental Fast and Slow learning Network) which is a modified
version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep
forecasting using pseudo targets. It uses a simple linear regressor of the
input sequence to extrapolate pseudo future samples (pseudo targets) and
calculate the loss from the rest of the forecast and keep updating the model.
The model benefits from the associative memory and adaptive structure
mechanisms of FSNet, at the same time the model incrementally improves by using
pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on
datasets with smooth degradation trajectories while it achieved 0.01588 RMSE
and 0.01234 MAE on datasets having irregular degradation trajectories with
capacity regeneration spikes.

</details>


### [85] [Learning to Optimize Capacity Planning in Semiconductor Manufacturing](https://arxiv.org/abs/2509.15767)
*Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang*

Main category: cs.LG

TL;DR: A neural network-based model using deep reinforcement learning and heterogeneous graph neural networks for capacity planning in semiconductor manufacturing, achieving 1.8% improvements in throughput and cycle time.


<details>
  <summary>Details</summary>
Motivation: Current heuristic-based capacity planning methods in semiconductor manufacturing cannot effectively account for complex interactions along the process flow that lead to bottlenecks, despite offering interpretability.

Method: Uses deep reinforcement learning with a heterogeneous graph neural network to represent policies, capturing diverse relationships among machines and processing steps for proactive decision-making. Includes scalability measures for handling large action spaces.

Result: Evaluation on Intel's Minifab model and SMT2020 testbed shows the trained policy increases throughput and decreases cycle time by about 1.8% each in the largest tested scenario.

Conclusion: The neural network-based approach demonstrates effectiveness in semiconductor capacity planning by better handling complex process interactions compared to traditional heuristic methods.

Abstract: In manufacturing, capacity planning is the process of allocating production
resources in accordance with variable demand. The current industry practice in
semiconductor manufacturing typically applies heuristic rules to prioritize
actions, such as future change lists that account for incoming machine and
recipe dedications. However, while offering interpretability, heuristics cannot
easily account for the complex interactions along the process flow that can
gradually lead to the formation of bottlenecks. Here, we present a neural
network-based model for capacity planning on the level of individual machines,
trained using deep reinforcement learning. By representing the policy using a
heterogeneous graph neural network, the model directly captures the diverse
relationships among machines and processing steps, allowing for proactive
decision-making. We describe several measures taken to achieve sufficient
scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and
preliminary experiments using the popular SMT2020 testbed. In the largest
tested scenario, our trained policy increases throughput and decreases cycle
time by about 1.8% each.

</details>


### [86] [Generalization and Optimization of SGD with Lookahead](https://arxiv.org/abs/2509.15776)
*Kangcheng Li,Yunwen Lei*

Main category: cs.LG

TL;DR: This paper provides a rigorous stability and generalization analysis of the Lookahead optimizer with minibatch SGD, addressing limitations of previous theoretical studies that focused mainly on convergence and had restrictive assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical analyses of Lookahead optimizer primarily focus on convergence on training data, with generalization capabilities remaining poorly understood. Current generalization analyses are limited by restrictive assumptions like global Lipschitz continuity and fail to fully capture the optimization-generalization relationship.

Method: The authors leverage on-average model stability to derive generalization bounds for Lookahead optimizer with minibatch SGD, analyzing both convex and strongly convex problems without the restrictive Lipschitzness assumption.

Result: The analysis demonstrates a linear speedup with respect to batch size in the convex setting, providing improved generalization bounds that better capture the relationship between optimization and generalization.

Conclusion: This work establishes rigorous generalization guarantees for Lookahead optimizer, overcoming limitations of previous analyses and providing theoretical insights into its generalization behavior across different problem settings.

Abstract: The Lookahead optimizer enhances deep learning models by employing a
dual-weight update mechanism, which has been shown to improve the performance
of underlying optimizers such as SGD. However, most theoretical studies focus
on its convergence on training data, leaving its generalization capabilities
less understood. Existing generalization analyses are often limited by
restrictive assumptions, such as requiring the loss function to be globally
Lipschitz continuous, and their bounds do not fully capture the relationship
between optimization and generalization. In this paper, we address these issues
by conducting a rigorous stability and generalization analysis of the Lookahead
optimizer with minibatch SGD. We leverage on-average model stability to derive
generalization bounds for both convex and strongly convex problems without the
restrictive Lipschitzness assumption. Our analysis demonstrates a linear
speedup with respect to the batch size in the convex setting.

</details>


### [87] [ThermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks](https://arxiv.org/abs/2509.15815)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Zhenyu Chen*

Main category: cs.LG

TL;DR: ThermalGuardian is a testing method for automotive deep learning frameworks that addresses quality issues caused by temperature variations in vehicular environments.


<details>
  <summary>Details</summary>
Motivation: Automotive deep learning frameworks are deployed in temperature-varying environments (-40°C to 50°C) which cause GPU frequency fluctuations, leading to quality issues that existing testing methods cannot detect.

Method: ThermalGuardian generates test input models using model mutation rules for temperature-sensitive operators, simulates GPU temperature fluctuations based on Newton's law of cooling, and controls GPU frequency based on real-time GPU temperature.

Result: The method can detect critical quality issues including delays/errors in compute-intensive operators, precision errors in high/mixed-precision operators, and synchronization issues in time-series operators.

Conclusion: ThermalGuardian is the first automotive deep learning framework testing method designed specifically for temperature-varying environments, addressing a critical gap in existing testing approaches.

Abstract: Deep learning models play a vital role in autonomous driving systems,
supporting critical functions such as environmental perception. To accelerate
model inference, these deep learning models' deployment relies on automotive
deep learning frameworks, for example, PaddleInference in Apollo and TensorRT
in AutoWare. However, unlike deploying deep learning models on the cloud,
vehicular environments experience extreme ambient temperatures varying from
-40{\deg}C to 50{\deg}C, significantly impacting GPU temperature. Additionally,
heats generated when computing further lead to the GPU temperature increase.
These temperature fluctuations lead to dynamic GPU frequency adjustments
through mechanisms such as DVFS. However, automotive deep learning frameworks
are designed without considering the impact of temperature-induced frequency
variations. When deployed on temperature-varying GPUs, these frameworks suffer
critical quality issues: compute-intensive operators face delays or errors,
high/mixed-precision operators suffer from precision errors, and time-series
operators suffer from synchronization issues. The above quality issues cannot
be detected by existing deep learning framework testing methods because they
ignore temperature's effect on the deep learning framework quality. To bridge
this gap, we propose ThermalGuardian, the first automotive deep learning
framework testing method under temperature-varying environments. Specifically,
ThermalGuardian generates test input models using model mutation rules
targeting temperature-sensitive operators, simulates GPU temperature
fluctuations based on Newton's law of cooling, and controls GPU frequency based
on real-time GPU temperature.

</details>


### [88] [On the Convergence of Muon and Beyond](https://arxiv.org/abs/2509.15816)
*Da Chang,Yongxiang Liu,Ganzhao Yuan*

Main category: cs.LG

TL;DR: This paper presents Muon-VR2, a variance-reduced variant of the Muon optimizer that achieves optimal convergence rate of Õ(T^{-1/3}) for stochastic non-convex optimization, matching the theoretical lower bound.


<details>
  <summary>Details</summary>
Motivation: There's a significant gap between Muon optimizer's practical success and theoretical understanding, with existing analyses showing only suboptimal convergence rates. The authors aim to explore the theoretical limits of the Muon framework.

Method: The authors construct and analyze Muon-VR2, which incorporates a variance-reduction mechanism into the Muon framework. They provide rigorous convergence proofs under both general non-convex settings and the Polyak-Łojasiewicz condition.

Result: Muon-VR2 achieves optimal convergence rate of Õ(T^{-1/3}), matching the theoretical lower bound. Extensive experiments on CIFAR-10 and C4 benchmarks confirm the theoretical findings on per-iteration convergence.

Conclusion: This work provides the first proof of optimality for a Muon-style optimizer and clarifies the path toward developing more practically efficient, accelerated variants.

Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling
matrix-structured parameters for training neural networks. However, a
significant gap persists between its practical performance and theoretical
understanding. Existing analyses indicate that the standard Muon variant
achieves only a suboptimal convergence rate of $\mathcal{O}(T^{-1/4})$ in
stochastic non-convex settings, where $T$ denotes the number of iterations. To
explore the theoretical limits of the Muon framework, we construct and analyze
a variance-reduced variant, termed Muon-VR2. We provide the first rigorous
proof that incorporating a variance-reduction mechanism enables Muon-VR2 to
attain an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby
matching the theoretical lower bound for this class of problems. Moreover, our
analysis establishes convergence guarantees for Muon variants under the
Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision
(CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on
per-iteration convergence. Overall, this work provides the first proof of
optimality for a Muon-style optimizer and clarifies the path toward developing
more practically efficient, accelerated variants.

</details>


### [89] [SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors](https://arxiv.org/abs/2509.15827)
*Baptiste Schubnel,Jelena Simeunović,Corentin Tissier,Pierre-Jean Alet,Rafael E. Carrillo*

Main category: cs.LG

TL;DR: SolarCrossFormer is a novel deep learning model that combines satellite images and ground-based meteorological data for high-resolution day-ahead solar irradiance forecasting using graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Current solar irradiance forecasting solutions lack the temporal and spatial resolution required by power grid operators for large-scale integration of solar PV systems.

Method: SolarCrossFormer uses graph neural networks to exploit inter- and intra-modal correlations between satellite images and time series from meteorological stations, generating probabilistic forecasts with 15-minute resolution for 24-hour horizons.

Result: The model achieves a normalized mean absolute error of 6.1% over a one-year dataset across 127 locations in Switzerland, with competitive performance against commercial numerical weather prediction services.

Conclusion: SolarCrossFormer provides robust, high-resolution forecasting that can incorporate new data without retraining and produce forecasts for locations without input data, making it suitable for real-life power grid operations.

Abstract: Accurate day-ahead forecasts of solar irradiance are required for the
large-scale integration of solar photovoltaic (PV) systems into the power grid.
However, current forecasting solutions lack the temporal and spatial resolution
required by system operators. In this paper, we introduce SolarCrossFormer, a
novel deep learning model for day-ahead irradiance forecasting, that combines
satellite images and time series from a ground-based network of meteorological
stations. SolarCrossFormer uses novel graph neural networks to exploit the
inter- and intra-modal correlations of the input data and improve the accuracy
and resolution of the forecasts. It generates probabilistic forecasts for any
location in Switzerland with a 15-minute resolution for horizons up to 24 hours
ahead. One of the key advantages of SolarCrossFormer its robustness in real
life operations. It can incorporate new time-series data without retraining the
model and, additionally, it can produce forecasts for locations without input
data by using only their coordinates. Experimental results over a dataset of
one year and 127 locations across Switzerland show that SolarCrossFormer yield
a normalized mean absolute error of 6.1 % over the forecasting horizon. The
results are competitive with those achieved by a commercial numerical weather
prediction service.

</details>


### [90] [HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs](https://arxiv.org/abs/2509.15828)
*Ning Xu,Junkai Zhang,Yang Wu,Huigen Ye,Hua Xu,Huiling Xu,Yifan Zhang*

Main category: cs.LG

TL;DR: HyP-ASO is a hybrid framework combining formula-based variable selection with RL-based neighborhood size prediction to accelerate large-scale Integer Linear Program solving.


<details>
  <summary>Details</summary>
Motivation: Traditional ILP solvers are slow for large-scale problems, and existing LNS frameworks struggle with generating effective neighborhoods efficiently.

Method: Combines a customized formula using feasible solutions to calculate variable selection probabilities with a deep RL policy network that predicts optimal neighborhood size.

Result: Significantly outperforms existing LNS-based approaches for large-scale ILPs, showing lightweight and highly scalable performance.

Conclusion: HyP-ASO provides an effective and scalable solution for accelerating large-scale ILP solving through hybrid policy-based adaptive search optimization.

Abstract: Directly solving large-scale Integer Linear Programs (ILPs) using traditional
solvers is slow due to their NP-hard nature. While recent frameworks based on
Large Neighborhood Search (LNS) can accelerate the solving process, their
performance is often constrained by the difficulty in generating sufficiently
effective neighborhoods. To address this challenge, we propose HyP-ASO, a
hybrid policy-based adaptive search optimization framework that combines a
customized formula with deep Reinforcement Learning (RL). The formula leverages
feasible solutions to calculate the selection probabilities for each variable
in the neighborhood generation process, and the RL policy network predicts the
neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly
outperforms existing LNS-based approaches for large-scale ILPs. Additional
experiments show it is lightweight and highly scalable, making it well-suited
for solving large-scale ILPs.

</details>


### [91] [Tsururu: A Python-based Time Series Forecasting Strategies Library](https://arxiv.org/abs/2509.15843)
*Alina Kostromina,Kseniia Kuvshinova,Aleksandr Yugay,Andrey Savchenko,Dmitry Simakov*

Main category: cs.LG

TL;DR: Tsururu is a Python library that addresses the gap in time series forecasting by providing flexible training approaches, combining global and multivariate methods with multi-step-ahead strategies, and enabling integration with various forecasting models.


<details>
  <summary>Details</summary>
Motivation: Current time series research focuses on developing new models but neglects optimal training approach selection, creating a gap between state-of-the-art research and practical industry applications.

Method: Developed Tsururu library that enables flexible combinations of global and multivariate approaches, multi-step-ahead forecasting strategies, and seamless integration with various forecasting models.

Result: Created an open-source Python library available at https://github.com/sb-ai-lab/tsururu that bridges research and industry applications.

Conclusion: Tsururu successfully addresses the underexplored area of training approach selection in time series forecasting, providing a practical tool that combines advanced research methodologies with industry-ready applications.

Abstract: While current time series research focuses on developing new models, crucial
questions of selecting an optimal approach for training such models are
underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA
research and industry by enabling flexible combinations of global and
multivariate approaches and multi-step-ahead forecasting strategies. It also
enables seamless integration with various forecasting models. Available at
https://github.com/sb-ai-lab/tsururu .

</details>


### [92] [FedHK-MVFC: Federated Heat Kernel Multi-View Clustering](https://arxiv.org/abs/2509.15844)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: A framework combining quantum field theory with federated learning for multi-view clustering in healthcare, using heat-kernel coefficients to create geometry-aware similarity measures with privacy-preserving protocols.


<details>
  <summary>Details</summary>
Motivation: To enable collaborative analysis of sensitive medical data across hospitals while maintaining privacy compliance (HIPAA) and capturing complex data structures through geometry-aware methods.

Method: Developed Heat Kernel Distance (HKD) transformation with convergence guarantees. Two algorithms: HK-MVFC for centralized analysis and FedHK-MVFC for federated learning using differential privacy and secure aggregation.

Result: 8-12% increase in clustering accuracy, 70% reduced communication, and 98.2% efficiency retention over centralized methods on cardiovascular patient datasets. Validated on 10,000 patient records across two hospitals.

Conclusion: Establishes a new standard for geometry-aware federated learning in healthcare, providing rigorous mathematical foundations with practical clinical applications while ensuring privacy and data security.

Abstract: In the realm of distributed AI and privacy-focused medical applications, we
propose a framework for multi-view clustering that links quantum field theory
with federated healthcare analytics. Our method uses heat-kernel coefficients
from spectral analysis to convert Euclidean distances into geometry-aware
similarity measures, capturing the structure of diverse medical data. We lay
this out through the Heat Kernel Distance (HKD) transformation with convergence
guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy
Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View
Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across
hospitals using differential privacy and secure aggregation to facilitate
HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular
patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced
communication, and $98.2 \%$ efficiency retention over centralized methods.
Validated on 10,000 patient records across two hospitals, it proves useful for
collaborative phenotyping involving ECG, cardiac imaging, and behavioral data.
Our theoretical contributions include update rules with proven convergence,
adaptive view weighting, and privacy-preserving protocols. This presents a new
standard for geometry-aware federated learning in healthcare, turning advanced
math into workable solutions for analyzing sensitive medical data while
ensuring both rigor and clinical relevance.

</details>


### [93] [Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data](https://arxiv.org/abs/2509.15859)
*Nakul Sharma*

Main category: cs.LG

TL;DR: A novel framework that uses Vision Foundation Models' latent space to generate synthetic data for training linear classifiers on imbalanced datasets, achieving state-of-the-art performance with high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning approaches for foundation models on imbalanced datasets are computationally expensive and fail to match performance of models trained on balanced datasets, highlighting the need for more efficient and effective solutions.

Method: Leverages rich semantic latent space of Vision Foundation Models to generate synthetic data, then trains a simple linear classifier using a mixture of real and synthetic data for long-tail classification, reducing trainable parameters to just those in the linear model.

Result: Sets new state-of-the-art on CIFAR-100-LT benchmark and demonstrates strong performance on Places-LT benchmark, showing effectiveness and adaptability of the approach.

Conclusion: The proposed simple and computationally efficient framework effectively addresses long-tail classification challenges by combining synthetic data generation from foundation models with linear classifiers, achieving superior performance with minimal trainable parameters.

Abstract: Imbalanced classification datasets pose significant challenges in machine
learning, often leading to biased models that perform poorly on
underrepresented classes. With the rise of foundation models, recent research
has focused on the full, partial, and parameter-efficient fine-tuning of these
models to deal with long-tail classification. Despite the impressive
performance of these works on the benchmark datasets, they still fail to close
the gap with the networks trained using the balanced datasets and still require
substantial computational resources, even for relatively smaller datasets.
Underscoring the importance of computational efficiency and simplicity, in this
work we propose a novel framework that leverages the rich semantic latent space
of Vision Foundation Models to generate synthetic data and train a simple
linear classifier using a mixture of real and synthetic data for long-tail
classification. The computational efficiency gain arises from the number of
trainable parameters that are reduced to just the number of parameters in the
linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT
benchmark and demonstrates strong performance on the Places-LT benchmark,
highlighting the effectiveness and adaptability of our simple and effective
approach.

</details>


### [94] [ToFU: Transforming How Federated Learning Systems Forget User Data](https://arxiv.org/abs/2509.15861)
*Van-Tuan Tran,Hong-Hanh Nguyen-Le,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: ToFU is a transformation-guided federated unlearning framework that incorporates transformations during learning to reduce memorization, enabling more efficient data removal from federated learning models.


<details>
  <summary>Details</summary>
Motivation: Current federated unlearning methods struggle to efficiently erase deeply memorized information from neural networks, requiring a paradigm shift from post-hoc unlearning to designing FL systems inherently amenable to forgetting.

Method: Proposes a learning-to-unlearn framework that uses transformation composition during the learning process to bound instance-specific information, making subsequent unlearning simpler. It works as a plug-and-play framework compatible with existing FU methods.

Result: Experiments on CIFAR-10, CIFAR-100, and MUFAC benchmark show ToFU outperforms existing FU baselines, enhances performance when integrated with current methods, and reduces unlearning time.

Conclusion: ToFU provides an effective approach to federated unlearning by reducing memorization through transformation-guided learning, offering both standalone improvements and compatibility with existing unlearning methods.

Abstract: Neural networks unintentionally memorize training data, creating privacy
risks in federated learning (FL) systems, such as inference and reconstruction
attacks on sensitive data. To mitigate these risks and to comply with privacy
regulations, Federated Unlearning (FU) has been introduced to enable
participants in FL systems to remove their data's influence from the global
model. However, current FU methods primarily act post-hoc, struggling to
efficiently erase information deeply memorized by neural networks. We argue
that effective unlearning necessitates a paradigm shift: designing FL systems
inherently amenable to forgetting. To this end, we propose a
learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework
that incorporates transformations during the learning process to reduce
memorization of specific instances. Our theoretical analysis reveals how
transformation composition provably bounds instance-specific information,
directly simplifying subsequent unlearning. Crucially, ToFU can work as a
plug-and-play framework that improves the performance of existing FU methods.
Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU
outperforms existing FU baselines, enhances performance when integrated with
current methods, and reduces unlearning time.

</details>


### [95] [SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion](https://arxiv.org/abs/2509.15865)
*Haoran Zhao,Tong Bai,Lei Huang,Xiaoyu Liang*

Main category: cs.LG

TL;DR: SAGE is a semantic-aware shared sampling framework that reduces diffusion model sampling cost by 25.5% while improving generation quality, by sharing early-stage sampling across semantically similar queries.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have high sampling costs requiring dozens of sequential model evaluations, which remains a major limitation. Prior acceleration methods treat each query independently.

Method: SAGE reduces total sampling steps by sharing early-stage sampling across semantically similar queries, integrating a shared sampling scheme for efficiency and a tailored training strategy for quality preservation.

Result: Extensive experiments show 25.5% reduction in sampling cost, while improving generation quality with 5.0% lower FID, 5.4% higher CLIP score, and 160% higher diversity over baselines.

Conclusion: SAGE enables efficiency gains without sacrificing quality by leveraging semantic similarity across queries, providing a novel approach to diffusion model acceleration.

Abstract: Diffusion models manifest evident benefits across diverse domains, yet their
high sampling cost, requiring dozens of sequential model evaluations, remains a
major limitation. Prior efforts mainly accelerate sampling via optimized
solvers or distillation, which treat each query independently. In contrast, we
reduce total number of steps by sharing early-stage sampling across
semantically similar queries. To enable such efficiency gains without
sacrificing quality, we propose SAGE, a semantic-aware shared sampling
framework that integrates a shared sampling scheme for efficiency and a
tailored training strategy for quality preservation. Extensive experiments show
that SAGE reduces sampling cost by 25.5%, while improving generation quality
with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over
baselines.

</details>


### [96] [Improving Monte Carlo Tree Search for Symbolic Regression](https://arxiv.org/abs/2509.15929)
*Zhengyao Huang,Daniel Zhengyu Huang,Tiannan Xiao,Dina Ma,Zhenyu Ming,Hao Shi,Yuanhui Wen*

Main category: cs.LG

TL;DR: An improved MCTS framework for symbolic regression with extreme bandit allocation and evolution-inspired state-jumping actions that achieves competitive performance with state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional MCTS approaches for symbolic regression have limitations in bandit strategies and sequential symbol construction, which restrict performance. The paper aims to address these limitations through innovative strategies.

Method: Proposes two key innovations: (1) extreme bandit allocation strategy for identifying globally optimal expressions with finite-time performance guarantees, and (2) evolution-inspired state-jumping actions (mutation and crossover) that enable non-local transitions and reshape the reward landscape.

Result: The approach achieves competitive performance with state-of-the-art symbolic regression libraries in terms of recovery rate and obtains favorable positions on the Pareto frontier of accuracy versus model complexity across various datasets.

Conclusion: The improved MCTS framework with extreme bandit allocation and state-jumping actions effectively addresses limitations of traditional MCTS for symbolic regression, demonstrating robust and efficient performance comparable to existing state-of-the-art methods.

Abstract: Symbolic regression aims to discover concise, interpretable mathematical
expressions that satisfy desired objectives, such as fitting data, posing a
highly combinatorial optimization problem. While genetic programming has been
the dominant approach, recent efforts have explored reinforcement learning
methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with
its ability to balance exploration and exploitation through guided search, has
emerged as a promising technique for symbolic expression discovery. However,
its traditional bandit strategies and sequential symbol construction often
limit performance. In this work, we propose an improved MCTS framework for
symbolic regression that addresses these limitations through two key
innovations: (1) an extreme bandit allocation strategy tailored for identifying
globally optimal expressions, with finite-time performance guarantees under
polynomial reward decay assumptions; and (2) evolution-inspired state-jumping
actions such as mutation and crossover, which enable non-local transitions to
promising regions of the search space. These state-jumping actions also reshape
the reward landscape during the search process, improving both robustness and
efficiency. We conduct a thorough numerical study to the impact of these
improvements and benchmark our approach against existing symbolic regression
methods on a variety of datasets, including both ground-truth and black-box
datasets. Our approach achieves competitive performance with state-of-the-art
libraries in terms of recovery rate, attains favorable positions on the Pareto
frontier of accuracy versus model complexity. Code is available at
https://github.com/PKU-CMEGroup/MCTS-4-SR.

</details>


### [97] [Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics](https://arxiv.org/abs/2509.15933)
*Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,David Pardo,Jose I. Aizpurua*

Main category: cs.LG

TL;DR: This paper introduces a Bayesian Physics-Informed Neural Network (B-PINN) framework for probabilistic prognostics estimation, addressing limitations in SciML applications for prognostics by incorporating uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Applications of Scientific Machine Learning in prognostics remain limited due to complexity of incorporating PDEs for ageing physics and scarcity of robust uncertainty quantification methods.

Method: Embedding Bayesian Neural Networks into PINN architecture to produce uncertainty-aware predictions, applied to transformer ageing case study using heat diffusion PDE as physical residual with different prior distributions.

Result: The B-PINN delivers more reliable prognostic predictions by accurately quantifying predictive uncertainty compared to dropout-PINN baseline, validated against finite element model with real measurements.

Conclusion: The proposed framework provides crucial capability for supporting robust and informed maintenance decision-making in critical power assets through principled uncertainty quantification.

Abstract: Scientific Machine Learning (SciML) integrates physics and data into the
learning process, offering improved generalization compared with purely
data-driven models. Despite its potential, applications of SciML in prognostics
remain limited, partly due to the complexity of incorporating partial
differential equations (PDEs) for ageing physics and the scarcity of robust
uncertainty quantification methods. This work introduces a Bayesian
Physics-Informed Neural Network (B-PINN) framework for probabilistic
prognostics estimation. By embedding Bayesian Neural Networks into the PINN
architecture, the proposed approach produces principled, uncertainty-aware
predictions. The method is applied to a transformer ageing case study, where
insulation degradation is primarily driven by thermal stress. The heat
diffusion PDE is used as the physical residual, and different prior
distributions are investigated to examine their impact on predictive posterior
distributions and their ability to encode a priori physical knowledge. The
framework is validated against a finite element model developed and tested with
real measurements from a solar power plant. Results, benchmarked against a
dropout-PINN baseline, show that the proposed B-PINN delivers more reliable
prognostic predictions by accurately quantifying predictive uncertainty. This
capability is crucial for supporting robust and informed maintenance
decision-making in critical power assets.

</details>


### [98] [UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation](https://arxiv.org/abs/2509.15934)
*Mingdong Wu,Long Yang,Jin Liu,Weiyao Huang,Lehong Wu,Zelin Chen,Daolin Ma,Hao Dong*

Main category: cs.LG

TL;DR: A novel three-stage framework for in-hand object pose estimation using energy-based diffusion models trained on simulated data, achieving high precision and generalization to unseen CAD models.


<details>
  <summary>Details</summary>
Motivation: Accurate in-hand pose estimation is crucial for industrial and everyday applications, but existing methods struggle with precision and generalizability to unseen CAD models.

Method: Three-stage framework: 1) sampling and pre-ranking pose candidates, 2) iterative refinement, 3) post-ranking. Uses unified energy-based diffusion model with render-compare architecture for sim-to-real transfer.

Result: Outperforms conventional baselines (regression, matching, registration) and shows strong intra-category generalization to unseen CAD models.

Conclusion: The approach successfully integrates tactile pose estimation, tracking, and uncertainty estimation into a unified framework with robust real-world performance.

Abstract: Accurate estimation of the in-hand pose of an object based on its CAD model
is crucial in both industrial applications and everyday tasks, ranging from
positioning workpieces and assembling components to seamlessly inserting
devices like USB connectors. While existing methods often rely on regression,
feature matching, or registration techniques, achieving high precision and
generalizability to unseen CAD models remains a significant challenge. In this
paper, we propose a novel three-stage framework for in-hand pose estimation.
The first stage involves sampling and pre-ranking pose candidates, followed by
iterative refinement of these candidates in the second stage. In the final
stage, post-ranking is applied to identify the most likely pose candidates.
These stages are governed by a unified energy-based diffusion model, which is
trained solely on simulated data. This energy model simultaneously generates
gradients to refine pose estimates and produces an energy scalar that
quantifies the quality of the pose estimates. Additionally, borrowing the idea
from the computer vision domain, we incorporate a render-compare architecture
within the energy-based score network to significantly enhance sim-to-real
performance, as demonstrated by our ablation studies. We conduct comprehensive
experiments to show that our method outperforms conventional baselines based on
regression, matching, and registration techniques, while also exhibiting strong
intra-category generalization to previously unseen CAD models. Moreover, our
approach integrates tactile object pose estimation, pose tracking, and
uncertainty estimation into a unified framework, enabling robust performance
across a variety of real-world conditions.

</details>


### [99] [Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions](https://arxiv.org/abs/2509.15950)
*Marko Tuononen,Heikki Penttinen,Ville Hautamäki*

Main category: cs.LG

TL;DR: First application of influence functions to deep learning-based wireless receivers (DeepRx) for identifying influential training samples and enabling targeted fine-tuning to improve bit error rate performance.


<details>
  <summary>Details</summary>
Motivation: To improve wireless receiver performance by identifying which training samples most influence bit predictions, allowing for targeted fine-tuning of poorly performing cases rather than random retraining.

Method: Applied influence analysis to DeepRx (fully convolutional receiver) using loss-relative influence with capacity-like binary cross-entropy loss, with first-order updates on beneficial samples and proposed second-order influence-aligned update strategy.

Result: Influence-based fine-tuning consistently improved bit error rate toward genie-aided performance, outperforming random fine-tuning in single-target scenarios, though multi-target adaptation was less effective.

Conclusion: Influence functions serve as both an interpretability tool and basis for efficient receiver adaptation, establishing their utility in deep learning-based wireless communications.

Abstract: We present the first use of influence functions for deep learning-based
wireless receivers. Applied to DeepRx, a fully convolutional receiver,
influence analysis reveals which training samples drive bit predictions,
enabling targeted fine-tuning of poorly performing cases. We show that
loss-relative influence with capacity-like binary cross-entropy loss and
first-order updates on beneficial samples most consistently improves bit error
rate toward genie-aided performance, outperforming random fine-tuning in
single-target scenarios. Multi-target adaptation proved less effective,
underscoring open challenges. Beyond experiments, we connect influence to
self-influence corrections and propose a second-order, influence-aligned update
strategy. Our results establish influence functions as both an interpretability
tool and a basis for efficient receiver adaptation.

</details>


### [100] [Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation](https://arxiv.org/abs/2509.15955)
*Zhangqi Jiang,Tingjin Luo,Xu Yang,Xinyan Liang*

Main category: cs.LG

TL;DR: AGF-TI addresses the sub-cluster problem in incomplete multi-view semi-supervised learning by using adversarial graph fusion and low-rank tensor learning to handle missing views while maintaining structural continuity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for incomplete multi-view learning ignore missing samples, which can create discontinuous local structures (sub-clusters) that violate the smoothness assumption in label propagation, leading to distorted graph fusion and poor classification performance.

Method: Proposes AGF-TI with: 1) adversarial graph fusion using min-max framework to learn robust consensus graphs, 2) low-rank tensor learning to recover incomplete structures from high-order consistency, 3) anchor-based strategy for computational efficiency, and 4) alternative optimization with reduced gradient descent.

Result: Extensive experiments show AGF-TI outperforms state-of-the-art methods on various datasets, demonstrating superior performance in handling incomplete multi-view data.

Conclusion: AGF-TI effectively addresses the sub-cluster problem in incomplete multi-view learning through adversarial graph fusion and tensor completion, providing a robust solution with theoretical convergence guarantees.

Abstract: View missing remains a significant challenge in graph-based multi-view
semi-supervised learning, hindering their real-world applications. To address
this issue, traditional methods introduce a missing indicator matrix and focus
on mining partial structure among existing samples in each view for label
propagation (LP). However, we argue that these disregarded missing samples
sometimes induce discontinuous local structures, i.e., sub-clusters, breaking
the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster
Problem (SCP) would distort graph fusion and degrade classification
performance. To alleviate SCP, we propose a novel incomplete multi-view
semi-supervised learning method, termed AGF-TI. Firstly, we design an
adversarial graph fusion scheme to learn a robust consensus graph against the
distorted local structure through a min-max framework. By stacking all
similarity matrices into a tensor, we further recover the incomplete structure
from the high-order consistency information based on the low-rank tensor
learning. Additionally, the anchor-based strategy is incorporated to reduce the
computational complexity. An efficient alternative optimization algorithm
combining a reduced gradient descent method is developed to solve the
formulated objective, with theoretical convergence. Extensive experimental
results on various datasets validate the superiority of our proposed AGF-TI as
compared to state-of-the-art methods. Code is available at
https://github.com/ZhangqiJiang07/AGF_TI.

</details>


### [101] [Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems](https://arxiv.org/abs/2509.15999)
*Alan A. Lahoud,Erik Schaffernicht,Johannes A. Stork*

Main category: cs.LG

TL;DR: IO-LVM learns latent representations of constrained optimization problem cost functions from observed solutions, using a solver in the loop to ensure feasibility and capturing distributions over cost functions rather than single solutions.


<details>
  <summary>Details</summary>
Motivation: Standard models like Autoencoders struggle to enforce constraints when decoding structured outputs for constrained optimization problems with unknown cost functions, and existing inverse optimization methods typically recover only single cost functions rather than distributions.

Method: Proposes Inverse Optimization Latent Variable Model (IO-LVM) that learns a latent space of COP cost functions using a solver in the loop, leveraging estimated gradients of a Fenchel-Young loss through a non-differentiable deterministic solver to shape the latent space.

Result: Validated on ship/taxi routes and synthetic graph paths, demonstrating ability to reconstruct paths/cycles, predict their distributions, and yield interpretable latent representations.

Conclusion: IO-LVM effectively captures distributions over cost functions, enabling identification of diverse solution behaviors from different agents or conditions not available during training.

Abstract: Learning representations for solutions of constrained optimization problems
(COPs) with unknown cost functions is challenging, as models like (Variational)
Autoencoders struggle to enforce constraints when decoding structured outputs.
We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a
latent space of COP cost functions from observed solutions and reconstructs
feasible outputs by solving a COP with a solver in the loop. Our approach
leverages estimated gradients of a Fenchel-Young loss through a
non-differentiable deterministic solver to shape the latent space. Unlike
standard Inverse Optimization or Inverse Reinforcement Learning methods, which
typically recover a single or context-specific cost function, IO-LVM captures a
distribution over cost functions, enabling the identification of diverse
solution behaviors arising from different agents or conditions not available
during the training process. We validate our method on real-world datasets of
ship and taxi routes, as well as paths in synthetic graphs, demonstrating its
ability to reconstruct paths and cycles, predict their distributions, and yield
interpretable latent representations.

</details>


### [102] [Predicting the descent into extremism and terrorism](https://arxiv.org/abs/2509.16014)
*R. O. Lane,W. J. Holmes,C. J. Taylor,H. M. State-Davey,A. J. Wragge*

Main category: cs.LG

TL;DR: A system for automatically detecting extremism and terrorism intentions from online statements using machine learning and tracking algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop an automated approach for analyzing online statements to identify potential extremist or terrorist intentions, enabling early detection and monitoring.

Method: Uses Universal Sentence Encoder to extract 512-dimensional vector features from quotes, trains SVM classifier with 10-fold cross-validation, and implements tracking algorithms for temporal analysis.

Result: Achieved 81% accuracy for detecting extremism and 97% for terrorism detection using 839 quotes, outperforming baseline n-gram systems. Tracking algorithms successfully detected trends and attitude changes over time.

Conclusion: The proposed system effectively detects extremist and terrorist intentions from online statements and can track attitude changes, providing a valuable tool for security analysis.

Abstract: This paper proposes an approach for automatically analysing and tracking
statements in material gathered online and detecting whether the authors of the
statements are likely to be involved in extremism or terrorism. The proposed
system comprises: online collation of statements that are then encoded in a
form amenable to machine learning (ML), an ML component to classify the encoded
text, a tracker, and a visualisation system for analysis of results. The
detection and tracking concept has been tested using quotes made by terrorists,
extremists, campaigners, and politicians, obtained from wikiquote.org. A set of
features was extracted for each quote using the state-of-the-art Universal
Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The
data were used to train and test a support vector machine (SVM) classifier
using 10-fold cross-validation. The system was able to correctly detect
intentions and attitudes associated with extremism 81% of the time and
terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was
higher than that which was achieved for a simple baseline system based on
n-gram text features. Tracking techniques were also used to perform a temporal
analysis of the data, with each quote considered to be a noisy measurement of a
person's state of mind. It was demonstrated that the tracking algorithms were
able to detect both trends over time and sharp changes in attitude that could
be attributed to major events.

</details>


### [103] [Time-adaptive SympNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.16026)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: This paper introduces TSympNets, an extension of SympNets that learns time-adaptive symplectic integrators for irregularly sampled Hamiltonian systems, including non-autonomous systems. The paper provides theoretical approximation guarantees for separable Hamiltonian systems and fixes an error in a previous theorem.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning methods for learning symplectic integrators (like SympNets and HénonNets) require training data with fixed step sizes, but real-world measurement data is often sampled irregularly on non-equidistant time grids. This limitation motivates the development of time-adaptive methods.

Method: The authors adapt and extend the TSympNets architecture from previous work to handle non-autonomous Hamiltonian systems. They provide a universal approximation theorem for separable Hamiltonian systems and conduct numerical experiments to validate the theoretical capabilities.

Result: The paper proves that TSympNets can universally approximate separable Hamiltonian systems but shows that this capability cannot be extended to non-separable Hamiltonian systems. Numerical experiments confirm these theoretical findings.

Conclusion: TSympNets provide an effective solution for learning time-adaptive symplectic integrators for irregularly sampled data, with proven approximation guarantees for separable systems. The paper also corrects a significant error in a previous theorem related to symplectic map approximation.

Abstract: Measurement data is often sampled irregularly i.e. not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [20] and
H\'enonNets [4] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets, which we
call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets
and extend them to non-autonomous Hamiltonian systems. So far the approximation
qualities of TSympNets were unknown. We close this gap by providing a universal
approximation theorem for separable Hamiltonian systems and show that it is not
possible to extend it to non-separable Hamiltonian systems. To investigate
these theoretical approximation capabilities, we perform different numerical
experiments. Furthermore we fix a mistake in a proof of a substantial theorem
[25, Theorem 2] for the approximation of symplectic maps in general, but
specifically for symplectic machine learning methods.

</details>


### [104] [Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria](https://arxiv.org/abs/2509.16040)
*Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels*

Main category: cs.LG

TL;DR: A framework for automated constitutive model discovery using three sparse regression algorithms (LASSO, LARS, OMP) paired with three model selection criteria (CV, AIC, BIC), enabling systematic exploration of sparsity-performance-cost trade-offs.


<details>
  <summary>Details</summary>
Motivation: To provide a fully automated alternative to traditional model calibration by systematically evaluating different sparse regression approaches for constitutive model discovery from data.

Method: Pairs LASSO, LARS, and OMP regression algorithms with K-fold cross-validation, AIC, and BIC criteria, creating nine distinct algorithms. LARS serves as efficient path-based solver, OMP as tractable heuristic for ℓ0-regularized selection.

Result: All nine algorithm-criterion combinations perform consistently well for discovering isotropic and anisotropic material models, yielding highly accurate constitutive models from both synthetic and experimental datasets.

Conclusion: The framework broadens viable discovery algorithms beyond ℓ1-based approaches like LASSO, demonstrating that multiple sparse regression methods can effectively automate constitutive model discovery.

Abstract: The automated discovery of constitutive models from data has recently emerged
as a promising alternative to the traditional model calibration paradigm. In
this work, we present a fully automated framework for constitutive model
discovery that systematically pairs three sparse regression algorithms (Least
Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression
(LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection
criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC),
and Bayesian Information Criterion (BIC). This pairing yields nine distinct
algorithms for model discovery and enables a systematic exploration of the
trade-off between sparsity, predictive performance, and computational cost.
While LARS serves as an efficient path-based solver for the
$\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for
$\ell_0$-regularized selection. The framework is applied to both isotropic and
anisotropic hyperelasticity, utilizing both synthetic and experimental
datasets. Results reveal that all nine algorithm-criterion combinations perform
consistently well for the discovery of isotropic and anisotropic materials,
yielding highly accurate constitutive models. These findings broaden the range
of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.

</details>


### [105] [SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection](https://arxiv.org/abs/2509.16060)
*Maithili Joshi,Palash Nandi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: SABER is a novel white-box jailbreak method that bypasses LLM safety mechanisms by adding residual connections between intermediate layers, achieving 51% improvement over baselines on HarmBench.


<details>
  <summary>Details</summary>
Motivation: Despite extensive safety alignment training, LLMs remain vulnerable to jailbreak attacks that can bypass their safety mechanisms, which are found to be predominantly embedded in middle-to-late layers.

Method: SABER connects two intermediate layers (s < e) through a residual connection to bypass safety mechanisms, leveraging the finding that safety features are concentrated in middle-to-late layers.

Result: Achieves 51% improvement over best-performing baseline on HarmBench test set with only marginal perplexity shift on validation set.

Conclusion: The method demonstrates that LLM safety mechanisms can be effectively bypassed through targeted layer manipulation, highlighting ongoing vulnerabilities despite alignment efforts.

Abstract: Large Language Models (LLMs) with safe-alignment training are powerful
instruments with robust language comprehension capabilities. These models
typically undergo meticulous alignment procedures involving human feedback to
ensure the acceptance of safe inputs while rejecting harmful or unsafe ones.
However, despite their massive scale and alignment efforts, LLMs remain
vulnerable to jailbreak attacks, where malicious users manipulate the model to
produce harmful outputs that it was explicitly trained to avoid. In this study,
we find that the safety mechanisms in LLMs are predominantly embedded in the
middle-to-late layers. Building on this insight, we introduce a novel white-box
jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which
connects two intermediate layers $s$ and $e$ such that $s < e$, through a
residual connection. Our approach achieves a 51% improvement over the
best-performing baseline on the HarmBench test set. Furthermore, SABER induces
only a marginal shift in perplexity when evaluated on the HarmBench validation
set. The source code is publicly available at
https://github.com/PalGitts/SABER.

</details>


### [106] [MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning](https://arxiv.org/abs/2509.16078)
*Yi Xu,Yitian Zhang,Yun Fu*

Main category: cs.LG

TL;DR: DMAE is a novel masked autoencoder framework for unsupervised multivariate time series representation learning that uses dual reconstruction tasks and feature-level alignment to learn high-quality representations.


<details>
  <summary>Details</summary>
Motivation: To extract compact and informative representations from raw multivariate time series without labels for efficient transfer to diverse downstream tasks.

Method: Proposes Dual-Masked Autoencoder (DMAE) with two complementary pretext tasks: reconstructing masked values and estimating latent representations of masked features guided by a teacher encoder, plus feature-level alignment constraint.

Result: Comprehensive evaluations across classification, regression, and forecasting tasks demonstrate consistent and superior performance over competitive baselines.

Conclusion: DMAE effectively learns temporally coherent and semantically rich representations through joint optimization of dual reconstruction objectives and feature alignment.

Abstract: Unsupervised multivariate time series (MTS) representation learning aims to
extract compact and informative representations from raw sequences without
relying on labels, enabling efficient transfer to diverse downstream tasks. In
this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked
time-series modeling framework for unsupervised MTS representation learning.
DMAE formulates two complementary pretext tasks: (1) reconstructing masked
values based on visible attributes, and (2) estimating latent representations
of masked features, guided by a teacher encoder. To further improve
representation quality, we introduce a feature-level alignment constraint that
encourages the predicted latent representations to align with the teacher's
outputs. By jointly optimizing these objectives, DMAE learns temporally
coherent and semantically rich representations. Comprehensive evaluations
across classification, regression, and forecasting tasks demonstrate that our
approach achieves consistent and superior performance over competitive
baselines.

</details>


### [107] [Rethinking Molecule Synthesizability with Chain-of-Reaction](https://arxiv.org/abs/2509.16084)
*Seul Lee,Karsten Kreis,Srimukh Prasad Veccham,Meng Liu,Danny Reidenbach,Saee Paliwal,Weili Nie,Arash Vahdat*

Main category: cs.LG

TL;DR: ReaSyn is a generative framework that generates synthesizable molecules by exploring synthetic pathways, using a chain-of-reaction notation inspired by LLM reasoning and reinforcement learning for optimization.


<details>
  <summary>Details</summary>
Motivation: Existing molecular generative models often produce unsynthesizable molecules, with limited coverage of synthesizable chemical space and poor optimization performance.

Method: Proposes chain-of-reaction (CoR) notation for synthetic pathways, supervised training with dense reaction step supervision, RL-based finetuning, and test-time compute scaling for synthesizable projection.

Result: Achieves highest reconstruction rate, pathway diversity, optimization performance, and significantly outperforms previous methods in synthesizable hit expansion.

Conclusion: ReaSyn demonstrates superior ability to navigate large synthesizable chemical space through explicit reaction pathway reasoning.

Abstract: A well-known pitfall of molecular generative models is that they are not
guaranteed to generate synthesizable molecules. There have been considerable
attempts to address this problem, but given the exponentially large
combinatorial space of synthesizable molecules, existing methods have shown
limited coverage of the space and poor molecular optimization performance. To
tackle these problems, we introduce ReaSyn, a generative framework for
synthesizable projection where the model explores the neighborhood of given
molecules in the synthesizable space by generating pathways that result in
synthesizable analogs. To fully utilize the chemical knowledge contained in the
synthetic pathways, we propose a novel perspective that views synthetic
pathways akin to reasoning paths in large language models (LLMs). Specifically,
inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the
chain-of-reaction (CoR) notation that explicitly states reactants, reaction
types, and intermediate products for each step in a pathway. With the CoR
notation, ReaSyn can get dense supervision in every reaction step to explicitly
learn chemical reaction rules during supervised training and perform
step-by-step reasoning. In addition, to further enhance the reasoning
capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning
and goal-directed test-time compute scaling tailored for synthesizable
projection. ReaSyn achieves the highest reconstruction rate and pathway
diversity in synthesizable molecule reconstruction and the highest optimization
performance in synthesizable goal-directed molecular optimization, and
significantly outperforms previous synthesizable projection methods in
synthesizable hit expansion. These results highlight ReaSyn's superior ability
to navigate combinatorially-large synthesizable chemical space.

</details>


### [108] [Randomized Smoothing Meets Vision-Language Models](https://arxiv.org/abs/2509.16088)
*Emmanouil Seferis,Changshun Wu,Stefanos Kollias,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: This paper extends randomized smoothing (RS) from classification to generative models by connecting generative outputs to oracle classification tasks, developing theory for robustness certification in VLMs with improved scaling laws.


<details>
  <summary>Details</summary>
Motivation: Randomized smoothing is well-established for classification models but unclear for generative models since their outputs are sequences rather than labels. The paper aims to make robustness certification feasible for state-of-the-art vision-language models.

Method: The authors connect generative outputs to oracle classification tasks (e.g., discrete actions, harm detection, semantic clustering) and develop theoretical foundations that associate sample counts with robustness radius. They derive improved scaling laws analytically relating certified radius and accuracy to sample numbers.

Result: The paper validates that robustness certification becomes both well-defined and computationally feasible for VLMs, showing that 2-3 orders of magnitude fewer samples suffice with minimal loss even under weaker assumptions. Validation is performed against jailbreak-style adversarial attacks.

Conclusion: Randomized smoothing can be successfully extended to generative models through oracle classification connections, making robustness certification practical for modern vision-language models with efficient sample requirements.

Abstract: Randomized smoothing (RS) is one of the prominent techniques to ensure the
correctness of machine learning models, where point-wise robustness
certificates can be derived analytically. While RS is well understood for
classification, its application to generative models is unclear, since their
outputs are sequences rather than labels. We resolve this by connecting
generative outputs to an oracle classification task and showing that RS can
still be enabled: the final response can be classified as a discrete action
(e.g., service-robot commands in VLAs), as harmful vs. harmless (content
moderation or toxicity detection in VLMs), or even applying oracles to cluster
answers into semantically equivalent ones. Provided that the error rate for the
oracle classifier comparison is bounded, we develop the theory that associates
the number of samples with the corresponding robustness radius. We further
derive improved scaling laws analytically relating the certified radius and
accuracy to the number of samples, showing that the earlier result of 2 to 3
orders of magnitude fewer samples sufficing with minimal loss remains valid
even under weaker assumptions. Together, these advances make robustness
certification both well-defined and computationally feasible for
state-of-the-art VLMs, as validated against recent jailbreak-style adversarial
attacks.

</details>


### [109] [Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering](https://arxiv.org/abs/2509.16101)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: A robust personalized federated learning framework using heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with advanced tensor decomposition techniques for efficient high-dimensional data handling.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of handling high-dimensional multi-view data in federated learning while preserving privacy and enabling personalization through efficient tensor representations.

Method: Integrates heat-kernel coefficients with Tucker and CP tensor decompositions, employs matriculation/vectorization techniques, and uses a dual-level optimization scheme with local tensorized kernel clustering and federated aggregation of tensor factors with differential privacy.

Result: The framework enables efficient representation of high-dimensional multi-view structures with significant communication savings through low-rank tensor approximations while maintaining privacy.

Conclusion: The proposed tensorized approach provides an effective solution for personalized federated learning with enhanced privacy protection and computational efficiency for complex multi-view data.

Abstract: We present a robust personalized federated learning framework that leverages
heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with
advanced tensor decomposition techniques. Our approach integrates heat-kernel
coefficients adapted from quantum field theory with Tucker decomposition and
canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional
distance metrics and efficiently represent high-dimensional multi-view
structures. The framework employs matriculation and vectorization techniques to
facilitate the discovery of hidden structures and multilinear relationships via
N-way generalized tensors. The proposed method introduces a dual-level
optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor
decomposition operating on order-N input tensors, and federated aggregation of
tensor factors with privacy-preserving personalization mechanisms. The local
stage employs tensorized kernel Euclidean distance transformations and Tucker
decomposition to discover client-specific patterns in multi-view tensor data,
while the global aggregation process coordinates tensor factors (core tensors
and factor matrices) across clients through differential privacy-preserving
protocols. This tensorized approach enables efficient handling of
high-dimensional multi-view data with significant communication savings through
low-rank tensor approximations.

</details>


### [110] [Dynamic Classifier-Free Diffusion Guidance via Online Feedback](https://arxiv.org/abs/2509.16131)
*Pinelopi Papalampidi,Olivia Wiles,Ira Ktena,Aleksandar Shtedritski,Emanuele Bugliarello,Ivana Kajic,Isabela Albuquerque,Aida Nematzadeh*

Main category: cs.LG

TL;DR: This paper introduces a dynamic CFG scheduling framework that adapts guidance scales per timestep based on online feedback from quality evaluations, improving text-to-image generation over static approaches.


<details>
  <summary>Details</summary>
Motivation: Static guidance scales in classifier-free guidance (CFG) fail to adapt to diverse prompt requirements, and prior solutions like gradient-based correction or fixed schedules are complex and don't generalize well.

Method: Leverages online feedback from latent-space evaluations (CLIP for alignment, discriminator for fidelity, human preference model) to assess generation quality at each diffusion step, then performs greedy search to select optimal CFG scale per timestep.

Result: Significant improvements in text alignment, visual quality, text rendering and numerical reasoning on Imagen 3, achieving up to 53.8% human preference win-rate overall and 55.5% on text rendering prompts.

Conclusion: Optimal guidance schedule is inherently dynamic and prompt-dependent, and the proposed framework provides an efficient and generalizable solution.

Abstract: Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion
models, yet its effectiveness is limited by the use of static guidance scales.
This "one-size-fits-all" approach fails to adapt to the diverse requirements of
different prompts; moreover, prior solutions like gradient-based correction or
fixed heuristic schedules introduce additional complexities and fail to
generalize. In this work, we challeng this static paradigm by introducing a
framework for dynamic CFG scheduling. Our method leverages online feedback from
a suite of general-purpose and specialized small-scale latent-space
evaluations, such as CLIP for alignment, a discriminator for fidelity and a
human preference reward model, to assess generation quality at each step of the
reverse diffusion process. Based on this feedback, we perform a greedy search
to select the optimal CFG scale for each timestep, creating a unique guidance
schedule tailored to every prompt and sample. We demonstrate the effectiveness
of our approach on both small-scale models and the state-of-the-art Imagen 3,
showing significant improvements in text alignment, visual quality, text
rendering and numerical reasoning. Notably, when compared against the default
Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate
for overall preference, a figure that increases up to to 55.5% on prompts
targeting specific capabilities like text rendering. Our work establishes that
the optimal guidance schedule is inherently dynamic and prompt-dependent, and
provides an efficient and generalizable framework to achieve it.

</details>


### [111] [Spatio-temporal, multi-field deep learning of shock propagation in meso-structured media](https://arxiv.org/abs/2509.16139)
*M. Giselle Fernández-Godino,Meir H. Shachar,Kevin Korner,Jonathan L. Belof,Mukul Kumar,Jonathan Lind,William J. Schill*

Main category: cs.LG

TL;DR: A multi-field spatio-temporal deep learning model (MSTM) that unifies seven coupled fields to predict shock wave propagation through porous and architected materials, running 1000x faster than direct simulation with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Predicting shock wave behavior in porous materials is crucial for planetary defense, national security, and inertial fusion energy, but capturing complex phenomena like pore collapse and localized heating has remained challenging despite recent advances.

Method: MSTM integrates seven coupled fields (pressure, density, temperature, energy, material distribution, and two velocity components) into a single autoregressive surrogate model trained on high-fidelity hydrocode data.

Result: MSTM achieves 1000x speedup over direct simulation with errors below 4% in porous materials and below 10% in lattice structures, while resolving sharp shock fronts and preserving integrated quantities within 5% accuracy.

Conclusion: The model transforms previously intractable problems into tractable design studies, providing a practical framework for optimizing meso-structured materials in critical applications like planetary impact mitigation and fusion energy.

Abstract: The ability to predict how shock waves traverse porous and architected
materials is a decisive factor in planetary defense, national security, and the
race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous
Hugoniot responses, and localized heating -- phenomena that can determine the
success of asteroid deflection or fusion ignition -- has remained a major
challenge despite recent advances in single-field and reduced representations.
We introduce a multi-field spatio-temporal deep learning model (MSTM) that
unifies seven coupled fields -- pressure, density, temperature, energy,
material distribution, and two velocity components -- into a single
autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs
about a thousand times faster than direct simulation, achieving errors below
4\% in porous materials and below 10\% in lattice structures. Unlike prior
single-field or operator-based surrogates, MSTM resolves sharp shock fronts
while preserving integrated quantities such as mass-averaged pressure and
temperature to within 5\%. This advance transforms problems once considered
intractable into tractable design studies, establishing a practical framework
for optimizing meso-structured materials in planetary impact mitigation,
inertial fusion energy, and national security.

</details>


### [112] [Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents](https://arxiv.org/abs/2509.16151)
*Isaiah J. King,Benjamin Bowman,H. Howie Huang*

Main category: cs.LG

TL;DR: This paper proposes using graph-based deep reinforcement learning for automated cyber defense, framing it as a two-player context-based partially observable Markov decision problem with attributed graph observations to enable zero-shot adaptation to new networks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL approaches for cyber defense overfit to specific network topologies and fail when faced with environmental perturbations. The authors aim to create more generalizable agents that can adapt to unseen networks.

Method: The approach represents networks as attributed graphs and frames automated cyber defense as a two-player context-based partially observable Markov decision problem, incorporating relational inductive bias to allow agents to reason about host interactions in a more general manner.

Result: The proposed method outperforms state-of-the-art approaches by a wide margin and enables agents to defend never-before-seen networks against various adversaries in complex, multi-agent environments.

Conclusion: Graph-based representation with relational inductive bias allows RL agents to better reason about network states and achieve zero-shot adaptation to new networks, making automated cyber defense more robust and generalizable.

Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for
automated cyber defense (ACD). The traditional RL approach represents networks
as a list of computers in various states of safety or threat. Unfortunately,
these models are forced to overfit to specific network topologies, rendering
them ineffective when faced with even small environmental perturbations. In
this work, we frame ACD as a two-player context-based partially observable
Markov decision problem with observations represented as attributed graphs.
This approach allows our agents to reason through the lens of relational
inductive bias. Agents learn how to reason about hosts interacting with other
system entities in a more general manner, and their actions are understood as
edits to the graph representing the environment. By introducing this bias, we
will show that our agents can better reason about the states of networks and
zero-shot adapt to new ones. We show that this approach outperforms the
state-of-the-art by a wide margin, and makes our agents capable of defending
never-before-seen networks against a wide range of adversaries in a variety of
complex, and multi-agent environments.

</details>


### [113] [DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation](https://arxiv.org/abs/2509.16173)
*Yuen Chen,Yian Wang,Hari Sundaram*

Main category: cs.LG

TL;DR: DiveBatch is a novel adaptive batch size SGD algorithm that dynamically adjusts batch size using gradient diversity to accelerate training while maintaining generalization performance.


<details>
  <summary>Details</summary>
Motivation: Training large-scale deep neural networks is computationally expensive, and while SGD variants are widely used, traditional approaches focus on learning rate tuning. The challenge is balancing efficiency (large batches) with convergence speed and generalization (small batches).

Method: Proposes DiveBatch, which dynamically adjusts batch size based on gradient diversity - a data-driven adaptation with strong theoretical justification from SGD convergence analysis. The algorithm maintains small-batch generalization while improving efficiency.

Result: Evaluations on synthetic data, CIFAR-10, CIFAR-100, and Tiny-ImageNet show DiveBatch converges 1.06-5.0x faster than standard SGD and AdaBatch, with slight performance trade-off.

Conclusion: DiveBatch successfully addresses the batch size adaptation challenge, achieving significant acceleration in training convergence while preserving generalization performance through gradient diversity-based adaptation.

Abstract: The goal of this paper is to accelerate the training of machine learning
models, a critical challenge since the training of large-scale deep neural
models can be computationally expensive. Stochastic gradient descent (SGD) and
its variants are widely used to train deep neural networks. In contrast to
traditional approaches that focus on tuning the learning rate, we propose a
novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts
the batch size. Adapting the batch size is challenging: using large batch sizes
is more efficient due to parallel computation, but small-batch training often
converges in fewer epochs and generalizes better. To address this challenge, we
introduce a data-driven adaptation based on gradient diversity, enabling
DiveBatch to maintain the generalization performance of small-batch training
while improving convergence speed and computational efficiency. Gradient
diversity has a strong theoretical justification: it emerges from the
convergence analysis of SGD. Evaluations of DiveBatch on synthetic and
CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges
significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a
slight trade-off in performance.

</details>


### [114] [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/abs/2509.16189)
*Andrew Kyle Lampinen,Martin Engelcke,Yuxuan Li,Arslan Chaudhry,James L. McClelland*

Main category: cs.LG

TL;DR: Machine learning systems fail to generalize due to lack of latent learning - learning information not immediately relevant but potentially useful later. Cognitive science suggests episodic memory and retrieval mechanisms can help address this issue.


<details>
  <summary>Details</summary>
Motivation: To understand why ML systems fail to generalize compared to natural intelligence, and explore how cognitive science concepts like latent learning and episodic memory could improve ML generalization capabilities.

Method: Drawing inspiration from cognitive science, the paper analyzes various ML failures (reversal curse, agent navigation) and tests how retrieval mechanisms with oracle access can improve generalization. Examines within-example in-context learning for effective information use across retrieved examples.

Result: A system with oracle retrieval mechanism can use learning experiences more flexibly to generalize better across challenges. Identifies essential components for effective retrieval, particularly the importance of within-example in-context learning.

Conclusion: Current ML systems' data inefficiency compared to natural intelligence may stem from lack of latent learning. Retrieval methods can complement parametric learning to improve generalization by enabling more flexible use of learned information.

Abstract: When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.

</details>


### [115] [Inverting Trojans in LLMs](https://arxiv.org/abs/2509.16203)
*Zhengxing Li,Guangmingmei Yang,Jayaram Raghuram,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: A novel LLM backdoor trigger inversion approach using discrete search, implicit blacklisting via activation similarity, and confidence-based detection that reliably detects and inverts ground-truth backdoor triggers.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor detection methods for images don't work well for LLMs due to discrete input space, combinatorial explosion of token combinations, and lack of good blacklists for certain domains.

Method: Three components: 1) discrete greedy search starting from singleton tokens, 2) implicit blacklisting using cosine similarity in activation space with clean target samples, 3) detection based on high misclassification rates with unusual confidence.

Result: The approach reliably detects and successfully inverts ground-truth backdoor trigger phrases, unlike many recent works.

Conclusion: The proposed method effectively addresses LLM-specific challenges in backdoor detection and provides a practical solution for trigger inversion in discrete input spaces.

Abstract: While effective backdoor detection and inversion schemes have been developed
for AIs used e.g. for images, there are challenges in "porting" these methods
to LLMs. First, the LLM input space is discrete, which precludes gradient-based
search over this space, central to many backdoor inversion methods. Second,
there are ~30,000^k k-tuples to consider, k the token-length of a putative
trigger. Third, for LLMs there is the need to blacklist tokens that have strong
marginal associations with the putative target response (class) of an attack,
as such tokens give false detection signals. However, good blacklists may not
exist for some domains. We propose a LLM trigger inversion approach with three
key components: i) discrete search, with putative triggers greedily accreted,
starting from a select list of singletons; ii) implicit blacklisting, achieved
by evaluating the average cosine similarity, in activation space, between a
candidate trigger and a small clean set of samples from the putative target
class; iii) detection when a candidate trigger elicits high misclassifications,
and with unusually high decision confidence. Unlike many recent works, we
demonstrate that our approach reliably detects and successfully inverts
ground-truth backdoor trigger phrases.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [116] [Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)](https://arxiv.org/abs/2509.15238)
*Dylan Léveillé*

Main category: cs.MA

TL;DR: A tool that automatically generates BDI plans using Alternating-Time Temporal Logic (ATL) to accommodate multi-agent cooperation and competition.


<details>
  <summary>Details</summary>
Motivation: Existing BDI plan generation approaches require significant manual effort and focus mainly on single-agent systems, lacking support for multi-agent interactions.

Method: Developed a tool that uses Alternating-Time Temporal Logic (ATL) to automatically generate BDI plans that account for possible competition or cooperation between agents.

Result: The tool successfully generated plans for an illustrative game requiring agent collaboration, demonstrating that the generated plans allow agents to successfully achieve shared goals.

Conclusion: The ATL-based approach effectively automates BDI plan generation for multi-agent systems, accommodating both cooperative and competitive scenarios.

Abstract: Belief-Desire-Intention (BDI) is a framework for modelling agents based on
their beliefs, desires, and intentions. Plans are a central component of BDI
agents, and define sequences of actions that an agent must undertake to achieve
a certain goal. Existing approaches to plan generation often require
significant manual effort, and are mainly focused on single-agent systems. As a
result, in this work, we have developed a tool that automatically generates BDI
plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans
generated accommodate for possible competition or cooperation between the
agents in the system. We demonstrate the effectiveness of the tool by
generating plans for an illustrative game that requires agent collaboration to
achieve a shared goal. We show that the generated plans allow the agents to
successfully attain this goal.

</details>


### [117] [Dynamic Agent Grouping ECBS: Scaling Windowed Multi-Agent Path Finding with Completeness Guarantees](https://arxiv.org/abs/2509.15381)
*Tiannan Zhang,Rishi Veerapaneni,Shao-Hung Chan,Jiaoyang Li,Maxim Likhachev*

Main category: cs.MA

TL;DR: This paper extends the WinC-MAPF framework to work with bounded suboptimal solvers (like ECBS) while maintaining completeness guarantees, introducing DAG-ECBS which dynamically creates agent groups and shows improved scalability over existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous WinC-MAPF methods required optimal MAPF solvers, which limited their practicality. The authors aim to enable the use of more efficient bounded suboptimal solvers while preserving the completeness guarantees of the WinC-MAPF framework.

Method: The paper introduces Dynamic Agent Grouping ECBS (DAG-ECBS), which dynamically creates and plans agent groups while ensuring each group's solution remains bounded suboptimal. This extends the WinC-MAPF framework to work with suboptimal solvers.

Result: DAG-ECBS demonstrates improved scalability compared to SS-CBS and can outperform windowed ECBS without completeness guarantees. The authors prove that DAG-ECBS maintains completeness within the WinC-MAPF framework.

Conclusion: The work provides a blueprint for designing more MAPF methods that can leverage the WinC-MAPF framework with bounded suboptimal solvers, making the framework more practical and scalable for real-world applications.

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths for a team of agents. Although several MAPF methods which
solve full-horizon MAPF have completeness guarantees, very few MAPF methods
that plan partial paths have completeness guarantees. Recent work introduced
the Windowed Complete MAPF (WinC-MAPF) framework, which shows how windowed
optimal MAPF solvers (e.g., SS-CBS) can use heuristic updates and disjoint
agent groups to maintain completeness even when planning partial paths
(Veerapaneni et al. 2024). A core limitation of WinC-MAPF is that they required
optimal MAPF solvers. Our main contribution is to extend WinC-MAPF by showing
how we can use a bounded suboptimal solver while maintaining completeness. In
particular, we design Dynamic Agent Grouping ECBS (DAG-ECBS) which dynamically
creates and plans agent groups while maintaining that each agent group solution
is bounded suboptimal. We prove how DAG-ECBS can maintain completeness in the
WinC-MAPF framework. DAG-ECBS shows improved scalability compared to SS-CBS and
can outperform windowed ECBS without completeness guarantees. More broadly, our
work serves as a blueprint for designing more MAPF methods that can use the
WinC-MAPF framework.

</details>
