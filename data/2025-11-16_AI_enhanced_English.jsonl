{"id": "2511.10030", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10030", "abs": "https://arxiv.org/abs/2511.10030", "authors": ["Tao Jiang", "Zichuan Lin", "Lihe Li", "Yi-Chen Li", "Cong Guan", "Lei Yuan", "Zongzhang Zhang", "Yang Yu", "Deheng Ye"], "title": "Multi-agent In-context Coordination via Decentralized Memory Retrieval", "comment": null, "summary": "Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.", "AI": {"tldr": "MAICC is a novel multi-agent reinforcement learning approach that uses decentralized memory retrieval and in-context coordination to enable faster adaptation to unseen cooperative tasks by balancing online test data with offline memory.", "motivation": "In cooperative MARL, decentralized policy deployment often leads to task alignment mismatches and inefficient reward assignment, limiting policy adaptation efficiency.", "method": "Train centralized embedding model for trajectory representations, then decentralized models to approximate it for team-level task info. Use learned embeddings to retrieve relevant trajectories as context, combined with current sub-trajectories for decision-making. Introduce memory mechanism balancing test-time online data with offline memory.", "result": "Extensive experiments on LBF and SMAC benchmarks show MAICC enables faster adaptation to unseen tasks compared to existing methods.", "conclusion": "MAICC effectively enhances coordination through fast adaptation in cooperative MARL by leveraging decentralized memory retrieval and hybrid utility scoring."}}
{"id": "2511.10283", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10283", "abs": "https://arxiv.org/abs/2511.10283", "authors": ["Won Ik Cho", "Woonghee Han", "Kyung Seo Ki", "Young Min Kim"], "title": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System", "comment": "10 pages, 1 figure, 2 tables", "summary": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.", "AI": {"tldr": "A training-free framework for private-domain multi-agent systems using behavior modeling, documentation, and structured specifications to handle heterogeneous tools and domain-specific requirements.", "motivation": "Existing open-domain frameworks struggle in private domains due to tool heterogeneity, domain jargon, API restrictions, and governance complexity. Traditional fine-tuning approaches are costly, brittle to domain shifts, and risk degrading general performance.", "method": "Uses an orchestrator, tool-calling agent, and general chat agent with tool integration via structured specifications and domain-informed instructions. Avoids training and synthetic data generation by relying on behavior modeling and documentation.", "result": "Enables scalable adaptation to private tools and evolving contexts without continual retraining. Supports lightweight deployment, API specification retrieval, and synthetic dialogue generation for evaluation.", "conclusion": "Provides a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems, addressing key limitations of current approaches."}}
