<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: The paper interprets transformer attention mechanisms as an approximate Vector Symbolic Architecture (VSA), explaining reasoning behaviors and failures, and proposes VSA-inspired improvements for more reliable AI systems.


<details>
  <summary>Details</summary>
Motivation: Transformer models show impressive reasoning but fail on symbolic tasks; this work aims to understand these limitations through a VSA lens to build more robust systems.

Method: The paper develops a unified perspective by mapping self-attention components to VSA operations (binding, superposition) and analyzes transformer behaviors through this algebraic framework.

Result: The VSA view explains reasoning successes (chain-of-thought) and failures (variable confusion), and inspires new architectural biases and training objectives for better symbolic handling.

Conclusion: Viewing attention as soft vector-symbolic computation provides a principled path toward more interpretable and logically reliable reasoning systems in transformers.

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [2] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: This paper addresses the limitation of existing KGQA benchmarks that assume complete knowledge graphs by proposing a methodology for creating benchmarks under KG incompleteness and introducing GR-Agent, an adaptive reasoning agent that outperforms baselines in both complete and incomplete settings.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based KGQA benchmarks assume complete KGs with direct supporting triples, which doesn't reflect real-world incomplete KGs where answers require inference from existing facts. This gap limits evaluation of true reasoning capabilities.

Method: Proposes a methodology for constructing KGQA benchmarks under incompleteness by removing direct supporting triples while preserving alternative reasoning paths. Introduces GR-Agent that constructs an interactive environment from KGs, formalizes KGQA as agent-environment interaction using graph reasoning tools, and maintains memory of reasoning evidence.

Result: Experiments show existing methods suffer consistent performance degradation under incompleteness. GR-Agent outperforms non-training baselines and performs comparably to training-based methods in both complete and incomplete settings.

Conclusion: The proposed benchmarking methodology reveals limitations of current KGQA methods, and GR-Agent demonstrates superior reasoning capability for handling incomplete knowledge graphs through adaptive graph reasoning.

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [3] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: This paper systematically investigates methods to improve LLM-generated Infrastructure as Code (Terraform) by injecting structured configuration knowledge (e.g., enhanced RAG, Graph RAG). While baseline LLMs performed poorly (27.1% success), knowledge injection significantly boosted technical correctness (75.3%), but a 'Correctness-Congruence Gap' remained where LLMs still struggled with nuanced user intent alignment.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC), specifically for Terraform. There is a need to systematically improve LLM-based IaC generation.

Method: The research enhanced an existing IaC-Eval benchmark with cloud emulation and automated error analysis, and developed a novel error taxonomy. It implemented and evaluated a series of knowledge injection techniques, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches (including semantic enrichment and modeling inter-resource dependencies).

Result: Experimental results showed that baseline LLM performance was poor (27.1% overall success). Injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%.

Conclusion: Injecting structured configuration knowledge significantly improves the technical correctness of LLM-generated IaC. However, despite these gains, intent alignment plateaued, revealing a 'Correctness-Congruence Gap'—LLMs can become proficient 'coders' but remain limited 'architects' in fulfilling nuanced user intent.

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [4] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI is a multi-agent AI system designed to help rural farmers make climate adaptation decisions through collaborative reasoning and real-time data integration.


<details>
  <summary>Details</summary>
Motivation: Agricultural regions face climate risks like droughts and heavy rainfall, needing adaptive solutions. Current AI systems lack dynamic collaboration for complex decision-making.

Method: Uses modular, role-specialized agents coordinated via chain-of-responsibility, integrating real-time tools and datasets with governance to reduce errors and support multilingual access.

Result: Experiments show AgroAskAI provides more actionable and inclusive outputs for climate-related agricultural queries after tool enhancements and prompt refinement.

Conclusion: Agentic AI like AgroAskAI holds promise for sustainable, accountable decision support in agriculture, especially for vulnerable communities.

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [5] [Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation](https://arxiv.org/abs/2512.15033)
*Xidan Song,Weiqi Wang,Ruifeng Cao,Qingya Hu*

Main category: cs.AI

TL;DR: This paper introduces a Geometric Stability Framework to evaluate LLMs in chess reasoning, revealing an Accuracy-Stability Paradox where high-accuracy models like GPT-5.1 show poor consistency under geometric transformations, while others like Claude Sonnet 4.5 excel in dual robustness.


<details>
  <summary>Details</summary>
Motivation: Standard accuracy metrics in LLM evaluation fail to distinguish genuine reasoning from memorization, particularly in chess where geometric transformations can expose weaknesses.

Method: The authors propose a Geometric Stability Framework testing model consistency under invariant transformations (e.g., board rotation, mirror symmetry) and apply it to six LLMs using a dataset of ~3,000 positions.

Result: GPT-5.1 shows catastrophic degradation under perturbations (error rates surge over 600% in rotation), while Claude Sonnet 4.5 and Kimi K2 Turbo maintain high consistency. Gemini 2.5 Flash leads in safety with 96.0% illegal state rejection.

Conclusion: Geometric stability is an essential orthogonal metric for AI evaluation, helping disentangle reasoning capabilities from data contamination in large-scale models.

Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

</details>


### [6] [LADY: Linear Attention for Autonomous Driving Efficiency without Transformers](https://arxiv.org/abs/2512.15038)
*Jihao Huang,Xi Xia,Zhiyuan Li,Tianle Liu,Jingke Wang,Junbo Chen,Tengju Ye*

Main category: cs.AI

TL;DR: LADY is a fully linear attention-based generative model for end-to-end autonomous driving that enables efficient long-range temporal modeling with constant computational cost while supporting cross-modal interactions.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based methods suffer from quadratic attention costs that limit long sequence modeling on resource-constrained platforms, while linear attention approaches lack support for crucial cross-modal and cross-temporal interactions needed for autonomous driving.

Method: Proposes LADY with linear attention mechanisms enabling constant computational/memory costs regardless of history length, plus a lightweight linear cross-attention for effective cross-modal fusion of camera and LiDAR features.

Result: Achieves state-of-the-art performance on NAVSIM and Bench2Drive benchmarks with constant-time complexity, showing improved planning performance and significantly reduced computational costs.

Conclusion: LADY demonstrates practical deployment on edge devices, offering an efficient solution for autonomous driving that maintains performance while overcoming computational limitations of traditional transformers.

Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.

</details>


### [7] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: Explores integration of agentic AI in ISAC systems for 6G networks to enable intelligent, autonomous operations through perception-reasoning-action loops.


<details>
  <summary>Details</summary>
Motivation: Increasingly dynamic and complex wireless environments require ISAC systems to become more intelligent and autonomous to maintain efficiency and adaptability.

Method: Comprehensive review of agentic AI and ISAC, analysis of optimization approaches, proposal of a novel agentic ISAC framework with a case study.

Result: Proposed framework demonstrates superiority in optimizing ISAC performance; highlights advantages of GenAI-based agentic AI.

Conclusion: Agentic AI holds significant application value for ISAC systems in 6G, with promising prospects for future intelligent networks.

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [8] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER is a cognitive-inspired framework that dynamically selects reasoning strategies for LLMs based on query difficulty, using RL-trained agents and tool-assisted reasoning to balance efficiency and accuracy, achieving significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM reasoning strategies struggle to balance efficiency and accuracy across queries with varying difficulty levels, as they mainly rely on either fast or slow thinking modes without considering query-specific requirements.

Method: The paper proposes CogER framework featuring query complexity assessment to predefined difficulty levels, Markov Decision Process modeling with reinforcement learning for strategy selection, and Cognitive Tool-Assisted Reasoning that enables autonomous tool invocation in chain-of-thought reasoning.

Result: CogER outperforms state-of-the-art Test-Time scaling methods with at least 13% relative improvement in average exact match on In-Domain tasks and 8% relative gain on Out-of-Domain tasks.

Conclusion: The authors conclude that Cognitive-Inspired Elastic Reasoning (CogER) effectively addresses the efficiency-accuracy trade-off in LLM reasoning by dynamically selecting strategies based on query difficulty, achieving significant performance improvements over existing methods.

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [9] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: Novel clustering-based framework for variable ordering in relaxed decision diagrams that partitions variables into clusters to reduce computational overhead while maintaining tight dual bounds.


<details>
  <summary>Details</summary>
Motivation: Standard dynamic variable ordering heuristics for decision diagrams incur high computational costs when applied globally to all unfixed variables, creating a performance trade-off.

Method: Two strategies: Cluster-to-Cluster (sequential cluster processing using aggregate criteria like vertex weights in MWISP) and Pick-and-Sort (iterative selection/sorting of representative variables). Also includes theoretical analysis and cluster size policies for MWISP.

Result: Integration into DD-based branch-and-bound algorithm shows consistent computational cost reduction compared to baseline dynamic ordering across MWISP benchmarks.

Conclusion: Clustering-based variable ordering effectively balances computational efficiency and bound quality, offering a practical improvement for discrete optimization problems.

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [10] [CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications](https://arxiv.org/abs/2512.15231)
*Zhengchao Chen,Haoran Wang,Jing Yao,Pedram Ghamisi,Jun Zhou,Peter M. Atkinson,Bing Zhang*

Main category: cs.AI

TL;DR: CangLing-KnowFlow is a unified intelligent agent framework for Earth observation that integrates expert knowledge, dynamic workflow adjustment, and continuous learning to handle diverse remote sensing tasks more reliably than existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing automated systems for remote sensing are task-specific and lack a unified framework for end-to-end workflows, creating gaps in managing diverse applications from data preprocessing to interpretation.

Method: The framework combines a Procedural Knowledge Base (1,008 expert-validated workflows across 162 tasks), Dynamic Workflow Adjustment for autonomous failure diagnosis and replanning, and an Evolutionary Memory Module for continuous learning from runtime events.

Result: Evaluation on KnowFlow-Bench (324 workflows) across 13 LLM backbones showed CangLing-KnowFlow outperformed the Reflexion baseline by at least 4% in Task Success Rate for all complex tasks.

Conclusion: CangLing-KnowFlow demonstrates strong potential as a robust, efficient, and scalable automated solution for complex Earth observation challenges by integrating expert knowledge into adaptive procedures.

Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).

</details>


### [11] [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)
*Toshihide Ubukata,Enhong Mu,Takuto Yamauchi,Mingyue Zhang,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: GCRL enhances RL-based controller synthesis by using Graph Neural Networks to encode exploration history into graphs for better context awareness.


<details>
  <summary>Details</summary>
Motivation: Current exploration policies for controller synthesis rely on fixed rules or RL with limited current features, lacking broader contextual understanding.

Method: Integrates Graph Neural Networks to encode Labeled Transition System exploration history into graph structures capturing non-current context.

Result: GCRL showed superior learning efficiency and generalization in 4/5 benchmark domains, except one with high symmetry and local interactions.

Conclusion: GCRL's graph-based history encoding effectively improves RL methods for controller synthesis, though challenges remain in highly symmetric domains.

Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.

</details>


### [12] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: Study analyzes AI's limitations in understanding scientific diagrams using CSAT questions, revealing perception-cognition gaps and computation flaws despite advanced capabilities.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about academic integrity as students increasingly use AI for assignments, by identifying AI's weaknesses to help design AI-resistant assessments.

Method: Experimental analysis using 2025 Korean CSAT Earth Science questions with three input conditions (full-page, individual items, optimized multimodal) tested on GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro.

Result: Unstructured inputs caused performance drops due to OCR issues. Even optimized conditions showed perception errors, calculation-conceptualization discrepancies, and process hallucinations in AI reasoning.

Conclusion: AI has critical reasoning flaws in interpreting scientific diagrams, providing basis for creating AI-resistant questions to ensure fair assessment by targeting these vulnerabilities.

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [13] [SCOPE: Prompt Evolution for Enhancing Agent Effectiveness](https://arxiv.org/abs/2512.15374)
*Zehua Pei,Hui-Ling Zhen,Shixiong Kai,Sinno Jialin Pan,Yunhe Wang,Mingxuan Yuan,Bei Yu*

Main category: cs.AI

TL;DR: SCOPE introduces self-evolving prompt optimization to dynamically manage LLM agent contexts, improving task success rates from 14.23% to 38.64%


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle with managing massive, dynamic contexts due to static prompts causing Corrective and Enhancement failures

Method: SCOPE frames context management as online optimization using Dual-Stream mechanism (balancing tactical specificity and strategic generality) and Perspective-Driven Exploration

Result: Experimental results on HLE benchmark show significant improvement in task success rates

Conclusion: SCOPE enables automatic prompt evolution for better context management without human intervention

Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.

</details>


### [14] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: Enhancing LLM route instructions using qualitative spatial relations for pedestrian navigation


<details>
  <summary>Details</summary>
Motivation: To improve pedestrian wayfinding capabilities of Large Language Models

Method: Utilizing qualitative spatial relations in route instructions

Result: Improved LLM performance in generating pedestrian navigation guidance

Conclusion: Qualitative spatial relations effectively enhance LLM route instruction capabilities for pedestrian wayfinders

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [15] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: A bootstrapping outer-learning framework that enhances prediction accuracy in multiplayer card games by expanding human game databases with AI-generated games.


<details>
  <summary>Details</summary>
Motivation: Early game decisions (bidding, game selection, initial card play) are critical in multiplayer card games but rely on limited human expert data at computational boundaries.

Method: Implementation of a general bootstrapping outer-learning framework using perfect feature hash functions to merge statistics from human and self-played AI games.

Result: Development of a self-improving card game engine validated through a Skat case study, supporting various in-game decisions.

Conclusion: The framework successfully improves decision-making accuracy by leveraging expanded game data, demonstrating practical utility in automated game strategies.

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [16] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: This paper introduces an integrated MILP-ASP system for vertiport scheduling that handles dynamic requirements and ambiguous human requests using three-valued logic and decision trees.


<details>
  <summary>Details</summary>
Motivation: Urban Air Mobility requires efficient scheduling under resource constraints, with challenges including dynamic operational needs and vague human rescheduling requests.

Method: Combines Mixed Integer Linear Programming (MILP) with Answer Set Programming (ASP), using three-valued logic to interpret ambiguous inputs and decision trees for integration.

Result: The framework provides optimized scheduling while transparently supporting human inputs, creating an explainable and adaptive system.

Conclusion: The integrated MILP-ASP approach offers a robust solution for explainable, adaptive vertiport scheduling in UAM.

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [17] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math is a large-scale dataset with 7.5M math reasoning traces created using GPT-OSS-120b, combining AoPS and StackExchange problems. It features three reasoning modes with/without Python tool integration, shows superior performance over OpenMathReasoning, and enables efficient long-context training with bucketed strategy.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diverse reasoning styles, long-form traces, and effective tool integration needed for high-quality mathematical reasoning supervision.

Method: Leveraged GPT-OSS-120b to generate 7.5M solution traces across high/medium/low reasoning modes with and without Python tool-integrated reasoning. Combined 85K AoPS problems with 262K StackExchange-Math problems and developed sequential bucketed strategy for efficient 128K context-length training.

Result: Outperformed OpenMathReasoning on AoPS problems. StackExchange integration improved robustness on HLE-Math while maintaining competition benchmark accuracy. Achieved 100% maj@16 accuracy on AIME 2024/2025 with Python TIR. Training acceleration of 2-3× without significant accuracy loss.

Conclusion: Nemotron-Math enables state-of-the-art mathematical reasoning performance through its large-scale, diverse dataset with effective tool integration and efficient training methods.

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [18] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: A new benchmark evaluates LLMs' scientific discovery capabilities through scenario-based questions and research projects, revealing significant gaps despite some promise.


<details>
  <summary>Details</summary>
Motivation: Existing science benchmarks focus on decontextualized knowledge without capturing iterative scientific processes like hypothesis generation and interpretation.

Method: A scenario-grounded benchmark across biology, chemistry, materials, and physics with domain expert-defined projects, assessed at question-level accuracy and project-level performance.

Result: LLMs show a performance gap compared to general benchmarks, diminishing returns from scaling, and systematic weaknesses, with varied success across scenarios.

Conclusion: Current LLMs are far from general scientific superintelligence but show promise; the framework provides a reproducible benchmark for advancing LLM development in science.

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [19] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: This paper develops a decision-theoretic framework to determine when to delegate decisions to AI systems, highlighting trade-offs between value alignment, epistemic accuracy, and reach under uncertainty.


<details>
  <summary>Details</summary>
Motivation: While AI value alignment techniques exist, there is insufficient guidance on deciding when imperfect alignment is acceptable for delegation, especially under uncertainty about AI capabilities.

Method: The authors introduce a formal framework balancing an AI agent's value (mis)alignment, epistemic accuracy, and reach (available actions), accounting for the principal's uncertainty.

Result: Analysis shows universal delegation requires near-perfect alignment and trust, which is impractical, but context-specific delegation can be optimal even with misalignment if the AI offers superior accuracy or expanded reach that improves expected outcomes.

Conclusion: The work shifts focus from perfect alignment to managing delegation risks and rewards under uncertainty, providing a principled method to decide when an AI is aligned enough for specific contexts.

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [20] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC framework integrates reasoning and self-critique in a single LLM using hybrid reinforcement learning, enabling built-in critical thinking for better problem-solving.


<details>
  <summary>Details</summary>
Motivation: Most existing LLMs decouple reasoning from verification, lacking immediate feedback or increasing system complexity. Inspired by human critical thinking where reasoning and evaluation are intertwined.

Method: Proposed STC framework that interleaves reasoning and self-critique at each step within a single model, trained with hybrid reinforcement learning combining reasoning rewards and critique-consistency rewards.

Result: Experiments on mathematical reasoning benchmarks show STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces.

Conclusion: Stepwise Think-Critique (STC) represents a step toward LLMs with built-in critical thinking capabilities, demonstrating strong critic-thinking and producing more interpretable reasoning traces.

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [21] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE framework improves LLM transparency by modeling inter-generational influences through attribution graphs


<details>
  <summary>Details</summary>
Motivation: Current context attribution methods for LLMs produce incomplete explanations by ignoring inter-generational influences

Method: Introduces attribution graph that quantifies how each generation is influenced by prompt and prior generations while preserving causality and row stochasticity

Result: CAGE improves context attribution faithfulness by up to 40% across multiple models, datasets, and metrics

Conclusion: Graph-based attribution framework provides more complete explanations for autoregressive LLM behavior

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


### [22] [Artism: AI-Driven Dual-Engine System for Art Generation and Critique](https://arxiv.org/abs/2512.15710)
*Shuai Liu,Yiqing Tian,Yang Chen,Mar Canet Sola*

Main category: cs.AI

TL;DR: Proposes a dual-engine AI architecture (AIDA and Ismism Machine) using deep learning and multi-agent collaboration for simulating art evolution trajectories and enabling reflexive critical analysis.


<details>
  <summary>Details</summary>
Motivation: To address the complex problem of exploring potential trajectories in art evolution and shift from traditional unidirectional critique to intelligent, interactive reflexive practice.

Method: Dual-engine AI architectural method with two interconnected components: AIDA (artificial artist social network) and Ismism Machine (critical analysis system), leveraging deep learning and multi-agent collaboration for multidimensional simulations.

Result: Framework developed for exploring art historical developments and conceptual innovation patterns; currently being applied in experimental studies on contemporary art concepts.

Conclusion: Introduces a general AI-driven critical loop methodology that offers new possibilities for computational art analysis and enables interactive reflexive practice.

Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.

</details>


### [23] [Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants](https://arxiv.org/abs/2512.15712)
*Vincent Huang,Dami Choi,Daniel D. Johnson,Sarah Schwettmann,Jacob Steinhardt*

Main category: cs.AI

TL;DR: Proposes Predictive Concept Decoder (PCD) - an end-to-end trained interpretability assistant that compresses neural activations into sparse concepts to predict model behavior through natural language questions.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods use hand-designed agents; authors want to make activation interpretation more scalable and effective by turning it into a training objective.

Method: Train encoder to compress activations into sparse concept list, decoder to answer natural language questions about model behavior from concepts. Pretrain on large data, finetune for specific questions.

Result: PCD shows favorable scaling: auto-interp score improves with data. Successfully detects jailbreaks, secret hints, implanted concepts, and surfaces latent user attributes.

Conclusion: Turning interpretability into end-to-end training objective creates scalable approach that improves with more data and works on various applications.

Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: This paper investigates how AI ecosystem diversity (multiple models trained on collective outputs) affects knowledge collapse, finding moderate diversity mitigates collapse better than low or extreme diversity.


<details>
  <summary>Details</summary>
Motivation: Concerns about AI causing knowledge collapse (reduction to dominant ideas) inspired by ecological diversity concepts, extending prior single-model collapse research to model ecosystems.

Method: Built ecosystems of language models trained on segmented data from their collective outputs, evaluated over ten self-training iterations with varying diversity levels.

Result: Moderate epistemic diversity optimally mitigates collapse; too little diversity causes rapid decay, while too much reduces individual model capacity and impairs performance from the start.

Conclusion: AI monoculture risks require monitoring diversity and policies incentivizing domain-specific models to maintain robust AI ecosystems.

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [25] [LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts](https://arxiv.org/abs/2512.14706)
*Krunal Jesani,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: LLM-guided NAS pipeline generates image-captioning models using CNN encoders and sequence decoders, evaluated on MS COCO with BLEU-4.


<details>
  <summary>Details</summary>
Motivation: Traditional NAS requires human expertise or automated trial-and-error; this work aims to automate model design via LLMs.

Method: Compose CNN encoders from LEMUR's classification backbones with decoders (LSTM/GRU/Transformer) under a Net API, using DeepSeek-R1-0528-Qwen3-8B for generation.

Result: Generated dozens of models, over half successfully trained; slight drop in success rate with more prompt components (10 vs. 5 snippets).

Conclusion: Highlights promise of LLM-guided NAS for automating architecture design, hyperparameters, and training, with challenges addressed via iterative fixes.

Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.

</details>


### [26] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: Proposes AutoS method for unsupervised multi-domain adaptation that autonomously selects relevant source samples and models while using pseudo-label enhancement to handle target domain label noise.


<details>
  <summary>Details</summary>
Motivation: Multiple source domains often contain redundant or unrelated information that can harm transfer performance, especially in massive-source domain settings. Need effective strategies to identify and select the most transferable knowledge.

Method: AutoS method uses density-driven selection strategy to choose source samples and determine contributing source models. Includes pseudo-label enhancement module based on pre-trained multimodal model to mitigate target label noise.

Result: Experiments on real-world datasets demonstrate the superiority of the proposed method.

Conclusion: AutoS effectively addresses unsupervised multi-domain adaptation by autonomously selecting transferable source knowledge while handling target domain challenges.

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [27] [SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI](https://arxiv.org/abs/2512.14712)
*Ryan Cartularo*

Main category: cs.LG

TL;DR: This paper compares two multimodal fusion approaches for sepsis prediction: End-to-End Deep Fusion (SepsisFusionFormer) and Context-Aware Stacking (SepsisLateFusion). While SepsisFusionFormer underperformed, SepsisLateFusion achieved state-of-the-art results using a mixture-of-experts architecture with specialized modality handlers and dynamic gating. The system is deployed in SepsisSuite.


<details>
  <summary>Details</summary>
Motivation: Sepsis remains a major ICU challenge, but existing prediction models struggle to effectively integrate heterogeneous data streams (vitals, text, imaging). Current methods are either modality-siloed or use brittle early fusion approaches that don't handle complex cross-modal interactions well.

Method: The study compares two approaches: 1) End-to-End Deep Fusion via SepsisFusionFormer (quad-modal hierarchical gated attention network), and 2) Context-Aware Stacking via SepsisLateFusion (mixture-of-experts architecture with three specialized experts: Static-Historian, Temporal-Monitor, NLP-Reader, dynamically gated by CatBoost meta-learner).

Result: SepsisFusionFormer suffered from "attention starvation" and overfitting (AUC 0.66), while SepsisLateFusion achieved SOTA performance: 0.915 AUC for 4-hour prediction. It reduced missed cases by 48% through calibrated decision thresholds. For antibiotic selection, a quad-modal ensemble achieved 0.72 AUC.

Conclusion: Context-Aware Stacking with specialized modality experts outperforms end-to-end deep fusion for sepsis prediction, particularly in small datasets. The SepsisLateFusion architecture provides clinically actionable predictions with reduced missed cases, enabling preventative interventions. The framework is available as SepsisSuite for clinical deployment.

Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info

</details>


### [28] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: Paper introduces Latent Class Reinforcement Learning (LCRL) model to capture heterogeneity in preference evolution during travel decisions, applied to driving simulator data with Variational Bayes estimation, identifying three distinct traveler classes with different adaptation patterns.


<details>
  <summary>Details</summary>
Motivation: Travel decisions involve experiential learning with individual heterogeneity in preferences and adaptation processes, requiring models that capture both phenomena.

Method: Developed a Latent Class Reinforcement Learning (LCRL) model estimated using Variational Bayes on driving simulator data.

Result: Identified three traveler classes: 1) context-dependent preferences with exploitative tendencies, 2) persistent exploitative strategy, 3) exploratory strategy with context-specific preferences.

Conclusion: The LCRL model effectively captures heterogeneity in preference evolution, providing insights for analyzing adaptive travel behavior.

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [29] [Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms](https://arxiv.org/abs/2512.14714)
*Lucas Cesar Ferreira Domingos,Russell Brinkworth,Paulo Eduardo Santos,Karl Sammut*

Main category: cs.LG

TL;DR: GSE ResNeXt, combining Gabor filters with ResNeXt and attention mechanisms, outperforms baselines in underwater acoustic classification, with 28% faster training and better robustness across environmental conditions.


<details>
  <summary>Details</summary>
Motivation: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses challenges. Issues such as limited dataset availability and lack of standardized experimentation hinder generalization and robustness.

Method: GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms.

Result: GSE ResNeXt consistently outperforms baseline models (Xception, ResNet, MobileNetV2) in classification performance. Addition of Gabor convolutions reduces training time by 28%. Model shows improved stability, convergence, and feature extraction capabilities.

Conclusion: Future developments should focus on mitigating the impact of environmental factors on input signals.

Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.

</details>


### [30] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: BLADE framework uses gradient-based sensitivity to identify specific bits in LLM weights that subtly alter semantic meaning in image captions while preserving grammar.


<details>
  <summary>Details</summary>
Motivation: Traditional fault analysis overlooks semantic dimensions in generative systems; bit flips can subtly shift semantic meaning in LLM-generated narratives.

Method: Differentiable fault analysis (BLADE) uses gradient-based sensitivity estimation to locate semantically critical bits, refined through a caption-level semantic-fluency objective.

Result: Identifies that specific bit perturbations can steer semantic output while maintaining syntax, revealing how meaning is encoded at bit level.

Conclusion: Bit-level faults can reshape semantic output in vision-language models, enabling applications in robustness testing, adversarial defense, and explainable AI.

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [31] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: Smaller GPT-OSS-20B matches larger models' financial NLP performance with significantly better efficiency, challenging scale-performance assumptions and enabling cost-effective deployment.


<details>
  <summary>Details</summary>
Motivation: To address the need for rigorous evaluation frameworks for LLMs in financial services, particularly assessing performance, efficiency, and practical applicability given rapid adoption in the industry.

Method: Comprehensive evaluation of GPT-OSS models (120B and 20B variants) alongside contemporary LLMs across ten diverse financial NLP tasks using real-world datasets (Financial PhraseBank, FiQA-SA, FLARE FINERORD) and novel efficiency metrics.

Result: GPT-OSS-20B achieves comparable accuracy to larger variants (65.1% vs 66.5%) with superior computational efficiency (198.4 Token Efficiency Score, 159.80 tokens/second), outperforming larger competitors including Qwen3-235B.

Conclusion: The GPT-OSS model family's architectural innovations allow smaller models to match larger competitors' performance with drastically improved efficiency, enabling sustainable LLM deployment in financial services.

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [32] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: SEED is a spectral entropy-guided framework for multivariate time series forecasting that addresses limitations in existing dependency modeling methods through dynamic dependency evaluation and signed graph construction.


<details>
  <summary>Details</summary>
Motivation: Existing attention- or graph-based methods face issues with disrupted temporal dependencies, ignored negative correlations due to softmax normalization, and lack of temporal position awareness in variables.

Method: Proposes a Dependency Evaluator using spectral entropy for adaptive CI/CD balancing, Spectral Entropy-based Fuser for refined dependency weights, Signed Graph Constructor for negative correlation preservation, and Context Spatial Extractor for temporal position awareness.

Result: Achieves state-of-the-art performance across 12 real-world datasets from various domains.

Conclusion: SEED effectively addresses key limitations in spatial-temporal dependency modeling and demonstrates strong generalization capabilities.

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [33] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: Proposes Class-Aware Attribution Prior (CAP) framework that extracts discriminative attribution priors to help language models better distinguish between semantically similar classes.


<details>
  <summary>Details</summary>
Motivation: Existing attribution methods highlight class-relevant tokens but focus on common keywords shared by similar classes, providing insufficient discriminative cues for model differentiation.

Method: CAP extracts attribution priors that capture fine-grained class distinctions, and CAP Hybrid combines these with existing attribution techniques for more balanced supervision.

Result: Extensive experiments show consistent improvements in interpretability and robustness across full-data, few-shot, and adversarial scenarios.

Conclusion: The proposed attribution framework effectively enhances model differentiation capabilities while improving both interpretability and robustness.

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [34] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: Automated generation of Synthea rules from cancer statistics enables privacy-compliant synthetic medical data that preserves statistical properties and disease patterns.


<details>
  <summary>Details</summary>
Motivation: To create realistic synthetic medical data in a privacy-compliant manner, addressing the challenge that manual rule creation for systems like Synthea is complex and requires extensive expert knowledge and sample data.

Method: The approach involves automatically generating Synthea rules from statistics extracted from tabular cancer report data, with a specific application to glioblastoma using real-world datasets.

Result: The synthetic glioblastoma dataset reproduced known disease courses and largely maintained the statistical properties of the original dataset.

Conclusion: Synthetic patient data shows significant potential for privacy-preserving medical research, particularly for hypothesis formulation and prototype development, though medical interpretation should account for the limitations of current synthetic data generation approaches.

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [35] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: Hierarchical Attention Transformers (HATs) with curriculum learning enable more efficient Groebner bases computation for large multivariate polynomial systems by leveraging tree-structured inductive bias.


<details>
  <summary>Details</summary>
Motivation: To improve upon Kera et al.'s transformer-based approach for computing Groebner bases, addressing the computational limitations of conventional flat attention models when dealing with the hierarchical structure present in multivariate polynomial systems.

Method: The approach uses Hierarchical Attention Transformers (HATs) with tree-structured inductive bias to model hierarchical relationships in polynomial data, combined with curriculum learning for training. The method generalizes to arbitrary depths and includes computational cost analysis.

Result: The method achieves significant computational savings compared to conventional flat attention models and successfully solves instances that are much larger than those in Kera et al.'s prior work.

Conclusion: The use of Hierarchical Attention Transformers (HATs) with curriculum learning significantly improves the scalability and computational efficiency of Groebner bases computation, enabling the solution of much larger polynomial systems than previous methods.

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [36] [Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion](https://arxiv.org/abs/2512.14725)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Petros Koumoutsakos,Soledad Le Clainche*

Main category: cs.LG

TL;DR: A generative diffusion framework using hierarchical graph neural networks to synthesize urban wind fields from geometry data alone, enabling rapid evaluation of urban design decisions without costly CFD simulations.


<details>
  <summary>Details</summary>
Motivation: Urban wind flow modeling is crucial for air quality assessment and sustainable city planning, but current approaches face challenges: low order models are limited in capturing geometric effects, while high-fidelity CFD simulations are prohibitively expensive across multiple geometries or wind conditions.

Method: The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. The model is trained across multiple mesh slices and wind angles.

Result: The model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes.

Conclusion: This work represents the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.

Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.

</details>


### [37] [Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning](https://arxiv.org/abs/2512.14726)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: The Quantum Decision Transformer (QDT) introduces quantum-inspired computational mechanisms to offline RL, achieving over 2,000% performance improvement over standard Decision Transformers through synergistic quantum attention and feedforward networks.


<details>
  <summary>Details</summary>
Motivation: Standard Decision Transformers struggle with long-horizon credit assignment and complex state-action dependencies in offline reinforcement learning.

Method: Integrates Quantum-Inspired Attention with entanglement for non-local feature correlations and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation.

Result: Demonstrates over 2,000% performance improvement compared to standard DTs, with superior generalization across varying data quality and strong synergistic effects between quantum components.

Conclusion: Quantum-inspired architecture design requires holistic co-design of interdependent mechanisms, establishing quantum principles as promising for advancing transformer architectures in sequential decision-making.

Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.

</details>


### [38] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: This paper critiques conformal prediction's practical utility in medical settings, showing that while statistical guarantees hold for any calibration set size, meaningful practical utility requires larger calibration sets than often available in healthcare.


<details>
  <summary>Details</summary>
Motivation: Machine learning needs reliable uncertainty estimates for safe clinical decisions, but standard ML models fail here. Conformal prediction offers statistical guarantees, but the promise of working with small calibration sets needs examination.

Method: The authors question CP's effectiveness with small calibration sets through theoretical analysis and empirical demonstration on a medical image classification task.

Result: Results show that although CP's statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees depends heavily on calibration set size.

Conclusion: CP's practical value in medical domains is limited by data scarcity, as small calibration sets yield statistically valid but practically useless uncertainty estimates.

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [39] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: A data-driven approach for inferring urban rail transit trajectories using AFC/AVL data, achieving over 90% accuracy during peak hours without needing external parameter data.


<details>
  <summary>Details</summary>
Motivation: Improved trajectory inference is crucial for transit operation organization, but existing methods often rely on external/survey data or synthetic validation, limiting robustness and applicability.

Method: Uses AFC and AVL data to infer trajectories through train alternative sets, data-driven adaptive inference via KLEM (KL divergence with EM algorithm) for parameter estimation, and trajectory construction.

Result: The method achieves high-precision trajectory inference with over 90% accuracy during peak hours, validated using real individual travel data.

Conclusion: The fully data-driven approach effectively infers passenger trajectories without external data dependency, enhancing robustness and practical applicability in urban rail transit systems.

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [40] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: A geometric framework that prevents hallucinated commitments in high-stakes decision-making by representing semantic meaning on a unit sphere and using policy constraints as explicit priors.


<details>
  <summary>Details</summary>
Motivation: To prevent hallucinated (false or unsubstantiated) commitments in high-stakes domains like regulated financial systems, where incorrect approvals or interpretations can have serious consequences.

Method: Represent semantic meaning as direction on a unit sphere, model evidence as witness vectors, define admissible interpretations as spherical convex regions, introduce policy constraints as explicit priors on the same manifold, and perform constrained optimization over admissible regions.

Result: Zero hallucinated approvals across multiple policy regimes in large-scale regulated financial data - the first such result at scale. The framework achieves information-theoretically optimal complexity bounds.

Conclusion: The geometric framework successfully prevents hallucinated commitments by separating policy constraints from evidence geometry and enabling refusal as a natural outcome under contradiction or policy exclusion, with provable theoretical properties and empirical validation.

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [41] [INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT](https://arxiv.org/abs/2512.14732)
*Idan Tankel,Nir Mazor,Rafi Brada,Christina LeBedis,Guy ben-Yosef*

Main category: cs.LG

TL;DR: Proposes an automated framework using LLMs and VLMs to detect, classify, and report incidental findings in abdominal CT scans, improving accuracy and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of incidental findings in CT scans is time-consuming and inconsistent. Automating this process can enhance efficiency and precision in clinical reporting.

Method: Uses a planner-executor framework: an LLM-based planner generates Python scripts based on medical guidelines, executed by VLMs, segmentation models, and image processing subroutines for automated detection and classification.

Result: The framework outperforms pure VLM-based approaches in accuracy and efficiency on a CT abdominal benchmark for three organs, demonstrating effectiveness in an end-to-end automatic manner.

Conclusion: The plan-and-execute agentic approach effectively automates incidental findings management, offering a scalable solution for improving clinical workflows in radiology.

Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.

</details>


### [42] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: Lightweight approach for intra-day personalization in video streaming that updates user features with recent watch history at inference time, improving engagement metrics by 0.47%.


<details>
  <summary>Details</summary>
Motivation: Batch-trained recommender systems have stale user features updated only daily, failing to incorporate recent user actions and leading to outdated recommendations.

Method: Model-agnostic approach that selectively injects recent watch history to override stale user features at inference time without requiring model retraining.

Result: Statistically significant 0.47% increase in key user engagement metrics, representing substantial improvement over daily-updated systems.

Conclusion: First evidence that intra-day personalization can drive meaningful impact in long-form video streaming, offering a practical alternative to real-time architectures requiring model retraining.

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [43] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: Developed AI models to automatically rank conceptual novelty of research papers using title/abstract analysis and semantic similarity, comparing binary classification vs pairwise comparison approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying truly novel research among the overwhelming volume of AI publications, providing a data-driven and scalable alternative to unstable manual assessment.

Method: Fine-tuned Qwen3-4B-Instruct-2507 and SciBERT models on two task formulations: binary classification for absolute novelty and pairwise comparison for relative novelty, comparing against GPT-5.1.

Result: Implementation is publicly available and models were benchmarked against GPT-5.1 to analyze performance differences between task formulations and modeling choices.

Conclusion: The study demonstrates that automated novelty assessment is feasible and provides a scalable solution to handle the overwhelming volume of academic publications, with pairwise novelty comparison showing particular promise.

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [44] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: Unsupervised discrete diffusion guidance solves Sudoku puzzles via constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for solving constraint satisfaction problems often require supervision or predefined rules; this paper aims to develop an unsupervised approach.

Method: Apply discrete diffusion guidance to constraint satisfaction problems, specifically using Sudoku as a test case without supervised training.

Result: The method successfully solves Sudoku puzzles without requiring labeled data or explicit rule encoding.

Conclusion: Discrete diffusion guidance is a viable unsupervised technique for solving CSPs, with potential applications beyond Sudoku.

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [45] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: Decision calibration framework shows forecast model performance at statistical level doesn't predict decision-making value; ML and classical NWP models rank differently across decision tasks.


<details>
  <summary>Details</summary>
Motivation: Weather forecasts are used for decision-making, so evaluation should focus on how forecasts improve decision quality rather than just statistical accuracy.

Method: Decision calibration framework evaluating forecast performance at the decision level rather than forecast level, applied to compare Machine Learning and classical numerical weather prediction models.

Result: Model performance at forecast level does not reliably translate to decision-making performance; some differences only appear at decision level, and model rankings change across different decision tasks.

Conclusion: Standard forecast evaluations are insufficient for selecting optimal models for specific decision tasks, as forecast-level performance does not reliably translate to decision-level performance.

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [46] [Unreliable Uncertainty Estimates with Monte Carlo Dropout](https://arxiv.org/abs/2512.14851)
*Aslak Djupskås,Alexander Johannes Stasik,Signe Riemer-Sørensen*

Main category: cs.LG

TL;DR: Monte Carlo Dropout (MCD) provides computationally efficient uncertainty estimation for deep learning but fails to match the reliability of traditional Bayesian methods like Gaussian Processes and Bayesian Neural Networks in capturing true epistemic and aleatoric uncertainty.


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty estimation is crucial for safety-critical machine learning applications, but exact Bayesian inference is computationally prohibitive for deep neural networks.

Method: Empirical investigation comparing MCD's uncertainty estimation capabilities against Gaussian Processes and Bayesian Neural Networks.

Result: MCD struggles to accurately reflect true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions where Bayesian models perform better.

Conclusion: Uncertainty estimates from MCD are not as reliable as those from traditional Bayesian approaches for capturing both epistemic and aleatoric uncertainty.

Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.

</details>


### [47] [How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal](https://arxiv.org/abs/2512.14873)
*Sam Jeong,Hae Yong Kim*

Main category: cs.LG

TL;DR: FAN's performance gains come from sine activation's local behavior near zero (not periodicity), mitigating vanishing gradients and dying-ReLU problem. This led to DAL, a more efficient activation that accelerates convergence.


<details>
  <summary>Details</summary>
Motivation: The motivation was to understand the underlying mechanism behind the performance improvements of Fourier Analysis Network (FAN), which replaces some ReLU activations with sine and cosine functions, and to determine whether the benefits come from the periodic nature or other properties of these functions.

Method: The authors analyzed the individual contributions of sine and cosine activations in FAN, investigated the mechanism behind performance gains, and developed the Dual-Activation Layer (DAL) as a more efficient alternative. Evaluation was conducted on three tasks: classification of noisy sinusoidal signals vs. pure noise, MNIST digit classification, and ECG-based biometric recognition.

Result: Results showed that only the sine activation contributes positively to performance, while cosine is detrimental. FAN primarily alleviates the dying-ReLU problem by providing a more stable gradient pathway. DAL models converged faster and achieved equal or higher validation accuracy compared to conventional activations in all three evaluated tasks.

Conclusion: The study concludes that the FAN's improvement stems from the sine function's local behavior near zero, which mitigates the vanishing gradient problem, rather than its periodic nature. This insight led to the development of DAL, a more efficient activation that accelerates convergence and improves performance across various tasks.

Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.

</details>


### [48] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: ERBP is an information-geometric framework that explains model collapse in self-referential learning and introduces an Entropy Reservoir concept with quantitative design rules.


<details>
  <summary>Details</summary>
Motivation: Self-referential learning suffers from model collapse (degeneration in language models, mode dropping in GANs, policy over-exploitation in RL), but existing fixes lack unifying principles.

Method: Models the closed loop as stochastic Bregman projection sequence in distribution space and introduces Entropy Reservoir (high-entropy distribution mixing) to stabilize dynamics.

Result: Provides necessary/sufficient conditions for collapse/stability, closed-form convergence rates, and experimental validation across LLMs, RL, and GANs.

Conclusion: ERBP unifies disparate stabilization methods into quantitative entropy flux management, transforming ad-hoc fixes into systematic design principles.

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [49] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: The paper demonstrates that linear task matrices can effectively adapt base models to new tasks, achieving performance approaching fine-tuning levels across multiple vision and text datasets.


<details>
  <summary>Details</summary>
Motivation: Previous interpretability work showed linear encodings exist in in-context prompted models, but similar linear representations in general adaptation regimes remained unexplored.

Method: Developed the concept of a task matrix - a linear transformation from base to fine-tuned embedding states. Evaluated on vision and text models across ten datasets.

Result: Base models augmented with task matrices surpassed linear probes and sometimes approached fine-tuned performance levels. Demonstrated cross-layer linear encodings between pretrained and fine-tuned architectures exist.

Conclusion: Task matrices provide an efficient, generalizable adaptation method that validates the existence of linear encodings in model adaptation, with implementation made publicly available.

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [50] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: OLR-WA is a novel online linear regression model that achieves batch regression comparable performance, handles data drift, converges rapidly, and uniquely manages confidence-based scenarios by prioritizing older, higher-confidence data.


<details>
  <summary>Details</summary>
Motivation: Online learning needs efficient incremental updates without costly recalculations, but must handle evolving data patterns (drift) and confidence variations in real-world scenarios.

Method: Proposes OLR-WA (Online Regression with Weighted Average), a multivariate online linear regression model that uses weighted updates prioritizing older data with higher confidence.

Result: OLR-WA matches batch regression performance, surpasses other online models in convergence (high r2 from first iteration), works with minimal initial data (1-10%), and uniquely handles confidence-based scenarios effectively.

Conclusion: OLR-WA is versatile and effective for online linear regression, solidifying its utility across different contexts including temporal drift and confidence challenges.

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [51] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: This paper proposes on-policy expert corrections (OECs) to address covariate shift in multi-turn LLM agent training, showing 13-14% improvements over imitation learning on SWE-bench.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for multi-turn LM agents suffers from covariate shift when student behavior diverges from expert demonstrations.

Method: Generate OEC data by starting rollouts with student model and switching to expert model mid-trajectory, combined with rejection sampling and supervised fine-tuning.

Result: OEC trajectories achieve relative 14% and 13% improvement over imitation learning in 7b and 32b settings on SWE-bench verified.

Conclusion: Combining expert demonstrations with on-policy data is essential for effective multi-turn LM agent training.

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [52] [ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs](https://arxiv.org/abs/2512.14908)
*Turja Kundu,Sanjukta Bhowmick*

Main category: cs.LG

TL;DR: ATLAS is a novel graph learning algorithm that uses multi-scale community topology with MLPs instead of GNN aggregation, achieving better accuracy on heterophilic graphs and improved scalability.


<details>
  <summary>Details</summary>
Motivation: GNNs face two key limitations: performance degradation on heterophilic graphs and poor scalability due to iterative aggregation.

Method: Extracts topological community information at multiple resolutions, concatenates community assignments to node features, and applies MLPs instead of GNN aggregation.

Result: ATLAS achieves comparable accuracy to baselines, with gains up to 20 percentage points over GCN on heterophilic graphs and 11 points over MLP on homophilic graphs.

Conclusion: The approach provides topological context without aggregation, offers better scalability, and introduces explainable graph learning through multi-resolution community features.

Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.

</details>


### [53] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: Proposed method efficiently selects regularization parameter for low-rank MMSE filters using Kronecker-product representation, showing strong link to rank selection and significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The regularization parameter is surprisingly linked to rank selection, and properly choosing it is crucial for low-rank settings in MMSE filters.

Method: A method to efficiently find the regularization parameter for low-rank MMSE filters based on Kronecker-product representation, showing the parameter is linked to rank selection.

Result: Simulations validate the proposed method, showing significant gains over commonly used methods.

Conclusion: The proposed method for selecting the regularization parameter in low-rank MMSE filters is validated as highly effective through simulations, showing significant performance improvements over conventional approaches.

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [54] [Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise](https://arxiv.org/abs/2512.14967)
*Felipe J. P. Antunes,Yuri F. Saporito,Sebastian Jaimungal*

Main category: cs.LG

TL;DR: Novel method for MV-FBSDEs using elicitability and deep learning avoids nested Monte Carlo, validated on systemic risk model and applied to economic growth model.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving MV-FBSDEs with common noise that avoids computationally expensive nested Monte Carlo simulations, leveraging the concept of elicitability to create a path-wise loss function for neural network training.

Method: The method combines Picard iterations, elicitability, and deep learning. It uses an elicitable score to derive a path-wise loss function for training neural networks without nested Monte Carlo. The mean-field interaction is parameterized via a recurrent neural network, and the backward process is approximated with a feedforward network representing the decoupling field.

Result: The algorithm accurately recovers the true solution in a systemic risk inter-bank borrowing and lending model where analytical solutions exist. It is successfully extended to quantile-mediated interactions and applied to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, demonstrating its applicability to complex mean-field games without closed-form solutions.

Conclusion: A novel numerical method for solving MV-FBSDEs with common noise is presented, combining Picard iterations, elicitability, and deep learning. The approach successfully avoids computationally expensive nested Monte Carlo simulations by using an elicitable path-wise loss function. The method is validated on a systemic risk model with analytical solutions, extended to quantile-mediated interactions, and applied to a complex economic growth model, demonstrating its accuracy and flexibility.

Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.

</details>


### [55] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: The paper introduces a new method to integrate constraints into diffusion models by modifying the denoiser, improving constraint compliance while maintaining flexibility when constraints are misspecified.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enforcing constraints in diffusion models introduce bias away from the true data distribution, especially problematic when constraints are misspecified in scientific applications.

Method: Instead of modifying the loss function or sampling loop, the authors incorporate a guidance-inspired adjustment directly into the denoiser to create a soft inductive bias toward constraint-compliant samples.

Result: The proposed softly constrained denoisers achieve better constraint compliance than standard denoisers while remaining flexible enough to deviate from misspecified constraints when data evidence contradicts them.

Conclusion: Integrating constraint guidance directly into the denoiser provides an effective balance between constraint enforcement and distribution fidelity, particularly valuable for scientific applications where constraint misspecification is common.

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [56] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: Simply repeating input prompts boosts performance in major AI models without extra computational overhead


<details>
  <summary>Details</summary>
Motivation: To improve model performance in non-reasoning scenarios without increasing token generation or latency

Method: Evaluating popular models (Gemini, GPT, Claude, Deepse   ek) by repeating input prompts during non-reasoning tasks

Result: Performance improves for all tested models when input prompts are repeated

Conclusion: Repeating input prompts enhances model performance without additional computational costs

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [57] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: A model-based reinforcement learning algorithm for unbounded diffusion processes with adaptive state-action space partitioning, achieving efficient learning with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Many real-world problems in finance, economics, and operations research involve reinforcement learning for diffusion processes with unbounded states, continuous actions, and polynomial rewards, but existing methods struggle in such continuous high-dimensional settings.

Method: Adaptive partitioning of joint state-action space with estimators for drift, volatility, and rewards in each partition, refining discretization when estimation bias exceeds statistical confidence.

Result: Established regret bounds depending on horizon, state dimension, reward growth, and zooming dimension, recovering bounded case results while extending to unbounded diffusion problems.

Conclusion: The algorithm effectively balances exploration and approximation for unbounded domains, validated through numerical experiments including multi-asset portfolio selection.

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [58] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-Code introduces function-based reasoning steps and meta-learning label correction for coding PRMs, achieving SOTA 80.9% pass@1 on LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of Process Reward Models in coding tasks due to lack of meaningful step decompositions and noise in Monte-Carlo-generated partial labels.

Method: Uses Chain-of-Function prompting to treat functions as reasoning steps, enabling modular code generation and meta-learning-based label correction through bi-level optimization.

Result: Achieved 80.9 pass@1 rate on LiveCodeBench, surpassing OpenAI o4-mini and establishing state-of-the-art performance.

Conclusion: DreamPRM-Code achieves state-of-the-art performance on coding benchmarks, significantly advancing the application of Process Reward Models in programming tasks.

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [59] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: SPA is a deterministic framework that extracts monotonic price runs, aligns them with public events, and generates transparent explanations using only daily OHLCV data and normalized events.


<details>
  <summary>Details</summary>
Motivation: Existing tools like technical indicators and predictive models lack transparency and auditability, creating challenges in settings requiring clear explanations.

Method: SPA uses deterministic segmentation to identify price runs, symmetric correlation windows to attach relevant events, and constrained explanation generation for interpretability.

Result: Evaluation on AAPL, NVDA, SCHW, and PGR shows SPA produces stable structural decompositions and contextual narratives, with ablation studies confirming the importance of each component.

Conclusion: SPA provides a transparent, reproducible view of historical price structure for analytical workflows, not forecasting, enhancing interpretability in finance.

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [60] [Spectral Representation-based Reinforcement Learning](https://arxiv.org/abs/2512.15036)
*Chenxiao Gao,Haotian Sun,Na Li,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: Introduces spectral representations as a theoretically sound solution to challenges in reinforcement learning with function approximations, showing competitive performance on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Function approximations like neural networks in RL face theoretical ambiguities, optimization instability, exploration difficulty, and high computational costs.

Method: Proposes spectral representations derived from transition operator decomposition, with methods for constructing representations under latent variable or energy-based structures and learning them from data.

Result: Developed RL algorithms achieve comparable or superior performance to state-of-the-art model-free and model-based baselines on over 20 DeepMind Control Suite tasks.

Conclusion: Spectral representations provide an effective abstraction with clear theoretical grounding, addressing key RL challenges and enabling practical, high-performing algorithms.

Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.

</details>


### [61] [EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks](https://arxiv.org/abs/2512.15067)
*Zijiang Yan,Yixiang Huang,Jianhua Pei,Hina Tabassum,Luca Chiaraviglio*

Main category: cs.LG

TL;DR: EMFusion is a multivariate diffusion-based forecasting framework that integrates contextual factors to predict electromagnetic field levels with uncertainty quantification, achieving significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing EMF forecasting methods use univariate approaches that cannot capture inter-operator and inter-frequency variations needed for proactive network planning, necessitating frequency-selective multivariate forecasting.

Method: EMFusion uses a residual U-Net backbone with cross-attention to integrate contextual factors (time, season, holidays) and treats forecasting as structural inpainting with imputation-based sampling for temporal coherence.

Result: EMFusion outperforms baselines by 23.85% in CRPS, 13.93% in normalized RMSE, and reduces prediction CRPS error by 22.47% when using working hours context.

Conclusion: EMFusion provides accurate, probabilistic EMF forecasting with explicit uncertainty quantification, essential for trustworthy decision-making in wireless network planning and compliance.

Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.

</details>


### [62] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: RAG systems still hallucinate despite retrieval; embedding-based detection fails due to 'semantic illusion'; GPT-4 shows better detection through reasoning


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection methods using semantic similarity and NLI have fundamental limitations that need rigorous characterization

Method: Applied conformal prediction to hallucination detection using calibration sets of ~600 examples; tested on synthetic and real benchmarks across multiple LLMs

Result: Embedding-based methods show unacceptable FPR (100% on HaluEval, 88% on RAGTruth, 50% on WikiBio) while GPT-4 achieves only 7% FPR

Conclusion: Semantic illusion makes embedding-based detection insufficient for production RAG; reasoning-based approaches like GPT-4 are necessary

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [63] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: FEAML is an automated feature engineering method for multi-label learning that uses LLMs and feedback loops to generate high-quality features.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based feature engineering methods don't model complex label dependencies well and aren't adapted for multi-label tasks.

Method: Uses LLMs' code generation capabilities guided by metadata and label co-occurrence matrices, with feature evaluation and redundancy detection via Pearson correlation.

Result: FEAML outperforms other feature engineering methods on various multi-label datasets.

Conclusion: FEAML creates an efficient, interpretable, and self-improving feature engineering paradigm for multi-label learning.

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [64] [Neural Modular Physics for Elastic Simulation](https://arxiv.org/abs/2512.15083)
*Yifei Li,Haixu Wu,Zeyi Xu,Tuur Stuyck,Wojciech Matusik*

Main category: cs.LG

TL;DR: Neural Modular Physics (NMP) is a hybrid framework that integrates neural networks with traditional physics simulation principles, dividing elastic dynamics into interpretable modules to improve reliability, generalization, and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Existing monolithic neural simulators lack physical interpretability and reliability compared to traditional numerical methods, which operate modularly.

Method: NMP decomposes elastic dynamics into neural modules connected by intermediate physical quantities, allowing supervised learning of intermediate steps and enforcement of physical constraints.

Result: NMP shows superior generalization to unseen initial conditions and resolutions, stable long-term simulation, better physical property preservation than neural simulators, and greater feasibility than traditional simulators in unknown dynamics.

Conclusion: Modular neural simulation combines neural networks' approximation power with traditional simulators' reliability, offering a promising direction for physics-based learning.

Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.

</details>


### [65] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: A new operator learning method called PIP^2 Net is proposed that uses partition-of-unity regularization to improve stability and accuracy in solving parameterized PDEs.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning architectures like DeepONet and FNO suffer from instability in trunk-network features, mode imbalance, lack of explicit physical structure, and require large training datasets.

Method: The authors develop PIP^2 Net by revising the POU-PI-DeepONet framework with a simplified and more principled partition penalty to improve coordinated trunk outputs while maintaining DeepONet's flexibility.

Result: PIP^2 Net consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness on three nonlinear PDEs: viscous Burgers equation, Allen-Cahn equation, and a diffusion-reaction system.

Conclusion: The proposed PIP^2 Net framework successfully addresses instability issues in operator learning through partition-of-unity regularization, demonstrating improved performance while maintaining model flexibility.

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


### [66] [SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs](https://arxiv.org/abs/2512.15088)
*Xianglin Wu,Chiheb Ben Hammouda,Cornelis W. Oosterlee*

Main category: cs.LG

TL;DR: SigMA integrates path signatures with attention for superior parameter estimation in fBm-driven SDEs compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Classical methods struggle with fBm-driven SDEs due to non-Markovian/non-semimartingale properties; need accurate yet tractable parameter inference.

Method: SigMA neural architecture combines path signatures, multi-head self-attention, convolutional preprocessing, and MLP for feature encoding.

Result: Outperforms CNN, LSTM, Transformer, and Deep Signature baselines in accuracy, robustness, and compactness on synthetic/real-world data.

Conclusion: Signature-attention fusion offers an effective, scalable framework for parameter inference in rough/persistent stochastic systems.

Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.

</details>


### [67] [Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption](https://arxiv.org/abs/2512.15112)
*Sunwoo Kim,Soo Yong Lee,Kyungho Kim,Hyunjin Hwang,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: FUEL is an unsupervised graph learning method that adaptively controls graph convolution usage to prevent over-smoothing by treating identified clusters as pseudo-classes for similarity optimization.


<details>
  <summary>Details</summary>
Motivation: Graph convolution's over-reliance can cause poor embeddings in non-homophilic graphs by making dissimilar nodes overly similar. Current adaptive convolution methods are mainly explored in supervised settings, leaving a gap in unsupervised learning.

Method: FUEL learns the optimal convolution degree by maximizing intra-cluster similarity and inter-cluster separability. It uses node features to identify clusters as pseudo-classes when labels are unavailable.

Result: Extensive experiments on 14 benchmarks with 15 baselines show FUEL achieves state-of-the-art performance across varying homophily levels in downstream tasks.

Conclusion: FUEL effectively addresses unsupervised graph learning by balancing convolution usage via cluster-aware optimization, demonstrating robustness on diverse graph types.

Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.

</details>


### [68] [How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models](https://arxiv.org/abs/2512.15115)
*Ali Ghodsi*

Main category: cs.LG

TL;DR: A unified theoretical framework reveals fundamental trade-offs: attention models have gradient advantages but limited expressivity, while state-space models are more expressive but suffer from gradient attenuation.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical understanding of expressive power and trainability differences among various sequence modeling architectures (RNNs, Transformers, state space models) where current understanding is fragmented.

Method: Introduce a unified framework through a data-dependent effective interaction operator W_ij(X), categorizing models into two classes: Unified Factorized Framework (attention-style mixing) and Structured Dynamics (latent dynamical systems). Derive three theoretical results: Interaction Rank Gap, Equivalence (Head-Count) Theorem, and Gradient Highway Result.

Result: Models in the Unified Factorized Framework have limited operator span and cannot represent certain structured dynamical maps. Representing a linear SSM requires heads equal to the lag operator subspace dimension. Attention layers maintain distance-independent gradient paths while stable linear dynamics suffer from distance-dependent gradient attenuation.

Conclusion: Attention-based architectures excel at long-range gradient flow but face expressivity limitations, while state-space models enable more expressive operations but struggle with efficient gradient propagation across long distances, revealing a formal trade-off that should guide architectural design.

Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.

</details>


### [69] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: FADTI is a diffusion-based time series imputation framework that uses frequency-informed feature modulation via Fourier Bias Projection to handle structured missing patterns better than Transformer/diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting generalization under structured missing patterns and distribution shifts in multivariate time series imputation.

Method: Proposes FADTI framework with learnable Fourier Bias Projection (FBP) module for frequency-informed feature modulation, combined with self-attention and gated convolution for temporal modeling. FBP supports multiple spectral bases for adaptive encoding of stationary/non-stationary patterns.

Result: Experiments on multiple benchmarks show FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates, including on a new biological time series dataset.

Conclusion: FADTI effectively injects frequency-domain inductive bias into generative imputation process, demonstrating superior performance for multivariate time series imputation tasks.

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [70] [Automatic Reward Shaping from Multi-Objective Human Heuristics](https://arxiv.org/abs/2512.15120)
*Yuqing Xie,Jiayu Chen,Wenhao Tang,Ya Zhang,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: MORSE is a framework that automatically combines multiple rewards into a unified function using bi-level optimization and stochastic exploration.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions is challenging in reinforcement learning, especially for multi-objective environments.

Method: MORSE formulates reward shaping as bi-level optimization: inner loop trains policies, outer loop updates rewards. It introduces stochasticity guided by task performance and neural network prediction error to avoid local minima.

Result: Experiments in MuJoCo and Isaac Sim show MORSE effectively balances multiple objectives across robotic tasks, achieving performance comparable to manually tuned rewards.

Conclusion: MORSE provides an automated approach to reward shaping that eliminates the need for manual tuning while maintaining competitive task performance.

Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.

</details>


### [71] [TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training](https://arxiv.org/abs/2512.15123)
*Mukur Gupta,Niharika Gupta,Saifur Rahman,Shantanu Pal,Chandan Karmakar*

Main category: cs.LG

TL;DR: TrajSyn enables server-side adversarial training in Federated Learning by synthesizing proxy datasets from client model updates without accessing raw data, improving robustness without client-side compute burden.


<details>
  <summary>Details</summary>
Motivation: Deep learning models on edge devices are vulnerable to adversarial attacks, but adversarial training is difficult in FL due to privacy constraints and limited client compute resources.

Method: TrajSyn synthesizes a proxy dataset from client model update trajectories to enable effective server-side adversarial training without accessing raw client data.

Result: TrajSyn consistently improves adversarial robustness on image classification benchmarks.

Conclusion: The framework provides effective adversarial defense in FL settings while preserving privacy and avoiding extra computational burden on client devices.

Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.

</details>


### [72] [From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?](https://arxiv.org/abs/2512.15134)
*Aaron Mueller,Andrew Lee,Shruti Joshi,Ekdeep Singh Lubana,Dhanya Sridhar,Patrik Reizinger*

Main category: cs.LG

TL;DR: Study examines if sparse autoencoders and probes create truly independent concept representations, revealing they don't maintain independence during steering despite appearing disentangled in correlational tests.


<details>
  <summary>Details</summary>
Motivation: Current interpretability methods evaluate concept representations in isolation, ignoring real-world correlations between concepts, making it unclear if they achieve true disentanglement.

Method: Controlled multi-concept evaluation with varying correlations between textual concepts (sentiment, domain, tense), testing featurizers' ability to learn disentangled representations and perform independent steering experiments.

Result: Features show one-to-many mapping (concepts spread across features) and steering affects multiple concepts simultaneously, indicating lack of true independence despite disjoint subspaces.

Conclusion: Correlational metrics are insufficient for establishing independence in steering; compositional evaluations are crucial for meaningful interpretability research.

Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.

</details>


### [73] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: Study compares ML models (XGBoost, Random Forest, LSTM, TCN) for crop yield prediction in Germany, finding models generalize poorly to temporal data despite good test performance, and reveals SHAP interpretations can appear reliable even when models fail to generalize.


<details>
  <summary>Details</summary>
Motivation: To examine generalization performance and interpretability of ML models for crop yield prediction, highlighting vulnerabilities in post hoc explainability methods when models don't generalize.

Method: Systematic comparison of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN) using a high-quality, long-term dataset from Germany's NUTS-3 regions with temporal validation.

Result: Models perform well on spatially split test sets but degrade substantially on temporally independent validation; SHAP feature importance appears credible even with poor generalization, exposing interpretability vulnerabilities.

Conclusion: Need validation-aware interpretation in agricultural ML; feature importance should not be trusted without proving generalization; advocates domain-aware validation and hybrid modeling for trustworthy explanations.

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [74] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: A new gradient-based membership inference attack for federated learning that analyzes gradient patterns across multiple rounds to infer membership and attributes.


<details>
  <summary>Details</summary>
Motivation: Federated learning reduces direct data exposure, but exchanging model updates can still leak sensitive information through gradient patterns.

Method: Uses shadow technique to learn round-wise gradient patterns without accessing private data, works for semi-honest/malicious adversaries, and extends to attribute inference.

Result: Strong attack performance on CIFAR-100, Purchase100, and Breast Cancer Wisconsin datasets, with comparable overhead to existing attacks.

Conclusion: Multi-round federated learning increases vulnerability; aggregators pose greater threat; attack effectiveness depends on data complexity.

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [75] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: The paper analyzes how different INR architectural components affect Neural Tangent Kernel conditioning and spectral bias through pairwise similarity factors and scaling terms.


<details>
  <summary>Details</summary>
Motivation: Implicit Neural Representations often converge slowly and struggle with high-frequency details due to spectral bias, but how specific architectural choices affect NTK conditioning remains unclear.

Method: The authors show INR mechanisms can be understood through their impact on pairwise similarity factors and scaling terms that determine NTK eigenvalue variance, providing closed-form decompositions for common components.

Result: Experiments confirm predicted variance reductions and demonstrate faster convergence with improved reconstruction quality across multiple tasks.

Conclusion: A unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning through mechanisms like positional encoding, spherical normalization, and Hadamard modulation.

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [76] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: DEER introduces diffusion-based speculative decoding to overcome limitations of autoregressive drafters, achieving significantly longer draft acceptance (32 vs 10 tokens) and higher speedups (5.54x vs 2.41x) than EAGLE-3.


<details>
  <summary>Details</summary>
Motivation: Autoregressive speculative decoding suffers from step-wise uncertainty accumulation and sequential decoding constraints, limiting speedup potential.

Method: DEER uses diffusion LLM drafters trained via two-stage alignment with target AR model, employing single-step decoding for parallel draft generation.

Result: Achieves draft acceptance length of 32 tokens and 5.54x speedup on HumanEval with Qwen3-30B-A3B, outperforming EAGLE-3's 10 tokens and 2.41x speedup.

Conclusion: Diffusion-based drafting effectively addresses AR drafter limitations, demonstrating superior efficiency for LLM reasoning systems.

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [77] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: Self-supervised ECG encoder + pretrained EEG encoder, fused via concatenation, achieves near SOTA emotion recognition—foundation models enable label-efficient multi-modal learning.


<details>
  <summary>Details</summary>
Motivation: Overcome multi-modal integration challenges (limited labeled data, modality differences) for physiological signals (ECG/EEG) to enable effective cross-modal learning.

Method: Adapt the CBraMod encoder for large-scale self-supervised ECG pretraining with dual-masking strategy; utilize pretrained CBraMod for EEG; train symmetric ECG encoder; fuse via simple embedding concatenation.

Result: Achieves near state-of-the-art performance on emotion recognition, demonstrating improved downstream performance.

Conclusion: The use of carefully designed physiological foundation models with straightforward multi-modal fusion significantly enhances downstream learning performance, demonstrating the potential for scalable and label-efficient healthcare/affective computing solutions.

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [78] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: Chorus is a context-aware, data-free model customization approach for IoT applications that adapts models to unseen deployment conditions without requiring target-domain data by learning effective context representations.


<details>
  <summary>Details</summary>
Motivation: Sensor data in IoT applications is affected by diverse contextual conditions, but traditional domain adaptation methods ignore context information or use simplistic integration strategies, making them ineffective for unseen context shifts.

Method: Chorus performs unsupervised cross-modal reconstruction between sensor data and language-based context embeddings, learns robust context representations, trains a lightweight gated head to balance sensor and context contributions, and uses a context-caching mechanism for efficiency.

Result: Experiments on IMU, speech, and WiFi sensing tasks show Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts while maintaining comparable latency on edge devices.

Conclusion: Chorus effectively handles context shifts in IoT applications through context-aware adaptation without target data, achieving superior performance and practical efficiency.

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [79] [Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions](https://arxiv.org/abs/2512.15286)
*Siva Sai,Ishika Goyal,Shubham Sharma,Sri Harshita Manuri,Vinay Chamola,Rajkumar Buyya*

Main category: cs.LG

TL;DR: Survey of Quantum Machine Learning applications in cybersecurity, covering key techniques and their use in security tasks like intrusion detection, malware classification, and cloud security, with discussion of limitations and future directions.


<details>
  <summary>Details</summary>
Motivation: Classical machine learning and defense strategies are failing against evolving cyber threats and high-volume data, prompting exploration of Quantum Machine Learning for better performance.

Method: Comprehensive overview and mapping of QML techniques (QNNs, QSVMs, VQCs, QGANs) across learning paradigms (supervised, unsupervised, generative) to cybersecurity tasks.

Result: QML offers improved encoding and processing for security applications but has limitations that need addressing.

Conclusion: QML has potential to enhance cybersecurity defenses, though current limitations require further research to realize its benefits.

Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.

</details>


### [80] [Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures](https://arxiv.org/abs/2512.15228)
*Songze Huo,Xiao-Ming Cao*

Main category: cs.LG

TL;DR: DBCata is a deep generative model that improves catalyst screening by generating accurate adsorption structures without explicit force data, outperforming current MLIPs and enhancing DFT accuracy in most cases.


<details>
  <summary>Details</summary>
Motivation: The limited distribution of training data for machine learning interatomic potentials (MLIPs) from near-equilibrium structures leads to unreliable adsorption structures and energy predictions in catalyst screening.

Method: DBCata integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures without requiring explicit energy or force information. It also employs a hybrid chemical-heuristic and self-supervised outlier detection approach to identify and refine anomalous predictions.

Result: DBCata generates high-fidelity adsorption geometries with an interatomic distance mean absolute error (DMAE) of 0.035 Å on the Catalysis-Hub dataset, nearly three times better than state-of-the-art MLIP models. It improves DFT accuracy within 0.1 eV in 94% of cases by refining anomalous predictions through outlier detection.

Conclusion: DBCata is a powerful tool for catalyst design and optimization, enabling accelerated high-throughput computational screening for efficient alloy catalysts in oxygen reduction reactions.

Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \textÅ on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.

</details>


### [81] [Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery](https://arxiv.org/abs/2512.15344)
*Hiroyoshi Nagahama,Katsufumi Inoue,Masayoshi Todorokihara,Michifumi Yoshioka*

Main category: cs.LG

TL;DR: This paper introduces two phase-aware preprocessing methods for vibration signals in predictive maintenance that address random phase variations, showing significant accuracy improvements (up to +5.4%) across various deep learning architectures.


<details>
  <summary>Details</summary>
Motivation: Most learning-based approaches for predictive maintenance of rotating machinery either discard phase information during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information, leaving potential performance gains unexploited.

Method: Two phase-aware preprocessing strategies: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase, and (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Evaluated using synchronized three-axis sensor data on six deep learning architectures under a two-stage learning framework.

Result: Both methods show architecture-independent improvements: three-axis independent method achieves consistent gains (+2.7% for Transformer), while single-axis reference approach delivers superior performance with up to 96.2% accuracy (+5.4%) by preserving spatial phase relationships.

Conclusion: Both phase alignment strategies are established as practical and scalable enhancements for predictive maintenance systems, demonstrating the importance of proper phase handling in vibration signal analysis.

Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.

</details>


### [82] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: O-EENC-SD is a hyperparameter-free, efficient online speaker diarization system using EEND-EDA with RNN stitching that achieves competitive performance on two-speaker telephone conversations.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods: hyperparameter-free solution compared to unsupervised clustering approaches, and more computationally efficient alternative to current online end-to-end methods that are computationally costly.

Method: An end-to-end online speaker diarization system based on EEND-EDA with a novel RNN-based stitching mechanism and centroid refinement decoder, operating on independent chunks with no overlap.

Result: The system is competitive with state-of-the-art methods on the CallHome dataset for two-speaker conversational telephone speech, providing a great trade-off between DER and complexity.

Conclusion: O-EENC-SD provides a compelling trade-off between diarization error rate and computational complexity, achieving competitive results in the two-speaker telephone speech domain while being extremely efficient.

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [83] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: A new RL algorithm FM-EAC combines model-based and model-free approaches with feature-based models and enhanced actor-critic framework, achieving better cross-task generalization in dynamic environments than existing methods.


<details>
  <summary>Details</summary>
Motivation: Modern RL methods struggle with effective transferability across tasks and scenarios, despite the convergence of MBRL and MFRL in approaches like Dyna-Q. The limitation in cross-task generalization motivated the development of a more versatile algorithm.

Method: The authors propose FM-EAC (Feature Model-Based Enhanced Actor-Critic), which integrates model-based and model-free reinforcement learning approaches. It uses novel feature-based models and an enhanced actor-critic framework to combine planning, acting, and learning for multi-task control.

Result: Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. The framework allows customization of different sub-networks according to user-specific requirements.

Conclusion: FM-EAC is a generalized framework that combines planning, acting, and learning for multi-task control in dynamic environments. It demonstrates superior performance compared to state-of-the-art methods and offers customization flexibility through modular sub-networks.

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [84] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: DHMBPO proposes using two separate rollout horizons - a long one for on-policy sampling and a short one for gradient estimation - to balance distribution shift and gradient stability in model-based RL.


<details>
  <summary>Details</summary>
Motivation: Standard MBRL faces a dilemma: long rollouts reduce distribution shift but amplify model bias and gradient variance, while short rollouts have opposite issues. The optimal horizon differs for these two objectives.

Method: Double Horizon Model-Based Policy Optimization (DHMBPO) splits rollouts into: (1) Distribution Rollout (DR) - long horizon for generating on-policy samples, and (2) Training Rollout (TR) - short horizon for differentiable transitions and stable gradient estimation.

Result: DHMBPO effectively balances distribution shift, model bias, and gradient instability, achieving superior performance over existing MBRL methods on continuous-control benchmarks in both sample efficiency and runtime.

Conclusion: The double-horizon approach successfully resolves the conflict between different optimal rollout horizons for distribution alignment and gradient estimation, advancing MBRL methodology.

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [85] [Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory](https://arxiv.org/abs/2512.15267)
*Huiyan Xue,Xuming Ran,Yaxin Li,Qi Xu,Enhui Li,Yi Xu,Qiang Zhang*

Main category: cs.LG

TL;DR: Selective Subnetwork Distillation (SSD) improves sparse neural network performance in continual learning by selectively transferring knowledge between tasks without requiring replay or task labels.


<details>
  <summary>Details</summary>
Motivation: Sparse neural systems face limitations in cross-task knowledge reuse and performance degradation under high sparsity despite their resilience to catastrophic forgetting.

Method: SSD identifies frequently activated neurons and selectively distills knowledge within previous Top-K subnetworks and output logits through structurally guided distillation.

Result: Experiments show SSD improves accuracy, retention, and representation coverage on Split CIFAR-10, CIFAR-100, and MNIST benchmarks.

Conclusion: SSD provides a structurally grounded solution that enables knowledge transfer while preserving sparse modularity in continual learning.

Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.

</details>


### [86] [Soft Geometric Inductive Bias for Object Centric Dynamics](https://arxiv.org/abs/2512.15493)
*Hampus Linander,Conor Heins,Alexander Tschantz,Marco Perin,Christopher Buckley*

Main category: cs.LG

TL;DR: Soft geometric inductive bias using geometric algebra neural networks improves physical dynamics prediction compared to non-equivariant models in rigid body simulations.


<details>
  <summary>Details</summary>
Motivation: Exact group equivariance can degrade performance when symmetries are broken, so a softer geometric prior is needed for better physical fidelity.

Method: Object-centric world models built with geometric algebra neural networks, trained autoregressively for next-step predictions in 2D rigid body dynamics with obstacles.

Result: Better long-horizon rollout performance in terms of physical fidelity compared to non-equivariant baseline models in simulated environments.

Conclusion: Geometric algebra provides an effective middle ground between hand-crafted physics and unstructured deep nets, enabling sample-efficient dynamics models for multi-object scenes.

Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.

</details>


### [87] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: Proposes Persistence, a topology-based metric for evaluating embedding quality without labels, using persistent homology to capture geometric structure and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Modern unsupervised/self-supervised representation learning lacks effective metrics for evaluating embedding quality without labels, creating an open challenge despite good generalization.

Method: Develops Persistence, an unsupervised metric based on persistent homology that quantifies geometric structure and topological richness of embedding spaces at multiple scales.

Result: Empirical results across diverse domains show Persistence achieves strong correlations with downstream performance, surpassing existing unsupervised metrics and aiding model/hyperparameter selection.

Conclusion: Persistence provides a reliable, topology-aware approach for unsupervised evaluation of embeddings, addressing limitations of previous metrics and enabling better model development.

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [88] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: This paper introduces N-simplicial attention, a higher-order generalization of graph attention mechanisms using Rotary Position Embeddings, with simplex selection for efficiency, and analyzes its Lipschitz continuity and over-smoothing properties.


<details>
  <summary>Details</summary>
Motivation: Moving beyond pairwise token interactions (like in GATs or Transformers) to capture higher-order interactions between tokens, while managing computational complexity.

Method: Proposes N-simplicial attention (generalizing attention to higher-order interactions), adapts it for Rotary Position Embeddings (RoPE), and introduces cost-effective simplex selection to focus computation on task-sensitive interactions.

Result: Introduces the mechanism and provides theoretical analysis showing the method has a Lipschitz upper-bound and suffers from over-smoothing despite enabling higher-order interactions.

Conclusion: N-simplicial attention extends attention mechanisms to higher orders with efficiency controls, but inherits over-smoothing issues, suggesting directions for future work in managing this trade-off.

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [89] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: G2RL proposes gradient-guided reinforcement learning that uses model's own update geometry instead of external heuristics for more effective exploration in LLM training.


<details>
  <summary>Details</summary>
Motivation: Current RL exploration mechanisms (entropy bonuses, external comparators) are misaligned with how LLMs actually learn, encouraging surface-level variation without guaranteeing meaningful gradient direction differences.

Method: G2RL constructs sequence-level features from model's final layer sensitivity (obtainable via forward pass), compares gradient directions within sampled groups, and rewards novel gradient directions with multiplicative scaling while deemphasizing redundant updates.

Result: Consistent improvements across math and reasoning benchmarks (MATH500, AMC, AIME24/25, GPQA, MMLUpro) on Qwen3 1.7B/4B models, with better pass@1, maj@16, and pass@k metrics compared to entropy-based GRPO and external embedding methods.

Conclusion: Policy's own update space provides more faithful and effective basis for guiding exploration in LLM reinforcement learning, enabling expanded exploration into orthogonal gradient directions while maintaining semantic coherence.

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [90] [Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference](https://arxiv.org/abs/2512.15335)
*Chenxiang Zhang,Tongxi Qu,Zhong Li,Tian Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: Lower-precision post-training quantization reduces neural network privacy vulnerability by up to 10x compared to full-precision models, with a trade-off in utility.


<details>
  <summary>Details</summary>
Motivation: Existing privacy analyses focus on full-precision models, but quantization alters parameters and outputs—creating a gap in understanding how bit-width affects privacy leakage.

Method: Used membership inference attacks to evaluate 3 PTQ algorithms (AdaRound, BRECQ, OBC) at 4-bit, 2-bit, and 1.58-bit precision on CIFAR-10, CIFAR-100, and TinyImageNet datasets.

Result: Low-precision PTQ consistently reduces privacy leakage; quantizing only the last layer at higher precision allows fine-grained control over privacy-utility trade-off.

Conclusion: PTQ offers actionable insights for balancing efficiency, utility, and privacy in real-world deployments, with lower precision reducing membership inference vulnerability substantially.

Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.

</details>


### [91] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: Fusion-3 (F3) adaptively fuses Rocket, SAX, and SFA representations to improve time series classification, showing consistent gains over Rocket on datasets with structured variability or rich frequency content.


<details>
  <summary>Details</summary>
Motivation: Kernel-based methods like Rocket perform inconsistently across datasets; the intuition that different representations capture complementary structure suggests fusion could yield improvements.

Method: Clustered UCR datasets into 6 groups using meta-features, then fused Rocket, SAX, and SFA representations adaptively within F3 framework.

Result: F3 yields small but consistent average improvements over Rocket on 113 UCR datasets, with significant gains in regimes with structured variability or rich frequency content.

Conclusion: Selective fusion provides dependable, interpretable extension to kernel-based methods, correcting weaknesses where data support it.

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [92] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: A framework for evaluating ML model robustness in power system protection under sensor/data degradation.


<details>
  <summary>Details</summary>
Motivation: Conventional protection schemes struggle with renewable integration; ML offers adaptive solutions but needs robustness validation.

Method: High-fidelity EMT simulations to model sensor outages, reduced sampling rates, and communication losses.

Result: FC stable under most degradations but drops 13% under single-phase loss; FL more sensitive, with 150% error increase under voltage loss.

Conclusion: Provides actionable guidance for robustness-aware design of ML-assisted protection systems.

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [93] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: A Bayesian RL algorithm called EUBRL uses epistemic guidance for principled exploration, achieving near-optimal regret bounds and strong empirical performance in challenging environments.


<details>
  <summary>Details</summary>
Motivation: Agents face the exploration-exploitation dilemma at knowledge boundaries, where epistemic uncertainty (systematic uncertainty from limited knowledge) occurs.

Method: Proposed EUBRL, a Bayesian reinforcement learning algorithm that uses epistemic guidance to adaptively reduce per-step regret from estimation errors.

Result: Theoretical guarantees show nearly minimax-optimal regret and sample complexity for expressive priors in infinite-horizon MDPs. Empirical tests on sparse-reward, long-horizon, stochastic tasks show superior sample efficiency, scalability, and consistency.

Conclusion: EUBRL effectively addresses exploration challenges through epistemic uncertainty, providing both theoretical optimality and practical benefits in complex RL environments.

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [94] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: FlowBind is an efficient any-to-any cross-modal generation framework using shared latent space and modality-specific flows, achieving competitive quality with significantly reduced parameters and faster training.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based approaches for any-to-any generation are inefficient due to large data requirements, high computational costs from joint distribution modeling, and complex multi-stage training.

Method: FlowBind learns a shared latent space with modality-specific invertible flows, jointly optimized under a single flow-matching objective, allowing direct translation across modalities during inference.

Result: Experiments show FlowBind achieves comparable generation quality while using 6x fewer parameters and training 10x faster than prior methods on text, image, and audio modalities.

Conclusion: FlowBind provides an efficient and simplified alternative for any-to-any generation by factorizing interactions through a shared latent space, substantially reducing data and computational requirements.

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [95] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: Analysis of min-max normalized eigenvalues in random matrices, including scaling laws for cumulative distribution and residual errors in matrix factorization, with experimental verification.


<details>
  <summary>Details</summary>
Motivation: Random matrices are fundamental in mathematics and machine learning, where data normalization is common; this study examines the statistical properties of min-max normalized eigenvalues to address practical data science needs.

Method: Applied existing effective distribution for normalized eigenvalues to derive scaling laws for cumulative distribution and residual errors in matrix factorization; conducted numerical experiments to validate theoretical results.

Result: Theoretical predictions for scaling laws and residual errors were developed and confirmed through numerical experiments, showing consistency with the proposed models.

Conclusion: The study successfully characterized min-max normalized eigenvalue behavior, providing useful tools for applications in data science involving normalized random matrices.

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [96] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: Investigating techniques to reduce copyrighted content generation in text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models may reproduce copyrighted training data, posing legal risks.

Method: Combining chain-of-thought and task instruction prompting with negative prompting and prompt rewriting.

Result: Studied image similarity to copyrighted content and relevance to user input across models.

Conclusion: Provided insights on effectiveness of techniques for varying model complexities.

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [97] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: This paper introduces Invertibility Loss (InvLoss) to quantitatively assess data reconstruction attack risks in federated learning systems, develops a risk estimator, and proposes defensive mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current federated learning systems lack a theoretically-grounded framework to characterize and assess data reconstruction attack risks despite their significant threat to client data privacy.

Method: Proposing Invertibility Loss (InvLoss) metric with computable upper bound, analyzing Jacobian spectral properties, developing InvRE risk estimator, and creating adaptive noise perturbation defenses.

Result: Establishes theoretical foundation for DRA risk quantification, provides unified explanation for defense effectiveness, and validates framework through extensive experiments on real-world datasets.

Conclusion: The proposed framework enables systematic evaluation and mitigation of data reconstruction attack risks in federated learning while maintaining classification accuracy.

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [98] [Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance](https://arxiv.org/abs/2512.15469)
*Ioannis Kalogeropoulos,Giorgos Bouritsas,Yannis Panagakis*

Main category: cs.LG

TL;DR: A metanetwork framework that edits neural networks in one inference step to meet requirements like fairness and efficiency while preserving utility, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enable efficient editing of machine learning models for compliance with various requirements without compromising performance, addressing limitations of post-processing and re-training approaches.

Method: A data-driven framework using a graph metanetwork as an editor that performs single-step inference to modify neural networks, trained to minimize an objective balancing requirement enforcement and utility preservation.

Result: Improved trade-offs between performance, requirement satisfaction, and time efficiency across tasks including data minimisation, bias mitigation, and weight pruning.

Conclusion: The approach provides a more efficient and effective alternative to post-processing or re-training methods for editing machine learning models to meet diverse requirements.

Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.

</details>


### [99] [Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs](https://arxiv.org/abs/2512.15483)
*Luca Torresi,Pascal Friederich*

Main category: cs.LG

TL;DR: Extended Bayesian optimization with proxy measurements improves autonomous experimentation in self-driving labs by enabling flexible multi-stage workflows and decision-making based on intermediate data.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian optimization used in self-driving labs relies on fixed experimental workflows with clear optimization parameters and measurable objectives, excluding on-the-fly decisions and intermediate measurements. This requires real-world experiments to be adapted and simplified.

Method: The paper introduces an extension to Bayesian optimization that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate proxy measurements.

Result: Proxy measurements yield substantial improvement over conventional Bayesian optimization across a wide range of scenarios, both in time to find good solutions and overall optimality of found solutions.

Conclusion: This extension of Bayesian optimization to handle proxy measurements enables more complex, realistic experimental workflows in self-driving labs and facilitates smooth integration of simulations and experiments in next-generation SDLs.

Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.

</details>


### [100] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: A comparison of two reliability assessment approaches for classifier predictions (Robustness Quantification and Uncertainty Quantification) shows they're complementary and can be combined into a superior hybrid method.


<details>
  <summary>Details</summary>
Motivation: To compare two conceptually different approaches for assessing the reliability of individual classifier predictions - Robustness Quantification (RQ) and Uncertainty Quantification (UQ).

Method: Comparing both RQ and UQ approaches on multiple benchmark datasets, then combining them into a hybrid approach.

Result: There's no clear winner between RQ and UQ individually, but they are complementary. The hybrid approach combining both outperforms either approach alone. The analysis also provides assessment of relative importance of uncertainty vs robustness as sources of unreliability for each dataset.

Conclusion: RQ and UQ should not be viewed as competing alternatives but as complementary approaches that can be effectively combined to create better reliability assessment methods for classifier predictions.

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [101] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: A method using infinite-dimensional Gaussian processes with Random Fourier Features to model and visualize temporal evolution of vector sets in domains like crime analysis and linguistics.


<details>
  <summary>Details</summary>
Motivation: Analyzing time-varying vector sets is challenging due to their complex evolving structures, yet crucial for understanding dynamics in domains like ecology, crime, and linguistics.

Method: Model the distribution of each vector set using infinite-dimensional Gaussian processes, approximating the latent function with Random Fourier Features to derive compact, comparable vector representations over time.

Result: The method effectively captures temporal dynamics, enabling tracking and visualization of vector set transitions in low-dimensional space for sociological (crime) and linguistic (word embeddings) data.

Conclusion: The proposed approach provides interpretable, robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [102] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: JUICE is a novel co-selection method for unlabeled incomplete multi-view data that jointly performs missing data imputation and feature/instance selection in a unified framework using cross-view neighborhood information.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat co-selection and missing data imputation as separate processes and fail to capture complementary information when merging multi-view data, limiting co-selection effectiveness.

Method: JUICE reconstructs incomplete multi-view data using available observations in a unified framework, leveraging cross-view neighborhood information to refine imputation and learn inter-sample relationships.

Result: Extensive experiments show JUICE outperforms state-of-the-art methods.

Conclusion: Joint learning of imputation and co-selection with cross-view information enables more representative feature and instance selection.

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [103] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: A correction-oriented post-training method enables diffusion language models to identify and refine incorrect tokens while preserving correct content, outperforming standard training on code revision tasks.


<details>
  <summary>Details</summary>
Motivation: Standard masked diffusion language model training fails to reliably induce corrective behavior where models can identify unreliable tokens and iteratively refine them while preserving correct content. This limits the potential of diffusion language models for iterative error correction applications.

Method: The authors study corrective behavior in diffusion language models, identifying limitations in standard masked diffusion training. They propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens to enable error-aware confidence and targeted refinement. They introduce the Code Revision Benchmark (CRB) to evaluate error localization and in-place correction capabilities.

Result: Experiments on code revision tasks and controlled settings show that models trained with the proposed approach substantially outperform standard masked diffusion language models in correction scenarios while also improving pure completion performance.

Conclusion: The paper concludes that the proposed correction-oriented post-training principle effectively enables diffusion language models to exhibit corrective behavior, improving both error correction and pure completion performance compared to standard masked diffusion language models.

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [104] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: This paper establishes a unifying mathematical framework connecting autoregressive models (ARMs) and energy-based models (EBMs), showing their equivalence and providing theoretical foundations for EBM-to-ARM distillation.


<details>
  <summary>Details</summary>
Motivation: While ARMs dominate LLM development, EBMs naturally characterize optimal alignment policies but remain less prevalent. The authors seek to bridge these model classes theoretically.

Method: Establishes a bijection between ARMs and EBMs using probability chain rule, connects to soft Bellman equations, derives equivalence between their supervised learning objectives, and analyzes distillation error bounds.

Result: Demonstrated functional equivalence between ARMs and EBMs, showing ARMs can implicitly plan ahead despite next-token prediction paradigm.

Conclusion: EBMs and ARMs are mathematically equivalent under certain conditions, providing theoretical justification for ARM's planning capabilities and EBM-to-ARM distillation.

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [105] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: BEAT: A framework tokenizing behavior into interpretable sequences using vector-quantized autoencoding, enabling transferable explainable recommendations through semantic alignment with frozen language models.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of existing explainable recommendation methods that rely on ID-based representations obscuring semantic meaning and impose structural constraints on language models, especially in complex real-world scenarios where user intents are entangled and collaborative signals don't align with linguistic semantics.

Method: Tokenizes user and item behaviors into discrete interpretable sequences using a vector-quantized autoencoding process to disentangle macro-level interests and micro-level intentions, with multi-level semantic supervision and semantic alignment regularization to embed behavior tokens into frozen language models.

Result: Experiments on three public datasets show improved zero-shot recommendation performance while generating coherent and informative explanations. The behavior tokens capture fine-grained semantics and provide a plug-and-play interface for integrating complex behavior patterns into large language models.

Conclusion: BEAT proposes a unified and transferable framework that effectively bridges behavioral patterns with language models through interpretable behavior tokens, offering plug-and-play compatibility while enhancing both recommendation performance and explanation quality.

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [106] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: SoFlow enables efficient one-step image generation using Flow Matching and solution consistency losses, outperforming MeanFlow on ImageNet 256×256.


<details>
  <summary>Details</summary>
Motivation: Multi-step denoising in diffusion/Flow Matching models is inefficient; need for few-step generation.

Method: Proposes Solution Flow Models (SoFlow) with Flow Matching loss (enables CFG) and Jacobian-free solution consistency loss.

Result: SoFlow achieves better FID-50K scores than MeanFlow on ImageNet 256×256 using same DiT architecture/training epochs.

Conclusion: SoFlow provides effective one-step generation with improved efficiency and performance.

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [107] [A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks](https://arxiv.org/abs/2512.15685)
*Oleg Melnikov,Yurii Dorofieiev,Yurii Shakhnovskiy,Huy Truong,Victoria Degeler*

Main category: cs.LG

TL;DR: SICAMS framework uses multivariate stats to detect/classify/localize water network anomalies via pressure/flow data, without hydraulic modeling.


<details>
  <summary>Details</summary>
Motivation: Need robust anomaly detection in water distribution networks that works without calibrated hydraulic models.

Method: Whitening transformation removes spatial correlations, Hotelling's T² statistic tests for anomalies, heuristic algorithm classifies leaks/malfunctions.

Result: High sensitivity/reliability on benchmark dataset, works under multiple leaks, enables water loss estimation.

Conclusion: Framework applicable to real-world operations, provides system health monitoring without hydraulic calibration.

Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.

</details>


### [108] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: A multi-modal semantic communication framework using text queries and cross-modal attention to adaptively transmit image regions at optimal resolutions based on task relevance and bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional transformer-based semantic communication struggles with complex scenes containing multiple objects due to lack of explicit task guidance from self-attention mechanisms.

Method: Integrates text queries with visual features via cross-modal attention to compute relevance scores, then transmits image patches at adaptive resolutions using trained encoder-decoder pairs while matching total bitrate to channel capacity.

Result: The system enables efficient transmission of task-critical information by reconstructing and combining patches at the receiver while maintaining bandwidth constraints.

Conclusion: The proposed framework provides flexible, goal-driven semantic communication that performs effectively in complex, bandwidth-limited environments.

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>


### [109] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: FrontierCS is a challenging computer science benchmark with 156 open-ended problems where optimal solutions are unknown but quality can be objectively evaluated through executable programs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on tasks with known optimal solutions, limiting assessment of true problem-solving capabilities at the frontier of CS difficulty.

Method: Models solve problems by implementing executable programs evaluated through automatic evaluators, with expert reference solutions provided for comparison.

Result: Frontier reasoning models significantly lag behind human experts on both algorithmic and research tracks, even with increased reasoning budgets, and tend to over-optimize for workable code rather than high-quality solutions.

Conclusion: The benchmark highlights limitations of current AI models in frontier computer science problem-solving and provides a framework for measurable progress in this area. our dis a period

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [110] [Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data](https://arxiv.org/abs/2512.15706)
*Kayode Olumoyin,Lamees El Naqa,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: Proposes PINN-based method to learn time-varying cell interactions and treatment responses in sparse biological data scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional fixed-parameter models fail to capture evolving dynamics in biological systems, especially with sparse temporal data like tumor volume measurements.

Method: Uses physics-informed neural networks (PINN) to predict subpopulation trajectories at unobserved time points by learning time-varying interactions.

Result: Demonstrated consistency with biological explanations of subpopulation trajectories.

Conclusion: Providers a framework for learning evolving biological interactions under external interventions like cancer treatments.

Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [111] [Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis](https://arxiv.org/abs/2512.15398)
*Zanxiang He,Meng Li,Liyun Shi,Weiye Daia,Liming Nie*

Main category: cs.MA

TL;DR: Mapis is a novel multi-agent framework for PCOS diagnosis, integrating clinical guidelines and a knowledge graph to improve accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: PCOS affects 10% of reproductive-aged women, but current ML tools rely on large labeled datasets and lack interpretability; multi-agent systems are underexplored for this task.

Method: Proposes Mapis, a knowledge-grounded multi-agent framework that uses a structured workflow based on the 2023 International Guideline, with specialized agents (e.g., gynecological endocrine, radiology) collaborating in a diagnostic process supported by a PCOS knowledge graph.

Result: Mapis outperforms traditional ML by 13.56%, single-agent by 6.55%, and previous multi-agent systems by 7.05% in accuracy on clinical datasets.

Conclusion: Mapis effectively addresses limitations in PCOS diagnosis by enhancing interpretability and accuracy through a guideline-based multi-agent approach.

Abstract: Polycystic Ovary Syndrome (PCOS) constitutes a significant public health issue affecting 10% of reproductive-aged women, highlighting the critical importance of developing effective diagnostic tools. Previous machine learning and deep learning detection tools are constrained by their reliance on large-scale labeled data and an lack of interpretability. Although multi-agent systems have demonstrated robust capabilities, the potential of such systems for PCOS detection remains largely unexplored. Existing medical multi-agent frameworks are predominantly designed for general medical tasks, suffering from insufficient domain integration and a lack of specific domain knowledge. To address these challenges, we propose Mapis, the first knowledge-grounded multi-agent framework explicitly designed for guideline-based PCOS diagnosis. Specifically, it built upon the 2023 International Guideline into a structured collaborative workflow that simulates the clinical diagnostic process. It decouples complex diagnostic tasks across specialized agents: a gynecological endocrine agent and a radiology agent collaborative to verify inclusion criteria, while an exclusion agent strictly rules out other causes. Furthermore, we construct a comprehensive PCOS knowledge graph to ensure verifiable, evidence-based decision-making. Extensive experiments on public benchmarks and specialized clinical datasets, benchmarking against nine diverse baselines, demonstrate that Mapis significantly outperforms competitive methods. On the clinical dataset, it surpasses traditional machine learning models by 13.56%, single-agent by 6.55%, and previous medical multi-agent systems by 7.05% in Accuracy.

</details>
