<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: Team Twente's third-place solution for Integrated Healthcare Timetabling Competition 2024 uses a 3-phase hybrid approach combining mixed-integer programming, constraint programming, and simulated annealing with problem decomposition.


<details>
  <summary>Details</summary>
Motivation: To develop an effective solution for the healthcare timetabling competition by leveraging multiple optimization techniques to handle the complex scheduling constraints.

Method: 3-phase solution approach using decomposition into subproblems, combining mixed-integer programming, constraint programming, and simulated annealing.

Result: Achieved third place in the competition and provided first-time lower bounds on optimal solution values for benchmark instances.

Conclusion: The hybrid approach was effective but could be improved by addressing identified open problems in the methodology.

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [2] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: Introduces epistemic reject-option predictor that abstains in high epistemic uncertainty regions, minimizing expected regret compared to Bayes-optimal predictor.


<details>
  <summary>Details</summary>
Motivation: Traditional reject-option methods only address aleatoric uncertainty, which is insufficient when limited training data makes epistemic uncertainty significant in practical scenarios.

Method: Builds on Bayesian learning to redefine optimal predictor as minimizing expected regret - the performance gap between learned model and Bayes-optimal predictor with full data distribution knowledge.

Result: First principled framework enabling learning predictors that identify inputs where training data is insufficient for reliable decisions, abstaining when regret exceeds specified rejection cost.

Conclusion: Provides a novel approach to reject-option prediction that properly handles epistemic uncertainty from limited data, offering more reliable uncertainty quantification in practical applications.

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [3] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA is an online learning framework that uses multi-granularity human feedback to dynamically align ranking in RAG systems, improving adaptation to evolving user intent while maintaining baseline retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems use static retrieval that cannot adapt to evolving user intent and content drift, limiting their effectiveness in interactive settings.

Method: DMA organizes document-, list-, and response-level feedback into a coherent pipeline: supervised training for pointwise/listwise rankers, policy optimization using response-level preferences, and knowledge distillation into a lightweight scorer for low-latency serving.

Result: Online deployment showed substantial improvements in human engagement, while offline tests on knowledge-intensive benchmarks (TriviaQA, HotpotQA) demonstrated competitive foundational retrieval with notable gains on conversational QA.

Conclusion: DMA provides a principled approach for feedback-driven, real-time adaptation in RAG systems without sacrificing baseline capability, positioning it as an effective solution for dynamic retrieval alignment.

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [4] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: Introduces real-time reasoning for agents in dynamic environments, compares reactive vs planning agent approaches, proposes AgileThinker hybrid method that outperforms single-paradigm approaches under time pressure.


<details>
  <summary>Details</summary>
Motivation: Real-world agents need to make both logical and timely judgments in dynamic environments where hazards emerge and opportunities arise while reasoning is ongoing, but current language model approaches fail to address this dynamic nature.

Method: Built Real-Time Reasoning Gym to study two language model deployment paradigms: reactive agents (bounded computation for rapid responses) and planning agents (extended computation for complex problems). Proposed AgileThinker which simultaneously engages both reasoning paradigms.

Result: State-of-the-art models struggle with logical and timely judgments in both paradigms. AgileThinker consistently outperforms single-paradigm agents as task difficulty and time pressure increase, effectively balancing reasoning depth and response latency.

Conclusion: Establishes real-time reasoning as a critical testbed for practical agents and provides foundation for temporally constrained AI systems, highlighting a path toward real-time capable agents.

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [5] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: ORCHID is a modular agentic system that uses RAG and human oversight for high-risk property classification at DOE sites, improving accuracy and auditability over traditional expert-only approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional expert-only workflows for HRP classification are time-consuming, backlog-prone, and struggle to keep pace with evolving export control policies, requiring a more efficient and transparent solution.

Method: The system employs small cooperating agents (retrieval, description refiner, classifier, validator, feedback logger) that coordinate via agent-to-agent messaging and use MCP for model-agnostic operation, following an Item to Evidence to Decision loop with step-by-step reasoning.

Result: In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts.

Conclusion: ORCHID demonstrates a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows through its modular design, human oversight, and audit-friendly features like append-only audit bundles.

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


### [6] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: A methodology for generating and evaluating military courses of action for mechanized battalions, using concurrent generation and evaluation processes to identify superior alternatives based on opponent status and battlefield conditions.


<details>
  <summary>Details</summary>
Motivation: To support decision-making during military ground combat operations by providing systematic recommendations for various courses of action, particularly focusing on mechanized battalion actions.

Method: Systematically generates thousands of individual action alternatives, evaluates them based on opponent status and actions, considers unit composition, force ratios, offense/defense types, and advance rates using field manuals, with concurrent generation and evaluation processes.

Result: Produces a variety of alternative courses of action with superior outcomes, facilitates management of new course generation based on previous evaluations, and formulates revised courses as combat conditions evolve.

Conclusion: The methodology effectively supports sequential decision-making in military operations by continuously generating and evaluating superior courses of action as battlefield conditions change.

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [7] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: LLM-based agents show promise for cleaning predictive maintenance logs, effectively handling generic noise types but struggling with domain-specific errors.


<details>
  <summary>Details</summary>
Motivation: Economic constraints, limited datasets, and expertise shortages hinder predictive maintenance adoption in automotive sector; LLMs offer opportunity to overcome these barriers.

Method: Evaluate LLM agents on cleaning tasks involving six distinct types of noise in maintenance logs (typos, missing fields, near-duplicates, incorrect dates, etc.).

Result: LLMs are effective at handling generic cleaning tasks and offer promising foundation for industrial applications, though domain-specific errors remain challenging.

Conclusion: LLM-based agents present viable solution for predictive maintenance data cleaning pipelines, with potential for further improvements through specialized training and enhanced agentic capabilities.

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


### [8] [Reasoning Is All You Need for Urban Planning AI](https://arxiv.org/abs/2511.05375)
*Sijie Yang,Jiatong Li,Filip Biljecki*

Main category: cs.AI

TL;DR: A framework for AI agents in urban planning that integrates reasoning capabilities to assist human planners by transparently evaluating constraints, stakeholder values, and trade-offs.


<details>
  <summary>Details</summary>
Motivation: AI has succeeded in learning patterns from data for urban planning predictions, but decision-making requires transparent reasoning about constraints and values that statistical learning alone cannot provide.

Method: The Agentic Urban Planning AI Framework uses a multi-agent collaboration structure with three cognitive layers (Perception, Foundation, Reasoning) and six logic components (Analysis, Generation, Verification, Evaluation, Collaboration, Decision).

Result: The framework enables systematic solution exploration, regulatory compliance verification, and transparent trade-off deliberation, augmenting human planners without replacing their judgment.

Conclusion: Reasoning-capable AI agents can amplify human planning by ensuring value-based, rule-grounded, and explainable decisions, addressing limitations of pure statistical learning and outlining future research directions.

Abstract: AI has proven highly successful at urban planning analysis -- learning
patterns from data to predict future conditions. The next frontier is
AI-assisted decision-making: agents that recommend sites, allocate resources,
and evaluate trade-offs while reasoning transparently about constraints and
stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,
ReAct, and multi-agent collaboration frameworks -- now make this vision
achievable.
  This position paper presents the Agentic Urban Planning AI Framework for
reasoning-capable planning agents that integrates three cognitive layers
(Perception, Foundation, Reasoning) with six logic components (Analysis,
Generation, Verification, Evaluation, Collaboration, Decision) through a
multi-agents collaboration framework. We demonstrate why planning decisions
require explicit reasoning capabilities that are value-based (applying
normative principles), rule-grounded (guaranteeing constraint satisfaction),
and explainable (generating transparent justifications) -- requirements that
statistical learning alone cannot fulfill. We compare reasoning agents with
statistical learning, present a comprehensive architecture with benchmark
evaluation metrics, and outline critical research challenges. This framework
shows how AI agents can augment human planners by systematically exploring
solution spaces, verifying regulatory compliance, and deliberating over
trade-offs transparently -- not replacing human judgment but amplifying it with
computational reasoning capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale](https://arxiv.org/abs/2511.04904)
*Bassel Al Omari,Michael Matthews,Alexander Rutherford,Jakob Nicolaus Foerster*

Main category: cs.LG

TL;DR: Craftax-MA extends Craftax to multi-agent RL with exceptional speed, and Craftax-Coop adds complex cooperation challenges where current MARL methods struggle.


<details>
  <summary>Details</summary>
Motivation: Existing MARL benchmarks are too narrow and short-horizon, failing to stress long-term dependencies and generalization needed for real multi-agent systems.

Method: Extend Craftax environment to support multiple agents (Craftax-MA) and add heterogeneous agents, trading, and cooperation mechanics (Craftax-Coop), implemented in JAX for high performance.

Result: Training runs complete in under an hour for 250M interactions, and analysis shows existing algorithms struggle with long-horizon credit assignment, exploration, and cooperation.

Conclusion: Craftax benchmarks provide compelling challenges that can drive long-term MARL research by exposing limitations of current methods in complex, open-ended environments.

Abstract: Progress in multi-agent reinforcement learning (MARL) requires challenging
benchmarks that assess the limits of current methods. However, existing
benchmarks often target narrow short-horizon challenges that do not adequately
stress the long-term dependencies and generalization capabilities inherent in
many multi-agent systems. To address this, we first present
\textit{Craftax-MA}: an extension of the popular open-ended RL environment,
Craftax, that supports multiple agents and evaluates a wide range of general
abilities within a single environment. Written in JAX, \textit{Craftax-MA} is
exceptionally fast with a training run using 250 million environment
interactions completing in under an hour. To provide a more compelling
challenge for MARL, we also present \textit{Craftax-Coop}, an extension
introducing heterogeneous agents, trading and more mechanics that require
complex cooperation among agents for success. We provide analysis demonstrating
that existing algorithms struggle with key challenges in this benchmark,
including long-horizon credit assignment, exploration and cooperation, and
argue for its potential to drive long-term research in MARL.

</details>


### [10] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV cache management in LLMs must preserve positional encoding integrity; simple contiguous block strategies outperform complex eviction methods that disrupt positional coherence.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of unbounded KV cache growth in stateful multi-turn LLM scenarios and the degradation of generation quality when cache exceeds model's trained context window.

Method: Empirical analysis using stateful benchmarking framework to evaluate KV cache management strategies, focusing on positional encoding integrity and architectural context limits.

Result: LLM generation quality degrades sharply when accumulated KV cache approaches/exceeds model's trained context window; simple contiguous context block preservation strategies yield more coherent generations than positionally disruptive eviction methods.

Conclusion: KV cache eviction techniques should respect architectural limits, preserve positional structure, and consider "cache health" holistically beyond mere size management.

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [11] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: Proposes a novel fMRI analysis framework using adaptive frequency decomposition and connectivity learning to improve brain disorder diagnosis by capturing frequency-specific neural disruptions.


<details>
  <summary>Details</summary>
Motivation: Current resting-state fMRI models treat BOLD signals as monolithic time series, ignoring the multi-frequency nature of neuronal oscillations and frequency-specific manifestations of neurological disorders, limiting diagnostic accuracy.

Method: Uses Adaptive Cascade Decomposition to learn task-relevant frequency sub-bands per brain region and Frequency-Coupled Connectivity Learning to model intra- and cross-band interactions, integrated via a Unified-GCN with message-passing for diagnostic prediction.

Result: Superior performance demonstrated on ADNI and ABIDE datasets compared to existing methods.

Conclusion: The framework effectively addresses limitations of current approaches by adaptively capturing frequency-specific neural disruptions, enhancing diagnostic sensitivity and specificity for brain disorders.

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [12] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: AWEMixer: A novel time series forecasting model that combines adaptive wavelet enhancement with frequency routing and coherent gated fusion to address non-stationary IoT sensor data, outperforming transformer and MLP-based methods on long-sequence forecasting.


<details>
  <summary>Details</summary>
Motivation: Long-term time series forecasting in IoT environments faces challenges from non-stationary, multi-scale sensor signals and error accumulation. Traditional time-domain methods and Fourier transform approaches either miss frequency information or blur temporal patterns of transient events.

Method: AWEMixer includes two key components: 1) Frequency Router that uses FFT-derived global periodicity to adaptively weight localized wavelet subbands, and 2) Coherent Gated Fusion Block that selectively integrates frequency features with multi-scale temporal representations using cross-attention and gating mechanisms.

Result: The model was validated on seven public benchmarks and consistently outperformed recent state-of-the-art transformer-based and MLP-based models in long-sequence time series forecasting.

Conclusion: AWEMixer effectively addresses time-frequency localization challenges in IoT forecasting while maintaining robustness to noise, demonstrating superior performance over existing approaches.

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [13] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: Novel RUL prediction framework combining TCNs with modified TFT and Bi-LSTM encoder-decoder achieves 5.5% RMSE improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing RUL prediction models struggle with capturing fine-grained temporal dependencies and dynamically prioritizing critical features over time.

Method: Integrates Temporal Convolutional Networks for local temporal feature extraction with modified Temporal Fusion Transformer enhanced by Bi-LSTM encoder-decoder architecture, using multi-time-window methodology.

Result: Extensive evaluations show up to 5.5% reduction in average RMSE compared to state-of-the-art methods.

Conclusion: The framework advances industrial prognostic systems by bridging short- and long-term dependencies while emphasizing salient temporal patterns, demonstrating the potential of advanced time-series transformers for RUL prediction.

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [14] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: Paper introduces GLISp-SR, a sensor-guided extension of GLISp that integrates quantitative sensor data with preference feedback for faster, better-calibrated human-in-the-loop optimization.


<details>
  <summary>Details</summary>
Motivation: Existing preference-based optimization methods like Preferential Bayesian Optimization or GLISp treat systems as black boxes, ignoring informative sensor measurements that could improve learning efficiency and solution quality.

Method: Proposed GLISp-SR integrates sensor measurements via a physics-informed hypothesis function and a least-squares regularization term, adding grey-box structure to combine subjective preferences with quantitative data.

Result: Numerical evaluations show faster convergence and superior final solutions compared to baseline GLISp on both an analytical benchmark and a vehicle suspension tuning task.

Conclusion: Integrating sensor guidance into preference learning yields more efficient human-in-the-loop calibration, balancing subjective feedback with quantitative information for improved performance.

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [15] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: Knowledge Distillation from grokked models enables generalization in data-scarce and distribution shift scenarios where traditional training fails.


<details>
  <summary>Details</summary>
Motivation: To address delayed generalization (grokking) challenges in data-scarce regimes and practical distribution shift scenarios where traditional methods underperform.

Method: Used Knowledge Distillation from pre-grokked models across single distributions, joint distributions, and continual pretraining setups with limited data.

Result: KD successfully induced/accelerated grokking below critical data thresholds, enabled generalization on joint distributions, and mitigated catastrophic forgetting in continual learning with only 10% data.

Conclusion: KD plays a central role in enabling generalization under knowledge transfer, providing new insights into grokking mechanics for low-data and evolving distribution settings.

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [16] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow compiler converts PyTorch sparse models to fused dataflow graphs for RDAs, supporting cross-expression fusion and optimizations. It enables design-space exploration showing fusion granularity depends on the model, achieving up to 2.7x speedup.


<details>
  <summary>Details</summary>
Motivation: Address efficiency challenges in scaling deep learning models through sparse computation and specialized dataflow hardware.

Method: Propose FuseFlow compiler that converts PyTorch sparse models to fused dataflow graphs for RDAs, supporting cross-expression fusion, parallelization, dataflow ordering, and sparsity blocking.

Result: Enables design-space exploration showing full fusion not always optimal; fusion granularity depends on model. Provides heuristic to prune suboptimal configurations. Achieves ~2.7x speedup over unfused baseline for GPT-3 with BigBird attention.

Conclusion: FuseFlow effectively optimizes sparse models for RDAs, demonstrating that adaptive fusion strategies tailored to specific models yield significant performance gains.

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [17] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: Proposes enhanced instruction prefetching for cloud workloads using compressed entries, hierarchical storage, and ML control to improve latency and energy efficiency while reducing on-chip state.


<details>
  <summary>Details</summary>
Motivation: Large-scale networked services with deep software stacks and microservice orchestration cause increased instruction footprints and frontend stalls, leading to higher tail latency and energy consumption.

Method: Building on Entangling Instruction Prefetcher (EIP), introduces Compressed Entry (36 bits for 8 destinations via spatial clustering), Hierarchical Metadata Storage (on-chip for L1/frequent entries, virtualized bulk metadata), and Online ML Controller (profitable prefetch scoring with context features and bandit-adjusted threshold).

Result: Preserves EIP-like speedups with smaller on-chip state and improves efficiency for networked services, particularly in ML-era data center applications.

Conclusion: The design aligns with SLO-driven, self-optimizing systems, effectively addressing instruction footprint and stall issues in modern cloud workloads.

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [18] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: CNODE proposes a continuous neural ODE model to forecast Parkinson's disease progression by modeling brain changes over time, handling irregular MRI data and capturing individual variability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with irregular/sparse MRI data and fail to capture individual heterogeneity in Parkinson's disease progression, limiting forecasting accuracy.

Method: Uses neural ODE to model brain morphology changes continuously, learns patient-specific initial time and progression speed to align trajectories.

Result: Outperforms state-of-the-art baselines on PPMI dataset for forecasting PD progression.

Conclusion: CNODE enables more accurate, individualized PD progression forecasting by addressing data irregularity and patient heterogeneity.

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [19] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: The paper proposes integrating representation learning with causal inference to address limitations of current representation learning methods in causal tasks, using multi-modal biomedical data for causal discovery and optimal perturbation design.


<details>
  <summary>Details</summary>
Motivation: Current representation learning methods excel in predictive tasks but fail in causal tasks like predicting intervention effects, highlighting the need to combine representation learning with causal inference, especially given the availability of diverse multi-modal biomedical data.

Method: A statistical and computational framework that leverages multi-modal data (observational and perturbational, imaging and sequencing data at various biological levels) to perform causal discovery on observed variables, learn causal variables from multiple views, and design optimal perturbations.

Result: The framework enables more effective use of observational and perturbational data for causal discovery, facilitates learning of causal variables from multi-modal system views, and provides methods for designing optimal perturbations.

Conclusion: Integrating representation learning with causal inference through a multi-modal data approach addresses fundamental limitations in current methods and opens new opportunities for causal understanding in biomedical applications.

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [20] [DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing](https://arxiv.org/abs/2511.04791)
*Lei Gao,Chaoyi Jiang,Hossein Entezari Zarch,Daniel Wong,Murali Annavaram*

Main category: cs.LG

TL;DR: DuetServe is a unified LLM serving framework that uses dynamic GPU spatial multiplexing to isolate prefill and decode phases only when needed, improving throughput by up to 1.3x while maintaining low latency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems either cause interference between compute-intensive prefill and memory-bound decode phases when aggregated on shared GPUs, or waste resources through disaggregation across GPUs with duplicated models and KV cache transfers.

Method: DuetServe operates in aggregated mode by default and activates SM-level GPU spatial multiplexing when time-between-tokens degradation is predicted. It incorporates an attention-aware roofline model for latency forecasting, a partitioning optimizer for optimal SM splits, and an interruption-free execution engine to avoid CPU-GPU sync overhead.

Result: Evaluations demonstrate that DuetServe enhances total throughput by up to 1.3x compared to state-of-the-art frameworks while ensuring low generation latency.

Conclusion: DuetServe effectively balances resource efficiency and performance by providing fine-grained, adaptive isolation of prefill and decode phases within a single GPU, overcoming limitations of existing approaches.

Abstract: Modern LLM serving systems must sustain high throughput while meeting strict
latency SLOs across two distinct inference phases: compute-intensive prefill
and memory-bound decode phases. Existing approaches either (1) aggregate both
phases on shared GPUs, leading to interference between prefill and decode
phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two
phases across GPUs, improving latency but wasting resources through duplicated
models and KV cache transfers. We present DuetServe, a unified LLM serving
framework that achieves disaggregation-level isolation within a single GPU.
DuetServe operates in aggregated mode by default and dynamically activates
SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key
idea is to decouple prefill and decode execution only when needed through
fine-grained, adaptive SM partitioning that provides phase isolation only when
contention threatens latency service level objectives (SLOs). DuetServe
integrates (1) an attention-aware roofline model to forecast iteration latency,
(2) a partitioning optimizer that selects the optimal SM split to maximize
throughput under TBT constraints, and (3) an interruption-free execution engine
that eliminates CPU-GPU synchronization overhead. Evaluations show that
DuetServe improves total throughput by up to 1.3x while maintaining low
generation latency compared to state-of-the-art frameworks.

</details>


### [21] [Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator](https://arxiv.org/abs/2511.04804)
*Chaymae Yahyati,Ismail Lamaakal,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SiFEN is a new neural network architecture that uses finite-element methods on a learned simplicial mesh to create piecewise-polynomial predictors with explicit locality, controllable smoothness, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current neural networks like MLPs lack interpretability, explicit locality, and theoretical grounding. SiFEN aims to provide a compact, interpretable alternative with better calibration and inference efficiency.

Method: SiFEN represents functions as finite-element fields on learned simplicial meshes using degree-m Bernstein-Bezier polynomials. It employs barycentric coordinates for activation, shape regularization, semi-discrete OT coverage, and differentiable edge flips during end-to-end training.

Result: SiFEN achieves the theoretical FEM approximation rate M^(-m/d) and empirically matches or surpasses MLPs and KANs in performance while improving calibration (lower ECE/Brier scores) and reducing inference latency due to geometric locality.

Conclusion: SiFEN provides a theoretically grounded, compact, and interpretable alternative to dense MLPs and edge-spline networks, with advantages in calibration, efficiency, and performance across various tasks.

Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial
predictor that represents f: R^d -> R^k as a globally C^r finite-element field
on a learned simplicial mesh in an optionally warped input space. Each query
activates exactly one simplex and at most d+1 basis functions via barycentric
coordinates, yielding explicit locality, controllable smoothness, and
cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with
a light invertible warp and trains end-to-end with shape regularization,
semi-discrete OT coverage, and differentiable edge flips. Under standard
shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic
FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic
approximation tasks, tabular regression/classification, and as a drop-in head
on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter
budgets, improves calibration (lower ECE/Brier), and reduces inference latency
due to geometric locality. These properties make SiFEN a compact,
interpretable, and theoretically grounded alternative to dense MLPs and
edge-spline networks.

</details>


### [22] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE is a training-free compression method for Mixture-of-Experts models that reduces memory usage by up to 50% while maintaining accuracy, using sparse expert merging and bit-packed encoding.


<details>
  <summary>Details</summary>
Motivation: MoE models have high memory overhead from storing all expert parameters, limiting deployment. Existing compression methods cause performance drops at high ratios.

Method: Uses sparse expert merging with dual-mask to identify redundant/specialized weights, and bit-packed encoding to reuse exponent bits for efficient storage.

Result: Achieves 50% compression with maintained accuracy, outperforms prior methods by up to 16.7% on MMLU, and achieves 1.28x inference speedup.

Conclusion: PuzzleMoE enables efficient MoE deployment with high compression and accuracy, advancing scalable language modeling.

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [23] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: The paper analyzes topological constraints and capabilities of autoencoders for data manifolds and dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental topological limitations and possibilities when constructing autoencoders that approximate identity maps on data manifolds.

Method: Analyzes continuous encoder-decoder pairs and their compositions, focusing on topological properties and constraints.

Result: Identifies specific topological limitations and capabilities for autoencoder construction and extends analysis to dynamical systems with invariant manifolds.

Conclusion: Autoencoders have inherent topological constraints when approximating identity maps, but also possess capabilities for handling dynamical systems with invariant manifolds.

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [24] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE dataset with 80,000+ peptides and transformer model improves antimicrobial peptide classification by 2.56% over previous methods.


<details>
  <summary>Details</summary>
Motivation: Address fragmented datasets, inconsistent annotations, and lack of standardized benchmarks hindering computational AMP discovery.

Method: Create ESCAPE framework integrating peptides from 27 repositories with multilabel hierarchy, plus transformer model using sequence/structural data.

Result: Model achieves 2.56% relative improvement in mean Average Precision over second-best method for multilabel classification.

Conclusion: ESCAPE provides reproducible evaluation framework to advance AI-driven antimicrobial peptide research.

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [25] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: The volume hypothesis is incomplete - large datasets change loss landscapes to make sharp but generalizing minima more accessible by increasing their relative volume.


<details>
  <summary>Details</summary>
Motivation: To understand why large datasets improve generalization in deep learning, challenging the conventional volume hypothesis that only flat minima generalize well.

Method: Measured minima volumes under varying training data sizes and analyzed how data quantity changes the loss landscape structure.

Result: Found that sharp minima can generalize well but are rarely found due to small volumes; increasing data makes these good sharp minima relatively larger and more discoverable.

Conclusion: Large datasets improve generalization not just by finding flat minima, but by reshaping the loss landscape to make previously inaccessible sharp generalizing minima more discoverable.

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [26] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: A method that replaces negative prompts with implicit negative embeddings via concept inversion to improve defense against harmful content in text-to-image models, without modifying existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current defenses against harmful content in text-to-image models (fine-tuning and training-free guidance) show incompatibility when combined, limiting effectiveness.

Method: Propose using implicit negative embeddings from concept inversion instead of negative prompts in training-free methods, allowing seamless integration.

Result: Improved defense success rates on nudity and violence benchmarks while maintaining input prompt semantics.

Conclusion: The method effectively enhances defense compatibility and performance without altering existing pipelines.

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [27] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: Label-free RL struggles with smaller models lacking reasoning capability; curriculum learning and data curation improve performance across all model sizes.


<details>
  <summary>Details</summary>
Motivation: To investigate generalizability of unsupervised RL methods across different model sizes and address limitations for smaller models.

Method: Systematic evaluation of label-free RL from 0.5B to 7B models; propose curriculum learning with progressive difficulty and masking no-majority rollouts; data curation pipeline for difficulty-controlled samples.

Result: Smaller models degrade below baseline due to insufficient reasoning diversity; proposed method shows consistent improvements across all model sizes.

Conclusion: Curriculum-based label-free RL enables more robust unsupervised reasoning enhancement, especially for resource-constrained models.

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [28] [Persistent reachability homology in machine learning applications](https://arxiv.org/abs/2511.04825)
*Luigi Caputi,Nicholas Meadows,Henri Riihimäki*

Main category: cs.LG

TL;DR: Persistent reachability homology (PRH) for digraphs outperforms directed flag complex-based persistent homology (DPH) in classifying epilepsy detection neural networks, using Betti curves and SVM.


<details>
  <summary>Details</summary>
Motivation: Traditional persistent homology based on directed flag complex (DPH) may be computationally intensive. PRH offers an advantage by analyzing smaller digraph condensations during filtration, potentially improving efficiency and effectiveness.

Method: Applied PRH and DPH to digraph data from epilepsy neural networks, using Betti curves and their integrals as topological features, with a support vector machine classifier.

Result: PRH achieved superior classification performance compared to DPH in the epilepsy detection task.

Conclusion: PRH is a more effective and computationally advantageous method for topological analysis of digraph data in neuroscience applications like epilepsy detection.

Abstract: We explore the recently introduced persistent reachability homology (PRH) of
digraph data, i.e. data in the form of directed graphs. In particular, we study
the effectiveness of PRH in network classification task in a key neuroscience
problem: epilepsy detection. PRH is a variation of the persistent homology of
digraphs, more traditionally based on the directed flag complex (DPH). A main
advantage of PRH is that it considers the condensations of the digraphs
appearing in the persistent filtration and thus is computed from smaller
digraphs. We compare the effectiveness of PRH to that of DPH and we show that
PRH outperforms DPH in the classification task. We use the Betti curves and
their integrals as topological features and implement our pipeline on support
vector machine.

</details>


### [29] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: Dual-Guided Loss (DGL) is a scalable decision-focused learning method that uses dual variables from optimization problems to guide model training, reducing solver dependence while maintaining decision alignment.


<details>
  <summary>Details</summary>
Motivation: Scaling decision-focused learning is challenging because current methods either differentiate through solvers or use task-specific surrogates, both requiring frequent and expensive optimizer calls.

Method: DGL leverages dual variables from downstream combinatorial selection problems, decouples optimization from gradient updates by solving problems only periodically, and trains on dual-adjusted targets using simple differentiable surrogate losses between refreshes.

Result: DGL matches or exceeds state-of-the-art DFL methods while using far fewer solver calls and substantially less training time, with proven asymptotically diminishing decision regret.

Conclusion: DGL provides a scalable approach to decision-focused learning that reduces computational costs while maintaining strong decision alignment, making it practical for real-world applications.

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [30] [TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems](https://arxiv.org/abs/2511.05269)
*Ishan Kavathekar,Hemang Jain,Ameya Rathod,Ponnurangam Kumaraguru,Tanuja Ganu*

Main category: cs.MA

TL;DR: TAMAS is a benchmark for evaluating safety and robustness of multi-agent LLM systems, revealing high vulnerability to adversarial attacks and introducing metrics to assess safety-effectiveness tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on single-agent settings, failing to capture vulnerabilities in multi-agent dynamics and coordination, creating a gap in safety evaluation for collaborative LLM systems.

Method: Developed TAMAS benchmark with 5 scenarios, 300 adversarial instances across 6 attack types, 211 tools, and 100 harmless tasks. Evaluated 10 backbone LLMs across 3 agent interaction configurations from Autogen and CrewAI frameworks.

Result: Multi-agent systems are highly vulnerable to adversarial attacks. The benchmark reveals critical challenges and failure modes in current deployments, with the proposed Effective Robustness Score (ERS) quantifying safety-effectiveness tradeoffs.

Conclusion: TAMAS provides a foundation for systematically studying and improving multi-agent LLM system safety, highlighting urgent need for stronger defenses against adversarial threats.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities as
autonomous agents through tool use, planning, and decision-making abilities,
leading to their widespread adoption across diverse tasks. As task complexity
grows, multi-agent LLM systems are increasingly used to solve problems
collaboratively. However, safety and security of these systems remains largely
under-explored. Existing benchmarks and datasets predominantly focus on
single-agent settings, failing to capture the unique vulnerabilities of
multi-agent dynamics and co-ordination. To address this gap, we introduce
$\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent
$\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the
robustness and safety of multi-agent LLM systems. TAMAS includes five distinct
scenarios comprising 300 adversarial instances across six attack types and 211
tools, along with 100 harmless tasks. We assess system performance across ten
backbone LLMs and three agent interaction configurations from Autogen and
CrewAI frameworks, highlighting critical challenges and failure modes in
current multi-agent deployments. Furthermore, we introduce Effective Robustness
Score (ERS) to assess the tradeoff between safety and task effectiveness of
these frameworks. Our findings show that multi-agent systems are highly
vulnerable to adversarial attacks, underscoring the urgent need for stronger
defenses. TAMAS provides a foundation for systematically studying and improving
the safety of multi-agent LLM systems.

</details>
