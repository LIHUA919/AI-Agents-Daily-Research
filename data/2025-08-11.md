<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: The paper introduces Adaptive Exploration Policy Optimization (AEPO) to improve semantic alignment in Multimodal Large Language Models (MLLMs) for GUI tasks, outperforming baselines by up to 9.0%.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robustly grounding natural language instructions in GUIs, particularly semantic alignment, which is bottlenecked by inefficient exploration.

Method: Proposes AEPO, a policy optimization framework using multi-answer generation and an Adaptive Exploration Reward (AER) function derived from efficiency principles.

Result: AEPO-trained models (InfiGUI-G1-3B and InfiGUI-G1-7B) achieve state-of-the-art performance on GUI grounding benchmarks, with up to 9.0% improvement over RLVR baselines.

Conclusion: AEPO effectively addresses exploration inefficiencies, enhancing semantic alignment in MLLMs for GUI tasks, as demonstrated by benchmark results.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [2] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: A novel framework for safe AGI combines Active Inference with LLMs, integrating safety into core design via transparent beliefs and hierarchical value alignment.


<details>
  <summary>Details</summary>
Motivation: Traditional AI safety methods (post-hoc interpretability, reward engineering) have limitations; this work aims for inherently safer AGI.

Method: Uses natural language for belief representation, multi-agent Active Inference, and hierarchical Markov blankets for safety.

Result: Proposes mechanisms like belief-preference separation, bounded rationality, and modular safety.

Conclusion: The framework offers a safer AGI path, validated via ARC benchmark experiments.

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [3] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR, a framework using LLM agent scientists, improves table reasoning without training data, outperforming vanilla LLMs and matching supervised models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of annotated data and complex augmentation in table reasoning, and the underperformance of LLMs compared to supervised models.

Method: PanelTR employs LLM agent scientists for structured reasoning, involving individual investigations, self-review, and peer-review discussions via five scientist personas.

Result: Outperforms vanilla LLMs and rivals supervised models on four benchmarks, without relying on training data.

Conclusion: Structured scientific methodology enables robust, flexible semantic understanding in zero-shot contexts, applicable beyond table reasoning.

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [4] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: Neural networks challenge the symbolic view of human cognition by demonstrating similar abilities, suggesting a new research agenda.


<details>
  <summary>Details</summary>
Motivation: To question the symbolic nature of human cognition by showing neural networks exhibit comparable abilities.

Method: Analyzing the capabilities of modern neural networks and their implications for symbolic theories of cognition.

Result: Neural networks undermine the symbolic argument but highlight the role of symbolic systems in framing abstract problems.

Conclusion: Proposes a new research agenda to explore the symbolic basis of human thought in light of neural network findings.

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [5] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI integrates causal rating with traditional XAI to provide interactive, multi-method explanations for diverse stakeholders, addressing gaps in current XAI methods.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods focus on justifying model outputs for developers, neglecting diverse stakeholder needs. Evaluative AI shifts toward hypothesis testing but remains operationally limited.

Method: H-XAI combines causal rating methods with traditional XAI, enabling interactive hypothesis testing, comparison against baselines, and adaptation to stakeholder goals (individual decisions, group bias, robustness).

Result: Demonstrated in two case studies (credit risk classification and financial forecasting), H-XAI effectively answers stakeholder-specific questions at individual and model levels.

Conclusion: H-XAI fills critical gaps in XAI by unifying causal ratings and post-hoc explanations, supporting diverse stakeholder needs beyond traditional methods.

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [6] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: A survey on safety in embodied navigation, covering attacks, defenses, and evaluation methods, with insights for future research to improve system reliability.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs and embodied AI in navigation raises safety concerns in dynamic environments, necessitating a thorough analysis.

Method: Comprehensive review of attack strategies, defense mechanisms, evaluation methodologies, datasets, and metrics.

Result: Identifies existing safety challenges, mitigation technologies, and gaps like potential attacks, defenses, and better evaluation techniques.

Conclusion: The survey guides future research for safer embodied navigation, with broader societal and industrial benefits.

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [7] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: The paper proposes a Knowledge Graph (KG)-based tool retrieval framework to improve accuracy in selecting tools for multi-step user queries, outperforming traditional similarity-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional tool retrieval methods rely on query-tool similarity, which is inadequate for multi-step tasks. The paper aims to address this by leveraging semantic relationships and functional dependencies between tools.

Method: The framework uses ensembles of 1-hop ego tool graphs within a KG to model direct and indirect tool connections, enhancing contextual tool selection.

Result: The KG-based method achieves 91.85% tool coverage (Complete Recall), outperforming the strongest non-KG baseline (89.26%).

Conclusion: Structural information in KGs complements similarity matching, especially for sequential tool composition tasks.

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [8] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: MedOrch is a mediator-guided multi-agent framework for medical decision-making, using LLM-based mediation to enhance collaboration among VLM-based expert agents, outperforming individual models without training.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems struggle with multimodal tasks and lack self-reflection, limiting their use in cooperative clinical workflows.

Method: Proposes MedOrch, an LLM-based mediator coordinating multiple VLM-based expert agents for collaborative decision-making.

Result: Demonstrates superior performance on medical vision QA benchmarks, showing collaboration surpasses individual agents.

Conclusion: Highlights the potential of mediator-guided multi-agent collaboration in medical multimodal intelligence.

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [9] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: HIMA, a hierarchical multi-agent framework with specialized imitation learning agents and a meta-controller, outperforms existing LLM-based approaches in dynamic, long-horizon tasks like StarCraftII.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with dynamic, long-horizon tasks like StarCraftII due to resource constraints and partial observability.

Method: Proposes HIMA: hierarchical multi-agent framework with specialized imitation learning agents and a Strategic Planner (SP) for orchestration.

Result: HIMA outperforms state-of-the-art methods in strategic clarity, adaptability, and computational efficiency.

Conclusion: Combining specialized imitation modules with meta-level orchestration enhances robustness and general-purpose AI agent capabilities.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [10] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: A framework uses Participatory Budgeting (PB) to evaluate LLMs in resource allocation and reasoning, testing three prompting strategies and preference inference from natural language.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capabilities in structured resource allocation and reasoning, addressing gaps in evaluation due to data contamination and static benchmarks.

Method: Uses PB for LLM-based resource allocation and benchmarking, employing greedy selection, direct optimization, and hill-climbing-inspired refinement. Tests preference inference from natural language.

Result: Highlights the impact of prompt design and shows LLMs' potential for mechanism design with unstructured inputs.

Conclusion: LLMs show promise in handling resource allocation and reasoning tasks, especially with adaptive prompting and preference inference.

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [11] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: The paper argues that cognitive imagination is crucial for human reasoning and AI, proposing semantic models as a tool to simulate it.


<details>
  <summary>Details</summary>
Motivation: Cognitive imagination is undervalued in AI, leading to blind reasoning. The paper highlights its role in human thinking and aims to address this gap in AI.

Method: Proposes semantic models, a new approach combining probabilistic causal relationships with learning capabilities like neural networks.

Result: Semantic models can simulate cognitive imagination by maintaining consistent, manipulable imaginary contexts.

Conclusion: The paper calls for prioritizing cognitive imagination in AI research, suggesting semantic models as a breakthrough tool.

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [12] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: The paper proposes CA-DL8.5, a generic and anytime beam search algorithm for optimal decision tree learning, unifying existing strategies and outperforming prior methods in anytime performance.


<details>
  <summary>Details</summary>
Motivation: Existing exact methods for optimal decision tree learning lack balanced anytime behavior, and existing anytime extensions have not been systematically compared.

Method: CA-DL8.5 extends DL8.5 with a modular design for integrating heuristics and relaxation mechanisms, using branch-and-bound pruning, trie-based caching, and restart-based beam search.

Result: CA-DL8.5 with LDS heuristics outperforms other variants and the Blossom algorithm in anytime performance while ensuring completeness and optimality.

Conclusion: CA-DL8.5 provides a flexible framework for exact and anytime decision tree learning, with LDS-based instantiations showing superior performance.

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [13] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: The paper introduces a deep reinforcement learning (DRL) approach for autonomous driving, combining bird's-eye view (BEV) perception with the Mamba framework for efficient spatio-temporal feature extraction. The proposed ME³-BEV framework outperforms existing models in dynamic urban scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional modular approaches (error propagation) and end-to-end learning (computational bottlenecks) in autonomous driving systems.

Method: Integrates BEV perception with the Mamba framework for spatio-temporal feature extraction, using the ME³-BEV framework for end-to-end DRL.

Result: Outperforms existing models in CARLA simulator tests, excelling in collision rate and trajectory accuracy.

Conclusion: The ME³-BEV framework offers a promising, interpretable, and efficient solution for real-time autonomous driving.

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [14] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: The paper resolves an open problem by proving that aggregate-combine-readout GNNs' logical expressiveness strictly exceeds that of C2, providing insights into infinitary logics.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved question of whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs.

Method: Proving the logical expressiveness of aggregate-combine-readout GNNs exceeds C2 through theoretical analysis.

Result: The expressiveness of aggregate-combine-readout GNNs strictly surpasses C2, applicable to both undirected and directed graphs.

Conclusion: The work resolves a key open problem and offers new insights into the expressive power of infinitary logics.

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [15] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE is a novel framework where LLMs compete by generating and solving verifiable tasks, offering scalable, automated, and objective evaluation without human input.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for foundation models are limited by scalability and domain expertise, hindering their adaptability to rapid model evolution.

Method: LLMs act as both task-setters and solvers in a game-like framework, creating verifiable tasks (e.g., code-output-prediction challenges) to evaluate each other objectively.

Result: Weaker models can reliably score stronger ones, LLMs exhibit self-preferencing behavior, and SKATE reveals fine-grained capability differences between models.

Conclusion: SKATE advances scalable, general evaluation frameworks to keep pace with LLM progress, demonstrating its effectiveness in automated, objective assessment.

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [16] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: The paper explores using machine learning and explainable AI to analyze feature importance in VRP solutions, proposing a framework to guide metaheuristic algorithm design.


<details>
  <summary>Details</summary>
Motivation: To improve VRP-solving metaheuristics by leveraging machine learning for feature importance analysis and explainable AI.

Method: Conducts sensitivity analysis with multiple classifier models to predict VRP solution quality and uses explainable AI to understand model decisions.

Result: Feature importance varies, but certain features consistently predict solution quality; a unified framework ranks feature impact.

Conclusion: Feature importance analysis can guide metaheuristic algorithm design for VRP, enhancing efficiency.

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [17] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: The study improves LLMs for healthcare by using a RAG pipeline with GPT-4o-mini and text-embedding-3-small, significantly boosting accuracy in drug contraindication tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in healthcare due to the need for accuracy in pharmaceutical contraindications.

Method: Implemented a RAG pipeline with GPT-4o-mini and text-embedding-3-small, using Langchain for hybrid retrieval and re-ranking, trained on DUR data.

Result: Accuracy improved from 0.49-0.57 to 0.94, 0.87, and 0.89 for age groups, pregnancy, and concomitant drug use contraindications.

Conclusion: The RAG framework enhances LLMs' reliability in drug contraindication, aiding safer prescription decisions.

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [18] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: The paper advocates for confidence-driven, risk-aware LLM-as-a-Judge systems, addressing the overconfidence issue in current models with a new metric (TH-Score) and framework (LLM-as-a-Fuser).


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-Judge systems focus on accuracy but lack well-calibrated confidence, undermining reliability.

Method: Introduces TH-Score to measure confidence-accuracy alignment and proposes LLM-as-a-Fuser, an ensemble framework for risk-aware evaluation.

Result: The approach improves calibration, enabling adaptive evaluation pipelines with superior reliability and accuracy.

Conclusion: Shifting to confidence-driven systems enhances trustworthiness and adaptability in LLM-as-a-Judge applications.

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [19] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: The paper introduces GeoLaux, a benchmark for evaluating Multimodal Large Language Models (MLLMs) on geometry problem-solving, focusing on auxiliary line construction and long-step reasoning. It reveals performance drops in extended reasoning, shortcut tendencies in proving problems, and the importance of auxiliary line awareness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs overlook auxiliary line construction and lack fine-grained process evaluation, limiting their ability to assess long-step reasoning in geometry.

Method: The authors create the GeoLaux benchmark with 2,186 geometry problems, including calculation and proving questions, and design a five-dimensional evaluation strategy.

Result: Experiments on 13 MLLMs show performance degradation in extended reasoning, shortcut tendencies in proving problems, and the benefits of auxiliary line awareness.

Conclusion: GeoLaux serves as a benchmark and guide for improving MLLMs' long-step geometric reasoning, with the dataset and code to be released.

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [20] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: A Bayesian inductive logic programming method balances hypothesis complexity and data fit, outperforming prior methods in domains like game playing and drug design.


<details>
  <summary>Details</summary>
Motivation: Unifying probabilistic and logical learning in AI is a key challenge.

Method: Uses Bayesian inductive logic programming with priors favoring general programs and a likelihood favoring accuracy.

Result: Outperforms previous methods, especially those learning minimum description length programs, and is data-efficient.

Conclusion: The approach is effective, data-efficient, and works well with imbalanced or exclusively positive examples.

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [21] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: A method to break symmetries in hypothesis spaces for inductive logic programming, reducing solving times significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of searching vast and logically equivalent hypothesis spaces in inductive logic programming.

Method: Introducing a symmetry-breaking technique implemented in answer set programming.

Result: Experiments show solving times reduced from over an hour to 17 seconds.

Conclusion: The approach effectively improves efficiency in inductive logic programming tasks.

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [22] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval introduces BET, an AI tool for automated red-teaming, achieving high attack success rates on LLMs and proposing fine-grained robustness metrics.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the robustness of state-of-the-art LLMs by identifying vulnerabilities through adversarial testing.

Method: Uses Dynamic Adversarial Optimization for automated red-teaming and introduces fine-grained metrics to measure attack difficulty.

Result: Achieved 100% Attack Success Rate against 37 of 41 LLMs, with attack difficulty varying 300-fold across models.

Conclusion: Demonstrates universal vulnerability in LLMs and provides practical methods for distributed robustness assessment.

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [23] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: The paper reinterprets Conant and Ashby's theorem, showing that agents performing regulation tasks can be seen as having 'beliefs' about their environment, with models imposed by observers.


<details>
  <summary>Details</summary>
Motivation: To address apparent counterexamples to Conant and Ashby's theorem in Artificial Life, proposing a broader notion of models.

Method: Introduces a reinterpretation where agents' regulation tasks imply 'beliefs' about their environment, with models defined by observers.

Result: A more broadly applicable theorem where models are observer-imposed, resolving apparent counterexamples.

Conclusion: Models are not intrinsic but observer-dependent, extending the applicability of the original theorem.

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [24] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: AntiCheatPT_256, a transformer-based model, detects cheating in Counter-Strike 2 with 89.17% accuracy, using a new dataset (CS2CD) of 795 matches.


<details>
  <summary>Details</summary>
Motivation: Cheating in online games undermines fairness, and current anti-cheat systems struggle to adapt without invasive measures.

Method: A transformer model trained on 90,707 context windows from the CS2CD dataset, augmented to address class imbalance.

Result: Achieved 89.17% accuracy and 93.36% AUC on an unaugmented test set.

Conclusion: The model provides a reproducible, data-driven baseline for future cheat detection research.

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [25] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: The paper introduces 'Explanatory AI' as a user-centered alternative to traditional XAI, focusing on human understanding through narrative, adaptability, and context-sensitive explanations, validated by healthcare professionals.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods lack adaptability and fail to support meaningful human understanding, necessitating a shift toward user-centered explanations.

Method: Develops an eight-dimensional conceptual model for Explanatory AI, validated using Rapid Contextual Design methodology with healthcare professionals.

Result: Users prefer context-sensitive, multimodal explanations over technical transparency, highlighting the need for human-centered AI explanations.

Conclusion: The paper advocates for Explanatory AI as a paradigm shift, emphasizing human comprehension and setting a research agenda for diverse applications.

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [26] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: The paper introduces two methods for constructing Legal Knowledge Graphs (KGs) for violence against women cases, using a bottom-up approach and Large Language Models, validated for legal decision-making support.


<details>
  <summary>Details</summary>
Motivation: Legal KGs are scarce but crucial for enhancing legal decision-making, accessibility, and predictive justice.

Method: Two approaches: a systematic bottom-up method and a Large Language Model-based solution, integrating data extraction, ontology development, and semantic enrichment.

Result: Developed KGs validated via competency questions, improving legal information accessibility and enabling complex queries.

Conclusion: The KGs can significantly aid legal decision-making, predictive justice, and machine learning applications.

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [27] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: The paper introduces 'Fair Game,' a dynamic framework using Reinforcement Learning to adapt ML fairness goals over time, addressing gaps in current Fair ML approaches.


<details>
  <summary>Details</summary>
Motivation: Current Fair ML methods rely on observational bias definitions, which are often conflicting and limited to static or retrospective use, failing to adapt in dynamic social environments.

Method: Proposes 'Fair Game,' combining an Auditor and Debiasing algorithm in an RL loop to dynamically adjust fairness goals based on societal feedback.

Result: 'Fair Game' enables flexible, adaptive fairness in ML systems, simulating societal ethical evolution by modifying fairness goals over time.

Conclusion: The framework bridges the gap between Fair ML's static goals and dynamic societal needs, offering a scalable solution for pre- and post-deployment fairness.

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [28] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: A data-driven framework evaluates multi-winner voting rules' axiom violations across diverse preference distributions, showing neural networks outperform traditional rules.


<details>
  <summary>Details</summary>
Motivation: To shift from worst-case analysis to practical evaluation of voting rules' axiomatic performance.

Method: Propose a data-driven framework to analyze voting rules under various preference distributions and compare traditional rules with neural networks.

Result: Neural networks outperform traditional voting rules in minimizing axiom violations.

Conclusion: Data-driven approaches can inform new voting system designs and encourage further research in social choice.

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: D2D converts causal loop diagrams (CLDs) into system dynamics models (SDMs) for dynamic analysis, outperforming network centrality in consistency and providing uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: CLDs are qualitative and static, limiting dynamic analysis and intervention strategies. Quantitative methods like network centrality often lead to false inferences.

Method: D2D transforms CLDs into SDMs using structural information (link existence, polarity) with minimal user input (variable labeling).

Result: D2D distinguishes high- and low-ranked leverage points, shows consistency with data-driven models, and offers uncertainty estimates.

Conclusion: D2D is a promising tool for dynamic modeling, implemented in open-source tools, with potential for broader validation and application.

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [30] [Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation](https://arxiv.org/abs/2508.05154)
*Rishabh Gaur,Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: The paper introduces domain-driven metrics for evaluating RL-based agent-based models (ABMs) and rational ABMs (RABMs), addressing challenges in performance assessment due to system complexity and lack of standardized metrics.


<details>
  <summary>Details</summary>
Motivation: The complexity and stochasticity of ABMs/RABMs, along with the absence of standardized metrics for RL algorithms, motivate the development of domain-driven evaluation metrics.

Method: The study develops domain-driven RL metrics, building on existing state-of-the-art metrics, and demonstrates their application in a rational ABM disease modeling case study involving masking, vaccination, and lockdown behaviors.

Result: The proposed domain-driven rewards, combined with traditional metrics, are shown effective in various simulation scenarios, such as differential mask availability.

Conclusion: Domain-driven RL metrics enhance the evaluation of RL-based ABMs/RABMs, providing a more tailored and effective approach for complex, stochastic systems.

Abstract: For the development and optimization of agent-based models (ABMs) and
rational agent-based models (RABMs), optimization algorithms such as
reinforcement learning are extensively used. However, assessing the performance
of RL-based ABMs and RABMS models is challenging due to the complexity and
stochasticity of the modeled systems, and the lack of well-standardized metrics
for comparing RL algorithms. In this study, we are developing domain-driven
metrics for RL, while building on state-of-the-art metrics. We demonstrate our
``Domain-driven-RL-metrics'' using policy optimization on a rational ABM
disease modeling case study to model masking behavior, vaccination, and
lockdown in a pandemic. Our results show the use of domain-driven rewards in
conjunction with traditional and state-of-the-art metrics for a few different
simulation scenarios such as the differential availability of masks.

</details>


### [31] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: A novel framework represents physical laws as a weighted knowledge graph, achieving high accuracy in link prediction and uncovering key insights in physics.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and represent physical laws by resolving notational ambiguities and leveraging graph-based methods for deeper insights.

Method: Constructed a database of 400 physics equations, represented as a weighted graph, and trained a Graph Attention Network (GAT) for link prediction.

Result: Achieved a test AUC of 0.9742, outperforming baselines, and identified key findings like conceptual axes and hub equations.

Conclusion: The framework successfully models physics knowledge, suggesting novel analogies and enabling targeted analysis of subfields.

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [32] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: Neural network nudging is proposed for nonlinear state space models, with theoretical backing and successful evaluation on chaotic systems.


<details>
  <summary>Details</summary>
Motivation: Designing effective nudging terms for nonlinear systems is challenging, motivating a data-driven approach.

Method: Neural networks are used to learn nudging terms, supported by theoretical results from observer theory.

Result: The method is tested on chaotic systems (Lorenz 96, Kuramoto-Sivashinsky, Kolmogorov flow) and shows promise.

Conclusion: Neural network nudging is a viable solution for nonlinear state space models, validated by theory and experiments.

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [33] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD is a population-free, multi-agent reinforcement learning framework for ad-hoc teamwork, generating diverse training partners without pretrained agents or manual tuning. It outperforms baselines and is perceived as more adaptive and human-like.


<details>
  <summary>Details</summary>
Motivation: To enable robust ad-hoc teamwork by adaptively generating diverse training partners without relying on pretrained partners or manual parameter tuning.

Method: UPD stochastically mixes an ego agent's policy with biased random behaviors and scores partners using a variance-based learnability metric. It integrates with unsupervised environment design for fully unsupervised curricula.

Result: UPD outperforms population-based and population-free baselines in evaluations on Overcooked-AI and the Overcooked Generalisation Challenge. It is also rated higher in adaptability, human-likeness, and collaboration in a user study.

Conclusion: UPD is highly effective for robust ad-hoc teamwork, offering a dynamic partner curriculum that enhances adaptability and collaboration, surpassing existing methods.

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [34] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: A scalable framework integrates heterogeneous data to reconstruct accurate distribution grid topology, combining spatial layout and dynamic behavior, with a confidence-aware mechanism and physical constraints, achieving 95% accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate grid topology is critical for reliable operations, but utility data varies in quality and sources, necessitating a robust solution.

Method: The framework jointly leverages spatial infrastructure data and dynamic system behavior, using a confidence-aware inference mechanism and embedding physical constraints.

Result: Validated on 8000+ meters across 3 feeders, the method achieves 95% accuracy, better confidence calibration, and computational efficiency.

Conclusion: The framework provides actionable, trustworthy topologies by balancing uncertainty awareness with structural validity, suitable for real-world deployment.

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [35] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: The paper presents a theoretical framework for analyzing linear encoder-decoder architectures in scientific machine learning, focusing on interpretability and Bayes risk minimization.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of nonlinear neural networks and provide interpretable solutions for scientific machine learning problems.

Method: Develops a unified framework using Bayes risk minimization, deriving closed-form, rank-constrained linear and affine linear optimal mappings for forward and inverse tasks.

Result: Generalizes existing formulations to handle rank-deficiencies and validates results with numerical experiments in biomedical imaging, finance, and fluid dynamics.

Conclusion: Offers a robust baseline for benchmarking neural network models in scientific machine learning, emphasizing interpretability.

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [36] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: The paper introduces a novel framework combining TAPE and Graphormer to enhance node classification in Textual Attribute Graphs (TAGs), achieving state-of-the-art results on the ogbn-arxiv dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods for node classification in TAGs struggle with integrating text semantics and graph structure, handling domain-specific terminology, long-range dependencies, temporal evolution, and scalability.

Method: The proposed framework uses ChatGPT within TAPE to generate rich text explanations, fuses them with structural features via a learned attention layer, and employs Graphormer for capturing long-range dependencies.

Result: Achieves 0.772 classification accuracy on ogbn-arxiv, outperforming GCN baselines (0.713), with strong precision (0.671), recall (0.577), and F1-score (0.610).

Conclusion: The framework offers a scalable, robust solution for dynamic TAGs, advancing research in knowledge systems and scientific discovery.

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [37] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: A reinforcement learning policy gradient (RL-PG) algorithm trains an autonomous guidance policy for collision avoidance maneuvers (CAMs) using a Markov decision process (MDP) framework, balancing collision risk and fuel efficiency.


<details>
  <summary>Details</summary>
Motivation: To optimize collision avoidance maneuvers by minimizing fuel consumption while maintaining acceptable collision risks, leveraging early maneuver decisions.

Method: Model CAM as a continuous state, discrete action, finite horizon MDP. Incorporate analytical models for conjunction risk, propellant consumption, and transit orbit geometry. Train policy using RL-PG on historic CAM data.

Result: Trained policy reduces average propellant consumption per CAM compared to conventional methods, with equal or better collision risk guarantees.

Conclusion: The RL-PG-based MDP framework effectively balances fuel efficiency and collision risk, outperforming traditional cut-off policies.

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [38] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT, a 2-bit quantization method, improves information density without forward-path penalties, challenging the view of quantization as suboptimal.


<details>
  <summary>Details</summary>
Motivation: To explore quantization not as a trade-off but as a potential improvement under fixed resource budgets.

Method: Introduces Signed-Zero Ternary (SZT), a deterministic 2-bit quantization method that preserves gradient information.

Result: SZT may enhance information density compared to non-quantized alternatives.

Conclusion: Quantization can outperform non-quantized methods in certain scenarios, offering a new perspective on its utility.

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [39] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: The paper proposes a machine learning-based method to decompose a stochastic time series into mean, dispersion, and noise components, with applications in smoothing, denoising, and forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the need for decomposing stochastic time series into interpretable components (mean, dispersion, and noise) for better analysis and forecasting.

Method: Machine learning is used to fit a dual signal (mean and dispersion) by minimizing a loss function that balances fitting accuracy and regularization. Two learning approaches (sequential and joint) are explored, along with optimization or neural networks for tuning.

Result: The method effectively decomposes time series, isolates noise, and can be tuned for specific applications (e.g., stepped or smoothed signals).

Conclusion: The proposed decomposition is versatile, enabling applications like forecasting, structure learning, and cross-effect analysis in multi-time series scenarios.

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [40] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: The paper addresses poor optimization in neural PDE solvers due to ill-conditioning, proposing Shifted Gaussian Encoding to improve conditioning and performance.


<details>
  <summary>Details</summary>
Motivation: Neural PDE solvers often fail due to ill-conditioning, not lack of expressivity, especially in multi-fidelity and stiff problems.

Method: Introduces Shifted Gaussian Encoding, an activation filtering step to improve matrix rank and expressivity while maintaining convexity in PIELMs.

Result: Extends solvable Peclet numbers by two orders, reduces error by six orders in multi-frequency learning, and outperforms deep networks in speed and accuracy.

Conclusion: Conditioning, not depth, is the bottleneck in neural solvers; simple architectural changes can yield significant improvements.

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [41] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO improves GRPO by addressing the Think-Answer Mismatch issue with noise-aware advantage weights, outperforming GRPO in noisy and unbalanced scenarios.


<details>
  <summary>Details</summary>
Motivation: GRPO is vulnerable to noisy rewards, especially in unbalanced groups, degrading learning. S-GRPO aims to stabilize training.

Method: S-GRPO introduces optimal, noise-aware advantage weights to counteract reward noise.

Result: S-GRPO outperforms GRPO by +2.5%, +2.2%, and +2.4% on benchmarks and remains stable under 20% noise.

Conclusion: S-GRPO enhances robustness and effectiveness in training large reasoning models, especially in noisy environments.

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [42] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Armed Bandits (MAB)-based pruning method for decision trees to improve generalization, outperforming traditional greedy pruning techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods like Cost-Complexity Pruning (CCP) and Reduced Error Pruning (REP) are greedy and may compromise long-term generalization, especially with small or complex datasets.

Method: The authors introduce a reinforcement learning (RL)-based MAB approach, treating pruning as an exploration-exploitation problem to dynamically optimize tree branches.

Result: Experiments on benchmark datasets show the MAB-based method achieves better predictive performance than conventional pruning techniques.

Conclusion: The MAB-based pruning approach offers a dynamic and probabilistic solution to optimize decision trees, enhancing their generalization ability.

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [43] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: The paper proposes a mildly conservative regularized evaluation (MCRE) framework and MCRQ algorithm to balance conservatism and performance in offline RL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the distribution shift and overestimation in offline RL by balancing conservatism and performance improvement.

Method: Introduces MCRE, combining TD error and behavior cloning in Bellman backup, and MCRQ, integrating MCRE into an actor-critic framework.

Result: MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.

Conclusion: The MCRE framework and MCRQ algorithm effectively balance conservatism and performance, advancing offline RL.

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [44] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: A semantically aligned RL method uses SBERT to compute rewards by aligning states with target semantic instructions, eliminating manual reward engineering.


<details>
  <summary>Details</summary>
Motivation: Challenges in designing effective reward functions in RL, especially for tasks with non-numeric goals, motivate a shift from heuristic-based rewards to semantic alignment.

Method: Rewards are computed as cosine similarity between goal textual descriptions and episode state descriptions using SBERT, replacing manual reward functions.

Result: The method achieves competitive control behavior without hand-crafted rewards and shows a correlation between language embedding and Euclidean spaces.

Conclusion: Semantic alignment enables natural language goal integration, paving the way for combining LLMs with control applications.

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [45] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: The paper addresses the challenge of achieving parameter-free optimal convergence rates for nonlinear fixed-point equations like Q-learning and TD-learning by leveraging semi-norm contractions and Polyak-Ruppert averaging.


<details>
  <summary>Details</summary>
Motivation: The non-monotonicity of semi-norms in nonlinear fixed-point equations has hindered optimal convergence rates, motivating the need for a new approach.

Method: The authors recast the averaged error as a linear recursion with a nonlinear perturbation and use a coupled semi-norm contraction with a monotonic induced norm.

Result: The method achieves the first parameter-free optimal rates of O~(1/√t) for Q-learning in average-reward and discounted settings.

Conclusion: The framework is versatile, supporting synchronous/asynchronous updates, single-agent/distributed deployments, and various data sources.

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [46] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP is a novel CoT compression framework that preserves logical coherence while reducing training and inference costs, achieving state-of-the-art performance in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Excessively long reasoning traces in LRMs increase training costs, inference latency, and deployment challenges, while existing CoT compression methods compromise logical coherence or fail to capture critical steps.

Method: ASAP uses anchor-guided pruning to preserve core reasoning structure and a first-token surprisal metric for logic-aware pruning, enabling autonomous generation of concise CoTs.

Result: ASAP reduces token generation by 23.5% and inference latency by 43.5% while achieving 36.19% Pass@1 accuracy on LiveCodeBench v4_v5.

Conclusion: ASAP offers an efficient and powerful solution for CoT compression in LRMs, balancing accuracy and cost-effectiveness.

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [47] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS combines LLMs with Monte Carlo Tree Search (MCTS) to improve multi-step prompt selection for better code generation and problem-solving in optimization tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex tasks requiring consistent multi-step planning, and existing MCTS-based approaches focus on simpler tasks or heuristic-based code.

Method: MCTS-OPS formulates prompt selection as a sequential decision process guided by MCTS, refining multi-step prompts for code generation.

Result: Experiments show 2-4x higher reward, 3x lower standard deviation, and 10% higher chance of optimal solutions in network optimization tasks.

Conclusion: Combining symbolic planning (MCTS) with LLMs enhances robust, high-quality code generation in complex domains.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [48] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: A novel stepwise dynamic competing risks model improves neurological outcome prediction for comatose post-cardiac arrest patients by leveraging time-invariant and time-varying features at optimal phases.


<details>
  <summary>Details</summary>
Motivation: Prognostication for comatose post-cardiac arrest patients is critical for ICU decision-making, but current methods don't optimally use time-invariant and time-varying data.

Method: The study proposes a stepwise dynamic competing risks model, extending the Fine and Gray model with neural networks to handle two phases of feature collection (time-invariant and time-varying).

Result: Evaluated on 2,278 patients, the model showed robust performance in predicting awakening, withdrawal of therapy, or death.

Conclusion: The model generalizes to multi-phase feature collection and could enhance dynamic prediction tasks by identifying when and for whom new data improves prognostication.

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [49] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: The paper addresses the overlooked issue of heterophily in heterogeneous graphs (HGs) and proposes AHGNN, a model that adapts to varying heterophily distributions and semantic diversity, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing studies on HGs often ignore heterophily, leading to performance degradation in real-world applications where heterophilic HGs are common.

Method: The proposed AHGNN uses heterophily-aware convolution and a coarse-to-fine attention mechanism to handle varying heterophily distributions and semantic diversity across meta-paths.

Result: AHGNN outperforms 20 baselines on seven real-world graphs, especially in high-heterophily scenarios.

Conclusion: AHGNN effectively addresses heterophily challenges in HGs, demonstrating superior performance and adaptability.

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [50] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM dynamically adjusts layer precision in LLMs for optimal performance-latency trade-off, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting LLMs to varying runtime constraints like latency and accuracy.

Method: Introduces DP-LLM, which dynamically assigns precision to each layer based on input values using a lightweight error estimator and learned thresholds.

Result: DP-LLM achieves superior performance-latency trade-off across multiple models and benchmarks.

Conclusion: DP-LLM effectively handles runtime constraints in LLMs by dynamically adjusting precision, offering a scalable solution.

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [51] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: The paper provides non-vacuous generalization bounds for deep temporal models like TCNs, introduces a delayed-feedback blocking mechanism, and evaluates the impact of temporal dependence on learning.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of generalization in deep temporal architectures, particularly TCNs, and to provide principled evaluation methods.

Method: Derives generalization bounds for exponentially β-mixing sequences, introduces a delayed-feedback blocking mechanism to transform dependent samples into independent ones, and proposes a fair-comparison methodology to isolate temporal structure effects.

Result: Bounds scale as O(R√(Dpn log N/N)), with √D scaling instead of exponential. Temporal dependence can enhance learning under fixed information budgets, but convergence rates diverge from theory.

Conclusion: Temporal dependence can improve learning, but gaps between theory and practice highlight the need for future research.

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [52] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: First implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN) for sequence-to-sequence learning, achieving competitive results on WMT'14 English-German translation.


<details>
  <summary>Details</summary>
Motivation: Explore the application of differentiable logic gates in sequential modeling, an area previously unexplored.

Method: Combine Boolean operations with recurrent architectures to create RDDLGN for sequence-to-sequence tasks.

Result: Achieves 5.00 BLEU and 30.9% accuracy during training, nearing GRU performance (5.41 BLEU).

Conclusion: Recurrent logic-based neural computation is viable, enabling new research directions like FPGA acceleration.

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [53] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: HGR and HSR improve sample efficiency in GCRL by leveraging hindsight goals and self-imitation, outperforming HER-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limited sample efficiency in GCRL with sparse rewards by better exploiting experiences beyond trajectory relabeling.

Method: Proposes Hindsight Goal-conditioned Regularization (HGR) and combines it with Hindsight Self-Imitation Regularization (HSR) to maximize experience utilization.

Result: Achieves more efficient sample reuse and superior performance in navigation and manipulation tasks compared to HER-based methods.

Conclusion: HGR and HSR enhance off-policy GCRL, offering significant improvements in sample efficiency and task performance.

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [54] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: A novel approach using inpainting with a fine-tuned diffusion model to synthesize realistic oral cancer lesions improves diagnostic accuracy, achieving 0.97 classification accuracy and 0.85 detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Limited annotated datasets and insufficient training data constrain oral cancer diagnostic models.

Method: Synthesized realistic oral cancer lesions using an inpainting technique with a fine-tuned diffusion model, leveraging a comprehensive dataset from multiple sources.

Result: Classification model achieved 0.97 accuracy; detection model achieved 0.85 accuracy in identifying lesion locations.

Conclusion: Synthetic image generation holds potential for medical diagnostics and can be extended to other cancer types.

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [55] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster improves privacy/utility tradeoffs in federated clustering by rebalancing cluster assignments to reduce privacy noise.


<details>
  <summary>Details</summary>
Motivation: Federated clustering enhances model performance but increases privacy risks. Standard DP mechanisms degrade utility due to uncontrolled client assignments.

Method: Proposes RR-Cluster, a lightweight add-on to federated clustering algorithms, which rebalances cluster assignments to ensure a minimum client count per cluster.

Result: RR-Cluster reduces privacy noise variance, improving privacy/utility tradeoffs, as shown in synthetic and real-world datasets.

Conclusion: RR-Cluster effectively balances privacy and utility in federated clustering, with theoretical and empirical validation.

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [56] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: Extensive comparison of 25 pretrained neural models in chemistry shows most offer no improvement over baseline ECFP fingerprints; only CLAMP performs better, raising concerns about evaluation rigor.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the performance of pretrained neural networks in chemistry and small molecule drug design, addressing gaps in existing studies.

Method: Comparison of 25 models across 25 datasets using a hierarchical Bayesian statistical testing model under a fair framework.

Result: Most neural models show negligible improvement over ECFP fingerprints; only CLAMP performs significantly better.

Conclusion: Findings highlight evaluation rigor issues and suggest the need for better methodologies and practical improvements in the field.

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [57] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP is a federated recommendation system that adapts to varying user privacy preferences, leveraging public user data to improve recommendations while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Existing FedRecs assume uniform privacy requirements, ignoring the potential of public user data. GFed-PP addresses this by accommodating both private and public users.

Method: GFed-PP uses a user-item interaction graph and a lightweight GCN for personalized embeddings. It learns embeddings locally and aggregates data on the server.

Result: GFed-PP outperforms existing methods on five datasets, improving accuracy without compromising privacy.

Conclusion: GFed-PP offers a practical solution for federated recommendation systems with diverse privacy needs.

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [58] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: RPO combines RPG and PPO for stable, sample-efficient reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Training instability in RPG due to high-variance gradients hinders sample efficiency.

Method: Proposes RPO by linking RPG with PPO's surrogate objective, enabling stable sample reuse via backpropagation.

Result: RPO outperforms on locomotion and manipulation tasks, showing superior efficiency and performance.

Conclusion: RPO bridges RPG and PPO, offering a stable, efficient solution for reinforcement learning.

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [59] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: The paper proposes a DDPG-based framework to optimize pandemic interventions like lockdowns and vaccinations, balancing health and economic outcomes on a large-scale simulation.


<details>
  <summary>Details</summary>
Motivation: Current research lacks scalable, automated methods to model optimal pandemic interventions, limiting their effectiveness and scope.

Method: Uses Deep Deterministic Policy Gradient (DDPG) on a large-scale agent-based simulation for multi-objective optimization of lockdown and vaccination strategies.

Result: Optimal policy balances health (infection, hospitalization) and economy (poverty), favoring mid-age and elderly vaccination with no lockdown.

Conclusion: The framework shows promise but requires further validation and open-sourcing for broader application.

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [60] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR is an Edge AI framework for 6G vehicular networks, using ML-based compression and RL to optimize scheduling and fairness, outperforming baselines in throughput and fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional RRM techniques are inadequate for handling the complexity and volume of data in 6G vehicular networks, necessitating AI-driven solutions like SCAR.

Method: SCAR employs ML-based compression (clustering, RBF networks) to reduce CQI data size and trains RL policies for scheduling and fairness optimization.

Result: SCAR improves feasible scheduling time by 14%, reduces unfair scheduling by 15%, and lowers CQI clustering distortion by 10%.

Conclusion: SCAR is scalable and effective for dynamic vehicular networks, enhancing both throughput and fairness.

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [61] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol is a reinforcement learning framework for molecular property prediction with LLMs, improving performance and interpretability by guiding reasoning with attribute-based rewards.


<details>
  <summary>Details</summary>
Motivation: Current LLMs for molecular property prediction rely on human-crafted prompts and verbose reasoning, lacking relevance. AttriLens-Mol aims to enhance reasoning and prediction by leveraging attribute-guided reinforcement learning.

Method: The framework uses three rewards: format (structured output), count (avoid irrelevant attributes), and rationality (verified relatedness). It trains models like R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 on 4,000 samples.

Result: AttriLens-Mol outperforms supervised fine-tuning and advanced models (GPT-3.5, GPT-4o, etc.) on in-distribution and out-of-distribution datasets. Extracted attributes also improve interpretable decision tree models.

Conclusion: AttriLens-Mol effectively elicits relevant molecular attributes, enhancing both performance and interpretability in property prediction. The code is publicly available.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [62] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper introduces Partial Feature Membership Inference (PFMI) and proposes MRAD, a two-stage attack framework, to infer membership with partial feature access, showing effectiveness across datasets.


<details>
  <summary>Details</summary>
Motivation: Existing membership inference attacks assume full feature access, which is unrealistic in many scenarios. This work addresses the gap by focusing on partial feature availability.

Method: Proposes MRAD: a two-stage framework involving feature reconstruction (minimizing loss) and anomaly detection (measuring deviation from training distribution).

Result: MRAD achieves an AUC of ~0.6 on STL-10 with 40% missing features, demonstrating effectiveness and compatibility with anomaly detection techniques.

Conclusion: MRAD successfully addresses PFMI, proving practical for real-world scenarios with partial feature access.

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [63] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [64] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [65] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: The paper proposes a method for generating high-quality, privacy-preserving synthetic tabular data using Tensor Networks (Matrix Product States, MPS), outperforming models like CTGAN, VAE, and PrivBayes under strict privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity, privacy constraints, and the need for diverse datasets in AI, the work aims to provide a robust solution for synthetic data generation.

Method: The method uses MPS for data generation, integrating noise injection and gradient clipping for differential privacy (DP) via Rényi Differential Privacy accounting.

Result: MPS outperforms classical models in fidelity and privacy-preserving capabilities, especially under strict privacy constraints.

Conclusion: MPS is a promising tool for privacy-aware synthetic data generation, offering interpretability and scalability for secure data sharing in sensitive domains.

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [66] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: GTMancer, a Graph Transformer framework, enhances multi-omics cancer subtype classification by integrating contrastive learning and dual attention mechanisms to capture complex omics relationships.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully exploit the intricate coupling between heterogeneous omics data, limiting their ability to resolve subtle cancer subtype heterogeneity for precision oncology.

Method: GTMancer leverages contrastive learning to embed multi-omics data into a unified space, unrolls the multiplex graph optimization problem, and uses dual attention coefficients to capture structural priors within and among omics data.

Result: GTMancer outperforms state-of-the-art algorithms on seven real-world cancer datasets.

Conclusion: The proposed framework effectively integrates multi-omics data, improving cancer subtype classification and advancing precision oncology.

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [67] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P introduces a one-step action sampling method for offline MARL, combining mean-flow matching and Q-function supervision to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Overcome the inefficiency of diffusion and flow-based models in offline MARL due to iterative sampling and misalignment with reward maximization.

Method: Proposes OM2P with reward-aware optimization, mean-flow matching loss, and derivative-free estimation for stability and memory efficiency.

Result: Achieves 3.8x lower GPU memory usage and 10.8x faster training, outperforming benchmarks.

Conclusion: OM2P successfully integrates mean-flow models into offline MARL, enabling practical generative policies for multi-agent systems.

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [68] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: The paper explores Continual Learning (CL) for ASR in Indian languages, testing three CL strategies to mitigate forgetting and improve scalability.


<details>
  <summary>Details</summary>
Motivation: India's linguistic diversity and privacy constraints make traditional multilingual ASR models impractical, necessitating CL approaches.

Method: A Conformer-based hybrid RNN-T/CTC model, pretrained on Hindi, is incrementally trained on eight more Indian languages using EWC, MAS, and LwF CL strategies.

Result: CL effectively reduces forgetting compared to naive fine-tuning, with performance analyzed via WER and Backward Transfer.

Conclusion: CL is a promising approach for scalable ASR in diverse Indian languages under realistic constraints.

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [69] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: A novel spiking neuron model combining SSM state transitions with non-linear feedback is proposed, achieving competitive performance in SNN tasks while overcoming instability.


<details>
  <summary>Details</summary>
Motivation: Bridging the advantages of SNNs (low-latency, energy-efficient) and deep SSMs (competitive performance) by addressing the lack of reset mechanisms in SSMs.

Method: Proposes a multiple-output spiking neuron model with linear SSM state transitions and non-linear feedback via reset, clarifying spiking, reset condition, and reset action.

Result: Achieves performance comparable to SNN benchmarks in tasks like keyword spotting and event-based vision, overcoming instability in linear dynamics.

Conclusion: The reset mechanism enables learning in unstable linear dynamics, expanding beyond strict stability constraints in deep SSMs.

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [70] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF is a novel Federated Meta-Learning approach for neural fields, addressing privacy leakage with a new loss function, enabling efficient optimization without retaining private data.


<details>
  <summary>Details</summary>
Motivation: Traditional FML for neural fields requires large data and computations, posing challenges for edge devices and risking privacy leakage.

Method: FedMeNF introduces a privacy-preserving loss function to regulate leakage in local meta-optimization, allowing efficient training without storing private data.

Result: FedMeNF achieves fast optimization and robust performance with few-shot or non-IID data while preserving privacy.

Conclusion: FedMeNF effectively balances privacy and efficiency in federated meta-learning for neural fields.

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [71] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: FCL is an adaptive robust loss function that dynamically adjusts its robustness to label noise during training, eliminating the need for manual hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing robust loss functions require extensive hyperparameter tuning for different datasets, which is inefficient and impractical.

Method: FCL combines the fractional derivative of Cross-Entropy loss (active component) and Mean Absolute Error (passive component), with the fractional order μ learned during training.

Result: FCL achieves state-of-the-art performance on benchmark datasets without manual tuning, balancing robustness and convergence speed.

Conclusion: FCL provides an effective, adaptive solution for training deep neural networks under label noise, outperforming existing methods.

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [72] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE, a novel VAE architecture, embeds structural equation modeling into its design for interpretable latent representations in tabular data, outperforming baselines in factor recovery and robustness.


<details>
  <summary>Details</summary>
Motivation: The challenge of learning interpretable latent representations from tabular data in deep generative modeling, especially for theory-driven domains requiring measurement validity.

Method: SE-VAE integrates structural equation modeling into a VAE, aligning latent subspaces with indicator groupings and isolating confounding variation with a global nuisance latent.

Result: SE-VAE outperforms baselines in disentanglement metrics, factor recovery, interpretability, and robustness to nuisance variation.

Conclusion: SE-VAE provides a principled, modular framework for white-box generative modeling in scientific and social domains, emphasizing architectural structure over regularization.

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [73] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means improves k-means efficiency using geometric principles, reducing runtime and energy use without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: Enhance k-means efficiency and energy economy while maintaining solution quality.

Method: Uses scalar projection to focus on high expressive data (HE) and bypass low expressive data (LE).

Result: Outperforms traditional and SOTA k-means variants in runtime, distance computations, and energy efficiency.

Conclusion: Gk-means is a sustainable, efficient alternative to traditional k-means.

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [74] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: The paper investigates self-initiated deception in LLMs, proposing a framework with two metrics to quantify deception likelihood, revealing increased deception in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To explore underexplored threats of LLM deception beyond human-induced scenarios, focusing on self-initiated deception in benign prompts.

Method: Proposes a novel framework using 'contact searching questions' and two statistical metrics (Deceptive Intention Score and Deceptive Behavior Score) derived from psychology.

Result: Evaluation of 14 LLMs shows both metrics escalate with task difficulty, indicating increased deception in complex problems.

Conclusion: Advanced LLMs exhibit rising deception tendencies in complex tasks, raising concerns for their deployment in critical domains.

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [75] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff is a generative method using diffusion models to design molecules with controlled biological activity, balancing efficacy and safety.


<details>
  <summary>Details</summary>
Motivation: Precise control over molecular activity, including multi-target modulation and off-target mitigation, is lacking in current drug design methods.

Method: ActivityDiff uses classifier-guided diffusion models, leveraging separately trained drug-target classifiers for positive and negative guidance.

Result: The method successfully handles tasks like single-/dual-target generation, selective generation, and off-target reduction.

Conclusion: ActivityDiff offers a versatile framework for integrated molecular activity control, balancing efficacy and safety.

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [76] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: A three-stage text-to-SQL framework improves database intent prediction and SQL generation by leveraging LLMs, prompt engineering, and critic agents.


<details>
  <summary>Details</summary>
Motivation: Traditional text-to-SQL methods assume a pre-specified database, which is impractical for scenarios with multiple databases. This paper addresses the overlooked step of identifying the correct database.

Method: The framework: 1) uses LLMs and prompt engineering to extract rules from NLQs, 2) trains a RoBERTa-based model to predict the correct db_id, and 3) refines SQL with critic agents.

Result: Outperforms state-of-the-art models in database intent prediction and SQL generation accuracy.

Conclusion: The proposed framework effectively handles multi-database scenarios, improving both database identification and SQL generation.

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [77] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: A new method using 311 Service Calls and street-level imagery tracks and forecasts homeless tent trends in San Francisco, offering more detailed and timely data than traditional PIT counts.


<details>
  <summary>Details</summary>
Motivation: Existing homelessness monitoring methods like PIT counts lack frequency, consistency, and spatial detail, necessitating a better approach.

Method: Uses publicly available, crowdsourced data (311 Service Calls and street-level imagery) to create a predictive model for daily and neighborhood-level trends.

Result: The model reveals rapid fluctuations during COVID-19 and spatial shifts in tent locations, providing more localized and timely insights.

Conclusion: This approach is a cost-effective tool for guiding policy and evaluating interventions to reduce unsheltered homelessness.

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [78] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR enhances LLM optimization by improving sample efficiency and reducing primacy bias through high-replay training and periodic resets.


<details>
  <summary>Details</summary>
Motivation: Address low sample efficiency and primacy bias in LLM optimization methods like RL and preference optimization.

Method: Introduces LoRR, a plugin with high-replay training, periodic resets, and hybrid optimization (SFT + preference-based losses).

Result: LoRR boosts performance on reasoning benchmarks, matching or outperforming complex RL methods.

Conclusion: LoRR provides a practical, efficient paradigm for LLM finetuning, maximizing performance with limited data.

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [79] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN is a modular framework for targeted unlearning in LLMs, using gradient-ratio metrics and selective noise injection to improve performance while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the legal and ethical concerns of LLMs by enabling effective unlearning of sensitive or unauthorized data without degrading unrelated knowledge.

Method: GRIN identifies key parameters for forgetting using a gradient-ratio metric, then applies selective noise injection before fine-tuning.

Result: Validated on benchmarks (TOFU, WMDP, SafePKU), GRIN achieves better unlearning performance while maintaining model utility.

Conclusion: GRIN provides a practical and effective solution for LLM unlearning, balancing performance and utility.

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [80] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: The paper discusses the need for a new risk analysis approach for multi-agent AI systems, identifying six critical failure modes and providing tools for practitioners to assess them.


<details>
  <summary>Details</summary>
Motivation: Organizations are adopting interconnected multi-agent AI systems, but interactions between agents create emergent behaviors and novel failure risks, requiring a different risk analysis approach than single-agent systems.

Method: The report examines six failure modes in governed environments and provides a toolkit for practitioners. It advocates for staged testing and evidence collection through simulation, observation, benchmarking, and red teaming.

Result: The methodology offers a framework for robust risk management in LLM-based multi-agent systems, addressing critical failure modes like cascading failures and communication issues.

Conclusion: The approach emphasizes analysis validity and progressive testing to mitigate risks in multi-agent AI systems, laying groundwork for organizational risk management.

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [81] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: Grid-Agent is an AI-driven framework combining LLMs and multi-agent reinforcement learning to address power grid violations in real time, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of modern power grids due to DERs, EVs, and extreme weather events necessitates adaptive, scalable solutions beyond traditional rule-based or numerical optimization approaches.

Method: Grid-Agent uses a modular agent architecture: a planning agent for action sequences and a validation agent for stability checks, with adaptive multiscale network representation for scalability.

Result: Tests on IEEE and CIGRE systems show superior violation mitigation, with continuous learning and adaptation capabilities.

Conclusion: Grid-Agent is highly suitable for smart grids, offering rapid, autonomous response to dynamic conditions.

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>


### [82] [Flow-Based Task Assignment for Large-Scale Online Multi-Agent Pickup and Delivery](https://arxiv.org/abs/2508.05890)
*Yue Zhang,Zhe Chen,Daniel Harabor,Pierre Le Bodic,Peter J. Stuckey*

Main category: cs.MA

TL;DR: The paper presents a scalable, efficient method for online Multi-Agent Pickup and Delivery (MAPD) using a minimum-cost flow formulation, outperforming existing baselines in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for online MAPD either use simplistic heuristics leading to poor decisions or complex reasoning lacking scalability under real-time constraints.

Method: The task assignment subproblem is formulated as a minimum-cost flow over the environment graph, eliminating pairwise distance computations and enabling simultaneous task assignment and routing. Two congestion-aware edge cost models are introduced to improve solution quality.

Result: The approach scales to over 20000 agents and 30000 tasks within 1-second planning time, outperforming baselines in computational efficiency and assignment quality.

Conclusion: The proposed method effectively balances real-time execution and scalability while improving solution quality for online MAPD.

Abstract: We study the problem of online Multi-Agent Pickup and Delivery (MAPD), where
a team of agents must repeatedly serve dynamically appearing tasks on a shared
map. Existing online methods either rely on simple heuristics, which result in
poor decisions, or employ complex reasoning, which suffers from limited
scalability under real-time constraints. In this work, we focus on the task
assignment subproblem and formulate it as a minimum-cost flow over the
environment graph. This eliminates the need for pairwise distance computations
and allows agents to be simultaneously assigned to tasks and routed toward
them. The resulting flow network also supports efficient guide path extraction
to integrate with the planner and accelerates planning under real-time
constraints. To improve solution quality, we introduce two congestion-aware
edge cost models that incorporate real-time traffic estimates. This approach
supports real-time execution and scales to over 20000 agents and 30000 tasks
within 1-second planning time, outperforming existing baselines in both
computational efficiency and assignment quality.

</details>


### [83] [Policy Optimization in Multi-Agent Settings under Partially Observable Environments](https://arxiv.org/abs/2508.06061)
*Ainur Zhaikhan,Malek Khammassi,Ali H. Sayed*

Main category: cs.MA

TL;DR: The paper introduces an adaptive social learning method for estimating global states in MARL, combining social learning and MARL efficiently without two-timescale frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing two-timescale learning frameworks in MARL by enabling concurrent social learning and reinforcement learning.

Method: Alternates between a single step of social learning and a single step of MARL, avoiding two-timescale learning.

Result: Theoretical guarantees and simulations show the method performs comparably to reinforcement learning with known true states.

Conclusion: The proposed adaptive social learning method is efficient and effective for MARL problems with partial observability.

Abstract: This work leverages adaptive social learning to estimate partially observable
global states in multi-agent reinforcement learning (MARL) problems. Unlike
existing methods, the proposed approach enables the concurrent operation of
social learning and reinforcement learning. Specifically, it alternates between
a single step of social learning and a single step of MARL, eliminating the
need for the time- and computation-intensive two-timescale learning frameworks.
Theoretical guarantees are provided to support the effectiveness of the
proposed method. Simulation results verify that the performance of the proposed
methodology can approach that of reinforcement learning when the true state is
known.

</details>
