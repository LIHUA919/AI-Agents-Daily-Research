{"id": "2602.11740", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11740", "abs": "https://arxiv.org/abs/2602.11740", "authors": ["Ayhan Alp Aydeniz", "Robert Loftin", "Kagan Tumer"], "title": "Counterfactual Conditional Likelihood Rewards for Multiagent Exploration", "comment": "9 pages, 5 figures", "summary": "Efficient exploration is critical for multiagent systems to discover coordinated strategies, particularly in open-ended domains such as search and rescue or planetary surveying. However, when exploration is encouraged only at the individual agent level, it often leads to redundancy, as agents act without awareness of how their teammates are exploring. In this work, we introduce Counterfactual Conditional Likelihood (CCL) rewards, which score each agent's exploration by isolating its unique contribution to team exploration. Unlike prior methods that reward agents solely for the novelty of their individual observations, CCL emphasizes observations that are informative with respect to the joint exploration of the team. Experiments in continuous multiagent domains show that CCL rewards accelerate learning for domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks that require tight coordination among agents.", "AI": {"tldr": "The paper introduces Counterfactual Conditional Likelihood (CCL) rewards to improve multiagent exploration by focusing on an agent's unique contribution to team exploration, enhancing coordination and learning efficiency.", "motivation": "Efficient exploration is crucial in multiagent systems, especially in open-ended domains like search and rescue. Current methods that encourage exploration at the individual level often lead to redundancy, as agents act without awareness of teammates' exploration.", "method": "The paper proposes Counterfactual Conditional Likelihood (CCL) rewards, which isolate each agent's unique contribution to team exploration, emphasizing observations that are informative with respect to joint exploration rather than just individual novelty.", "result": "Experiments in continuous multiagent domains show that CCL rewards accelerate learning in domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks requiring tight coordination.", "conclusion": "CCL rewards offer a more efficient approach to multiagent exploration by reducing redundancy and promoting coordination, potentially benefiting applications like search and rescue."}}
{"id": "2602.11754", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.11754", "abs": "https://arxiv.org/abs/2602.11754", "authors": ["Keita Nishimoto", "Kimitaka Asatani", "Ichiro Sakata"], "title": "Cooperation Breakdown in LLM Agents Under Communication Delays", "comment": null, "summary": "LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. We propose the FLCOA framework (Five Layers for Cooperation/Coordination among Autonomous Agents) to conceptualize how cooperation and coordination emerge in groups of autonomous agents, and highlight that the influence of lower-layer factors - especially computational and communication resources - has been largely overlooked. To examine the effect of communication delay, we introduce a Continuous Prisoner's Dilemma with Communication Delay and conduct simulations with LLM-based agents. As delay increases, agents begin to exploit slower responses even without explicit instructions. Interestingly, excessive delay reduces cycles of exploitation, yielding a U-shaped relationship between delay magnitude and mutual cooperation. These results suggest that fostering cooperation requires attention not only to high-level institutional design but also to lower-layer factors such as communication delay and resource allocation, pointing to new directions for MAS research.", "AI": {"tldr": "The FLCOA framework conceptualizes cooperation in LLM-MAS, revealing that communication delay in a simulated Prisoner's Dilemma leads to exploitation initially but reduces at high delays, forming a U-shaped relationship with cooperation.", "motivation": "LLM-based multi-agent systems need to operate under real-world computational and communication constraints for societal deployment, but lower-layer factors like resources are often overlooked in cooperation studies.", "method": "Introduce the FLCOA framework to analyze cooperation layers and conduct simulations using a Continuous Prisoner's Dilemma with Communication Delay with LLM-based agents.", "result": "As communication delay increases, agents exploit slower responses without explicit instructions; excessive delay reduces exploitation cycles, creating a U-shaped link between delay and mutual cooperation.", "conclusion": "Fostering cooperation in MAS requires attention to both high-level design and lower-layer factors like communication delay, offering new research directions."}}
{"id": "2602.11977", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.11977", "abs": "https://arxiv.org/abs/2602.11977", "authors": ["Michael Otte", "Roderich Gro\u00df"], "title": "Multi-Defender Single-Attacker Perimeter Defense Game on a Cylinder: Special Case in which the Attacker Starts at the Boundary", "comment": "4 pages, 3 figures", "summary": "We describe a multi-agent perimeter defense game played on a cylinder. A team of n slow-moving defenders must prevent a single fast-moving attacker from crossing the boundary of a defensive perimeter. We describe the conditions necessary for the attacker to win in the special case that the intruder starts close to the boundary and in a region that is currently defended.", "AI": {"tldr": "Analysis of a multi-agent perimeter defense game on a cylinder where slow defenders try to stop a fast attacker from crossing the boundary.", "motivation": "The motivation is to model and analyze defense strategies in scenarios where defenders are slower but numerous, focusing on conditions for attacker success.", "method": "The method involves describing a game setup on a cylindrical geometry, specifying defender and attacker parameters (n slow defenders, one fast attacker), and analyzing win conditions for the attacker.", "result": "The result identifies conditions under which the attacker can win, specifically when the attacker starts near the boundary in a defended region.", "conclusion": "The conclusion suggests that attacker success depends on initial proximity to the boundary and defender positions, with implications for defensive strategy design."}}
{"id": "2602.12102", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12102", "abs": "https://arxiv.org/abs/2602.12102", "authors": ["Zhijian Gao", "Shuxin Li", "Bo An"], "title": "DEpiABS: Differentiable Epidemic Agent-Based Simulator", "comment": "17 pages, 9 figures, to be published in AAMAS 2026", "summary": "The COVID-19 pandemic highlighted the limitations of existing epidemic simulation tools. These tools provide information that guides non-pharmaceutical interventions (NPIs), yet many struggle to capture complex dynamics while remaining computationally practical and interpretable. We introduce DEpiABS, a scalable, differentiable agent-based model (DABM) that balances mechanistic detail, computational efficiency and interpretability. DEpiABS captures individual-level heterogeneity in health status, behaviour, and resource constraints, while also modelling epidemic processes like viral mutation and reinfection dynamics. The model is fully differentiable, enabling fast simulation and gradient-based parameter calibration. Building on this foundation, we introduce a z-score-based scaling method that maps small-scale simulations to any real-world population sizes with negligible loss in output granularity, reducing the computational burden when modelling large populations. We validate DEpiABS through sensitivity analysis and calibration to COVID-19 and flu data from ten regions of varying scales. Compared to the baseline, DEpiABS is more detailed, fully interpretable, and has reduced the average normal deviation in forecasting from 0.97 to 0.92 on COVID-19 mortality data and from 0.41 to 0.32 on influenza-like-illness data. Critically, these improvements are achieved without relying on auxiliary data, making DEpiABS a reliable, generalisable, and data-efficient framework for future epidemic response modelling.", "AI": {"tldr": "DEpiABS is a scalable, differentiable agent-based model for epidemic simulation, balancing detail, efficiency, and interpretability, with improved accuracy in COVID-19 and flu forecasts.", "motivation": "Existing epidemic simulation tools often fail to capture complex dynamics while maintaining computational practicality and interpretability, as highlighted by the COVID-19 pandemic.", "method": "Introduces a scalable, differentiable agent-based model (DABM) that captures individual-level heterogeneity and epidemic processes, along with a z-score-based scaling method to map simulations to large populations with minimal computational loss.", "result": "DEpiABS reduces average normal deviation in forecasting from 0.97 to 0.92 on COVID-19 mortality data and from 0.41 to 0.32 on influenza-like-illness data across ten regions.", "conclusion": "DEpiABS provides a reliable, generalisable, and data-efficient framework for epidemic response modelling, achieving improvements without relying on auxiliary data."}}
{"id": "2602.11159", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11159", "abs": "https://arxiv.org/abs/2602.11159", "authors": ["Natalia Abarca", "Andr\u00e9s Carvallo", "Claudia L\u00f3pez Moncada", "Felipe Bravo-Marquez"], "title": "Explaining AI Without Code: A User Study on Explainable AI", "comment": "LatinX in AI Workshop @ NeurIPS-25", "summary": "The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\u03b1$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\u03b1$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.", "AI": {"tldr": "This paper presents a human-centered XAI module integrated into the no-code ML platform DashAI, showing that explanations improve usability and trust, especially for novices.", "motivation": "Machine Learning is increasingly used in sensitive domains, raising transparency concerns. While XAI addresses this, most methods are technically demanding, limiting accessibility, particularly in no-code ML platforms aimed at democratizing AI.", "method": "Developed and integrated an XAI module in DashAI with three techniques: Partial Dependence Plots, Permutation Feature Importance, and KernelSHAP, and conducted a user study with 20 participants (ML novices and experts) to evaluate usability and impact.", "result": "High task success (\u226580%) across explainability tasks, novices rated explanations as useful and trustworthy, while experts were critical of sufficiency. Explanations improved perceived predictability and confidence, with novices showing higher trust than experts.", "conclusion": "The findings highlight a key challenge for XAI in no-code ML: balancing accessibility for novices with sufficient detail for experts. This approach can help make AI more transparent and trustworthy in democratized environments."}}
{"id": "2602.11164", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11164", "abs": "https://arxiv.org/abs/2602.11164", "authors": ["Weiting Liu", "Han Wu", "Yufei Kuang", "Xiongwei Han", "Tao Zhong", "Jianfeng Feng", "Wenlian Lu"], "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "comment": null, "summary": "Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\\textbf{m}ated opt\\textbf{i}mization modeli\\textbf{n}g via a localizable error-\\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \\textbf{D}ynamic Supervised \\textbf{F}ine-Tuning \\textbf{P}olicy \\textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.", "AI": {"tldr": "MIND is a novel error-driven learning framework for automated optimization modeling via LLMs, addressing data sparsity and reward issues through localized error patterns and DFPO, outperforming state-of-the-art methods.", "motivation": "Existing automated optimization modeling approaches suffer from limitations: (L1) sparsity of error-specific problems and (L2) sparse rewards for difficult problems, hindering effective post-training of LLMs due to scarce and underutilized high-quality data.", "method": "Proposes MIND, leveraging localizable error propagation patterns in optimization modeling. It constructs a focused, high-density training corpus and introduces DFPO (Dynamic Supervised Fine-Tuning Policy Optimization) for localized refinement on difficult problems.", "result": "MIND consistently outperforms all state-of-the-art automated optimization modeling approaches in experiments on six benchmarks.", "conclusion": "MIND effectively overcomes limitations in existing approaches by customizing the training framework from data synthesis to post-training, enhancing LLM capabilities in complex decision-making tasks through error-driven learning."}}
{"id": "2602.12243", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12243", "abs": "https://arxiv.org/abs/2602.12243", "authors": ["Sanket A. Salunkhe", "George P. Kontoudis"], "title": "Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems", "comment": "Accepted at 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.", "AI": {"tldr": "A new distributed Gaussian Process framework called pxpGP for large-scale multi-robot systems improves scalability and accuracy in probabilistic modeling.", "motivation": "Multi-robot systems need scalable federated modeling methods under computational/communication constraints; GPs have cubic complexity limits in large-scale deployments.", "method": "Uses sparse variational inference for local pseudo-representation, sparse variational optimization, and global scaled proximal-inexact consensus ADMM with adaptive updates and warm-start.", "result": "pxpGP and dec-pxpGP outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, especially in large-scale networks.", "conclusion": "pxpGP is effective for centralized and decentralized multi-robot networks, addressing scalability challenges in GP modeling."}}
{"id": "2602.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11229", "abs": "https://arxiv.org/abs/2602.11229", "authors": ["Zituo Chen", "Haixu Wu", "Sili Deng"], "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation", "comment": null, "summary": "We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.", "AI": {"tldr": "Latent Generative Solvers (LGS) is a two-stage framework for simulating PDEs using a shared latent space and probabilistic dynamics, improving long-horizon stability and efficiency.", "motivation": "To address the challenge of long-horizon surrogate simulation across heterogeneous PDE systems, aiming to reduce rollout drift and improve reliability for forecasting and scientific workflows.", "method": "Uses a pretrained VAE to map PDE states into a shared latent space, learns probabilistic dynamics with a Transformer via flow matching, incorporates an uncertainty knob for perturbation, and employs flow forcing for system descriptor updates.", "result": "LGS matches deterministic baselines on short horizons while reducing rollout drift on long horizons, achieves up to 70x lower FLOPs, and adapts efficiently to out-of-distribution data under limited finetuning.", "conclusion": "LGS provides a practical, scalable, and uncertainty-aware approach for generalizable neural PDE solvers, enhancing long-term reliability and efficiency in scientific applications."}}
{"id": "2602.11184", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11184", "abs": "https://arxiv.org/abs/2602.11184", "authors": ["Zukang Xu", "Zhixiong Zhao", "Xing Hu", "Zhixuan Chen", "Dawei Yang"], "title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models", "comment": "Accepted by ICLR 2026", "summary": "Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.", "AI": {"tldr": "KBVQ-MoE is a novel vector quantization framework that overcomes redundancy and bias issues in Mixture of Experts models, enabling efficient 3-bit quantization with minimal accuracy loss.", "motivation": "MoE models are hard to deploy in resource-constrained environments due to large parameter sizes, and direct VQ application causes performance degradation from redundant representations and cumulative output bias.", "method": "KBVQ-MoE integrates input-driven redundancy elimination using KLT-guided SVD to share dominant weight components across experts, and bias-corrected output stabilization with channel-wise affine compensation for quantized outputs.", "result": "KBVQ-MoE preserves accuracy substantially better than existing methods, e.g., achieving 67.99 average accuracy for 3-bit Qwen1.5-MoE-A2.7B vs. 68.07 FP16 baseline.", "conclusion": "KBVQ-MoE demonstrates strong potential for efficient deployment of MoE-based LLMs on edge devices and other resource-constrained platforms."}}
{"id": "2602.11437", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.11437", "abs": "https://arxiv.org/abs/2602.11437", "authors": ["Chengrui Qu", "Christopher Yeh", "Kishan Panaganti", "Eric Mazumdar", "Adam Wierman"], "title": "Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization", "comment": "ICLR 2026", "summary": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.", "AI": {"tldr": "The paper introduces DrIGM, a distributionally robust IGM principle for cooperative multi-agent reinforcement learning to address uncertainties in real-world settings, and develops robust variants of existing value-factorization methods that enhance out-of-distribution performance.", "motivation": "In cooperative MARL, centralized training with decentralized execution relies on the IGM principle, but this can be unreliable due to environmental uncertainties such as sim-to-real gaps and system noise. There is a gap in ensuring robust team-optimal actions under such conditions.", "method": "The authors propose Distributionally robust IGM (DrIGM), a principle that aligns each agent's robust greedy action with the robust team-optimal joint action. They define robust individual action values compatible with decentralized execution and develop DrIGM-compliant variants of architectures like VDN, QMIX, and QTRAN, which train on robust Q-targets while maintaining scalability and integration ease.", "result": "The methods, tested on SustainGym simulators and a StarCraft environment, consistently improve out-of-distribution performance, demonstrating enhanced robustness to uncertainties without requiring per-agent reward shaping.", "conclusion": "DrIGM provides a reliable framework for robust multi-agent reinforcement learning under real-world uncertainties, with practical implementations that improve performance in out-of-distribution scenarios while maintaining scalability and compatibility with existing systems."}}
{"id": "2602.11295", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11295", "abs": "https://arxiv.org/abs/2602.11295", "authors": ["Gil Raitses"], "title": "On Decision-Valued Maps and Representational Dependence", "comment": "10 pages, 3 figures, 5 tables", "summary": "A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.", "AI": {"tldr": "DecisionDB is an infrastructure that logs, replays, and audits decision-valued maps, which track how different data representations affect computational outcomes, ensuring deterministic replay and enabling checkable decision reuse.", "motivation": "Computational engines can yield different outcomes based on data representations, leading to potential inconsistencies and audit challenges, prompting a need for formalization and infrastructure to manage these relationships reliably.", "method": "The paper formalizes decision-valued maps and develops DecisionDB, which logs decisions using identifiers derived from content and artifacts stored in write-once form, enabling deterministic replay to recover exact decision identifiers.", "result": "The infrastructure successfully partitions representation space into persistence regions and boundaries, with deterministic replay ensuring all identifying fields match stored values, facilitating mechanical verification of decision reuse.", "conclusion": "DecisionDB provides a robust framework for auditing and reproducing computational outcomes across varying representations, enhancing reliability and verifiability in data processing systems."}}
{"id": "2602.11185", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11185", "abs": "https://arxiv.org/abs/2602.11185", "authors": ["Zhendong Huang", "Hengjie Cao", "Fang Dong", "Ruijun Huang", "Mengyi Chen", "Yifeng Yang", "Xin Zhang", "Anrui Chen", "Mingzhi Dong", "Yujiang Wang", "Jinlong Hou", "Qin Lv", "Robert P. Dick", "Yuan Cheng", "Fan Yang", "Tun Lu", "Li Shang"], "title": "Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy", "comment": null, "summary": "Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.", "AI": {"tldr": "Spectra optimizer improves LLM training by addressing gradient anisotropy, speeding up convergence, reducing memory, and boosting accuracy.", "motivation": "Gradients in LLM training are anisotropic, with a dominant spike subspace suppressing learning in the tail, which optimizer statistics like AdamW don't effectively handle, leading to inefficiencies.", "method": "Develop Spectra, a spike-aware optimizer that tracks the dominant low-rank spike subspace via cached, warm-started power iteration and applies low-rank spectral shaping without amplifying the noise-sensitive tail.", "result": "Spectra trains LLaMA3 8B 30% faster to the same loss as AdamW, reduces per-step overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves downstream accuracy by 1.62%. Compared to Muon, it's 5.1x faster in processing, achieves lower final loss, and improves accuracy by 0.66%.", "conclusion": "Spectra effectively mitigates gradient anisotropy issues in LLM training, offering faster convergence, reduced resource usage, and better model performance, demonstrating its superiority over existing optimizers."}}
{"id": "2602.12055", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12055", "abs": "https://arxiv.org/abs/2602.12055", "authors": ["Amath Sow", "Mauricio Rodriguez Cesen", "Fabiola Martins Campos de Oliveira", "Mariusz Wzorek", "Daniel de Leng", "Mattias Tiger", "Fredrik Heintz", "Christian Esteve Rothenberg"], "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace", "comment": "AAMAS 2026 accepted paper", "summary": "Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.", "AI": {"tldr": "DTAPP-IICR is a scalable preflight planning method for large UAV fleets in dynamic airspace, combining prioritized planning with iterative conflict resolution to handle up to 1,000 UAVs efficiently.", "motivation": "Preflight planning for large UAV fleets faces challenges like temporal NFZs, heterogeneous vehicles, and strict deadlines, requiring scalable and flexible solutions beyond existing MAPF methods.", "method": "The method uses prioritized planning based on urgency, computes trajectories with SFIPP-ST (a 4D single-agent planner), and applies iterative Large Neighborhood Search with geometric conflict graphs and directional pruning for conflict resolution.", "result": "DTAPP-IICR achieves near-100% success with up to 1,000 UAVs, reduces runtime by up to 50% with pruning, and outperforms batch Enhanced Conflict-Based Search, scaling effectively in city-scale operations.", "conclusion": "DTAPP-IICR is a practical and scalable solution for preflight planning in dense, dynamic urban airspace, addressing real-world UTM needs where other priority-based methods fail."}}
{"id": "2602.11298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11298", "abs": "https://arxiv.org/abs/2602.11298", "authors": ["Alexander H. Liu", "Andy Ehrenberg", "Andy Lo", "Chen-Yo Sun", "Guillaume Lample", "Jean-Malo Delignon", "Khyathi Raghavi Chandu", "Patrick von Platen", "Pavankumar Reddy Muddireddy", "Rohin Arora", "Sanchit Gandhi", "Sandeep Subramanian", "Soham Ghosh", "Srijan Mishra", "Abhinav Rastogi", "Alan Jeffares", "Albert Jiang", "Alexandre Sablayrolles", "Am\u00e9lie H\u00e9liou", "Andrew Bai", "Angele Lenglemetz", "Anmol Agarwal", "Anton Eliseev", "Antonia Calvi", "Arjun Majumdar", "Baptiste Bout", "Baptiste Rozi\u00e8re", "Baudouin De Monicault", "Benjamin Tibi", "Cl\u00e9mence Lanfranchi", "Connor Chen", "Corentin Barreau", "Corentin Sautier", "Cyprien Courtot", "Darius Dabert", "Diego de las Casas", "Elliot Chane-Sane", "Enguerrand Paquin", "Faruk Ahmed", "Federico Baldassarre", "Gabrielle Berrada", "Ga\u00ebtan Ecrepont", "Gauthier Guinet", "Genevieve Hayes", "Georgii Novikov", "Giada Pistilli", "Guillaume Martin", "Gunjan Dhanuka", "Gunshi Gupta", "Han Zhou", "Indraneel Mukherjee", "Irene Zhang", "Jaeyoung Kim", "Jan Ludziejewski", "Jason Rute", "Joachim Studnia", "John Harvill", "Jonas Amar", "Josselin Somerville Roberts", "Julien Tauran", "Karmesh Yadav", "Kartik Khandelwal", "Kush Jain", "Laurence Aitchison", "L\u00e9onard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Maarten Buyl", "Manan Sharma", "Margaret Jennings", "Marie Pellat", "Mark Prins", "Mathieu Poir\u00e9e", "Mathilde Guillaumin", "Matthieu Dinot", "Matthieu Futeral", "Maxime Darrin", "Maximilian Augustin", "Mert Unsal", "Mia Chiquier", "Nathan Grinsztajn", "Neha Gupta", "Olivier Bousquet", "Olivier Duchenne", "Patricia Wang", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philom\u00e8ne Chagniot", "Pierre Stock", "Piotr Mi\u0142o\u015b", "Prateek Gupta", "Pravesh Agrawal", "Quentin Torroba", "Ram Ramrakhya", "Rishi Shah", "Romain Sauvestre", "Roman Soletskyi", "Rosalie Millner", "Sagar Vaze", "Samuel Humeau", "Siddharth Gandhi", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Th\u00e9o Cachet", "Theo Simon Sorg", "Thibaut Lavril", "Thomas Chabal", "Thomas Foubert", "Thomas Robert", "Thomas Wang", "Tim Lawson", "Tom Bewley", "Tom Edwards", "Tyler Wang", "Valeriia Nemychnikova", "Van Phung", "Vedant Nanda", "Victor Jouault", "Virgile Richard", "Vladislav Bataev", "Wassim Bouaziz", "Wen-Ding Li", "William Marshall", "Xinghui Li", "Xingran Guo", "Xinyu Yang", "Yannic Neuhaus", "Yihan Wang", "Zaccharie Ramzi", "Zhenlin Xu"], "title": "Voxtral Realtime", "comment": null, "summary": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.", "AI": {"tldr": "Voxtral Realtime is a streaming ASR model achieving offline-quality transcription at low latency with end-to-end training.", "motivation": "Existing offline ASR models require adaptation for streaming, often with quality loss; a natively streaming model can improve real-time applications.", "method": "Uses Delayed Streams Modeling framework with a causal audio encoder and Ada RMS-Norm, trained end-to-end on a multilingual dataset of 13 languages.", "result": "Matches Whisper's performance at 480ms delay, making it comparable to widely used offline systems.", "conclusion": "Voxtral Realtime offers high-quality, low-latency streaming transcription and is released as open-source under Apache 2.0."}}
{"id": "2602.11186", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.11186", "abs": "https://arxiv.org/abs/2602.11186", "authors": ["Zhihan Zeng", "Kaihe Wang", "Zhongpei Zhang", "Yue Xiu"], "title": "GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices", "comment": null, "summary": "The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0\\%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter--approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal \"always-on\" security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.", "AI": {"tldr": "GAC-KAN is a lightweight framework using physics-guided simulation and a KAN-based architecture for GNSS jamming detection in GenAI-powered consumer electronics, achieving 98.0% accuracy with only 0.13M parameters.", "motivation": "GenAI integration in consumer electronics increases computational demands, leaving limited resources for security tasks like GNSS protection, and real-world interference data for training robust classifiers is scarce.", "method": "Proposes GAC-KAN framework: uses physics-guided simulation to generate large-scale jamming datasets and designs a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone with ACB and Ghost modules, replacing MLP with a Kolmogorov-Arnold Network (KAN) for efficient non-linear mapping.", "result": "Achieves 98.0% overall accuracy, outperforming state-of-the-art baselines, with only 0.13 million parameters\u2014about 660 times fewer than Vision Transformer baselines.", "conclusion": "GAC-KAN is an extremely lightweight model ideal as an 'always-on' security companion for GNSS reliability in GenAI systems, minimizing resource contention."}}
{"id": "2602.11301", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11301", "abs": "https://arxiv.org/abs/2602.11301", "authors": ["John M. Willis"], "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates", "comment": "43 pages, plus 12 pages of appendices. One Figure", "summary": "Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.\n  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.", "AI": {"tldr": "This paper introduces the PBSAI Governance Ecosystem, a multi-agent reference architecture for securing enterprise and hyperscale AI estates, aligning with NIST AI RMF functions and providing a structured foundation for open ecosystem development.", "motivation": "Existing governance and security frameworks lack implementable architectures for multi-agent, AI-enabled cyber defense in the context of rapidly deployed large language models and AI estates in enterprise and hyperscale environments.", "method": "The paper proposes a twelve-domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts, incorporating baseline enterprise security capabilities and key systems security techniques like analytic monitoring.", "result": "PBSAI demonstrates alignment with NIST AI RMF functions and is illustrated in enterprise SOC and hyperscale defensive environments, offering a lightweight formal model for traceability, provenance, and human-in-the-loop guarantees.", "conclusion": "PBSAI is proposed as a structured, evidence-centric foundation for open ecosystem development and future empirical validation to address security challenges in AI estates."}}
{"id": "2602.11187", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11187", "abs": "https://arxiv.org/abs/2602.11187", "authors": ["Yubo Hou", "Furen Zhuang", "Partha Pratim Kundu", "Sezin Ata Kircali", "Jie Wang", "Mihai Dragos Rotaru", "Dutta Rahul", "Ashish James"], "title": "TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning", "comment": null, "summary": "The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.", "AI": {"tldr": "A novel multi-agent reinforcement learning framework called TDPNavigator-Placer is proposed to optimize chiplet placement in 2.5D integrated circuits by dynamically addressing conflicting objectives of wirelength reduction and thermal management.", "motivation": "The rapid growth of electronics has increased adoption of 2.5D integrated circuits, requiring effective automated chiplet placement as systems scale. Existing methods focus on minimizing wirelength or use weighted sums for multi-objective optimization, which are inadequate for handling inherently conflicting design requirements like wirelength reduction and thermal management.", "method": "The paper proposes TDPNavigator-Placer, a multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). It assigns conflicting objectives to specialized agents operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm.", "result": "Experimental results show that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.", "conclusion": "TDPNavigator-Placer effectively addresses the challenge of competing design requirements in chiplet placement, providing a superior multi-objective optimization approach compared to existing methods, with practical implications for deployment in large-scale heterogeneous chiplet assemblies."}}
{"id": "2602.11318", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.11318", "abs": "https://arxiv.org/abs/2602.11318", "authors": ["Sheza Munir", "Benjamin Mah", "Krisha Kalsi", "Shivani Kapania", "Julian Posada", "Edith Law", "Ding Wang", "Syed Ishtiaque Ahmed"], "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation", "comment": null, "summary": "In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the \"noisy sensor\" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular \"right\" answer to mapping the diversity of human experience.", "AI": {"tldr": "Systematic review critiques the 'ground truth' paradigm in ML, showing it traps consensus, biases via model mediation and Western norms, and calls for pluralistic annotation to value disagreement.", "motivation": "The paper challenges the positivistic fallacy in machine learning's 'ground truth' paradigm, where human disagreement is treated as noise rather than a sociotechnical signal, aiming to reveal how this leads to a 'consensus trap' in data annotation practices.", "method": "Conducted a systematic literature review of 30,897 records from 2020-2025 across seven premier venues, refined via tiered keyword filtration to 3,042 for screening, resulting in 346 papers for qualitative synthesis using reflexive thematic analysis.", "result": "Findings show systemic failures due to positional legibility and model-mediated annotations introduce anchoring bias, remove human voices, enforce Western norms through geographic hegemony and precarious worker compliance, misdiagnosing cultural pluralism as error.", "conclusion": "Argues for reclaiming disagreement as high-fidelity signal and proposes a roadmap for pluralistic annotation infrastructures to map human diversity instead of seeking a singular 'right' answer."}}
{"id": "2602.11190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11190", "abs": "https://arxiv.org/abs/2602.11190", "authors": ["Fan Zhang", "Shiming Fan", "Hua Wang"], "title": "Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have found that existing methods typically employ a strategy of embedding each time step as an independent token. This paradigm introduces a fundamental information bottleneck when processing long sequences, the root cause of which is that independent token embedding destroys a crucial structure within the sequence - what we term as multi-offset temporal correlation. This refers to the fine-grained dependencies embedded within the sequence that span across different time steps, which is especially prevalent in regular Web data. To fundamentally address this issue, we propose a new perspective on time series embedding. We provide an upper bound on the approximate reconstruction performance of token embedding, which guides our design of a concise yet effective Multi-Offset Time Embedding method to mitigate the performance degradation caused by standard token embedding. Furthermore, our MOTE can be integrated into various existing models and serve as a universal building block. Based on this paradigm, we further design a novel forecasting architecture named Time-TK. This architecture first utilizes a Multi-Offset Interactive KAN to learn and represent specific temporal patterns among multiple offset sub-sequences. Subsequently, it employs an efficient Multi-Offset Temporal Interaction mechanism to effectively capture the complex dependencies between these sub-sequences, achieving global information integration. Extensive experiments on 14 real-world benchmark datasets, covering domains such as traffic flow and BTC/USDT throughput, demonstrate that Time-TK significantly outperforms all baseline models, achieving state-of-the-art forecasting accuracy.", "AI": {"tldr": "The paper proposes MOTE, a novel time series embedding method, and Time-TK architecture to address the information bottleneck in existing forecasting models by preserving multi-offset temporal correlations, achieving state-of-the-art results.", "motivation": "Existing time series forecasting methods use independent token embeddings per time step, which destroys multi-offset temporal correlations\u2014fine-grained dependencies across different steps common in Web data\u2014leading to performance degradation in long sequences.", "method": "Introduces Multi-Offset Time Embedding (MOTE) as a universal building block to mitigate token embedding issues and designs Time-TK architecture with Multi-Offset Interactive KAN for learning temporal patterns and Multi-Offset Temporal Interaction mechanism for capturing complex dependencies.", "result": "Extensive experiments on 14 real-world datasets (e.g., traffic flow, BTC/USDT throughput) show Time-TK significantly outperforms all baselines, achieving state-of-the-art forecasting accuracy.", "conclusion": "The proposed MOTE and Time-TK effectively address the limitations of standard token embeddings, enhancing time series forecasting performance by leveraging multi-offset correlations, with potential applications in web services like intelligent transportation."}}
{"id": "2602.11340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11340", "abs": "https://arxiv.org/abs/2602.11340", "authors": ["Bo Pan", "Xuan Kan", "Kaitai Zhang", "Yan Yan", "Shunwen Tan", "Zihao He", "Zixin Ding", "Junjie Wu", "Liang Zhao"], "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.", "AI": {"tldr": "BLPO: Bi-level prompt optimization framework for multimodal LLM judges that converts images to text to overcome context window constraints for better image evaluation alignment with human judgments.", "motivation": "While LLMs are widely used as automated judges, aligning them with human judgments remains challenging. Existing auto prompt optimization methods target text-only evaluations and are underexplored in multimodal settings. There's a key bottleneck: multimodal models have limited context window capacity for visual examples, hindering effective prompt refinement for AI-generated image evaluation.", "method": "Proposes BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. The approach jointly refines both the judge prompt and the image-to-text (I2T) prompt through bi-level optimization to maintain fidelity under limited context budgets.", "result": "Experiments on four datasets and three LLM judges demonstrate the effectiveness of BLPO. The method shows improved alignment between LLM-based image evaluation and human judgments compared to existing approaches.", "conclusion": "BLPO addresses the context window limitation in multimodal LLM judges through a novel bi-level optimization framework that efficiently converts images to text representations. This enables more effective prompt optimization for AI-generated image evaluation with better human judgment alignment."}}
{"id": "2602.11192", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11192", "abs": "https://arxiv.org/abs/2602.11192", "authors": ["Arian Raje", "Anupam Nayak", "Gauri Joshi"], "title": "MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models", "comment": null, "summary": "Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\\times$ over efficient baselines and up to $14.7\\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.", "AI": {"tldr": "MELINOE is a fine-tuning method for Mixture-of-Experts models that reduces the number of activated experts per sequence, allowing caching to improve inference efficiency by up to 14.7\u00d7.", "motivation": "MoE models reduce activated parameters per token but require all parameters to be loaded into GPU memory, causing memory bottlenecks. Prior solutions offload experts to CPU, but suffer from I/O latency during transfer.", "method": "Fine-tune the MoE model to strongly prefer activating a smaller number of experts per sequence, enabling these preferred experts to be cached in GPU memory to reduce expert churn and transfer overhead.", "result": "Increases throughput by 1.2-3\u00d7 over efficient baselines and up to 14.7\u00d7 over transfer-heavy baselines, while maintaining or even improving model performance on downstream tasks.", "conclusion": "MELINOE is a reliable method for improving MoE inference efficiency by mitigating memory and transfer issues through fine-tuning and caching."}}
{"id": "2602.11348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11348", "abs": "https://arxiv.org/abs/2602.11348", "authors": ["Ruipeng Wang", "Yuxin Chen", "Yukai Wang", "Chang Wu", "Junfeng Fang", "Xiaodong Cai", "Qi Gu", "Hui Su", "An Zhang", "Xiang Wang", "Xunliang Cai", "Tat-Seng Chua"], "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition", "comment": null, "summary": "Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.", "AI": {"tldr": "AgentNoiseBench is a framework for evaluating the robustness of LLM-based agents in noisy environments by injecting controlled noise into benchmarks, revealing sensitivity to real-world perturbations.", "motivation": "Real-world deployments of LLM-based agents underperform on benchmarks due to idealized assumptions that ignore stochasticity and noise in environments, creating a gap between lab and practical performance.", "method": "Analyze real-world noise, categorize it into user-noise and tool-noise, and develop an automated pipeline to inject controllable noise into existing agent benchmarks while maintaining task solvability.", "result": "Extensive evaluations show consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental noise.", "conclusion": "The AgentNoiseBench framework bridges the gap by systematically assessing robustness, emphasizing the need for training and evaluation paradigms that account for real-world noise to improve agent reliability."}}
