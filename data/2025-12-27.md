<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: Using multi-agent with multi-persona debaters instead of single LLM reflections reduces thought degeneration and boosts reasoning accuracy in QA and programming tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit degeneration of thought when reflecting on their own mistakes, leading to repetitive errors despite awareness, necessitating an alternative reflection method to improve reasoning diversity and performance.

Method: Introduced multi-agent with multi-persona debaters to generate reflections, contrasting with single LLM reflections.

Result: Achieved 47% EM on HotPot QA and 82.7% on HumanEval, outperforming single LLM reflections.

Conclusion: The multi-agent with multi-persona debate method effectively mitigates degeneration of thought in LLMs, proving to be a promising approach for enhancing reasoning performance through diverse reflections.

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [2] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta,Shabista Haider*

Main category: cs.AI

TL;DR: BitRL-Light combines 1-bit quantized LLMs with DQN reinforcement learning for adaptive smart home lighting control on edge devices, reducing energy use by 32% vs. rule-based systems.


<details>
  <summary>Details</summary>
Motivation: Smart home lighting systems consume significant energy but lack adaptive intelligence to balance user comfort and energy efficiency simultaneously.

Method: Deploy a 1-bit quantized Llama-3.2-1B model on Raspberry Pi with Deep Q-Network reinforcement learning for real-time control, learning from user feedback via natural language commands and manual overrides.

Result: 71.4x energy reduction vs. full-precision models, 32% energy savings vs. rule-based systems, inference latency <200ms, 95% user satisfaction, 5.07x speedup over 2-bit models on ARM processors, and 92% task accuracy.

Conclusion: This work provides a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [3] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: A blockchain-enhanced multi-agent AI framework ensures trust and auditability in autonomous systems across healthcare, smart cities, and supply chains.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems are increasingly used for autonomous decision-making but raise concerns about trust, oversight, and data integrity.

Method: Proposes a LangChain-based multi-agent system integrated with permissioned blockchain (Hyperledger Fabric) for monitoring, policy enforcement, and auditing.

Result: The framework efficiently prevents unauthorized actions, provides traceability, and keeps operational latency within reasonable limits.

Conclusion: The architecture offers a scalable solution for implementing responsible and high-impact autonomous AI applications.

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [4] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi,Javad Vahidi*

Main category: cs.AI

TL;DR: Quantum-inspired MARL framework for UAV 6G networks outperforms classical methods in exploration-exploitation balance and coverage.


<details>
  <summary>Details</summary>
Motivation: To optimize exploration-exploitation tradeoffs in multiagent reinforcement learning for UAV-assisted 6G network deployment under conditions of partial observability and dynamics.

Method: Integrates classical MARL with quantum-inspired optimization using variational quantum circuits (VQCs) and QAOA, complemented by Bayesian inference, Gaussian processes, and variational inference. Uses centralized training with decentralized execution (CTDE) with shared memory and local view grids.

Result: Experiments show improved sample efficiency, accelerated convergence, enhanced coverage performance, robustness, and superior balance between exploration and exploitation compared to classical baselines (PPO, DDPG).

Conclusion: The study concludes that the quantum-inspired framework successfully balances exploration-exploitation tradeoffs in MARL for UAV-assisted 6G networks, demonstrating improved sample efficiency, faster convergence, and better coverage compared to classical methods. The availability of code ensures reproducibility.

Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [5] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: A price-aware AI system combines personal finance and diet optimization to create affordable, nutritious meal plans that adapt to market changes, using a modular multi-agent architecture.


<details>
  <summary>Details</summary>
Motivation: Limited household budgets and fluctuating food prices, especially in middle-income environments, challenge nutritional demands, requiring solutions that balance affordability and nutritional adequacy.

Method: Developed a modular multi-agent framework with agents for budgeting, nutrition, price monitoring, and health personalization, using a shared knowledge base and substitution graph to maintain nutritional quality at minimal cost.

Result: Simulations with a Saudi household case study show cost reductions of 12-18% compared to static menus, nutrient adequacy over 95%, and high performance under price changes of 20-30%.

Conclusion: The framework effectively combines affordability with nutritional adequacy, supporting sustainable and fair diet planning in line with SDGs on Zero Hunger and Good Health.

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [6] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: This paper proposes a multimodal knowledge graph-based RAG method that integrates visual cues into knowledge graphs, retrieval, and generation to enhance cross-modal reasoning for better understanding of long-form, domain-specific content like books.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG approaches struggle with high-level conceptual understanding and holistic comprehension of long-form, domain-specific content due to limited context windows and text-only limitations, while visual documents require integration of textual, visual, and spatial cues.

Method: The method introduces a multimodal knowledge graph-based RAG system that incorporates visual cues into knowledge graph construction, retrieval phase, and answer generation process to enable cross-modal reasoning.

Result: Experimental results show that the approach consistently outperforms existing RAG-based methods on both textual and multimodal corpora in global and fine-grained question answering tasks.

Conclusion: The proposed multimodal knowledge graph-based RAG effectively addresses limitations in reasoning over long-form content by leveraging visual information, offering improved performance and broader applicability in content understanding.

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [7] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025) from December 3-5, 2025 in Nagaoka, Japan.


<details>
  <summary>Details</summary>
Motivation: To document and disseminate multidisplinary research from the conference on AI, knowledge engineering, HCI, and creativity support systems.

Method: Proceeding compilation based on double-blind peer-reviewed papers selected for the conference, with some recommended for journal publication.

Result: A published volume containing accepted papers via the IEICE Proceedings Series.

Conclusion: The proceedings successfully capture current advancements and foster collaboration in the field, enhancing scholarly communication.

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [8] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: We introduce microprobe, a method for efficiently assessing foundation model reliability using only 100 probe examples via strategic prompt selection and uncertainty quantification, achieving high scores and cost reduction.


<details>
  <summary>Details</summary>
Motivation: Foundation model reliability assessment typically requires thousands of examples, making it computationally expensive and slow for real-world deployment, creating a need for more efficient methods.

Method: Our method combines strategic prompt diversity across five reliability dimensions with advanced uncertainty quantification and adaptive weighting to detect failure modes efficiently.

Result: microprobe achieves 23.5% higher composite reliability scores than random baselines, with p < 0.001 and d = 1.21, reduces assessment cost by 90%, maintains 95% coverage, and receives expert ratings of 4.14/5.0.

Conclusion: microprobe addresses a critical gap in efficient AI model evaluation, enabling faster and more cost-effective reliability assessment for responsible deployment.

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [9] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1 is an AI healthcare assistant based on Qwen-3 with 500GB medical knowledge, using pre-training and retrieval generation for accurate symptom diagnosis in 3-5 interactions.


<details>
  <summary>Details</summary>
Motivation: To create a secure, reliable AI health advisor that enhances primary healthcare and health management by providing professional diagnostic suggestions and guidance.

Method: Integrates 500GB structured medical knowledge via a hybrid approach combining enhanced pre-training and retrieval-enhanced generation, optimized for efficient 3-5 interaction rounds.

Result: Erkang-Diagnosis-1.1 outperforms GPT-4 on comprehensive medical exams, demonstrating its ability to accurately understand symptoms and provide diagnostic suggestions.

Conclusion: The model is designed as an intelligent health companion that empowers healthcare management, showing promise for practical applications in AI-assisted medical consultations.

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [10] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: Research investigates if reasoning chains started by one model can be continued by another, revealing that such interchangeability often preserves or enhances accuracy, highlighting modular AI reasoning potential.


<details>
  <summary>Details</summary>
Motivation: The study tackles the lack of understanding regarding whether reasoning processes from one model can be continued coherently and reliably by another, aiming to assess interchangeability as a measure of model trustworthiness.

Method: It employs token-level log-probability thresholds to truncate reasoning chains at early, mid, and late stages from baseline models, then tests continuation with other models, using a Process Reward Model (PRM) for evaluation.

Result: Results indicate that hybrid reasoning chains often maintain or improve final accuracy and logical structure, with cross-family and intra-family tests showing reasoning interchangeability as a viable property.

Conclusion: This paper concludes that reasoning interchangeability across different models can preserve or sometimes even enhance accuracy and logical structure, underscoring its potential for modular reasoning in collaborative AI.

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [11] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AAT: blockchain-based framework for AI usage traffic recording, uses DID/VC for trusted entities, interaction graph + risk diffusion algorithm for risk traceability, shows scalable performance.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI-driven applications powered by large language models has created security, accountability, and risk traceability challenges due to increasing AI interaction data.

Method: AAT uses decentralized identity (DID), verifiable credentials (VC), and a dynamic interaction graph with blockchain for recording. A risk diffusion algorithm traces risky behavior origins and propagates warnings.

Result: The system shows feasibility and stability under large-scale recording, demonstrated using blockchain TPS metrics.

Conclusion: AAT's blockchain-based AI traffic recording framework effectively enables AI accountability, risk traceability, and cross-system governance in multi-agent environments.

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [12] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: MoAS dynamically routes each token to the best attention scheme (MHA, GQA, or MQA) via a learned router, improving performance over static mixing and matching MHA quality with conditional compute efficiency.


<details>
  <summary>Details</summary>
Motivation: There exists a trade-off between modeling quality (best achieved by Multi-Head Attention) and inference efficiency (improved by Multi-Query Attention and Grouped-Query Attention due to reduced KV cache memory). Current solutions force a single static choice. The authors aim to develop a method that dynamically selects the appropriate attention scheme per token to achieve better quality-efficiency balance.

Method: The authors introduce Mixture of Attention Schemes (MoAS), which includes a learned router that dynamically selects the optimal attention scheme (Multi-Head Attention, Grouped-Query Attention, or Multi-Query Attention) for each token, rather than using a static combination.

Result: Experiments on WikiText-2 show that dynamic routing achieves a validation loss of 2.3074, outperforming a static mixture (2.3093) and demonstrating competitive performance with the Multi-Head Attention baseline while offering potential for conditional compute efficiency.

Conclusion: The paper proposes Mixture of Attention Schemes (MoAS) as a flexible solution to balance Transformer inference efficiency and performance. Dynamic routing of attention schemes per token outperforms static averaging and achieves competitive quality with Multi-Head Attention while enabling conditional computational savings.

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [13] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear is a memory architecture for LLMs, improving long-term dialogue by integrating multimodal perception, dynamic memory, and cognitive services.


<details>
  <summary>Details</summary>
Motivation: LLMs have memory limitations like restricted context windows, forgetting, redundancy, and hallucinations, hindering sustained dialogue and personalized services.

Method: Constructs a human-like memory architecture based on cognitive science with multimodal information perception, dynamic memory maintenance, and adaptive cognitive services.

Result: Outperforms solutions like Mem0, MemGPT, and Graphiti in accuracy, token efficiency, and response latency across healthcare, enterprise, and education domains.

Conclusion: Memory Bear significantly enhances knowledge fidelity, retrieval efficiency, reduces hallucinations, and promotes AI advancement from 'memory' to 'cognition'.

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [14] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova,Andrii Zelenchuk,Dmytro Filatov*

Main category: cs.AI

TL;DR: An AI-driven multi-agent hiring assistant improves early-stage candidate validation by integrating data preprocessing, profile construction, verification, scoring, and human review, reducing time per qualified candidate from 3.33 to 1.70 hours.


<details>
  <summary>Details</summary>
Motivation: Early-stage candidate validation is a bottleneck due to heterogeneous inputs like resumes and assessments, requiring efficient reconciliation.

Method: Modular multi-agent system with document/video preprocessing, structured profiles, public-data verification, technical/culture-fit scoring with risk penalties, LLM orchestration, and interactive interface for human-in-the-loop validation.

Result: Evaluated on 64 real applicants for a Python backend role, with experienced recruiter as baseline: system achieves 1.70 hours per qualified candidate vs. 3.33 hours for experienced recruiter, improving throughput and reducing screening cost.

Conclusion: The system enhances hiring efficiency while maintaining human authority, demonstrating practical value in candidate validation through reduced time and cost.

Abstract: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [15] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: A novel adversarial feedback mechanism called Adversarial Feedback for Attention (AFA) improves sentiment analysis by optimizing Transformer attention to focus on relevant words, achieving SOTA results and boosting large language model performance by 12.6%.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models often underperform in sentiment analysis because they tend to concentrate attention on common words rather than task-relevant terms.

Method: Proposes AFA training with a dynamic masking strategy to adversarially redistribute attention. Uses a discriminator to detect masking differences and a policy gradient to optimize attention distributions based on token perturbations.

Result: Achieves state-of-the-art performance on three public datasets and further improves large language model accuracy by 12.6% when applied.

Conclusion: The AFA mechanism effectively addresses attention misallocation in Transformers for sentiment analysis, enhancing accuracy without manual annotations.

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [16] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: This paper analyzes behavioral artifacts (laziness, decoding suboptimality, and context degradation) in LLMs through controlled experiments, finding widespread laziness but limited decoding issues and surprising context robustness, with recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit behavioral artifacts like laziness, suboptimal decoding, and context degradation, which can undermine reliability. The study aims to quantify these phenomena to understand their prevalence and impact.

Method: The authors conducted three controlled experiments (A, B, C) across advanced LLMs (e.g., OpenAI GPT-4 variant, DeepSeek) to assess laziness (via multi-part instruction compliance), decoding suboptimality (in reasoning tasks), and context degradation (in long conversations).

Result: Results show widespread laziness in multi-part instructions (omissions and length failures), limited evidence of decoding suboptimality (greedy answers aligned with high confidence), and surprising robustness against context degradation in 200-turn chaotic conversations (models maintained key facts and instructions).

Conclusion: While compliance with detailed instructions is a challenge, modern LLMs may mitigate some hypothesized failure modes like context forgetting in retrieval scenarios. Implications for reliability are discussed, with strategies like self-refinement and dynamic prompting recommended to reduce laziness and improve multi-instruction compliance.

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [17] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: The paper introduces Eidoku, a System-2 gate that verifies LLM reasoning by modeling it as a Constraint Satisfaction Problem focusing on structural consistency rather than generation likelihood, effectively rejecting structurally inconsistent hallucinations.


<details>
  <summary>Details</summary>
Motivation: LLMs often assign high likelihood to hallucinated statements, indicating that probability-based verification fails because hallucination is a structural consistency issue, not a low-confidence one.

Method: The authors formulate verification as a CSP independent of likelihood, using structural violation cost (graph connectivity, feature space consistency, logical entailment) and implement Eidoku, a lightweight gate that rejects reasoning steps exceeding a context-derived cost threshold.

Result: Eidoku successfully rejects 'smooth falsehoods'—probable but structurally disconnected statements—that probability-based verifiers cannot detect, as shown in experiments on a diagnostic dataset, enabling deterministic rejection of such hallucinations.

Conclusion: This approach provides a neuro-symbolic sanity check by explicitly enforcing structural constraints, addressing a key limitation of probability-based methods and enhancing the reliability of generative reasoning in LLMs.

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [18] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: Position paper proposes semantic language to bridge gap between functional and normative trustworthy AI for better assessment and implementation.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between Functional Trustworthy AI (implementation-focused) and Normative Trustworthy AI (regulation-focused) to enable better assessment of AI system trustworthiness.

Method: Position paper analyzing state-of-the-art, identifying gaps, discussing starting points for semantic language development, and proposing key considerations for future actions.

Result: The paper identifies the FTAI-NTAI gap, proposes a semantic language as a bridging solution, and outlines development considerations and future actions for TAI assessment.

Conclusion: The paper concludes that a semantic language bridging FTAI and NTAI is crucial for trustworthy AI assessment. It identifies key considerations for development and outlines future actions needed to advance TAI assessment frameworks.

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Parameter-Efficient Neural CDEs via Implicit Function Jacobians](https://arxiv.org/abs/2512.20625)
*Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: This paper introduces a parameter-efficient alternative to Neural CDEs, positioning it as a 'Continuous RNN'.


<details>
  <summary>Details</summary>
Motivation: Neural CDEs are designed for temporal sequence analysis but suffer from high parameter requirements, limiting their efficiency.

Method: Proposes a novel parameter-efficient approach to Neural CDEs, drawing an analogy to 'Continuous RNN' to enhance logical coherence.

Result: The alternative method significantly reduces the number of required parameters compared to standard Neural CDEs.

Conclusion: The proposed parameter-efficient Neural CDE offers a more practical and logically sound framework for sequence modeling, addressing key drawbacks of existing methods.

Abstract: Neural Controlled Differential Equations (Neural CDEs, NCDEs) are a unique branch of methods, specifically tailored for analysing temporal sequences. However, they come with drawbacks, the main one being the number of parameters, required for the method's operation. In this paper, we propose an alternative, parameter-efficient look at Neural CDEs. It requires much fewer parameters, while also presenting a very logical analogy as the "Continuous RNN", which the Neural CDEs aspire to.

</details>


### [20] [Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning](https://arxiv.org/abs/2512.20629)
*Wenlong Tang*

Main category: cs.LG

TL;DR: A multi-agent framework enables continual strategy evolution without parameter fine-tuning, using dual-loop architecture to update external latent vectors through environmental interaction and reflection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable continual strategy evolution in language models without fine-tuning their parameters, allowing agents to develop stable and disentangled strategic styles through environmental interaction and reinforcement feedback.

Method: The approach uses a multi-agent language framework with a dual-loop architecture: a behavior loop that adjusts action preferences based on environmental rewards, and a language loop that updates external latent vectors through reflection on semantic embeddings of generated text.

Result: Experimental results show agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, structured shifts at critical moments, and emergent ability to implicitly infer and adapt to emotional agents without shared rewards.

Conclusion: The study concludes that an external latent space provides language agents with a low-cost, scalable, and interpretable form of abstract strategic representation without modifying model parameters, enabling high-level strategic behavior learning through reflection-driven updates.

Abstract: This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.
  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.

</details>


### [21] [Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams](https://arxiv.org/abs/2512.20631)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: A study showing transformer sentiment models degrade significantly during real-world events, with new zero-training drift metrics for real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: Analyze temporal drift in transformer sentiment models during major events using authentic social media data to identify instability and develop practical monitoring solutions.

Method: Systematic evaluation of three transformer architectures on 12,279 social media posts with statistical validation; introduction of four novel drift metrics compared to embedding-based baselines.

Result: Model accuracy dropped up to 23.4% during events; strong correlation between confidence drops (max 13.0%) and performance degradation; robust detection exceeding industry thresholds.

Conclusion: Zero-training drift analysis enables deployment for real-time sentiment monitoring, providing insights into transformer behavior during dynamic content periods.

Abstract: We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.

</details>


### [22] [Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models](https://arxiv.org/abs/2512.20633)
*MunHwan Lee,Shaika Chowdhury,Xiaodi Li,Sivaraman Rajaganapathy,Eric W Klee,Ping Yang,Terence Sio,Liewei Wang,James Cerhan,Nansu NA Zong*

Main category: cs.LG

TL;DR: LLM-based knowledge curation framework outperforms traditional methods for lung cancer outcome prediction by converting multimodal clinical data into task-aligned semantic representations.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of treatment outcomes in lung cancer remains challenging due to sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models fail to capture semantic information across multimodal streams, while large-scale fine-tuning approaches are impractical in clinical workflows.

Method: Introduced a framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to convert laboratory, genomic, and medication data into high-fidelity, task-aligned features. This operates as an offline preprocessing step that integrates naturally into hospital informatics pipelines.

Result: Achieved mean AUROC of 0.803 (95% CI: 0.799-0.807) using a lung cancer cohort (N=184), outperforming all baselines including expert-engineered features, direct text embeddings, and an end-to-end transformer. Ablation study confirmed the complementary value of combining all three modalities (laboratory, genomic, and medication data).

Conclusion: The paper concludes that reframing LLMs as knowledge curation engines rather than black-box predictors provides a scalable, interpretable, and workflow-compatible pathway for advancing AI-driven decision support in oncology. The quality of semantic representation is identified as a key determinant of predictive accuracy in sparse clinical data settings.

Abstract: Accurate prediction of treatment outcomes in lung cancer remains challenging due to the sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models often fail to capture semantic information across multimodal streams, while large-scale fine-tuning approaches are impractical in clinical workflows. We introduce a framework that uses Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to convert laboratory, genomic, and medication data into high-fidelity, task-aligned features. Unlike generic embeddings, GKC produces representations tailored to the prediction objective and operates as an offline preprocessing step that integrates naturally into hospital informatics pipelines. Using a lung cancer cohort (N=184), we benchmarked GKC against expert-engineered features, direct text embeddings, and an end-to-end transformer. Our approach achieved a mean AUROC of 0.803 (95% CI: 0.799-0.807) and outperformed all baselines. An ablation study further confirmed the complementary value of combining all three modalities. These results show that the quality of semantic representation is a key determinant of predictive accuracy in sparse clinical data settings. By reframing LLMs as knowledge curation engines rather than black-box predictors, this work demonstrates a scalable, interpretable, and workflow-compatible pathway for advancing AI-driven decision support in oncology.

</details>


### [23] [Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning](https://arxiv.org/abs/2512.20634)
*Weiwei Wang*

Main category: cs.LG

TL;DR: This paper analyzes catastrophic forgetting in continual learning for large language models, proposing a framework to measure and promote deep alignment to mitigate spurious forgetting.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting is a key issue in continual learning, with recent work suggesting performance loss may be due to spurious forgetting from task alignment disruption rather than true knowledge loss, but existing approaches lack quantitative measures and automatic distinction mechanisms.

Method: Introduces a shallow versus deep alignment framework with quantitative metrics to measure alignment depth across token positions, real-time detection methods during training, specialized analysis tools for visualization, and adaptive mitigation strategies.

Result: Experiments on multiple datasets and model architectures (Qwen2.5-3B to Qwen2.5-32B) show 86.2-90.6% identification accuracy for shallow alignment and that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.

Conclusion: The framework successfully quantifies alignment depth, provides tools for detection and visualization, and demonstrates that promoting deep alignment effectively reduces spurious forgetting in continual learning.

Abstract: Catastrophic forgetting remains a fundamental challenge in continual learning for large language models. Recent work revealed that performance degradation may stem from spurious forgetting caused by task alignment disruption rather than true knowledge loss. However, this work only qualitatively describes alignment, relies on post-hoc analysis, and lacks automatic distinction mechanisms.
  We introduce the shallow versus deep alignment framework, providing the first quantitative characterization of alignment depth. We identify that current task alignment approaches suffer from shallow alignment - maintained only over the first few output tokens (approximately 3-5) - making models vulnerable to forgetting. This explains why spurious forgetting occurs, why it is reversible, and why fine-tuning attacks are effective.
  We propose a comprehensive framework addressing all gaps: (1) quantitative metrics (0-1 scale) to measure alignment depth across token positions; (2) real-time detection methods for identifying shallow alignment during training; (3) specialized analysis tools for visualization and recovery prediction; and (4) adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment. Extensive experiments on multiple datasets and model architectures (Qwen2.5-3B to Qwen2.5-32B) demonstrate 86.2-90.6% identification accuracy and show that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.

</details>


### [24] [SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression](https://arxiv.org/abs/2512.20635)
*Zeli Su,Ziyin Zhang,Wenzheng Zhang,Zhou Liu,Guixian Xu,Wentao Zhang*

Main category: cs.LG

TL;DR: SHRP is a structured pruning framework that reduces Transformer redundancy by removing redundant attention heads and enabling efficient compression with minimal accuracy loss, achieving significant parameter and FLOPs reductions for web-scale deployment.


<details>
  <summary>Details</summary>
Motivation: Transformer models used in web services for NLP tasks like text classification and semantic retrieval suffer from high inference latency and memory consumption due to architectural redundancy, especially in attention modules with independent heads, limiting real-time serving and scalability.

Method: Proposes SHRP (Specialized Head Routing and Pruning) with Expert Attention, treating each attention head as an independent expert, followed by a shared feed-forward network for output refinement, using a unified Top-1 usage mechanism for dynamic routing during training and deterministic pruning at deployment.

Result: On the GLUE benchmark with BERT-base, SHRP achieves 93% of original model accuracy with a 48% parameter reduction. Under extreme compression (11/12 layers pruned), it maintains 84% accuracy, shows a 4.2x throughput gain, and reduces computation to 11.5% of original FLOPs.

Conclusion: SHRP effectively addresses Transformer inefficiency by specializing and pruning attention heads, balancing accuracy and performance for large-scale, latency-sensitive web deployments, with strong compression results and practical utility.

Abstract: Transformer encoders are widely deployed in large-scale web services for natural language understanding tasks such as text classification, semantic retrieval, and content ranking. However, their high inference latency and memory consumption pose significant challenges for real-time serving and scalability. These limitations stem largely from architectural redundancy, particularly in the attention module. The inherent parameter redundancy of the attention mechanism, coupled with the fact that its attention heads operate with a degree of independence, makes it particularly amenable to structured model compression. In this paper, we propose SHRP (Specialized Head Routing and Pruning), a novel structured pruning framework that automatically identifies and removes redundant attention heads while preserving most of the model's accuracy and compatibility. SHRP introduces Expert Attention, a modular design that treats each attention head as an independent expert, followed by a lightweight shared expander feed-forward network that refines their outputs. The framework employs a unified Top-1 usage-driven mechanism to jointly perform dynamic routing during training and deterministic pruning at deployment. Experimental results on the GLUE benchmark using a BERT-base encoder show that SHRP achieves 93% of the original model accuracy while reducing parameters by 48 percent. Under an extreme compression scenario where 11/12 of the layers are pruned, the model still maintains 84% accuracy and delivers a 4.2x throughput gain while reducing computation to as low as 11.5 percent of the original FLOPs, demonstrating its practical utility for large-scale and latency-sensitive web deployments.

</details>


### [25] [Data-Free Pruning of Self-Attention Layers in LLMs](https://arxiv.org/abs/2512.20636)
*Dhananjay Saikumar,Blesson Varghese*

Main category: cs.LG

TL;DR: Gate-Norm prunes attention sublayers in LLMs based on query-key coupling, achieving faster inference with minimal accuracy loss, without needing data or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. This is due to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the representation.

Method: Gate-Norm, a one-shot, weight-only criterion that ranks attention sublayers by query-key coupling and removes the least coupled ones, requiring no calibration data, no forward passes, no fine-tuning, and no specialized kernels.

Result: On 40-layer, 13B-parameter LLaMA models, Gate-Norm prunes the model in under a second. Pruning 8-16 attention sublayers yields up to 1.30x higher inference throughput while keeping average zero-shot accuracy within 2% of the unpruned baseline across BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy/Challenge, and OpenBookQA. Across these settings, Gate-Norm matches data-driven pruning methods in accuracy while being ~1000x faster to score layers.

Conclusion: The attention sublayers in LLMs can be pruned based on query-key coupling, and pruning 8-16 attention sublayers yields up to 1.30x higher inference throughput while keeping average zero-shot accuracy within 2% of the unpruned baseline.

Abstract: Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the representation. We propose Gate-Norm, a one-shot, weight-only criterion that ranks attention sublayers by query--key coupling and removes the least coupled ones, requiring no calibration data, no forward passes, no fine-tuning, and no specialized kernels. On 40-layer, 13B-parameter LLaMA models, Gate-Norm prunes the model in under a second. Pruning $8$--$16$ attention sublayers yields up to $1.30\times$ higher inference throughput while keeping average zero-shot accuracy within $2\%$ of the unpruned baseline across BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy/Challenge, and OpenBookQA. Across these settings, Gate-Norm matches data-driven pruning methods in accuracy while being $\sim 1000\times$ faster to score layers, enabling practical, data-free compression of LLMs.

</details>


### [26] [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/abs/2512.20643)
*Suriya R S,Prathamesh Dinesh Joshi,Rajat Dandekar,Raj Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: This paper uses Scientific ML (Neural ODEs and UDEs) in Julia to efficiently forecast n-body trajectories with minimal training data, finding UDEs need only 20% data vs. Neural ODEs' 90%.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning models for the n-body problem are data-intensive black boxes that ignore physical laws and lack interpretability, needing more interpretable and data-efficient approaches.

Method: Robust modeling in Julia using Scientific ML frameworks (Neural ODEs and Universal Differential Equations) on synthetic noisy data, with analysis of forecasting breakdown point.

Result: The UDE model is more data-efficient, requiring only 20% of training data for accurate forecasts, while the Neural ODE needs 90%.

Conclusion: Embracing physical laws via UDEs in Scientific ML enhances data efficiency and interpretability for n-body simulations, outperforming Neural ODEs in training data requirements.

Abstract: The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.

</details>


### [27] [Q-RUN: Quantum-Inspired Data Re-uploading Networks](https://arxiv.org/abs/2512.20654)
*Wenbo Qiao,Shuaixian Wang,Peng Zhang,Yan Ming,Jiaming Zhao*

Main category: cs.LG

TL;DR: A quantum-inspired classical network (Q-RUN) mimics DRQC to outperform classic models without quantum hardware, serving as a drop-in replacement for fully connected layers.


<details>
  <summary>Details</summary>
Motivation: Data re-uploading quantum circuits (DRQC) are a key approach to implementing quantum neural networks and have been shown to outperform classical neural networks in fitting high-frequency functions, but their practical application is limited by the scalability of current quantum hardware.

Method: Introducing the mathematical paradigm of data re-uploading quantum circuits (DRQC) into classical models by proposing a quantum-inspired data re-uploading network (Q-RUN).

Result: Q-RUN delivers superior performance across both data modeling and predictive modeling tasks, reducing model parameters while decreasing error by approximately one to three orders of magnitude on certain tasks compared to fully connected layers and state-of-the-art neural network layers.

Conclusion: This work illustrates how principles from quantum machine learning can guide the design of more expressive artificial intelligence.

Abstract: Data re-uploading quantum circuits (DRQC) are a key approach to implementing quantum neural networks and have been shown to outperform classical neural networks in fitting high-frequency functions. However, their practical application is limited by the scalability of current quantum hardware. In this paper, we introduce the mathematical paradigm of DRQC into classical models by proposing a quantum-inspired data re-uploading network (Q-RUN), which retains the Fourier-expressive advantages of quantum models without any quantum hardware. Experimental results demonstrate that Q-RUN delivers superior performance across both data modeling and predictive modeling tasks. Compared to the fully connected layers and the state-of-the-art neural network layers, Q-RUN reduces model parameters while decreasing error by approximately one to three orders of magnitude on certain tasks. Notably, Q-RUN can serve as a drop-in replacement for standard fully connected layers, improving the performance of a wide range of neural architectures. This work illustrates how principles from quantum machine learning can guide the design of more expressive artificial intelligence.

</details>


### [28] [MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing](https://arxiv.org/abs/2512.20655)
*Yuting Hu,Lei Zhuang,Hua Xiang,Jinjun Xiong,Gi-Joon Nam*

Main category: cs.LG

TL;DR: MaskOpt provides a large-scale context-aware dataset from real IC designs to improve deep learning for mask optimization at 45nm.


<details>
  <summary>Details</summary>
Motivation: As IC dimensions shrink, model-based mask optimization is computationally expensive and existing deep learning datasets rely on synthetic layouts, lacking real-design context and cell hierarchy needed for practical applications.

Method: Construct MaskOpt dataset from real 45nm IC designs with over 226,000 tiles for metal and via layers, preserving standard-cell information and capturing surrounding geometric contexts for optical proximity effects. Evaluate state-of-the-art deep learning models on this dataset.

Result: Benchmark evaluations reveal trade-offs across models. Context size and input ablation studies show that both surrounding geometries and cell-aware inputs are critical for accurate mask generation.

Conclusion: MaskOpt advances deep learning in mask optimization by providing a realistic, context-aware dataset that highlights the importance of cell hierarchy and neighboring shapes, facilitating scalable and accurate solutions.

Abstract: As integrated circuit (IC) dimensions shrink below the lithographic wavelength, optical lithography faces growing challenges from diffraction and process variability. Model-based optical proximity correction (OPC) and inverse lithography technique (ILT) remain indispensable but computationally expensive, requiring repeated simulations that limit scalability. Although deep learning has been applied to mask optimization, existing datasets often rely on synthetic layouts, disregard standard-cell hierarchy, and neglect the surrounding contexts around the mask optimization targets, thereby constraining their applicability to practical mask optimization. To advance deep learning for cell- and context-aware mask optimization, we present MaskOpt, a large-scale benchmark dataset constructed from real IC designs at the 45$\mathrm{nm}$ node. MaskOpt includes 104,714 metal-layer tiles and 121,952 via-layer tiles. Each tile is clipped at a standard-cell placement to preserve cell information, exploiting repeated logic gate occurrences. Different context window sizes are supported in MaskOpt to capture the influence of neighboring shapes from optical proximity effects. We evaluate state-of-the-art deep learning models for IC mask optimization to build up benchmarks, and the evaluation results expose distinct trade-offs across baseline models. Further context size analysis and input ablation studies confirm the importance of both surrounding geometries and cell-aware inputs in achieving accurate mask generation.

</details>


### [29] [Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering](https://arxiv.org/abs/2512.20660)
*Matthew Thompson*

Main category: cs.LG

TL;DR: This paper proposes a dual-state architecture to treat the LLM as a stochastic environment rather than a decision-making agent, improving code generation success rates with deterministic workflows.


<details>
  <summary>Details</summary>
Motivation: Current AI coding agents often let LLMs make decisions that should be deterministic, leading to stochastic failures like gaming unit tests or hallucinating syntax. The paper draws on software engineering practices to manage this unpredictability.

Method: A Dual-State Architecture is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation), using Atomic Action Pairs with Guard Functions to couple generation with verification as indivisible transactions.

Result: Validated on three code generation tasks across 13 LLMs (1.3B-15B parameters), the framework improved task success rates by up to 66 percentage points for qualified models at computational costs of 1.2-2.1× baseline.

Conclusion: Architectural constraints can effectively substitute for parameter scale in achieving reliable code generation, offering a robust alternative to scaling model size.

Abstract: Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.
  A \textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.

</details>


### [30] [Dominating vs. Dominated: Generative Collapse in Diffusion Models](https://arxiv.org/abs/2512.20666)
*Hayeon Jeong,Jong-Seok Lee*

Main category: cs.LG

TL;DR: A study identifies Dominant-vs-Dominated (DvD) imbalance in multi-concept text-to-image diffusion models, analyzes its causes via DominanceBench, and offers insights for improved controllability.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often generate imbalanced images from multi-concept prompts, with one concept token dominating others, limiting diverse and accurate generation.

Method: Introduce DominanceBench to systematically analyze DvD imbalance, examining causes from data (limited instance diversity) and architecture (cross-attention dynamics, head ablation studies).

Result: Findings show training data's limited diversity exacerbates inter-concept interference, dominant tokens saturate attention across timesteps, and DvD arises from distributed attention mechanisms.

Conclusion: The study advances understanding of generative collapse in diffusion models, providing insights for more reliable and controllable text-to-image generation by addressing DvD imbalance.

Abstract: Text-to-image diffusion models have drawn significant attention for their ability to generate diverse and high-fidelity images. However, when generating from multi-concept prompts, one concept token often dominates the generation, suppressing the others-a phenomenon we term the Dominant-vs-Dominated (DvD) imbalance. To systematically analyze this imbalance, we introduce DominanceBench and examine its causes from both data and architectural perspectives. Through various experiments, we show that the limited instance diversity in training data exacerbates the inter-concept interference. Analysis of cross-attention dynamics further reveals that dominant tokens rapidly saturate attention, progressively suppressing others across diffusion timesteps. In addition, head ablation studies show that the DvD behavior arises from distributed attention mechanisms across multiple heads. Our findings provide key insights into generative collapse, advancing toward more reliable and controllable text-to-image generation.

</details>


### [31] [Forward Only Learning for Orthogonal Neural Networks of any Depth](https://arxiv.org/abs/2512.20668)
*Paul Caillon,Alex Colagrande,Erwan Fagnou,Blaise Delattre,Alexandre Allauzen*

Main category: cs.LG

TL;DR: FOTON: a forward-only algorithm that trains neural networks without backward passes, outperforming PEPITA and enabling training of any depth networks.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's computational cost is burdensome for modern large architectures. Existing alternatives like PEPITA fail to scale beyond a few hidden layers, limiting their use.

Method: FOTON bridges the gap with backpropagation by relaxing linear and orthogonal assumptions, allowing training of hidden layers without a backward pass.

Result: FOTON outperforms PEPITA in experiments, successfully training neural networks of any depth, including convolutional networks, without a backward pass.

Conclusion: FOTON addresses backpropagation's limitations, enabling more efficient training of deep neural networks and offering promise for advanced architectures.

Abstract: Backpropagation is still the de facto algorithm used today to
  train neural networks.
  With the exponential growth of recent architectures, the
  computational cost of this algorithm also becomes a burden. The
  recent PEPITA and forward-only frameworks have proposed promising
  alternatives, but they failed to scale up to a handful of hidden
  layers, yet limiting their use.
  In this paper, we first analyze theoretically the main limitations of
  these approaches. It allows us the design of a forward-only
  algorithm, which is equivalent to backpropagation under the linear
  and orthogonal assumptions. By relaxing the linear assumption, we
  then introduce FOTON (Forward-Only Training of Orthogonal Networks)
  that bridges the gap with the backpropagation
  algorithm. Experimental results show that it outperforms PEPITA,
  enabling us to train neural networks of any depth, without the need
  for a backward pass.
  Moreover its performance on convolutional networks clearly opens up avenues for its application to more
  advanced architectures. The code is open-sourced at https://github.com/p0lcAi/FOTON .

</details>


### [32] [Improving Cardiac Risk Prediction Using Data Generation Techniques](https://arxiv.org/abs/2512.20669)
*Alexandre Cabodevila,Pedro Gamallo-Fernandez,Juan C. Vidal,Manuel Lama*

Main category: cs.LG

TL;DR: This paper proposes a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records for cardiac rehabilitation datasets, aiming to enhance cardiac risk prediction models and reduce the need for invasive diagnostic tests.


<details>
  <summary>Details</summary>
Motivation: Real-world medical databases in cardiac rehabilitation face limitations: scarce data due to economic and time constraints, unsuitable records for analysis, and high prevalence of missing values from varied patient tests.

Method: The study employs a Conditional Variational Autoencoder (CVAE) architecture to synthesize realistic and coherent clinical records based on observed data.

Result: The CVAE architecture successfully generates synthetic data that improves the accuracy of classifiers for cardiac risk detection, outperforming state-of-the-art deep learning approaches in synthetic data generation.

Conclusion: The proposed CVAE-based architecture effectively addresses dataset limitations in cardiac rehabilitation, enhancing risk prediction models and reducing reliance on hazardous diagnostic procedures like exercise stress testing.

Abstract: Cardiac rehabilitation constitutes a structured clinical process involving multiple interdependent phases, individualized medical decisions, and the coordinated participation of diverse healthcare professionals. This sequential and adaptive nature enables the program to be modeled as a business process, thereby facilitating its analysis. Nevertheless, studies in this context face significant limitations inherent to real-world medical databases: data are often scarce due to both economic costs and the time required for collection; many existing records are not suitable for specific analytical purposes; and, finally, there is a high prevalence of missing values, as not all patients undergo the same diagnostic tests. To address these limitations, this work proposes an architecture based on a Conditional Variational Autoencoder (CVAE) for the synthesis of realistic clinical records that are coherent with real-world observations. The primary objective is to increase the size and diversity of the available datasets in order to enhance the performance of cardiac risk prediction models and to reduce the need for potentially hazardous diagnostic procedures, such as exercise stress testing. The results demonstrate that the proposed architecture is capable of generating coherent and realistic synthetic data, whose use improves the accuracy of the various classifiers employed for cardiac risk detection, outperforming state-of-the-art deep learning approaches for synthetic data generation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication](https://arxiv.org/abs/2512.20778)
*Moshe Rafaeli Shimron,Vadim Indelman*

Main category: cs.MA

TL;DR: A decentralized multi-agent decision-making framework that handles inconsistent beliefs with probabilistic guarantees and selective communication, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-agent scenarios often involve agents with different beliefs due to limited communication, leading to poor coordination and unsafe outcomes, which existing approaches ignore by assuming identical beliefs.

Method: Introduces a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies, provides probabilistic guarantees for action consistency and performance relative to an open-loop multi-agent POMDP, and selectively triggers communication when necessary. Also addresses whether to share data to improve inference performance after joint action selection.

Result: The framework demonstrates superior performance compared to state-of-the-art algorithms in simulations.

Conclusion: This approach effectively tackles the challenge of belief inconsistency in multi-agent systems, enhancing coordination and safety in decentralized operations with minimal, on-demand communication.

Abstract: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.

</details>


### [34] [DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination](https://arxiv.org/abs/2512.20973)
*Yihan Xia,Taotao Wang,Wenxin Xu,Shengli Zhang*

Main category: cs.MA

TL;DR: DAO-Agent: A hybrid on-chain/off-chain framework using ZKP and DAO to enable auditable, private coordination for autonomous LLM agents in trustless environments with minimal on-chain costs.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems face challenges in trustless environments: Centralized coordination cannot ensure transparent contribution measurement and equitable incentives, while blockchain solutions incur high computation costs and risk exposing sensitive agent information.

Method: The authors propose DAO-Agent, featuring three technical innovations: 1) on-chain DAO governance for transparent coordination, 2) ZKP mechanism for off-chain Shapley-based contribution measurement, and 3) hybrid on-chain/off-chain architecture for minimal-cost verification.

Result: Experimental evaluation shows DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that scales well with coalition size.

Conclusion: The DAO-Agent framework provides an efficient solution for decentralized multi-agent coordination that significantly reduces on-chain costs while maintaining strategic privacy. The hybrid architecture with ZKP-based verification and DAO governance establishes a scalable foundation for autonomous LLM agent systems in trustless environments.

Abstract: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.

</details>


### [35] [A Plan Reuse Mechanism for LLM-Driven Agent](https://arxiv.org/abs/2512.21309)
*Guopeng Li,Ruiqi Wu,Haisheng Tan*

Main category: cs.MA

TL;DR: AgentReuse is a plan reuse mechanism for LLM-driven agents that reduces latency by reusing similar plans based on semantic similarity and intent classification.


<details>
  <summary>Details</summary>
Motivation: LLM-driven agents in personal assistants face high latency (tens of seconds) in plan generation, degrading user experience, and many requests (30%) are similar, allowing plan reuse, but similarity is hard to define directly from text.

Method: AgentReuse evaluates request similarity using semantic analysis and intent classification, enabling reuse of previously generated plans to avoid redundant LLM processing.

Result: AgentReuse achieves 93% effective plan reuse rate, F1 score of 0.9718, and accuracy of 0.9459 in similarity evaluation, reducing latency by 93.12% compared to baselines.

Conclusion: The proposed plan reuse mechanism significantly improves efficiency and user experience for LLM-driven agents by leveraging request semantics.

Abstract: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.

</details>
