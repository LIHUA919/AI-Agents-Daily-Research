<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.LG](#cs.LG) [Total: 138]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP is a training-free framework using multi-agent orchestration and runtime personalization to improve workflow adherence in LLM-based TOD systems, outperforming baselines in fidelity, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long, conditional workflows involving external tools and user-specific data in TOD systems.

Method: WARPP combines multi-agent orchestration with runtime personalization, dynamically pruning branches and tailoring execution paths via a Personalizer agent.

Result: WARPP outperforms non-personalized and ReAct baselines, improving fidelity, tool accuracy, and reducing token usage as intent complexity grows.

Conclusion: WARPP effectively enhances LLM-based TOD systems without additional training, offering scalable improvements for complex workflows.

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [2] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: The paper reviews hypergame theory's applications in multi-agent systems (MAS), highlighting its ability to model subjective perceptions and addressing gaps like limited HNF adoption and lack of formal languages.


<details>
  <summary>Details</summary>
Motivation: Classical game theory's assumptions (rationality, complete information) often fail in real-world MAS due to uncertainty and misaligned beliefs. Hypergame theory addresses these limitations by modeling agents' subjective perceptions.

Method: A systematic review of 44 studies across domains like cybersecurity and robotics, introducing agent-compatibility criteria and a classification framework to assess hypergame theory's practical use.

Result: Findings include the prevalence of hierarchical models in deceptive reasoning, simplification of theoretical frameworks, and gaps like limited HNF adoption and unexplored human-agent misalignment.

Conclusion: The review provides a roadmap for enhancing strategic modeling in dynamic MAS using hypergame theory, identifying trends, challenges, and future research directions.

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [3] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA is an open-source platform for integrating AI into clinical workflows, enabling collaboration among clinicians, researchers, and developers.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI innovation and practical healthcare applications by fostering interdisciplinary collaboration.

Method: Built on Kubernetes, MAIA provides a modular, scalable environment with tools for data management, model development, annotation, deployment, and clinical feedback.

Result: MAIA supports real-world use cases in medical imaging AI, with deployments in academic and clinical settings.

Conclusion: MAIA accelerates AI translation into clinical solutions, emphasizing reproducibility, transparency, and user-centered design.

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [4] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: A multimodal large language model (MLLM)-based multi-agent system was developed for automatic chemical information extraction, achieving an F1 score of 80.8%, significantly outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: High-quality chemical databases are crucial for AI-powered chemical research, but current methods struggle with the multimodality and variability of chemical information in literature.

Method: The system uses an MLLM's reasoning to understand complex chemical graphics, decomposes tasks into sub-tasks, and coordinates specialized agents to solve them.

Result: Achieved an F1 score of 80.8% on a benchmark dataset, surpassing the previous state-of-the-art (35.6%), with improvements in sub-tasks like molecular image recognition and reaction parsing.

Conclusion: This system advances automated chemical information extraction, supporting AI-driven chemical research.

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [5] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: The paper introduces a corrigibility framework with five utility heads for safety in AI, proving guarantees in multi-step environments and addressing adversarial risks.


<details>
  <summary>Details</summary>
Motivation: To ensure AI systems remain corrigible (safe and controllable) even in complex, partially observed environments and adversarial settings.

Method: Uses five structurally separate utility heads combined lexicographically, with theoretical proofs for corrigibility in single-round and multi-step scenarios.

Result: Theorems prove corrigibility guarantees, and a decidable island for safety certification is identified, shifting risks to evaluation quality.

Conclusion: The framework provides clearer implementation guidance for AI safety, focusing on evaluation rather than hidden incentives.

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [6] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM is a training-free framework for efficient LLM inference on edge devices by exploiting temporal sparsity in attention patterns, achieving up to 60% sparsity with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on edge devices is challenging due to high computational demands, especially with long sequences. Existing solutions are designed for GPUs/TPUs and are unsuitable for edge scenarios.

Method: DeltaLLM uses a delta matrix construction strategy for temporal sparsity and a hybrid attention mechanism combining full attention locally with delta approximation globally.

Result: Achieves 60% sparsity in prefilling and 57% overall on BitNet and Llama models, with slight accuracy improvements or negligible drops.

Conclusion: DeltaLLM provides an efficient, no-fine-tuning solution for edge deployment, seamlessly integrating with existing pipelines.

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [7] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS introduces a team of LLM-based scientists for gene expression analysis, combining structured workflows and autonomous agents to improve precision and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current automation tools for gene expression analysis are either too rigid or lack precision, limiting their effectiveness in scientific research.

Method: GenoMAS uses six specialized LLM agents coordinated via typed message-passing protocols, with a guided-planning framework for task execution.

Result: GenoMAS achieves 89.13% Composite Similarity Correlation for preprocessing and 60.48% F1 score for gene identification, outperforming prior methods.

Conclusion: GenoMAS offers a robust, adaptable solution for gene expression analysis, validated by performance metrics and biological plausibility.

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [8] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: This survey explores alignment techniques for large language models (LLMs), covering methods like supervised fine-tuning and preference-based approaches, and highlights challenges like reward misspecification and scalability.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs align with human values is critical due to their societal impact, necessitating a comprehensive review of alignment methods and challenges.

Method: The survey analyzes alignment techniques such as Direct Preference Optimization (DPO), Constitutional AI, and brain-inspired methods, alongside evaluation frameworks and benchmarking datasets.

Result: Preference-based methods outperform supervised fine-tuning for nuanced alignment, but challenges like reward misspecification and scalable oversight persist.

Conclusion: Open problems in oversight, value pluralism, and robustness remain, guiding future research and practice in LLM alignment.

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [9] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: Large language models (LLMs) face inherent limits in improving prediction uncertainty due to scaling laws, leading to reliability issues for scientific standards. Their learning mechanism, while powerful, may cause error pileup and degenerative behavior. Avoiding this requires prioritizing insight and problem understanding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to highlight the limitations of LLMs in achieving reliable uncertainty predictions, which is crucial for scientific applications, and explores the underlying causes of these limitations.

Method: The study analyzes the scaling laws of LLMs and their impact on prediction uncertainty, linking their learning mechanisms (non-Gaussian outputs from Gaussian inputs) to error pileup and degenerative behavior.

Result: The findings suggest that LLMs' scaling laws severely restrict their ability to improve prediction reliability, with their learning mechanisms contributing to error accumulation and potential degenerative AI pathways.

Conclusion: To mitigate these issues, future AI research must prioritize deeper insight and understanding of problem structures, rather than relying solely on scaling up LLMs.

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [10] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: The paper investigates how Intrinsic Motivation (IM) methods impact RL agents' behavior in games, revealing reward hacking issues and testing GRM as a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: Games pose challenges for RL due to sparse rewards. IM methods help but introduce reward hacking, a poorly understood problem. This study evaluates IM's behavioral impact and tests GRM's effectiveness.

Method: Empirical evaluation of three IM techniques in the MiniGrid environment, comparing them with GRM to assess behavior changes and reward hacking mitigation.

Result: IM increases initial rewards and alters agent behavior. GRM partially mitigates reward hacking in some cases.

Conclusion: IM significantly affects RL agent behavior, and GRM shows promise in addressing reward hacking, though further research is needed.

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [11] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG integrates EHR data with KGs using hypergraph models to improve healthcare predictions by contextualizing knowledge.


<details>
  <summary>Details</summary>
Motivation: General KGs lack patient-specific contexts, while EHRs provide rich personal data. Combining these can enhance precision healthcare.

Method: Uses entity-linking to connect KGs with EHRs, then hypergraph models and transformers to learn contextualized representations.

Result: HypKG improves healthcare predictions significantly and enhances KG quality by adjusting entity representations.

Conclusion: HypKG effectively bridges KGs and EHRs, improving both prediction accuracy and knowledge utility in healthcare.

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [12] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: The paper explores using ontology-structured knowledge graphs for predicting future events, leveraging BFO and CCO. It introduces 'spatiotemporal instant' for semantics, critiques current probability models, and proposes a Markov chain-based approach for predictions, integrating results back into the graph.


<details>
  <summary>Details</summary>
Motivation: To enhance predictive analytics by organizing and retrieving data from knowledge graphs using ontologies, improving future event predictions.

Method: Uses BFO and CCO to structure data, introduces 'spatiotemporal instant,' critiques existing probability models, and employs Markov chains for predictions.

Result: Demonstrates how Markov chain models can predict future states and integrates these predictions back into the knowledge graph for analysis.

Conclusion: Proposes a refined ontological model for probability and showcases the practical integration of predictive analytics with knowledge graphs.

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [13] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench introduces a benchmark for evaluating LLMs in ASP tasks, revealing their limitations in core ASP solving.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in ASP are limited, lacking support for complex ASP features and dedicated benchmarks.

Method: ASPBench includes three tasks: ASP entailment, answer set verification, and answer set computation, evaluated on 14 LLMs.

Result: LLMs perform well on simpler tasks but struggle with answer set computation, the core of ASP solving.

Conclusion: The findings highlight the need for better integration of symbolic reasoning in LLMs for ASP.

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [14] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: A multi-objective supply chain optimization model using reinforcement learning outperforms traditional methods in balancing economic, environmental, and social goals.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing supply chains with non-stationary markets while considering economic, environmental, and social factors.

Method: Uses a Markov decision process and multi-objective reinforcement learning, compared against modified single-objective RL and MOEA-based approaches.

Result: The primary method achieves better trade-offs, with 75% higher hypervolume than MOEA and denser solutions than single-objective RL.

Conclusion: The proposed model effectively balances competing objectives, ensuring robust and stable supply chain performance.

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [15] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: The paper introduces DiCap, a diffusion-based counterfactual prompt learning model, to address limitations in existing prompt learning methods by ensuring causally invariant prompts and robust feature generalization.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods lack theoretical grounding, leading to difficulties in achieving causally invariant prompts and robust feature generalization across categories.

Method: DiCap uses a diffusion process to sample gradients from causal model distributions, generating counterfactuals that meet minimal sufficiency. It employs contrastive learning to refine prompts aligned with causal features.

Result: DiCap excels in tasks like image classification, image-text retrieval, and visual question answering, especially in unseen categories.

Conclusion: The DiCap framework, with its theoretical rigor and practical effectiveness, advances prompt learning by ensuring causal invariance and robust generalization.

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [16] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: The paper argues that AI is inherently tied to human cognition, analyzing its impact through displacement, enhancement, or replacement of human cognitive labor, and warns against obfuscating this relationship.


<details>
  <summary>Details</summary>
Motivation: To clarify the relationship between AI and human cognition, emphasizing the need to center human experience in AI development.

Method: Uses examples (e.g., abacus vs. mental arithmetic) and novel definitions to analyze sociotechnical relationships.

Result: Identifies three types of AI impact: displacement (harmful), enhancement (beneficial), and replacement (neutral) of human cognitive labor.

Conclusion: Obfuscating AI's cognitive ties distorts understanding and hinders human-centered AI development; transparency is crucial.

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [17] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: Fine-tuned open-source LLMs with CoT supervision match GPT-4o in accurately extracting PCL features and assigning risk categories from radiology reports, enabling scalable PCL research.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of PCL features is labor-intensive, hindering large-scale studies. Automating this process with LLMs can advance PCL research.

Method: Two open-source LLMs (LLaMA, DeepSeek) were fine-tuned using QLoRA on GPT-4o-generated CoT data to extract features and categorize risk from MRI/CT reports.

Result: Fine-tuned models achieved high accuracy (97-98%) in feature extraction and risk categorization (F1 scores 0.94-0.95), matching GPT-4o performance. Radiologist agreement was also high (Fleiss' Kappa ~0.89).

Conclusion: Fine-tuned LLMs with CoT supervision provide accurate, interpretable, and efficient PCL phenotyping, comparable to GPT-4o, for large-scale research.

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [18] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: A digital twin channel (DTC)-enabled framework improves 6G resource allocation by predicting CSI via environmental sensing, outperforming pilot-based methods by 11.5% in throughput.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of conventional statistical methods and excessive pilot overhead in dynamic 6G environments for applications like holographic communication and autonomous driving.

Method: Uses DTC to predict CSI based on environmental sensing and lightweight game-theoretic algorithms for online resource allocation.

Result: Achieves 11.5% higher throughput than pilot-based ideal CSI schemes in simulations.

Conclusion: The DTC framework is effective for scalable, low-overhead, and environment-aware communication in 6G networks.

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [19] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: The paper proposes a framework combining LLMs with GRAPHYP's network to make AI reasoning transparent and personalized for human preferences.


<details>
  <summary>Details</summary>
Motivation: To enhance AI transparency and trust by enabling users to understand and influence AI decision-making through structured conversations.

Method: Introduces D-LLMs, a system with three components: reasoning processes, preference classification, and dialogue approaches.

Result: A conceptual framework for interpretable AI that shows how answers are derived from human preferences.

Conclusion: The framework aims to make AI more transparent and trustworthy by embedding user preferences and enabling traceable reasoning.

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [20] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*MÃ¼ge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: The paper explores 'good-enough' matchings for the Stable Roommates problem, incorporating agents' habits, preferences, and friend networks to generate personalized solutions.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often lack solutions for stable roommates problems, prompting the need for acceptable, if not perfect, matchings.

Method: The method integrates agents' habits, habitual preferences, and friend networks to create personalized solutions.

Result: Examples and empirical evaluations demonstrate the method's effectiveness in generating acceptable matchings.

Conclusion: The approach provides a practical solution for stable roommates problems when perfect stability is unattainable.

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [21] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA is a novel framework for aligning LLM outputs with user preferences during inference, eliminating the need for a pre-trained reward model by integrating preference feedback directly into token generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on pre-trained reward models, which can be unstable due to dependency on human preference feedback. PITA aims to bypass this limitation.

Method: PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, using stochastic search and iterative refinement.

Result: PITA is evaluated on tasks like mathematical reasoning and sentiment classification, showing effectiveness in aligning LLM outputs with user preferences.

Conclusion: PITA provides a computationally efficient and stable alternative to reward-model-dependent methods for inference-time alignment of LLMs.

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [22] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: The paper introduces CMQ, an interpretable value decomposition framework for MARL, addressing transparency and interoperability issues by using human-like cooperation concepts.


<details>
  <summary>Details</summary>
Motivation: Current NN-based MARL lacks transparency and interoperability due to black-box networks, limiting understanding of implicit cooperative mechanisms.

Method: Proposes CMQ, a value-based method using concept bottleneck models to represent cooperation concepts as supervised vectors, enhancing interpretability and performance.

Result: CMQ outperforms state-of-the-art methods in StarCraft II and LBF, offering better cooperation concept representation and enabling test-time interventions.

Conclusion: CMQ bridges the performance-interpretability gap in MARL, providing meaningful cooperation insights and detecting biases or artifacts.

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [23] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: The paper introduces a mathematical framework to analyze RL stability in LLMs/LRMs, explaining policy brittleness due to non-unique optimal actions and proposing entropy regularization as a solution.


<details>
  <summary>Details</summary>
Motivation: RL in LLMs/LRMs often leads to brittle policies causing failures like spurious reasoning and deceptive alignment, lacking a unified theoretical explanation.

Method: The paper develops a rigorous mathematical framework to analyze the reward-to-policy mapping, extending it to multi-reward RL and testing with perturbation experiments.

Result: The framework explains policy brittleness via non-unique optimal actions and shows entropy regularization stabilizes policies, validated empirically.

Conclusion: This work advances policy-stability theory, offering insights for safer AI design by unifying explanations for RL failures in LLMs/LRMs.

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [24] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover Preview is a language model for formal theorem proving, achieving 70% success on miniF2F-test via reinforcement learning and tool-integrated reasoning.


<details>
  <summary>Details</summary>
Motivation: To advance automated theorem proving by emulating human-like problem-solving with tool-integrated reasoning.

Method: Uses reinforcement learning with tool-based interactions to iteratively refine Lean 4 proofs.

Result: Achieves a 70.0% pass@1 success rate on miniF2F-test.

Conclusion: Introduces a framework for tool-integrated reasoning models, promising for Math AI and theorem proving.

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [25] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: HFrame is a graph neural network framework for subgraph homomorphism, combining traditional algorithms with ML. It outperforms standard GNNs, is faster than exact matching, and achieves high accuracy.


<details>
  <summary>Details</summary>
Motivation: Subgraph homomorphism is complex and lacks efficient solutions. Traditional methods are slow, and standard GNNs struggle with distinguishing non-homomorphic pairs.

Method: HFrame integrates traditional algorithms with machine learning, using graph neural networks to solve subgraph homomorphism.

Result: HFrame is 101.91x faster than exact matching, achieves 0.962 accuracy, and generalizes better than standard GNNs.

Conclusion: HFrame is a scalable, accurate solution for subgraph homomorphism, bridging traditional and ML approaches.

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [26] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent is an LLM-powered agent that automates scientific tools across multiple domains, outperforming existing methods with intelligent tool selection and safety checks.


<details>
  <summary>Details</summary>
Motivation: Specialized computational tools require domain expertise, and current LLMs struggle to integrate multiple tools for complex workflows.

Method: SciToolAgent uses a scientific tool knowledge graph for tool selection and execution, along with a safety-checking module.

Result: It outperforms existing approaches in evaluations and successfully automates workflows in protein engineering, chemical reactivity, synthesis, and material screening.

Conclusion: SciToolAgent makes advanced research tools accessible to experts and non-experts, automating complex workflows effectively.

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [27] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: An AI-powered platform using LLMs improves industrial R&D scouting by automating solution discovery from patents and market data, enhancing efficiency and decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional R&D methods are slow, manual, and fragmented, relying heavily on domain expertise and incomplete data sources.

Method: The platform uses LLMs for semantic understanding, contextual reasoning, and knowledge extraction to analyze patents and market data, organizing solutions into standardized categories.

Result: The system reduces manual effort, speeds up innovation, and provides comprehensive insights into technical novelty, feasibility, and scalability.

Conclusion: The AI-driven platform transforms R&D scouting by automating and enhancing solution discovery, improving efficiency and decision-making.

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [28] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: The paper explores how high-dimensional representations in large language models (LLMs) can be exploited to bypass safety alignment, proposing dimensional reduction as a solution.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of LLMs raises concerns about safety alignment, especially as increasing model dimensions may introduce vulnerabilities like activation engineering.

Method: The study visualizes linear subspaces in activation spaces and tests dimensional reduction to mitigate jailbreaking risks.

Result: Projecting representations onto lower dimensions reduces susceptibility to jailbreaking while preserving alignment.

Conclusion: High-dimensional representations in LLMs are a double-edged sword for safety, requiring careful management.

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [29] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner integrates a vision-language model (VLM) with a real-time planner for autonomous driving, improving decision-making by leveraging visual context and dynamic inference adjustments.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack visual context, which is crucial for robust decision-making in complex driving environments.

Method: VLMPlanner combines a learning-based planner with a VLM to process multi-view images and uses the CAI-Gate mechanism for dynamic inference frequency.

Result: Superior planning performance in complex scenarios on the nuPlan benchmark.

Conclusion: VLMPlanner enhances robustness and safety in autonomous driving by integrating visual reasoning and adaptive inference.

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [30] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: Proposes HAG-PS, a multi-agent reinforcement learning method for dynamic mobility resource allocation, addressing policy sharing and memory efficiency in urban settings.


<details>
  <summary>Details</summary>
Motivation: To rebalance mobility demand and supply by dynamically allocating resources like bikes/e-scooters and ride-sharing vehicles.

Method: Uses hierarchical adaptive grouping (HAG-PS) with global/local information, adaptive agent grouping, and learnable ID embeddings for specialization.

Result: Outperforms baselines in real-world NYC bike-sharing data (1.2M trips), improving bike availability.

Conclusion: HAG-PS effectively addresses dynamic policy sharing and memory efficiency for urban-scale mobility resource allocation.

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [31] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: The paper introduces MazeEval, a benchmark to evaluate LLMs' pure spatial reasoning in maze navigation tasks without visual cues, revealing performance disparities and cross-linguistic limitations.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' spatial reasoning abilities for reliable real-world deployment in robotics and embodied AI, especially without visual input.

Method: Uses a function-calling interface for LLMs to navigate mazes (5x5 to 15x15 grids) with coordinate feedback and distance-to-wall data, excluding visual input. Evaluates eight LLMs in English and Icelandic.

Result: Performance varies widely: OpenAI's O3 excels (perfect navigation up to 30x30 mazes), while others fail beyond 9x9 mazes due to looping. Icelandic performance is worse, suggesting linguistic dependency.

Conclusion: Spatial reasoning in LLMs is tied to training data and linguistic patterns, highlighting the need for architectural improvements for reliable cross-linguistic deployment.

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [32] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: A novel approach using General Artificial Intelligence Lifelong Learning Systems and Federated Layering Techniques (FLT) to enhance QoS in edge computing, improving efficiency, accuracy, and privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing increased data volume and complexity in 6G networks by focusing on QoS in edge computing.

Method: Proposes a federated layering-based small model collaborative mechanism with negotiation and debate among AI models, integrating privacy protection.

Result: Enhances learning efficiency, reasoning accuracy, and privacy protection in edge computing.

Conclusion: Provides a resilient solution for lifelong learning systems, significantly improving QoS in edge environments.

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [33] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention Network, improves traffic accident severity prediction by integrating spatial, temporal, and contextual data, achieving high performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of traffic accident severity is crucial for road safety, emergency response, and infrastructure design, but existing methods fail to model complex interdependencies among variables.

Method: STARN-GAT uses adaptive graph construction and modality-aware attention mechanisms to unify road network topology, temporal traffic patterns, and environmental context.

Result: The model achieves high scores (Macro F1: 85%, ROC-AUC: 0.91, recall: 81%) on the FARS dataset and generalizes well to the ARI-BUET dataset (Macro F1: 0.84, recall: 0.78, ROC-AUC: 0.89).

Conclusion: STARN-GAT effectively bridges advanced graph neural networks with practical road safety applications, offering interpretability and real-time potential.

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [34] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: A study evaluated the trustworthiness of LLM-powered AI agents under adversarial attacks, revealing widespread policy violations and proposing a benchmark for security assessment.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI agents can adhere to deployment policies in realistic, adversarial environments.

Method: Conducted a large-scale red-teaming competition with 1.8 million prompt-injection attacks on 22 AI agents across 44 scenarios.

Result: Over 60,000 attacks succeeded, exposing vulnerabilities like unauthorized data access and regulatory noncompliance. Most agents violated policies within 10-100 queries.

Conclusion: AI agents are vulnerable to adversarial misuse, and current defenses are insufficient. The ART benchmark aims to improve security assessments for safer deployment.

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [35] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA is a metacognitive LLM-driven architecture for Automatic Heuristic Design (AHD), evolving prompts instead of heuristic code, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve heuristic design by leveraging metacognitive principles and LLMs, moving beyond direct code evolution.

Method: Uses prompt evolution, a problem analyzer, error diagnosis, and metacognitive search to refine prompts iteratively.

Result: Generates more effective and robust heuristics, surpassing state-of-the-art methods in experiments.

Conclusion: Demonstrates the potential of cognitive science-inspired AI for robust and interpretable AHD.

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [36] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: GraphDPO is a novel approximate unlearning framework for knowledge graphs (KGs) that addresses incomplete forgetting and boundary knowledge preservation by reframing unlearning as a preference optimization problem and introducing boundary recall mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing KG unlearning methods face challenges in fully removing outdated or erroneous knowledge due to KG connectivity and local data focus, leading to incomplete forgetting and weakened boundary knowledge.

Method: GraphDPO reframes unlearning as a preference optimization problem using direct preference optimization (DPO) and introduces out-boundary sampling and boundary recall mechanisms to mitigate incomplete forgetting and preserve boundary knowledge.

Result: GraphDPO outperforms state-of-the-art baselines by up to 10.1% in MRR_Avg and 14.0% in MRR_F1 across eight datasets.

Conclusion: GraphDPO effectively addresses the limitations of existing unlearning methods in KGs, achieving superior performance in both forgetting targeted knowledge and preserving boundary knowledge.

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [37] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: Proposes an adaptive search algorithm for compressing Large Multimodal Models (LMMs) by optimizing sparsity and KV cache compression, achieving efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Compressing LMMs for edge device deployment is challenging; current methods lack efficiency or compromise accuracy.

Method: Uses Tree-structured Parzen Estimator to dynamically adjust pruning ratios and KV cache quantization, combining pruning with fast pruning and KV cache compression.

Result: Outperforms SparseGPT and Wanda on benchmarks (LLaVA-1.5 7B/13B), achieving memory efficiency with minimal performance loss.

Conclusion: The framework sets a new standard for LMM optimization, balancing efficiency and performance.

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [38] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: MoCME improves MMKGC by leveraging intra- and inter-modal complementarity and dynamic negative sampling, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address the imbalance and overlooked complementarity in multimodal knowledge graphs for robust entity representation.

Method: Proposes MoCME with CMKF for modality fusion and EGNS for dynamic negative sampling.

Result: Outperforms existing methods on five benchmark datasets.

Conclusion: MoCME effectively enhances entity representation and training robustness in MMKGC.

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [39] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: The paper proposes a novel convolutional architecture with adaptive fuzzified temporal data to improve spatio-temporal dependency and global information synthesis in time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Current forecasting models lack the ability to capture spatio-temporal dependency and synthesize global information during learning.

Method: The method includes improved fuzzy time series construction, a bilateral Atrous algorithm for reduced computation, and a partially asymmetric convolutional architecture for flexible feature mining.

Result: The proposed method achieves state-of-the-art results on popular time series datasets.

Conclusion: The approach effectively enhances forecasting accuracy by addressing spatio-temporal dependency and global information synthesis.

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [40] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: The paper introduces Dynamic MAPF (D-MAPF), a variant of the Multi-Agent Path Finding problem, addressing dynamic changes like agent arrivals/departures and obstacle movements. It proposes a general definition, a flexible framework, and an ASP-based method with 'tunnels' for efficient replanning. Experimental results evaluate its performance and solution quality.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from real-world applications, such as warehouses with human presence, requiring adaptable solutions for dynamic environments where agents and obstacles can change.

Method: The paper introduces a general D-MAPF definition, a multi-shot computation framework, and an ASP-based method using 'tunnels' to combine replanning and repairing approaches.

Result: Experimental evaluations highlight the method's computational performance and solution quality, showcasing its strengths and weaknesses.

Conclusion: The proposed D-MAPF framework and ASP-based method offer a flexible and efficient solution for dynamic environments, with potential applications in real-world scenarios like warehouses.

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [41] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: The paper proposes a framework for analyzing fairness in AI as a runtime property, using a coin-toss model to study monitoring and enforcement strategies under evolving biases.


<details>
  <summary>Details</summary>
Motivation: Traditional fairness studies treat fairness as static, but real-world AI systems operate dynamically, requiring runtime fairness analysis.

Method: A minimal coin-toss model is used to explore monitoring and enforcing fairness under evolving biases, with strategies parametrized by dynamics, prediction horizon, and confidence.

Result: General results are provided under simple assumptions, with existing solutions surveyed for Markovian/additive dynamics (monitoring) and static settings (enforcement).

Conclusion: Fairness in AI must adapt to dynamic environments, and the proposed framework offers flexible strategies for runtime analysis.

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [42] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*AndrÃ©s Holgado-SÃ¡nchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: The paper proposes a method for learning societal value systems using heuristic deep clustering, addressing the challenge of aligning AI with diverse human values.


<details>
  <summary>Details</summary>
Motivation: Aligning AI with human values is crucial for ethical AI, but manually eliciting and calibrating value systems is difficult. Societies consist of diverse groups with distinct value systems, not just aggregated individual values.

Method: The method uses heuristic deep clustering to learn shared value groundings and diverse societal value systems from qualitative human preference data.

Result: The method is evaluated in a real-world use case involving traveling decisions, demonstrating its effectiveness.

Conclusion: The approach provides a scalable way to model societal value systems, enhancing AI's alignment with human values.

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [43] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI-targeted voice calls improve listenership and health behaviors in maternal and child health programs.


<details>
  <summary>Details</summary>
Motivation: To determine if AI-driven listenership improvements translate to better health knowledge and behaviors.

Method: Used an AI model (restless bandit) to target beneficiaries for live service calls, then measured health behavior changes.

Result: AI interventions led to significant improvements in health behaviors (e.g., supplement intake) and knowledge.

Conclusion: AI can meaningfully enhance maternal and child health outcomes by boosting engagement and behavior changes.

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [44] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: The paper analyzes Chain-of-Thought (CoT) prompting's internal mechanisms, revealing it acts as a decoding space pruner and modulates neuron activation task-dependently.


<details>
  <summary>Details</summary>
Motivation: To understand the operational principles of CoT prompting, which enhances model reasoning but lacks mechanistic clarity.

Method: Reverse tracing of information flow across decoding, projection, and activation phases, with quantitative analysis.

Result: CoT serves as a decoding space pruner, guided by answer templates, and modulates neuron activation differently in open- vs. closed-domain tasks.

Conclusion: The findings provide a mechanistic interpretability framework and insights for designing more efficient and robust CoT prompts.

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [45] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: A framework called evalSmarT is introduced for evaluating smart contract comments using LLMs, addressing limitations of traditional metrics and human evaluation.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating smart contract comments (BLEU, ROUGE, human evaluation) are either inadequate or impractical.

Method: The paper presents evalSmarT, a modular framework combining 40 LLMs with 10 prompting strategies for scalable evaluation.

Result: Prompt design affects alignment with human judgment, and LLM-based evaluation is scalable and semantically rich.

Conclusion: evalSmarT provides a flexible and effective alternative for evaluating smart contract comment quality.

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [46] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG improves Retrieval-Augmented Generation by using scene graphs and multimodal knowledge graphs to enhance cross-modal reasoning and retrieval.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG lacks multimodal integration and logical structure, while existing multimodal RAG methods struggle with knowledge structure and generalization.

Method: MMGraphRAG refines visual content with scene graphs, builds a multimodal knowledge graph, and uses spectral clustering for cross-modal linking and reasoning-path retrieval.

Result: Achieves state-of-the-art performance on DocBench and MMLongBench, showing strong adaptability and clear reasoning.

Conclusion: MMGraphRAG effectively addresses multimodal RAG limitations by integrating structured knowledge and reasoning paths.

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [47] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: POMCGS is a new offline algorithm for solving large POMDPs by constructing a policy graph on the fly, reducing computations and enabling pre-execution validation. It outperforms previous offline methods and competes with online algorithms.


<details>
  <summary>Details</summary>
Motivation: Offline policies are preferred for POMDPs with time/energy constraints, but existing offline methods fail to scale. POMCGS addresses this gap.

Method: POMCGS folds a search tree into a policy graph dynamically, using action progressive widening and observation clustering to handle continuous POMDPs.

Result: POMCGS solves previously intractable POMDPs offline, producing policies competitive with state-of-the-art online methods.

Conclusion: POMCGS is a scalable, efficient offline solution for large POMDPs, bridging the gap between offline and online performance.

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [48] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: The paper proposes a model linking neural network depth to logical reasoning limits, showing each layer adds one level of logic. It proves depth bounds logical expressiveness, explaining phenomena like hallucination and repetition, and suggests future architectural improvements.


<details>
  <summary>Details</summary>
Motivation: To formally understand the reasoning limitations of large neural language models by grounding them in the depth of their architecture.

Method: Treats neural networks as linear operators over logic predicate space, analyzing how each layer's depth affects logical reasoning capabilities.

Result: Proves neural networks of a certain depth cannot represent higher-order logic predicates, explaining phenomena like hallucination and repetition.

Conclusion: The findings motivate architectural extensions and interpretability strategies for future language model development.

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [49] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench is a unified benchmark for evaluating hallucinations in LLM-based agents, introducing a taxonomy and scalable evaluation method.


<details>
  <summary>Details</summary>
Motivation: Address fragmented evaluations and lack of a principled testbed for hallucinative actions in LLM agents.

Method: Introduces a three-part taxonomy, audits benchmarks, synthesizes test cases, and uses LLM-as-a-Judge for evaluation.

Result: Provides actionable insights on LLM agent failures and scalable assessment of hallucinations.

Conclusion: Lays groundwork for mitigating hallucinations in interactive LLM-agent scenarios.

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [50] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia BalÃ¡Å¾ovÃ¡,Richard Comploi-Taupe,Susana Hahn,Nicolas RÃ¼hling,Gottfried Schenner*

Main category: cs.AI

TL;DR: The paper proposes an ASP-based solver for interactive product configuration, enhancing performance with smart expansion functions and a user interface.


<details>
  <summary>Details</summary>
Motivation: To address challenges in guiding users through large-scale industrial product configuration processes effectively.

Method: Enhances the classical incremental approach with four smart expansion functions, leveraging cautious and brave consequences to reduce search space and costly checks.

Result: Improved solving performance by limiting unsatisfiability checks and reducing search space.

Conclusion: The approach effectively supports interactive configuration with better performance and an intuitive user interface.

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [51] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper reviews self-evolving agents as a solution to the static nature of LLMs, focusing on what, when, and how to evolve, and highlights applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: LLMs are static and cannot adapt to novel tasks or dynamic contexts, limiting their effectiveness in interactive environments. This necessitates the development of self-evolving agents.

Method: The survey systematically reviews self-evolving agents, categorizing evolutionary mechanisms, adaptation methods, and designs (e.g., scalar rewards, multi-agent systems).

Result: It provides a structured framework for designing self-evolving agents, analyzes evaluation metrics, and highlights applications in coding, education, and healthcare.

Conclusion: The survey establishes a roadmap for advancing adaptive agentic systems, aiming to pave the way for Artificial Super Intelligence (ASI).

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: The paper introduces a transformer-based method to model urban mobility for shift workers, addressing their underrepresentation in traditional transportation data.


<details>
  <summary>Details</summary>
Motivation: Shift workers (15-20% of the workforce) are underrepresented in transportation surveys, leading to biased planning. This study aims to correct this gap.

Method: A novel transformer-based approach uses fragmented GPS data to generate complete activity patterns for shift workers, employing period-aware embeddings and a transition-focused loss function.

Result: The method achieves strong alignment with GPS data (Average JSD < 0.02), providing representative activity patterns for shift workers.

Conclusion: The approach offers a valuable tool for inclusive transportation planning by accurately modeling 24/7 mobility needs.

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [53] [VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets](https://arxiv.org/abs/2507.19844)
*Biswarup Mukherjee,Li Zhou,S. Gokul Krishnan,Milad Kabirifar,Subhash Lakshminarayana,Charalambos Konstantinou*

Main category: cs.LG

TL;DR: A model for coordinating prosumers in local energy markets using reinforcement learning and adversarial pricing strategies, showing financial losses for some prosumers but improved fairness in larger markets.


<details>
  <summary>Details</summary>
Motivation: To address the coordination of prosumers with diverse energy resources in dynamic local energy markets and explore the impact of adversarial pricing.

Method: Uses multi-agent deep deterministic policy gradient (MADDPG) for real-time decision-making and VAE-GAN for price manipulation.

Result: Prosumers, especially those without generation capabilities, face financial losses under adversarial pricing, but larger markets stabilize and improve fairness.

Conclusion: The model effectively coordinates prosumers but highlights vulnerabilities to adversarial pricing, with market size influencing fairness and stability.

Abstract: This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.

</details>


### [54] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: A lightweight dual-path Spatiotemporal Network improves traffic forecasting by combining sLSTM for temporal modeling and Conv3D for spatial feature extraction, outperforming ConvLSTM with 23% lower MAE and 30% better generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate traffic forecasting is crucial for 5G+ resource management, but existing methods struggle with complex spatiotemporal patterns due to user mobility.

Method: Proposes a dual-path network: sLSTM for temporal modeling and Conv3D for spatial features, fused into a cohesive representation.

Result: Achieves 23% lower MAE than ConvLSTM and 30% better generalization on real-world datasets.

Conclusion: The method is effective for large-scale deployments, offering robust forecasting and improved convergence.

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [55] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: A spectral learning framework replaces neural layers with wavelet-domain operations, achieving competitive performance with fewer parameters and memory, while enabling faster convergence and lower inference costs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of traditional neural models (e.g., Transformers) by leveraging spectral sparsity and wavelet transforms for compact, interpretable, and efficient learning.

Method: The model operates entirely in the wavelet domain, applying learnable nonlinear transformations (soft-thresholding, gain-phase modulation) and adaptive wavelet basis selection (Haar, Daubechies, Biorthogonal). Implemented in PyTorch with 3D support, it avoids spatial convolutions or attention.

Result: Achieves 89.3% accuracy on SST-2 (vs. 90.1% for a 4-layer Transformer) with 72% fewer parameters, 58% less memory, and faster convergence. Linear-time wavelet transforms reduce inference costs.

Conclusion: The framework demonstrates the viability of spectral learning for vision and language tasks, offering a principled alternative to overparameterized neural models.

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [56] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: Deep learning models outperform traditional methods (ARIMA, ETS) in predicting Influenza A outbreaks, with Transformers showing the best performance.


<details>
  <summary>Details</summary>
Motivation: To compare traditional and deep learning models for predicting Influenza A outbreaks, given its significant annual mortality and the need for better forecasting tools.

Method: Comparative analysis using historical data (2009-2023) of traditional models (ARIMA, ETS) vs. six deep learning architectures (Simple RNN, LSTM, GRU, BiLSTM, BiGRU, Transformer).

Result: Deep learning models, especially Transformers, outperformed traditional models, with lower MSE and MAE values.

Conclusion: Deep learning enhances infectious disease prediction, suggesting its integration into public health systems for real-time forecasting and preparedness.

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [57] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: BikeVAE-GNN, a dual-task framework combining Hybrid-GNN and VAE, improves bicycle volume estimation in sparse networks, outperforming baselines with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate bicycle volume estimation is crucial for urban planning but hindered by sparse data in global bicycling networks.

Method: Uses a Hybrid-GNN (GCN, GAT, GraphSAGE) with VAE to model spatial relationships and enrich graph structure, performing regression and classification tasks.

Result: Achieves MAE of 30.82 bicycles/day, 99% accuracy, and F1-score of 0.99 on Melbourne data with 99% sparsity.

Conclusion: BikeVAE-GNN advances sparse network bicycle volume estimation, offering insights for sustainable infrastructure.

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [58] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: Proposes an efficient GNN-based method for subgraph matching in large circuits, outperforming traditional and existing deep learning approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and node-to-node matching methods are limited in generalization and efficiency for large circuits, while existing deep learning models fail to capture global subgraph embeddings effectively.

Method: Uses GNNs to predict high-probability regions for target circuits, constructs negative samples for accurate learning, and directly extracts global subgraph embeddings.

Result: Significantly improves time efficiency and target region prediction compared to existing methods.

Conclusion: Offers a scalable and effective solution for subgraph matching in large-scale circuits.

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [59] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: The paper proposes using the Modal Assurance Criterion (MAC) to select features for transfer learning in structural health monitoring (SHM) to address data scarcity and distribution differences between structures.


<details>
  <summary>Details</summary>
Motivation: Training data for SHM systems is costly and scarce, especially labeled data, and differences between structures hinder generalization of conventional machine learning methods.

Method: Leverages physics knowledge and the MAC to select features with invariant conditional distributions, enabling transfer learning across structures.

Result: The MAC correlates well with supervised metrics for joint-distribution similarity, indicating its effectiveness in selecting generalizable features.

Conclusion: The MAC-based feature selection method is validated through numerical and experimental studies, proving effective for SHM applications.

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [60] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: The paper explores using simpler ML models (logistic regression, k-nearest neighbors, random forest) for exoplanet discovery, showing promising results but noting biases. Data augmentation improves recall and precision.


<details>
  <summary>Details</summary>
Motivation: Manual exoplanet discovery is slow; ML can enhance efficiency. Existing ML models are complex, so simpler alternatives are explored.

Method: Tested logistic regression, k-nearest neighbors, and random forest on NASA's Kepler dataset. Used data augmentation to address biases.

Result: Initial results were promising but revealed biases. Data augmentation improved recall and precision, though accuracy varied by model.

Conclusion: Simpler ML models with data augmentation can effectively aid exoplanet discovery, improving fairness and generalization.

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [61] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: PINNs solve forward and inverse problems in differential equations by embedding prior knowledge (differential equations) into the loss function, improving performance and handling sparse data without overfitting.


<details>
  <summary>Details</summary>
Motivation: To leverage prior analytical knowledge (differential equations) in neural networks for solving complex differential equations and optimizing parameters efficiently.

Method: Use Physics-Informed Neural Networks (PINNs) with residuals of varying complexity (linear, quadratic, heat equation, etc.), implemented in Python using PyTorch.

Result: PINNs simultaneously solve forward and inverse problems by minimizing residuals and optimizing neural network weights and model parameters.

Conclusion: PINNs are effective for solving differential equations and parameter optimization, especially with sparse data, by integrating prior knowledge into the model.

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [62] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: ATGC-Gen, a transformer-based model, generates controllable DNA sequences by integrating biological signals, outperforming prior methods in controllability and relevance.


<details>
  <summary>Details</summary>
Motivation: To explore the application of language models (LMs) like GPT and BERT in DNA sequence generation, which remains underexplored despite their success in natural language.

Method: ATGC-Gen uses cross-modal encoding with decoder-only and encoder-only transformer architectures for flexible training and generation under autoregressive or masked recovery objectives.

Result: The model generates fluent, diverse, and biologically relevant sequences, showing improvements in controllability and functional relevance over prior methods.

Conclusion: ATGC-Gen demonstrates the potential of LMs in advancing programmable genomic design, with code publicly available.

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [63] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: The paper benchmarks traditional Autoencoders (AEs) against Kolmogorov-Arnold Networks (KANs) in tasks like reconstruction, generation, denoising, inpainting, and anomaly detection using cardiological signals.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of KAN-based architectures compared to traditional AEs, especially in medical signal processing tasks.

Method: Benchmarked Linear, Convolutional, and Variational AEs against their KAN counterparts with similar or fewer parameters, using the AbnormalHeartbeat dataset.

Result: Expected to show improved performance of KAN-based models in tasks like reconstruction, generation, denoising, inpainting, and anomaly detection.

Conclusion: KAN-based architectures may outperform traditional AEs in medical signal processing, offering potential advancements in deep learning applications.

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [64] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: MMCircuitEval is a multimodal benchmark for evaluating MLLMs in EDA tasks, addressing gaps in existing benchmarks with 3614 QA pairs across diverse circuit design stages.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs in EDA are narrow, limiting comprehensive evaluation. MMCircuitEval fills this gap by providing a detailed, expert-reviewed benchmark.

Method: The benchmark includes 3614 QA pairs from diverse sources, categorized by design stage, circuit type, abilities tested, and difficulty.

Result: Evaluations show performance gaps in MLLMs, especially in back-end design and complex computations.

Conclusion: MMCircuitEval is a foundational resource for advancing MLLMs in EDA, aiding real-world integration.

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [65] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: STAG is a self-supervised framework that quantizes graph structural information into discrete tokens for LLM-compatible graph learning, enabling zero-shot transfer without labeled data.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to embed graph structure into LLM-compatible formats, often losing details or requiring expensive alignment. Labeled data constraints further limit adaptability.

Method: STAG uses soft assignment and KL divergence-guided quantization to tokenize graph structures, avoiding manual verbalization or alignment. It supports both LLM-based and traditional learning.

Result: STAG achieves state-of-the-art performance in node classification benchmarks and works with various LLM architectures.

Conclusion: STAG bridges graph learning and LLMs effectively, offering a scalable and adaptable solution without dependency on labeled data.

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [66] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: The paper compares traditional graph algorithms and Graph Neural Networks (GNNs), showing GNNs outperform by 43-70% in accuracy for node classification and clustering. It also explores integration strategies between the two.


<details>
  <summary>Details</summary>
Motivation: Graph-structured data is common but challenging for traditional ML methods. The study aims to evaluate and compare classical graph algorithms and GNNs to advance graph representation learning.

Method: The study conducts comparative experiments on node classification and clustering tasks, analyzing performance differences between traditional algorithms and GNNs.

Result: GNNs achieve 43% to 70% higher accuracy than traditional methods in the evaluated tasks.

Conclusion: The paper highlights GNNs' superiority and explores integration strategies, providing theoretical guidance for future graph learning research.

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [67] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: The paper proposes an AI-based decision support system using meteorological data to predict maintenance risks for green hydrogen infrastructure in Oman, addressing the lack of historical operational data.


<details>
  <summary>Details</summary>
Motivation: The absence of historical maintenance and performance data for large-scale green hydrogen projects in desert environments creates a knowledge gap for risk assessment and planning.

Method: An AI decision support system is developed, leveraging meteorological data to create a Maintenance Pressure Index (MPI) for predicting infrastructure risks.

Result: The MPI tool enables temporal benchmarking and risk assessment, aiding regulatory and operational decisions despite missing historical data.

Conclusion: The proposed system enhances foresight in hydrogen infrastructure planning by integrating environmental risk factors into auction evaluations.

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [68] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: A comprehensive ML framework for BP prediction in ICUs addresses data leakage, uncertainty quantification, and cross-institutional validation, achieving clinically acceptable performance and enabling risk-stratified protocols.


<details>
  <summary>Details</summary>
Motivation: Current ML approaches for BP monitoring in ICUs lack external validation, uncertainty quantification, and data leakage prevention, limiting their reliability and deployment potential.

Method: The study combines Gradient Boosting, Random Forest, and XGBoost with 74 features across five physiological domains, implementing leakage prevention, quantile regression for uncertainty, and cross-validation between MIMIC-III and eICU databases.

Result: Internal validation met AAMI standards (SBP: RÂ²=0.86, RMSE=6.03 mmHg; DBP: RÂ²=0.49, RMSE=7.13 mmHg), but external validation showed 30% degradation. Uncertainty quantification provided valid prediction intervals (80.3% SBP, 79.9% DBP coverage).

Conclusion: The framework offers realistic deployment expectations for AI-assisted BP monitoring in ICUs, with publicly available source code for broader adoption.

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


### [69] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: FedDPG introduces a dynamic prompt generator for PLMs in federated learning, improving flexibility, privacy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address computational overhead and inflexibility in prompt-tuning for PLMs, while tackling FL challenges like privacy and resource constraints.

Method: Uses a dynamic prompt generator to create context-aware prompts, keeping PLM parameters frozen and reducing FL communication costs.

Result: Outperforms state-of-the-art methods in global model performance, reduces calculation time, and minimizes parameter transmission.

Conclusion: FedDPG effectively balances flexibility, privacy, and efficiency in federated learning for NLP tasks.

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [70] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: The paper proposes a data-driven approach using graph neural networks and Wikipedia embeddings to efficiently design new metallic glasses and other materials.


<details>
  <summary>Details</summary>
Motivation: Traditional material synthesis is slow and expensive, especially for metallic glasses, due to the need for optimal multi-element combinations. Data scarcity and poor encoding limit current machine learning methods.

Method: The study uses graph neural networks with material network representations, encoding node elements from Wikipedia via a language model. It leverages Wikipedia embeddings in multiple languages.

Result: The approach demonstrates potential for uncovering hidden material relationships and improving predictability in materials design.

Conclusion: This work introduces a novel AI-driven paradigm for discovering new amorphous materials, addressing limitations of traditional and statistical learning methods.

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [71] [Swift-Sarsa: Fast and Robust Linear Control](https://arxiv.org/abs/2507.19539)
*Khurram Javed,Richard S. Sutton*

Main category: cs.LG

TL;DR: SwiftTD is extended to control problems as Swift-Sarsa, outperforming existing methods on a new benchmark for linear on-policy control.


<details>
  <summary>Details</summary>
Motivation: To adapt SwiftTD's success in prediction tasks to control problems and address challenges in differentiating relevant signals from noise.

Method: Combines SwiftTD with True Online Sarsa(Î») to create Swift-Sarsa, tested on the operant conditioning benchmark.

Result: Swift-Sarsa effectively assigns credit to relevant signals without prior knowledge, handling noisy features robustly.

Conclusion: Swift-Sarsa enables scalable representation learning by efficiently managing noisy features, promising for complex control tasks.

Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD($\lambda$) and
TD($\lambda$) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy
reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the
$\textit{operant conditioning benchmark}$. The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.

</details>


### [72] [Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection](https://arxiv.org/abs/2507.19547)
*Pablo Peiro-Corbacho,Long Lin,Pablo Ãvila,Alejandro Carta-Bergaz,Ãngel Arenal,Carlos Sevilla-Salcedo,Gonzalo R. RÃ­os-MuÃ±oz*

Main category: cs.LG

TL;DR: A deep learning framework using convolutional autoencoders for unsupervised feature extraction from EGMs to detect AF drivers in ablation procedures.


<details>
  <summary>Details</summary>
Motivation: Current ablation therapies are often ineffective for persistent AF due to non-pulmonary vein drivers. Unsupervised learning can help identify these drivers.

Method: Convolutional autoencoders extract latent features from unipolar and bipolar EGMs. Downstream classifiers detect rotational/focal activity and EGM entanglement.

Result: Moderate performance in detecting rotational/focal activity (AUC 0.73-0.76) and high performance in identifying EGM entanglement (AUC 0.93).

Conclusion: The method enables real-time integration into clinical systems, showcasing unsupervised learning's potential for meaningful feature extraction from cardiac signals.

Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet
current ablation therapies, including pulmonary vein isolation, are frequently
ineffective in persistent AF due to the involvement of non-pulmonary vein
drivers. This study proposes a deep learning framework using convolutional
autoencoders for unsupervised feature extraction from unipolar and bipolar
intracavitary electrograms (EGMs) recorded during AF in ablation studies. These
latent representations of atrial electrical activity enable the
characterization and automation of EGM analysis, facilitating the detection of
AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients,
containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders
successfully learned latent representations with low reconstruction loss,
preserving the morphological features. The extracted embeddings allowed
downstream classifiers to detect rotational and focal activity with moderate
performance (AUC 0.73-0.76) and achieved high discriminative performance in
identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into
clinical electroanatomical mapping systems to assist in identifying
arrhythmogenic regions during ablation procedures. This work highlights the
potential of unsupervised learning to uncover physiologically meaningful
features from intracardiac signals.

</details>


### [73] [Harnessing intuitive local evolution rules for physical learning](https://arxiv.org/abs/2507.19561)
*Roie Ezraty,Menachem Stern,Shmuel M. Rubinstein*

Main category: cs.LG

TL;DR: A training scheme for physical systems (BEASTS) minimizes power dissipation by controlling only boundary parameters, enabling autonomous learning with local physical rules.


<details>
  <summary>Details</summary>
Motivation: Address the computational intensity and high power consumption of traditional Machine Learning by exploring alternative physical implementations.

Method: Introduces BEASTAL, a scheme for Boundary-Enabled Adaptive State Tuning Systems, using local physical rules for learning without complex architectures.

Result: Demonstrates autonomous learning for regression and classification tasks, achieving best performance with non-linear local evolution rules.

Conclusion: BEASTAL advances physical learning by simplifying architectures and leveraging intuitive local rules, suitable for linear tasks with non-linear enhancements.

Abstract: Machine Learning, however popular and accessible, is computationally
intensive and highly power-consuming, prompting interest in alternative
physical implementations of learning tasks. We introduce a training scheme for
physical systems that minimize power dissipation in which only boundary
parameters (i.e. inputs and outputs) are externally controlled. Using this
scheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by
exploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the
closest analog of the Adaline algorithm for such systems. We demonstrate this
autonomous learning in silico for regression and classification tasks. Our
approach advances previous physical learning schemes by using intuitive, local
evolution rules without requiring large-scale memory or complex internal
architectures. BEASTAL can perform any linear task, achieving best performance
when the local evolution rule is non-linear.

</details>


### [74] [Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition](https://arxiv.org/abs/2507.19627)
*Zhengqi Lin,Andrzej RuszczyÅski*

Main category: cs.LG

TL;DR: An efficient federated dual decomposition algorithm for Wasserstein barycenter computation, avoiding local data access and matrix-vector operations, ensuring low complexity and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently computing Wasserstein barycenters of distributions without accessing local data or solving repeated mass transportation problems.

Method: Proposes a federated dual decomposition algorithm that uses highly aggregated information and avoids matrix-vector operations.

Result: The algorithm demonstrates low per-iteration complexity and scalability, outperforming state-of-the-art methods in examples of mixture models.

Conclusion: The proposed method is efficient, scalable, and avoids common computational bottlenecks, making it suitable for practical applications.

Abstract: We propose an efficient federated dual decomposition algorithm for
calculating the Wasserstein barycenter of several distributions, including
choosing the support of the solution. The algorithm does not access local data
and uses only highly aggregated information. It also does not require repeated
solutions to mass transportation problems. Because of the absence of any
matrix-vector operations, the algorithm exhibits a very low complexity of each
iteration and significant scalability. We illustrate its virtues and compare it
to the state-of-the-art methods on several examples of mixture models.

</details>


### [75] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: The paper proposes a system for dynamic orchestration of AI agent workloads on heterogeneous compute infrastructure, optimizing for cost and performance.


<details>
  <summary>Details</summary>
Motivation: AI agents are complex and dynamic, requiring scalable infrastructure for efficient deployment. Current systems lack the ability to optimize for heterogeneous hardware.

Method: The system includes a framework for planning and optimizing execution graphs, an MLIR-based compilation system, and dynamic orchestration for heterogeneous hardware.

Result: Preliminary results show significant TCO benefits, with some workloads performing similarly on mixed older and newer hardware compared to homogeneous setups.

Conclusion: The proposed system enables cost-effective scaling of AI agents by leveraging heterogeneous infrastructure, potentially extending the lifespan of existing hardware.

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [76] [Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions](https://arxiv.org/abs/2507.19639)
*Devroop Kar,Zimeng Lyu,Sheeraja Rajakrishnan,Hao Zhang,Alex Ororbia,Travis Desell,Daniel Krutz*

Main category: cs.LG

TL;DR: Proposed four novel loss functions for stock trading strategies, outperforming reinforcement learning methods with significant profits.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of stock market volatility by improving trading decision-making.

Method: Developed loss functions for training neural networks, tested on S&P 500 stocks using Crossformer.

Result: Achieved consistent profits (48-51%) outperforming PPO, DDPG, and buy-and-hold.

Conclusion: Novel loss functions enable effective trading strategies, demonstrating superior performance in volatile markets.

Abstract: Stock trading has always been a challenging task due to the highly volatile
nature of the stock market. Making sound trading decisions to generate profit
is particularly difficult under such conditions. To address this, we propose
four novel loss functions to drive decision-making for a portfolio of stocks.
These functions account for the potential profits or losses based with respect
to buying or shorting respective stocks, enabling potentially any artificial
neural network to directly learn an effective trading strategy. Despite the
high volatility in stock market fluctuations over time, training time-series
models such as transformers on these loss functions resulted in trading
strategies that generated significant profits on a portfolio of 50 different
S&P 500 company stocks as compared to a benchmark reinforcment learning
techniques and a baseline buy and hold method. As an example, using 2021, 2022
and 2023 as three test periods, the Crossformer model adapted with our best
loss function was most consistent, resulting in returns of 51.42%, 51.04% and
48.62% respectively. In comparison, the best performing state-of-the-art
reinforcement learning methods, PPO and DDPG, only delivered maximum profits of
around 41%, 2.81% and 41.58% for the same periods. The code is available at
https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.

</details>


### [77] [Feature learning is decoupled from generalization in high capacity neural networks](https://arxiv.org/abs/2507.19680)
*Niclas Alexander GÃ¶ring,Charles London,Abdurrahman Hadi Erturk,Chris Mingard,Yoonsoo Nam,Ard A. Louis*

Main category: cs.LG

TL;DR: Neural networks outperform kernel methods due to feature learning, but current theories focus on feature learning strength, not feature quality, limiting generalization theories.


<details>
  <summary>Details</summary>
Motivation: To understand why neural networks outperform kernel methods and to evaluate the quality of learned features, not just the strength of feature learning.

Method: Introduce 'feature quality' as a measure, analyze existing feature learning theories, and demonstrate their focus on strength rather than quality.

Result: Current theories assess feature learning strength but fail to address feature quality, hindering generalization theories.

Conclusion: A new focus on feature quality is needed to advance theories of neural network generalization.

Abstract: Neural networks outperform kernel methods, sometimes by orders of magnitude,
e.g. on staircase functions. This advantage stems from the ability of neural
networks to learn features, adapting their hidden representations to better
capture the data. We introduce a concept we call feature quality to measure
this performance improvement. We examine existing theories of feature learning
and demonstrate empirically that they primarily assess the strength of feature
learning, rather than the quality of the learned features themselves.
Consequently, current theories of feature learning do not provide a sufficient
foundation for developing theories of neural network generalization.

</details>


### [78] [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
*Bermet Burkanova,Payam Jome Yazdian,Chuxuan Zhang,Trinity Evans,Paige TuttÃ¶sÃ­,Angelica Lim*

Main category: cs.LG

TL;DR: CoMPAS3D is a large, diverse salsa dance motion capture dataset designed to advance interactive, expressive humanoid AI, featuring 3 hours of dances with expert annotations and benchmarking tasks for leader/follower and duet generation.


<details>
  <summary>Details</summary>
Motivation: Human communication extends beyond text to embodied movement and coordination, yet current AI systems lack capabilities in such interactive, physical tasks. CoMPAS3D addresses this gap by providing a rich dataset for modeling coupled humanoid interactions.

Method: The dataset includes 3 hours of salsa dances from 18 dancers of varying skill levels, annotated with move types, errors, and styles. It benchmarks tasks like leader/follower generation and duet generation, paralleling spoken language processing.

Result: CoMPAS3D offers the first fine-grained salsa annotations and a multitask SalsaAgent model, enabling research in interactive AI and expressive motion generation.

Conclusion: CoMPAS3D and its resources aim to advance socially interactive embodied AI, fostering creative and expressive humanoid motion generation for partner dance with humans.

Abstract: Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

</details>


### [79] [KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System](https://arxiv.org/abs/2507.19686)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: KD-GAT, a CAN intrusion detection framework, combines Graph Attention Networks and knowledge distillation for high accuracy and low complexity.


<details>
  <summary>Details</summary>
Motivation: The CAN protocol lacks security, making it vulnerable to attacks. A lightweight yet accurate detection method is needed.

Method: CAN traffic is modeled as graphs. A teacher GAT trains a compact student GAT via supervised pretraining and distillation.

Result: High accuracy (99.97% and 99.31%) on two datasets; reduced performance on an imbalanced dataset.

Conclusion: KD-GAT is effective but needs improvement for imbalanced data.

Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle
communication but lacks inherent security mechanisms, making it vulnerable to
cyberattacks. This paper introduces KD-GAT, an intrusion detection framework
that combines Graph Attention Networks (GATs) with knowledge distillation (KD)
to enhance detection accuracy while reducing computational complexity. In our
approach, CAN traffic is represented as graphs using a sliding window to
capture temporal and relational patterns. A multi-layer GAT with jumping
knowledge aggregation acting as the teacher model, while a compact student
GAT--only 6.32% the size of the teacher--is trained via a two-phase process
involving supervised pretraining and knowledge distillation with both soft and
hard label supervision. Experiments on three benchmark datasets--Car-Hacking,
Car-Survival, and can-train-and-test demonstrate that both teacher and student
models achieve strong results, with the student model attaining 99.97% and
99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,
significant class imbalance in can-train-and-test has led to reduced
performance for both models on this dataset. Addressing this imbalance remains
an important direction for future work.

</details>


### [80] [NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology](https://arxiv.org/abs/2507.19697)
*Yazeed Alrubyli,Omar Alomeir,Abrar Wafa,DiÃ¡na HidvÃ©gi,Hend Alrasheed,Mohsen Bahrami*

Main category: cs.LG

TL;DR: The paper introduces NAICS-aware GraphSAGE, a graph neural network that uses business taxonomy to predict co-visitation patterns, outperforming traditional spatial models.


<details>
  <summary>Details</summary>
Motivation: Understanding co-visitation patterns is vital for urban planning and retail analytics, but existing methods fail due to data sparsity and lack of business semantics.

Method: The proposed method integrates business taxonomy (NAICS codes) into GraphSAGE, combining spatial, temporal, and socioeconomic features for scalable prediction.

Result: The model achieves a 157% improvement in R-squared (0.243 to 0.625) and 32% better ranking quality (NDCG@10) on a large dataset of 94.9M co-visitation records.

Conclusion: Incorporating business semantics significantly enhances co-visitation prediction, demonstrating the value of combining spatial and business relationship data.

Abstract: Understanding where people go after visiting one business is crucial for
urban planning, retail analytics, and location-based services. However,
predicting these co-visitation patterns across millions of venues remains
challenging due to extreme data sparsity and the complex interplay between
spatial proximity and business relationships. Traditional approaches using only
geographic distance fail to capture why coffee shops attract different customer
flows than fine dining restaurants, even when co-located. We introduce
NAICS-aware GraphSAGE, a novel graph neural network that integrates business
taxonomy knowledge through learnable embeddings to predict population-scale
co-visitation patterns. Our key insight is that business semantics, captured
through detailed industry codes, provide crucial signals that pure spatial
models cannot explain. The approach scales to massive datasets (4.2 billion
potential venue pairs) through efficient state-wise decomposition while
combining spatial, temporal, and socioeconomic features in an end-to-end
framework. Evaluated on our POI-Graph dataset comprising 94.9 million
co-visitation records across 92,486 brands and 48 US states, our method
achieves significant improvements over state-of-the-art baselines: the
R-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with
strong gains in ranking quality (32 percent improvement in NDCG at 10).

</details>


### [81] [Disjoint Generative Models](https://arxiv.org/abs/2507.19700)
*Anton Danholt Lautrup,Muhammad Rajabinasab,Tobias Hyrup,Arthur Zimek,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: A framework for generating synthetic datasets using disjoint generative models, enhancing privacy with minimal utility loss.


<details>
  <summary>Details</summary>
Motivation: To improve privacy in synthetic data generation by partitioning datasets and using separate generative models.

Method: Partition datasets into disjoint subsets, apply separate generative models, and combine results post hoc without common identifiers.

Result: Demonstrated success in case studies, showing increased privacy with low utility cost and feasibility for certain models.

Conclusion: Disjoint generative models offer significant privacy benefits and practical advantages for synthetic data generation.

Abstract: We propose a new framework for generating cross-sectional synthetic datasets
via disjoint generative models. In this paradigm, a dataset is partitioned into
disjoint subsets that are supplied to separate instances of generative models.
The results are then combined post hoc by a joining operation that works in the
absence of common variables/identifiers. The success of the framework is
demonstrated through several case studies and examples on tabular data that
helps illuminate some of the design choices that one may make. The principal
benefit of disjoint generative models is significantly increased privacy at
only a low utility cost. Additional findings include increased effectiveness
and feasibility for certain model types and the possibility for mixed-model
synthesis.

</details>


### [82] [Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search](https://arxiv.org/abs/2507.19715)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: The paper introduces semantic compression for vector retrieval, prioritizing diversity and coverage over traditional top-k ANN search, using submodular optimization and graph-augmented methods.


<details>
  <summary>Details</summary>
Motivation: Traditional ANN search often produces redundant results, lacking the diversity and context needed for applications like RAG and multi-hop QA.

Method: Proposes semantic compression via submodular optimization and graph-augmented vector retrieval (e.g., kNN or knowledge graphs) for multi-hop, context-aware search.

Result: The approach generalizes top-k retrieval, improves semantic coverage, and addresses high-dimensional concentration limitations.

Conclusion: The work lays a foundation for meaning-centric vector search systems, advocating hybrid indexing and diversity-aware querying, with open implementation for future research.

Abstract: Vector databases typically rely on approximate nearest neighbor (ANN) search
to retrieve the top-k closest vectors to a query in embedding space. While
effective, this approach often yields semantically redundant results, missing
the diversity and contextual richness required by applications such as
retrieval-augmented generation (RAG), multi-hop QA, and memory-augmented
agents. We introduce a new retrieval paradigm: semantic compression, which aims
to select a compact, representative set of vectors that captures the broader
semantic structure around a query. We formalize this objective using principles
from submodular optimization and information geometry, and show that it
generalizes traditional top-k retrieval by prioritizing coverage and diversity.
To operationalize this idea, we propose graph-augmented vector retrieval, which
overlays semantic graphs (e.g., kNN or knowledge-based links) atop vector
spaces to enable multi-hop, context-aware search. We theoretically analyze the
limitations of proximity-based retrieval under high-dimensional concentration
and highlight how graph structures can improve semantic coverage. Our work
outlines a foundation for meaning-centric vector search systems, emphasizing
hybrid indexing, diversity-aware querying, and structured semantic retrieval.
We make our implementation publicly available to foster future research in this
area.

</details>


### [83] [Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning](https://arxiv.org/abs/2507.19737)
*Yinzhou Tang,Huandong Wang,Xiaochen Fan,Yong Li*

Main category: cs.LG

TL;DR: DisasterMobLLM is a framework for predicting human mobility in disaster scenarios, improving accuracy by 32.8% (Acc@1) and 35.0% (F1-score) over baselines.


<details>
  <summary>Details</summary>
Motivation: Urbanization and climate change increase cities' vulnerability to disasters, necessitating better mobility prediction for early warnings and resource allocation. Existing models fail in disaster scenarios due to shifted mobility patterns.

Method: DisasterMobLLM integrates LLMs to model mobility intentions and transfer disaster knowledge between cities. It uses a RAG-Enhanced Intention Predictor, LLM-based Intention Refiner, and Intention-Modulated Location Predictor.

Result: Achieves 32.8% improvement in Acc@1 and 35.0% in F1-score for immobility prediction compared to baselines.

Conclusion: DisasterMobLLM effectively addresses the gap in disaster scenario mobility prediction, offering significant improvements over existing methods.

Abstract: The vulnerability of cities to natural disasters has increased with
urbanization and climate change, making it more important to predict human
mobility in the disaster scenarios for downstream tasks including
location-based early disaster warning and pre-allocating rescue resources, etc.
However, existing human mobility prediction models are mainly designed for
normal scenarios, and fail to adapt to disaster scenarios due to the shift of
human mobility patterns under disaster. To address this issue, we introduce
\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios
that can be integrated into existing deep mobility prediction methods by
leveraging LLMs to model the mobility intention and transferring the common
knowledge of how different disasters affect mobility intentions between cities.
This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next
intention, refines it with an LLM-based Intention Refiner, and then maps the
intention to an exact location using an Intention-Modulated Location Predictor.
Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\%
improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score
of predicting immobility compared to the baselines. The code is available at
https://github.com/tsinghua-fib-lab/DisasterMobLLM.

</details>


### [84] [Modeling enzyme temperature stability from sequence segment perspective](https://arxiv.org/abs/2507.19755)
*Ziqi Zhang,Shiheng Chen,Runze Yang,Zhisheng Wei,Wei Zhang,Lei Wang,Zhanzhi Liu,Fengshan Zhang,Jing Wu,Xiaoyong Pan,Hongbin Shen,Longbing Cao,Zhaohong Deng*

Main category: cs.LG

TL;DR: A novel deep learning framework, Segment Transformer, is introduced for predicting enzyme temperature stability, achieving state-of-the-art performance and successfully guiding enzyme engineering.


<details>
  <summary>Details</summary>
Motivation: Experimental determination of enzyme thermal stability is costly and time-consuming, while existing computational methods face data limitations.

Method: A curated dataset is used to train the Segment Transformer, a deep learning model leveraging segment-level representations for predicting thermal stability.

Result: The model achieves RMSE of 24.03, MAE of 18.09, and correlations of 0.33. Applied to cutinase enzyme, it improved heat resistance by 1.64-fold with 17 mutations.

Conclusion: The Segment Transformer effectively predicts enzyme thermal stability and aids in enzyme engineering, demonstrating practical utility.

Abstract: Developing enzymes with desired thermal properties is crucial for a wide
range of industrial and research applications, and determining temperature
stability is an essential step in this process. Experimental determination of
thermal parameters is labor-intensive, time-consuming, and costly. Moreover,
existing computational approaches are often hindered by limited data
availability and imbalanced distributions. To address these challenges, we
introduce a curated temperature stability dataset designed for model
development and benchmarking in enzyme thermal modeling. Leveraging this
dataset, we present the \textit{Segment Transformer}, a novel deep learning
framework that enables efficient and accurate prediction of enzyme temperature
stability. The model achieves state-of-the-art performance with an RMSE of
24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,
respectively. These results highlight the effectiveness of incorporating
segment-level representations, grounded in the biological observation that
different regions of a protein sequence contribute unequally to thermal
behavior. As a proof of concept, we applied the Segment Transformer to guide
the engineering of a cutinase enzyme. Experimental validation demonstrated a
1.64-fold improvement in relative activity following heat treatment, achieved
through only 17 mutations and without compromising catalytic function.

</details>


### [85] [Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation](https://arxiv.org/abs/2507.19771)
*Xin Zhang,Lissette Iturburu,Juan Nicolas Villamizar,Xiaoyu Liu,Manuel Salmeron,Shirley J. Dyke,Julio Ramirez*

Main category: cs.LG

TL;DR: A generative AI method using LLM and RAG automates structural drawing creation from natural language descriptions, reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: Structural drawings are labor-intensive despite software advancements. Automating this process can save time and reduce workload for engineers.

Method: Uses a large language model (LLM) with retrieval-augmented generation (RAG) to interpret natural language and generate AutoCAD drawings.

Result: Efficient conversion of natural language descriptions into AutoCAD drawings, streamlining the design process.

Conclusion: The AI-based method simplifies structural drawing generation, enhancing productivity and reducing manual effort.

Abstract: Structural drawings are widely used in many fields, e.g., mechanical
engineering, civil engineering, etc. In civil engineering, structural drawings
serve as the main communication tool between architects, engineers, and
builders to avoid conflicts, act as legal documentation, and provide a
reference for future maintenance or evaluation needs. They are often organized
using key elements such as title/subtitle blocks, scales, plan views, elevation
view, sections, and detailed sections, which are annotated with standardized
symbols and line types for interpretation by engineers and contractors. Despite
advances in software capabilities, the task of generating a structural drawing
remains labor-intensive and time-consuming for structural engineers. Here we
introduce a novel generative AI-based method for generating structural drawings
employing a large language model (LLM) agent. The method incorporates a
retrieval-augmented generation (RAG) technique using externally-sourced facts
to enhance the accuracy and reliability of the language model. This method is
capable of understanding varied natural language descriptions, processing these
to extract necessary information, and generating code to produce the desired
structural drawing in AutoCAD. The approach developed, demonstrated and
evaluated herein enables the efficient and direct conversion of a structural
drawing's natural language description into an AutoCAD drawing, significantly
reducing the workload compared to current working process associated with
manual drawing production, facilitating the typical iterative process of
engineers for expressing design ideas in a simplified way.

</details>


### [86] [AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines](https://arxiv.org/abs/2507.19803)
*Saram Abbas,Naeem Soomro,Rishad Shafik,Rakesh Heer,Kabita Adhikari*

Main category: cs.LG

TL;DR: An interpretable AI model using the Tsetlin Machine (TM) outperforms traditional methods in predicting bladder cancer recurrence, offering transparency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Bladder cancer recurrence rates are high, and current clinical tools like EORTC risk tables are outdated and unreliable, especially for intermediate-risk cases.

Method: The study uses the Tsetlin Machine (TM), a symbolic learner, to create a transparent, human-readable AI model. It was tested on the PHOTO trial dataset (n=330).

Result: TM achieved an F1-score of 0.80, outperforming XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). It provides interpretable clauses based on clinical features.

Conclusion: TM is a powerful, trustworthy decision-support tool for bladder cancer recurrence prediction, ready for real-world adoption due to its accuracy and transparency.

Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are
diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur
after treatment, triggering a relentless cycle of surgeries, monitoring, and
risk of progression. Clinical tools like the EORTC risk tables are outdated and
unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a
symbolic learner that outputs transparent, human-readable logic. Tested on the
PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming
XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the
exact clauses behind each prediction, grounded in clinical features like tumour
count, surgeon experience, and hospital stay - offering accuracy and full
transparency. This makes TM a powerful, trustworthy decision-support tool ready
for real-world adoption.

</details>


### [87] [Debunking Optimization Myths in Federated Learning for Medical Image Classification](https://arxiv.org/abs/2507.19822)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: Vanilla FL's performance is more affected by local optimizers and learning rates than by FL methods, with local training epochs' impact varying by method.


<details>
  <summary>Details</summary>
Motivation: To understand how local factors like optimizers and learning rates impact FL performance in medical imaging.

Method: Benchmarked recent FL methods on colorectal pathology and blood cell classification tasks, analyzing local configurations.

Result: Local optimizer and learning rate choices significantly affect performance; local training epochs' impact varies by FL method.

Conclusion: Edge-specific configurations are more critical than algorithmic complexity for effective FL.

Abstract: Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

</details>


### [88] [GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning](https://arxiv.org/abs/2507.19839)
*Tiantian Peng,Yuyang Liu,Shuo Yang,Qiuhe Hong,YongHong Tian*

Main category: cs.LG

TL;DR: GNSP is a continual learning method for CLIP that prevents catastrophic forgetting by projecting gradients onto the null space of prior knowledge, preserving zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: CLIP's zero-shot performance degrades during fine-tuning due to catastrophic forgetting and embedding misalignment.

Method: Proposes Gradient Null Space Projection (GNSP) and combines it with knowledge distillation and modality alignment preservation loss.

Result: Achieves SOTA on MTIL benchmark (11 tasks) and maintains CLIP's modality gap and cross-modal retrieval performance.

Conclusion: GNSP effectively preserves CLIP's generalization and alignment during continual learning.

Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot
generalization by aligning visual and textual modalities in a shared embedding
space. However, when continuously fine-tuned on diverse tasks, CLIP suffers
from catastrophic forgetting and degradation of its embedding alignment,
undermining its zero-shot capabilities. In this work, we propose Gradient Null
Space Projection (GNSP), an efficient continual learning method that projects
task-specific gradients onto the null space of previously learned knowledge.
This orthogonal projection mathematically prevents interference with previous
tasks without relying on rehearsal or architectural modification. Furthermore,
to preserve the inherent generalization property of CLIP, we introduce
knowledge distillation and combine it with a modality alignment preservation
loss inspired by CLIP pre-training to stabilize the structure of the multimodal
embedding space during fine-tuning. On the MTIL benchmark consisting of 11
tasks, our method achieved SOTA performance on both the Average and Last key
metrics. More importantly, experiments show that our method successfully
maintains the original modality gap and cross-modal retrieval performance of
CLIP, confirming its effectiveness in maintaining a robust visual-language
space throughout the continual learning process.

</details>


### [89] [A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets](https://arxiv.org/abs/2507.19846)
*Harish S,Chetana K Nayak,Joy Bose*

Main category: cs.LG

TL;DR: The paper proposes an ML-driven solution using clustering, supervised learning, and NLP to resolve telecom problem tickets, addressing challenges like data drift and missing data.


<details>
  <summary>Details</summary>
Motivation: To improve incident resolution in telecom billing systems by leveraging historical ticket data despite challenges like data drift and incomplete records.

Method: Combines clustering, supervised learning (LDA, Siamese networks, One-shot learning), and NLP (Index embedding) for resolution prediction. Includes a real-time dashboard and Kubernetes deployment.

Result: High prediction accuracy demonstrated on both open-source (Bitext) and proprietary telecom datasets.

Conclusion: The proposed solution effectively tackles ticket resolution challenges, offering a robust, scalable, and accurate ML-driven approach.

Abstract: Resolution of incidents or problem tickets is a common theme in service
industries in any sector, including billing and charging systems in telecom
domain. Machine learning can help to identify patterns and suggest resolutions
for the problem tickets, based on patterns in the historical data of the
tickets. However, this process may be complicated due to a variety of phenomena
such as data drift and issues such as missing data, lack of data pertaining to
resolutions of past incidents, too many similar sounding resolutions due to
free text and similar sounding text. This paper proposes a robust ML-driven
solution employing clustering, supervised learning, and advanced NLP models to
tackle these challenges effectively. Building on previous work, we demonstrate
clustering-based resolution identification, supervised classification with LDA,
Siamese networks, and One-shot learning, Index embedding. Additionally, we
present a real-time dashboard and a highly available Kubernetes-based
production deployment. Our experiments with both the open-source Bitext
customer-support dataset and proprietary telecom datasets demonstrate high
prediction accuracy.

</details>


### [90] [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
*Guanting Dong,Hangyu Mao,Kai Ma,Licheng Bao,Yifei Chen,Zhongyuan Wang,Zhongxia Chen,Jiazhen Du,Huiyang Wang,Fuzheng Zhang,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: ARPO is a novel RL algorithm for multi-turn LLM agents, balancing reasoning and tool interactions, outperforming existing methods with half the tool-use budget.


<details>
  <summary>Details</summary>
Motivation: Current RL algorithms fail to balance LLMs' reasoning and multi-turn tool interactions, limiting their effectiveness in realistic scenarios.

Method: ARPO introduces an entropy-based adaptive rollout mechanism and advantage attribution estimation to optimize stepwise tool-use interactions.

Result: ARPO outperforms trajectory-level RL algorithms across 13 benchmarks, achieving better performance with reduced tool-use budget.

Conclusion: ARPO offers a scalable solution for aligning LLM-based agents with dynamic environments, enhancing efficiency and performance.

Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

</details>


### [91] [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)
*Aditya Sharma,Linh Nguyen,Ananya Gupta,Chengyu Wang,Chiamaka Adebayo,Jakub Kowalski*

Main category: cs.LG

TL;DR: CWMI embeds causal physics in LLMs via a Causal Physics Module and Causal Intervention Loss, improving zero-shot physical reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs lack intuitive understanding of physical dynamics, limiting real-world causal reasoning.

Method: Introduces CWMI with a Causal Physics Module and Causal Intervention Loss to learn cause-and-effect from multimodal data.

Result: Outperforms state-of-the-art LLMs on zero-shot tasks like PIQA and PhysiCa-Bench.

Conclusion: Inducing a causal world model enhances AI reliability and generalizability.

Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities,
fundamentally lack an intuitive understanding of physical dynamics, which
limits their effectiveness in real-world scenarios that require causal
reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a
novel framework designed to embed an explicit model of causal physics within an
LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a
new training objective called Causal Intervention Loss, encouraging the model
to learn cause-and-effect relationships from multimodal data. By training the
model to predict the outcomes of hypothetical interventions instead of merely
capturing statistical correlations, CWMI develops a robust internal
representation of physical laws. Experimental results show that CWMI
significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning
tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench
dataset. These findings demonstrate that inducing a causal world model is a
critical step toward more reliable and generalizable AI systems.

</details>


### [92] [RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives](https://arxiv.org/abs/2507.19873)
*BjÃ¶rn Kischelewski,Benjamin Guedj,David Wahl*

Main category: cs.LG

TL;DR: RestoreAI uses AI to predict landmine risk from spatial patterns, improving clearance efficiency with linear, curved, and Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: Existing AI methods for landmine detection lack focus on spatial pattern-based risk prediction, which could enhance clearance efficiency.

Method: RestoreAI implements three deminers: linear (PCA-based), curved (principal curves), and Bayesian (incorporating expert knowledge).

Result: RestoreAI improved clearance efficiency by 14.37% in landmine detection and reduced time by 24.45% compared to baselines.

Conclusion: Pattern-based AI methods like RestoreAI significantly enhance landmine clearance, with linear patterns being as effective as curved ones.

Abstract: Landmine removal is a slow, resource-intensive process affecting over 60
countries. While AI has been proposed to enhance explosive ordnance (EO)
detection, existing methods primarily focus on object recognition, with limited
attention to prediction of landmine risk based on spatial pattern information.
This work aims to answer the following research question: How can AI be used to
predict landmine risk from landmine patterns to improve clearance time
efficiency? To that effect, we introduce RestoreAI, an AI system for
pattern-based risk estimation of remaining explosives. RestoreAI is the first
AI system that leverages landmine patterns for risk prediction, improving the
accuracy of estimating the residual risk of missing EO prior to land release.
We particularly focus on the implementation of three instances of RestoreAI,
respectively, linear, curved and Bayesian pattern deminers. First, the linear
pattern deminer uses linear landmine patterns from a principal component
analysis (PCA) for the landmine risk prediction. Second, the curved pattern
deminer uses curved landmine patterns from principal curves. Finally, the
Bayesian pattern deminer incorporates prior expert knowledge by using a
Bayesian pattern risk prediction. Evaluated on real-world landmine data,
RestoreAI significantly boosts clearance efficiency. The top-performing
pattern-based deminers achieved a 14.37 percentage point increase in the
average share of cleared landmines per timestep and required 24.45% less time
than the best baseline deminer to locate all landmines. Interestingly, linear
and curved pattern deminers showed no significant performance difference,
suggesting that more efficient linear patterns are a viable option for risk
prediction.

</details>


### [93] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,RenÃ© Schuster*

Main category: cs.LG

TL;DR: CLoRA introduces a parameter-efficient method for continual learning, reducing computational demands while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the computational constraints in real-world continual learning scenarios, where retraining large models is impractical.

Method: Uses Low-Rank Adaptation (LoRA) to fine-tune a small set of parameters across tasks, avoiding full model retraining.

Result: CLoRA matches or exceeds baseline performance while significantly reducing hardware requirements.

Conclusion: CLoRA is a resource-efficient solution for continual learning in constrained environments.

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of
catastrophic forgetting in neural networks, that arises when incrementally
learning a sequence of tasks. Current CL methods function within the confines
of limited data access, without any restrictions imposed on computational
resources. However, in real-world scenarios, the latter takes precedence as
deployed systems are often computationally constrained. A major drawback of
most CL methods is the need to retrain the entire model for each new task. The
computational demands of retraining large models can be prohibitive, limiting
the applicability of CL in environments with limited resources. Through CLoRA,
we explore the applicability of Low-Rank Adaptation (LoRA), a
parameter-efficient fine-tuning method for class-incremental semantic
segmentation. CLoRA leverages a small set of parameters of the model and uses
the same set for learning across all tasks. Results demonstrate the efficacy of
CLoRA, achieving performance on par with and exceeding the baseline methods. We
further evaluate CLoRA using NetScore, underscoring the need to factor in
resource efficiency and evaluate CL methods beyond task performance. CLoRA
significantly reduces the hardware requirements for training, making it
well-suited for CL in resource-constrained environments after deployment.

</details>


### [94] [A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction](https://arxiv.org/abs/2507.19894)
*Xiaohua Feng,Jiaming Zhang,Fengyuan Yu,Chengye Wang,Li Zhang,Kaixiang Li,Yuyuan Li,Chaochao Chen,Jianwei Yin*

Main category: cs.LG

TL;DR: The paper reviews Generative Model Unlearning (GenMU), proposes a unified framework for organizing research, and highlights challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in generative models by adapting machine unlearning techniques, despite the lack of a unified framework for comparison.

Method: Comprehensive review of GenMU, proposing a framework to categorize objectives, strategies, and metrics, and exploring connections to related techniques.

Result: Identifies gaps in current research, provides a systematic framework, and highlights practical applications of unlearning techniques.

Conclusion: The paper lays a foundation for future GenMU research, emphasizing the need for standardized evaluation and integration with related fields.

Abstract: With the rapid advancement of generative models, associated privacy concerns
have attracted growing attention. To address this, researchers have begun
adapting machine unlearning techniques from traditional classification models
to generative settings. Although notable progress has been made in this area, a
unified framework for systematically organizing and integrating existing work
is still lacking. The substantial differences among current studies in terms of
unlearning objectives and evaluation protocols hinder the objective and fair
comparison of various approaches. While some studies focus on specific types of
generative models, they often overlook the commonalities and systematic
characteristics inherent in Generative Model Unlearning (GenMU). To bridge this
gap, we provide a comprehensive review of current research on GenMU and propose
a unified analytical framework for categorizing unlearning objectives,
methodological strategies, and evaluation metrics. In addition, we explore the
connections between GenMU and related techniques, including model editing,
reinforcement learning from human feedback, and controllable generation. We
further highlight the potential practical value of unlearning techniques in
real-world applications. Finally, we identify key challenges and outline future
research directions aimed at laying a solid foundation for further advancements
in this field. We consistently maintain the related open-source materials at
https://github.com/caxLee/Generative-model-unlearning-survey.

</details>


### [95] [Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks](https://arxiv.org/abs/2507.19964)
*Kunhao Li,Di Wu,Jun Bai,Jing Xu,Lei Yang,Ziyi Zhang,Yiliao Song,Wencheng Yang,Taotao Cai,Yan Li*

Main category: cs.LG

TL;DR: The paper studies cross-client membership inference attacks (CC-MIA) in federated GNNs, revealing privacy risks in node classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address privacy threats in federated GNNs, focusing on sample-to-client attribution, a unique risk in decentralized settings.

Method: A general attack framework exploiting aggregation behaviors, gradient updates, and embedding proximity to link samples to clients.

Result: High performance in membership inference and ownership identification across realistic FL setups.

Conclusion: Highlights a new privacy threat in federated graph learning, urging the need for robust GNN designs.

Abstract: Graph-structured data is prevalent in many real-world applications, including
social networks, financial systems, and molecular biology. Graph Neural
Networks (GNNs) have become the de facto standard for learning from such data
due to their strong representation capabilities. As GNNs are increasingly
deployed in federated learning (FL) settings to preserve data locality and
privacy, new privacy threats arise from the interaction between graph
structures and decentralized training. In this paper, we present the first
systematic study of cross-client membership inference attacks (CC-MIA) against
node classification tasks of federated GNNs (FedGNNs), where a malicious client
aims to infer which client owns the given data. Unlike prior
centralized-focused work that focuses on whether a sample was included in
training, our attack targets sample-to-client attribution, a finer-grained
privacy risk unique to federated settings. We design a general attack framework
that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding
proximity to link samples to their source clients across training rounds. We
evaluate our attack across multiple graph datasets under realistic FL setups.
Results show that our method achieves high performance on both membership
inference and ownership identification. Our findings highlight a new privacy
threat in federated graph learning-client identity leakage through structural
and model-level cues, motivating the need for attribution-robust GNN design.

</details>


### [96] [Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training](https://arxiv.org/abs/2507.19968)
*Yue Hu,Zanxia Cao,Yingchao Liu*

Main category: cs.LG

TL;DR: DEO is a novel first-order optimization method inspired by the Dimer technique from molecular dynamics. It estimates curvature without full Hessian computation, helping escape saddle points and flat regions in neural network training.


<details>
  <summary>Details</summary>
Motivation: First-order methods like SGD and Adam struggle with complex loss landscapes (flat regions, plateaus, saddle points). Second-order methods are computationally expensive. DEO bridges this gap by efficiently estimating curvature.

Method: DEO adapts the Dimer method to approximate the Hessian's smallest eigenvector using gradient information. It projects gradients orthogonally to escape saddle points and flat regions.

Result: Preliminary experiments on a Transformer model show DEO competes with standard first-order methods, improving navigation of complex loss landscapes.

Conclusion: DEO successfully repurposes physics-inspired curvature estimation to enhance neural network training, offering a computationally feasible alternative to second-order methods.

Abstract: First-order optimization methods, such as SGD and Adam, are widely used for
training large-scale deep neural networks due to their computational efficiency
and robust performance. However, relying solely on gradient information, these
methods often struggle to navigate complex loss landscapes with flat regions,
plateaus, and saddle points. Second-order methods, which use curvature
information from the Hessian matrix, can address these challenges but are
computationally infeasible for large models. The Dimer method, a first-order
technique that constructs two closely spaced points to probe the local geometry
of a potential energy surface, efficiently estimates curvature using only
gradient information. Inspired by its use in molecular dynamics simulations for
locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel
framework to escape saddle points in neural network training. DEO adapts the
Dimer method to explore a broader region of the loss landscape, approximating
the Hessian's smallest eigenvector without computing the full matrix. By
periodically projecting the gradient onto the subspace orthogonal to the
minimum curvature direction, DEO guides the optimizer away from saddle points
and flat regions, enhancing training efficiency with non-stepwise updates.
Preliminary experiments on a Transformer toy model show DEO achieves
competitive performance compared to standard first-order methods, improving
navigation of complex loss landscapes. Our work repurposes physics-inspired,
first-order curvature estimation to enhance neural network training in
high-dimensional spaces.

</details>


### [97] [Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost](https://arxiv.org/abs/2507.20008)
*Padmavathi Moorthy*

Main category: cs.LG

TL;DR: The study compares GAT, XGBoost, and TimesNet for taxi fare prediction using a large dataset, analyzing data quality impact and model robustness.


<details>
  <summary>Details</summary>
Motivation: Precise fare prediction is vital for ride-hailing and urban mobility, requiring robust models under real-world conditions.

Method: Evaluated three models (GAT, XGBoost, TimesNet) on raw and denoised data, assessing accuracy, calibration, OOD robustness, and feature sensitivity. Pre-processing included KNN imputation, noise injection, and autoencoder denoising.

Result: Revealed differences between classical and deep learning models, providing guidelines for robust fare prediction systems.

Conclusion: The study offers practical insights for building scalable and robust fare prediction models in urban mobility.

Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban
mobility systems. This study examines three machine learning models-Graph
Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive
capabilities for taxi fares using a real-world dataset comprising over 55
million records. Both raw (noisy) and denoised versions of the dataset are
analyzed to assess the impact of data quality on model performance. The study
evaluated the models along multiple axes, including predictive accuracy,
calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and
feature sensitivity. We also explore pre-processing strategies, including KNN
imputation, Gaussian noise injection, and autoencoder-based denoising. The
study reveals critical differences between classical and deep learning models
under realistic conditions, offering practical guidelines for building robust
and scalable models in urban fare prediction systems.

</details>


### [98] [FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging](https://arxiv.org/abs/2507.20016)
*Liu junkang,Yuanyuan Liu,Fanhua Shang,Hongying Liu,Jin Liu,Wei Feng*

Main category: cs.LG

TL;DR: The paper proposes FedSWA and FedMoSWA, two federated learning algorithms designed to improve generalization in highly heterogeneous data settings, outperforming FedSAM and FedAvg.


<details>
  <summary>Details</summary>
Motivation: To address the poor generalization of FedSAM in highly heterogeneous data and improve FL performance.

Method: Introduces FedSWA (Stochastic Weight Averaging) and FedMoSWA (momentum-based stochastic controlled weight averaging) to find flatter minima and align local-global models.

Result: Theoretical analysis shows better convergence and generalization bounds for FedSWA and FedMoSWA. Empirical results on CIFAR10/100 and Tiny ImageNet confirm superiority.

Conclusion: FedSWA and FedMoSWA are effective for FL in heterogeneous data, offering better optimization and generalization than existing methods.

Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization
capability is crucial for real-word applications. In this paper, we revisit the
generalization problem in FL and investigate the impact of data heterogeneity
on FL generalization. We find that FedSAM usually performs worse than FedAvg in
the case of highly heterogeneous data, and thus propose a novel and effective
federated learning algorithm with Stochastic Weight Averaging (called
\texttt{FedSWA}), which aims to find flatter minima in the setting of highly
heterogeneous data. Moreover, we introduce a new momentum-based stochastic
controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed
to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds
for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization
and generalization errors of \texttt{FedMoSWA} are smaller than those of their
counterparts, including FedSAM and its variants. Empirically, experimental
results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the
proposed algorithms compared to their counterparts. Open source code at:
https://github.com/junkangLiu0/FedSWA.

</details>


### [99] [Irredundant $k$-Fold Cross-Validation](https://arxiv.org/abs/2507.20048)
*Jesus S. Aguilar-Ruiz*

Main category: cs.LG

TL;DR: Introduces Irredundant k-fold cross-validation, ensuring each instance is used once for training and once for testing, reducing redundancy and computational cost.


<details>
  <summary>Details</summary>
Motivation: Addresses redundancy and overfitting in traditional k-fold cross-validation by balancing dataset utilization.

Method: Irredundant k-fold cross-validation guarantees non-overlapping training partitions, ensuring each instance is used once for training and once for testing.

Result: Delivers consistent performance estimates with less optimistic variance and reduced computational cost compared to traditional k-fold.

Conclusion: The method improves model evaluation by mitigating overfitting and enabling sharper comparative analysis while remaining model-agnostic.

Abstract: In traditional k-fold cross-validation, each instance is used ($k-1$) times
for training and once for testing, leading to redundancy that lets many
instances disproportionately influence the learning phase. We introduce
Irredundant $k$-fold cross-validation, a novel method that guarantees each
instance is used exactly once for training and once for testing across the
entire validation procedure. This approach ensures a more balanced utilization
of the dataset, mitigates overfitting due to instance repetition, and enables
sharper distinctions in comparative model analysis. The method preserves
stratification and remains model-agnostic, i.e., compatible with any
classifier. Experimental results demonstrate that it delivers consistent
performance estimates across diverse datasets -- comparable to $k$-fold
cross-validation -- while providing less optimistic variance estimates because
training partitions are non-overlapping, and significantly reducing the overall
computational cost.

</details>


### [100] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: $K^4$ is a fast, unsupervised, parser-independent Log Anomaly Detection framework using compact 4D descriptors for high-performance online detection.


<details>
  <summary>Details</summary>
Motivation: Existing LogAD methods are slow, parsing-dependent, and use unrealistic evaluation protocols.

Method: $K^4$ transforms log embeddings into 4D descriptors (Precision, Recall, Density, Coverage) using k-NN statistics, enabling lightweight anomaly scoring.

Result: $K^4$ achieves AUROC 0.995-0.999, outperforms baselines significantly, and is orders of magnitude faster (training <4s, inference ~4Âµs).

Conclusion: $K^4$ sets a new state-of-the-art for LogAD with speed, accuracy, and practicality.

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [101] [What Can Grokking Teach Us About Learning Under Nonstationarity?](https://arxiv.org/abs/2507.20057)
*Clare Lyle,Gharda Sokar,Razvan Pascanu,Andras Gyorgy*

Main category: cs.LG

TL;DR: The paper explores how feature-learning dynamics, similar to those in grokking, can address primacy bias in continual learning by overwriting outdated neural network representations. It proposes increasing the effective learning rate to induce these dynamics, improving generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Primacy bias in neural networks hinders adaptation to new data in continual learning. The paper aims to leverage feature-learning dynamics, observed in grokking, to mitigate this bias.

Method: The authors propose increasing the effective learning rate (ratio of parameter to update norms) to induce feature-learning dynamics, tested in grokking, warm-starting, and reinforcement learning tasks.

Result: The method successfully facilitates feature-learning and enhances generalization in various settings, including grokking and reinforcement learning.

Conclusion: Feature-learning dynamics, accelerated by adjusting the effective learning rate, offer a promising solution to primacy bias in non-stationary learning problems.

Abstract: In continual learning problems, it is often necessary to overwrite components
of a neural network's learned representation in response to changes in the data
stream; however, neural networks often exhibit \primacy bias, whereby early
training data hinders the network's ability to generalize on later tasks. While
feature-learning dynamics of nonstationary learning problems are not well
studied, the emergence of feature-learning dynamics is known to drive the
phenomenon of grokking, wherein neural networks initially memorize their
training data and only later exhibit perfect generalization. This work
conjectures that the same feature-learning dynamics which facilitate
generalization in grokking also underlie the ability to overwrite previous
learned features as well, and methods which accelerate grokking by facilitating
feature-learning dynamics are promising candidates for addressing primacy bias
in non-stationary learning problems. We then propose a straightforward method
to induce feature-learning dynamics as needed throughout training by increasing
the effective learning rate, i.e. the ratio between parameter and update norms.
We show that this approach both facilitates feature-learning and improves
generalization in a variety of settings, including grokking, warm-starting
neural network training, and reinforcement learning tasks.

</details>


### [102] [ModShift: Model Privacy via Designed Shifts](https://arxiv.org/abs/2507.20060)
*Nomaan A. Kherani,Urbashi Mitra*

Main category: cs.LG

TL;DR: The paper introduces shifts to protect model privacy in federated learning by treating learning as a parameter estimation problem, driving Fisher Information to singularity to hinder eavesdroppers. It ensures accuracy via secure shift sharing and includes a convergence test for tampering detection.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy in federated learning against eavesdroppers while maintaining model accuracy and efficiency.

Method: Uses shifts to obscure model updates, derives Fisher Information matrix to complicate estimation for eavesdroppers, and shares shifts securely. Includes a convergence test for tampering detection.

Result: Achieves higher model shift than noise injection with less bandwidth, and passes the convergence test.

Conclusion: The proposed scheme effectively protects privacy, maintains accuracy, and is efficient in bandwidth usage.

Abstract: In this paper, shifts are introduced to preserve model privacy against an
eavesdropper in federated learning. Model learning is treated as a parameter
estimation problem. This perspective allows us to derive the Fisher Information
matrix of the model updates from the shifted updates and drive them to
singularity, thus posing a hard estimation problem for Eve. The shifts are
securely shared with the central server to maintain model accuracy at the
server and participating devices. A convergence test is proposed to detect if
model updates have been tampered with and we show that our scheme passes this
test. Numerical results show that our scheme achieves a higher model shift when
compared to a noise injection scheme while requiring a lesser bandwidth secret
channel.

</details>


### [103] [Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?](https://arxiv.org/abs/2507.20061)
*Saba Ahmadi,Avrim Blum,Haifeng Xu,Fan Yao*

Main category: cs.LG

TL;DR: The paper addresses the challenge of balancing free speech and reducing harmful content manipulation on social media through mechanism design, proposing practical methods to approximate optimal solutions despite NP-hard complexity.


<details>
  <summary>Details</summary>
Motivation: User-generated content (UGC) on social media is prone to manipulation and incitement, requiring effective moderation. However, strict moderation can lead to strategic user behavior, necessitating a balance between free speech and minimizing social distortion.

Method: The study uses mechanism design to optimize the trade-off between minimizing social distortion and maximizing free speech. It proposes practical approximation methods for the NP-hard problem and provides generalization guarantees for offline data requirements.

Result: The paper demonstrates that while the optimal trade-off is NP-hard, practical methods can approximate it effectively. It also quantifies the offline data needed for such approximations.

Conclusion: The research highlights the feasibility of balancing free speech and content moderation through mechanism design, offering actionable insights for social media platforms.

Abstract: User-generated content (UGC) on social media platforms is vulnerable to
incitements and manipulations, necessitating effective regulations. To address
these challenges, those platforms often deploy automated content moderators
tasked with evaluating the harmfulness of UGC and filtering out content that
violates established guidelines. However, such moderation inevitably gives rise
to strategic responses from users, who strive to express themselves within the
confines of guidelines. Such phenomena call for a careful balance between: 1.
ensuring freedom of speech -- by minimizing the restriction of expression; and
2. reducing social distortion -- measured by the total amount of content
manipulation. We tackle the problem of optimizing this balance through the lens
of mechanism design, aiming at optimizing the trade-off between minimizing
social distortion and maximizing free speech. Although determining the optimal
trade-off is NP-hard, we propose practical methods to approximate the optimal
solution. Additionally, we provide generalization guarantees determining the
amount of finite offline data required to approximate the optimal moderator
effectively.

</details>


### [104] [Geometric Operator Learning with Optimal Transport](https://arxiv.org/abs/2507.20065)
*Xinyi Li,Zongyi Li,Nikola Kovachki,Anima Anandkumar*

Main category: cs.LG

TL;DR: Integrating optimal transport (OT) into operator learning for PDEs on complex geometries, using mesh density functions and instance-dependent deformation for better flexibility and efficiency.


<details>
  <summary>Details</summary>
Motivation: Classical geometric learning methods rely on meshes, graphs, or point clouds, which may lack flexibility. The goal is to improve accuracy and computational efficiency for PDEs on complex geometries.

Method: Formulate geometry embedding as an OT problem, mapping mesh density functions to a uniform density in a reference space. Use instance-dependent deformation for flexibility. For 3D surfaces, embed into a 2D latent space for efficiency.

Result: Achieves better accuracy and reduces computational expenses (time and memory) compared to existing methods, validated on RANS equations for ShapeNet-Car and DrivAerNet-Car datasets. Improved accuracy on FlowBench dataset.

Conclusion: OT-based operator learning with instance-dependent deformation enhances flexibility, accuracy, and computational efficiency for PDEs on complex geometries.

Abstract: We propose integrating optimal transport (OT) into operator learning for
partial differential equations (PDEs) on complex geometries. Classical
geometric learning methods typically represent domains as meshes, graphs, or
point clouds. Our approach generalizes discretized meshes to mesh density
functions, formulating geometry embedding as an OT problem that maps these
functions to a uniform density in a reference space. Compared to previous
methods relying on interpolation or shared deformation, our OT-based method
employs instance-dependent deformation, offering enhanced flexibility and
effectiveness. For 3D simulations focused on surfaces, our OT-based neural
operator embeds the surface geometry into a 2D parameterized latent space. By
performing computations directly on this 2D representation of the surface
manifold, it achieves significant computational efficiency gains compared to
volumetric simulation. Experiments with Reynolds-averaged Navier-Stokes
equations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our
method achieves better accuracy and also reduces computational expenses in
terms of both time and memory usage compared to existing machine learning
models. Additionally, our model demonstrates significantly improved accuracy on
the FlowBench dataset, underscoring the benefits of employing
instance-dependent deformation for datasets with highly variable geometries.

</details>


### [105] [PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data](https://arxiv.org/abs/2507.20068)
*Aishwarya Mandyam,Jason Meng,Ge Gao,Jiankai Sun,Mac Schwager,Barbara E. Engelhardt,Emma Brunskill*

Main category: cs.LG

TL;DR: The paper proposes two methods for constructing valid confidence intervals in off-policy evaluation (OPE) using data augmentation, addressing bias and uncertainty quantification in high-stakes settings like healthcare.


<details>
  <summary>Details</summary>
Motivation: Existing OPE methods lack principled uncertainty quantification when using auxiliary datasets, which is critical for reliable policy comparisons in high-stakes applications.

Method: 1) A conformal prediction method for high-dimensional state MDPs to estimate policy performance conditioned on an initial state. 2) Doubly robust estimation and prediction-powered inference for average policy performance over many initial states.

Result: The methods consistently produce valid confidence intervals covering ground truth values across simulators (robotics, healthcare, inventory management) and a real healthcare dataset (MIMIC-IV).

Conclusion: The proposed approaches effectively leverage augmented data while ensuring reliable uncertainty quantification, outperforming prior methods.

Abstract: Off-policy evaluation (OPE) methods aim to estimate the value of a new
reinforcement learning (RL) policy prior to deployment. Recent advances have
shown that leveraging auxiliary datasets, such as those synthesized by
generative models, can improve the accuracy of these value estimates.
Unfortunately, such auxiliary datasets may also be biased, and existing methods
for using data augmentation for OPE in RL lack principled uncertainty
quantification. In high stakes settings like healthcare, reliable uncertainty
estimates are important for comparing policy value estimates. In this work, we
propose two approaches to construct valid confidence intervals for OPE when
using data augmentation. The first provides a confidence interval over the
policy performance conditioned on a particular initial state $V^{\pi}(s_0)$--
such intervals are particularly important for human-centered applications. To
do so we introduce a new conformal prediction method for high dimensional state
MDPs. Second, we consider the more common task of estimating the average policy
performance over many initial states; to do so we draw on ideas from doubly
robust estimation and prediction powered inference. Across simulators spanning
robotics, healthcare and inventory management, and a real healthcare dataset
from MIMIC-IV, we find that our methods can use augmented data and still
consistently produce intervals that cover the ground truth values, unlike
previously proposed methods.

</details>


### [106] [Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems](https://arxiv.org/abs/2507.20072)
*Jiaqiang Li,Jianbin Tan,Xueqin Wang*

Main category: cs.LG

TL;DR: The paper introduces Sparse Equation Matching (SEM), a derivative-free framework for discovering equations in complex systems, validated through simulations and EEG data analysis.


<details>
  <summary>Details</summary>
Motivation: Existing equation discovery methods rely on accurate derivative estimation and are limited to first-order systems, restricting real-world applicability.

Method: SEM uses integral-based sparse regression with Green's functions for derivative-free estimation in general-order dynamical systems.

Result: SEM outperforms derivative-based methods in simulations and successfully identifies brain connectivity patterns in EEG data.

Conclusion: SEM provides a versatile and effective approach for equation discovery, offering insights into complex systems like brain connectivity.

Abstract: Equation discovery is a fundamental learning task for uncovering the
underlying dynamics of complex systems, with wide-ranging applications in areas
such as brain connectivity analysis, climate modeling, gene regulation, and
physical system simulation. However, many existing approaches rely on accurate
derivative estimation and are limited to first-order dynamical systems,
restricting their applicability to real-world scenarios. In this work, we
propose sparse equation matching (SEM), a unified framework that encompasses
several existing equation discovery methods under a common formulation. SEM
introduces an integral-based sparse regression method using Green's functions,
enabling derivative-free estimation of differential operators and their
associated driving functions in general-order dynamical systems. The
effectiveness of SEM is demonstrated through extensive simulations,
benchmarking its performance against derivative-based approaches. We then apply
SEM to electroencephalographic (EEG) data recorded during multiple oculomotor
tasks, collected from 52 participants in a brain-computer interface experiment.
Our method identifies active brain regions across participants and reveals
task-specific connectivity patterns. These findings offer valuable insights
into brain connectivity and the underlying neural mechanisms.

</details>


### [107] [Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection](https://arxiv.org/abs/2507.20078)
*Adelaide Danilov,Aria Nourbakhsh,Christoph Schommer*

Main category: cs.LG

TL;DR: A novel framework combining cross-entropy loss with Cluster Purge Loss improves embedding space structure for equivalent code mutant detection, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for transformer models fail to adequately structure embedding spaces for nuanced intra-class relationships, crucial for tasks like equivalent mutant detection.

Method: Integrates cross-entropy loss with Cluster Purge Loss, focusing on fine-grained intra-class differences and dynamically adjusting borders for semantic separation.

Result: Achieves state-of-the-art performance in equivalent mutant detection and produces a more interpretable embedding space.

Conclusion: The proposed framework enhances embedding space quality and model performance for code processing tasks.

Abstract: Recent pre-trained transformer models achieve superior performance in various
code processing objectives. However, although effective at optimizing decision
boundaries, common approaches for fine-tuning them for downstream
classification tasks - distance-based methods or training an additional
classification head - often fail to thoroughly structure the embedding space to
reflect nuanced intra-class semantic relationships. Equivalent code mutant
detection is one of these tasks, where the quality of the embedding space is
crucial to the performance of the models. We introduce a novel framework that
integrates cross-entropy loss with a deep metric learning objective, termed
Cluster Purge Loss. This objective, unlike conventional approaches,
concentrates on adjusting fine-grained differences within each class,
encouraging the separation of instances based on semantical equivalency to the
class center using dynamically adjusted borders. Employing UniXCoder as the
base model, our approach demonstrates state-of-the-art performance in the
domain of equivalent mutant detection and produces a more interpretable
embedding space.

</details>


### [108] [Feed-anywhere ANN (I) Steady Discrete $\to$ Diffusing on Graph Hidden States](https://arxiv.org/abs/2507.20088)
*Dmitry Pasechnyuk-Vilensky,Daniil Doroshenko*

Main category: cs.LG

TL;DR: A novel framework for learning hidden graph structures using geometric analysis and nonlinear dynamics, ensuring topological correctness and efficient generalization.


<details>
  <summary>Details</summary>
Motivation: To develop a method for uncovering hidden graph structures from data with theoretical guarantees on correctness and convergence.

Method: Defines discrete Sobolev spaces, introduces gauge-equivalent nonlinear dynamics, and develops a stochastic gradient algorithm with sparsity regularization.

Result: Achieves stronger generalization bounds than standard neural networks, with complexity tied to data manifold topology.

Conclusion: The dynamics-based model effectively learns graph structures with theoretical and practical advantages.

Abstract: We propose a novel framework for learning hidden graph structures from data
using geometric analysis and nonlinear dynamics. Our approach: (1) Defines
discrete Sobolev spaces on graphs for scalar/vector fields, establishing key
functional properties; (2) Introduces gauge-equivalent nonlinear Schr\"odinger
and Landau--Lifshitz dynamics with provable stable stationary solutions
smoothly dependent on input data and graph weights; (3) Develops a stochastic
gradient algorithm over graph moduli spaces with sparsity regularization.
Theoretically, we guarantee: topological correctness (homology recovery),
metric convergence (Gromov--Hausdorff), and efficient search space utilization.
Our dynamics-based model achieves stronger generalization bounds than standard
neural networks, with complexity dependent on the data manifold's topology.

</details>


### [109] [Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning](https://arxiv.org/abs/2507.20089)
*Ziyi Liang,Annie Qu,Babak Shahbaba*

Main category: cs.LG

TL;DR: Meta Fusion is a flexible framework unifying traditional multimodal data fusion methods, enhancing predictive performance through soft information sharing and model-agnostic latent representation learning.


<details>
  <summary>Details</summary>
Motivation: To improve predictive power in machine learning by unifying and enhancing traditional multimodal data fusion strategies.

Method: Introduces Meta Fusion, a framework combining latent representations across modalities and using soft information sharing within a model cohort.

Result: Outperforms conventional fusion strategies in simulations and real-world applications like Alzheimer's disease detection.

Conclusion: Meta Fusion is a versatile and effective approach for multimodal data fusion, supported by theoretical and empirical evidence.

Abstract: Developing effective multimodal data fusion strategies has become
increasingly essential for improving the predictive power of statistical
machine learning methods across a wide range of applications, from autonomous
driving to medical diagnosis. Traditional fusion methods, including early,
intermediate, and late fusion, integrate data at different stages, each
offering distinct advantages and limitations. In this paper, we introduce Meta
Fusion, a flexible and principled framework that unifies these existing
strategies as special cases. Motivated by deep mutual learning and ensemble
learning, Meta Fusion constructs a cohort of models based on various
combinations of latent representations across modalities, and further boosts
predictive performance through soft information sharing within the cohort. Our
approach is model-agnostic in learning the latent representations, allowing it
to flexibly adapt to the unique characteristics of each modality.
Theoretically, our soft information sharing mechanism reduces the
generalization error. Empirically, Meta Fusion consistently outperforms
conventional fusion strategies in extensive simulation studies. We further
validate our approach on real-world applications, including Alzheimer's disease
detection and neural decoding.

</details>


### [110] [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
*Xin Gao,Xingming Xu*

Main category: cs.LG

TL;DR: EcoTransformer replaces dot-product attention with a Laplacian kernel convolution, reducing energy use while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The scaled dot-product attention in Transformers is computationally intensive and energy-consuming.

Method: EcoTransformer uses a Laplacian kernel convolution (L1 metric) to compute attention scores, avoiding matrix multiplication.

Result: Performs comparably or better than dot-product attention in NLP, bioinformatics, and vision tasks with lower energy consumption.

Conclusion: EcoTransformer offers an efficient alternative to traditional attention mechanisms, reducing energy costs without sacrificing performance.

Abstract: The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

</details>


### [111] [Graded Transformers: A Symbolic-Geometric Approach to Structured Learning](https://arxiv.org/abs/2507.20108)
*Tony Shaska Sr*

Main category: cs.LG

TL;DR: The paper introduces the Graded Transformer framework, embedding algebraic biases into sequence models with two architectures (LGT and EGT), offering theoretical guarantees and applications in diverse fields.


<details>
  <summary>Details</summary>
Motivation: To enhance structured data efficiency and hierarchical learning by integrating algebraic and geometric principles into transformers, overcoming limitations of fixed-grade models.

Method: Proposes Linearly Graded Transformer (LGT) and Exponentially Graded Transformer (EGT) using parameterized scaling operators and graded loss functions for adaptive feature prioritization.

Result: Theoretical guarantees include universal approximation, reduced sample complexity, robustness, and gradient stability. Applications span algebraic geometry, physics, NLP, and more.

Conclusion: The Graded Transformer advances structured deep learning by combining geometric principles with attention, enabling interpretable and efficient systems for complex domains.

Abstract: We introduce the Graded Transformer framework, a novel class of sequence
models that embeds algebraic inductive biases through grading transformations
on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we
propose two architectures: the Linearly Graded Transformer (LGT) and the
Exponentially Graded Transformer (EGT). These models apply parameterized
scaling operators-governed by fixed or learnable grading tuples and, for EGT,
exponential factors to infuse hierarchical structure into attention and
representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation
theorems for continuous and Sobolev functions, reduced sample complexity via
effective VC dimension bounds, Lipschitz continuity of graded operations, and
robustness to adversarial perturbations. A graded loss function ensures
gradient stability and alignment with domain priors during optimization. By
treating grades as differentiable parameters, the framework enables adaptive
feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical
learning and neurosymbolic reasoning, with applications spanning algebraic
geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale
simulations), natural language processing (e.g., syntactic parsing), biological
sequence analysis (e.g., variant prediction), and emerging areas like graph
neural networks and financial modeling. This work advances structured deep
learning by fusing geometric and algebraic principles with attention
mechanisms, offering a mathematically grounded alternative to data-driven
models and paving the way for interpretable, efficient systems in complex
domains.

</details>


### [112] [Online Learning with Probing for Sequential User-Centric Selection](https://arxiv.org/abs/2507.20112)
*Tianyi Xu,Yiting Chen,Henger Li,Zheyong Bian,Emiliano Dall'Anese,Zizhan Zheng*

Main category: cs.LG

TL;DR: The paper introduces the PUCS framework for sequential decision-making with costly information acquisition, offering offline and online solutions with theoretical guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address scenarios like ridesharing and content recommendation where resources and rewards are initially unknown, and probing is costly.

Method: Offline: greedy probing algorithm with approximation guarantee. Online: OLPA algorithm for stochastic combinatorial bandit with regret bounds.

Result: Offline achieves a constant-factor approximation. Online achieves tight regret bounds. Experiments validate effectiveness.

Conclusion: PUCS framework and proposed algorithms effectively solve sequential decision-making with costly probing, supported by theory and experiments.

Abstract: We formalize sequential decision-making with information acquisition as the
probing-augmented user-centric selection (PUCS) framework, where a learner
first probes a subset of arms to obtain side information on resources and
rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such
as ridesharing, wireless scheduling, and content recommendation, in which both
resources and payoffs are initially unknown and probing is costly. For the
offline setting with known distributions, we present a greedy probing algorithm
with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the
online setting with unknown distributions, we introduce OLPA, a stochastic
combinatorial bandit algorithm that achieves a regret bound
$\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound
$\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic
factors. Experiments on real-world data demonstrate the effectiveness of our
solutions.

</details>


### [113] [Wine Characterisation with Spectral Information and Predictive Artificial Intelligence](https://arxiv.org/abs/2507.20114)
*Jianping Yao,Son N. Tran,Hieu Nguyen,Samantha Sawyer,Rocco Longo*

Main category: cs.LG

TL;DR: The paper uses UV-Vis spectroscopy and ML to predict grape juice attributes and classify wine origin, achieving high accuracy with SVM.


<details>
  <summary>Details</summary>
Motivation: To improve traditional wine analysis methods by integrating spectroscopy and ML for sensory data and origin classification.

Method: Combined UV-Vis spectroscopy with ML techniques, focusing on SVM for prediction tasks.

Result: SVM achieved over 91% accuracy and F1 score in origin prediction, with influential wavelengths identified at 250-420 nm.

Conclusion: The study offers a foundation for integrating big data and IoT in the wine industry, advancing 'Smart Wineries'.

Abstract: The purpose of this paper is to use absorbance data obtained by human tasting
and an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the
attributes of grape juice (GJ) and to classify the wine's origin, respectively.
The approach combined machine learning (ML) techniques with spectroscopy to
find a relatively simple way to apply them in two stages of winemaking and help
improve the traditional wine analysis methods regarding sensory data and wine's
origins. This new technique has overcome the disadvantages of the complex
sensors by taking advantage of spectral fingerprinting technology and forming a
comprehensive study of the employment of AI in the wine analysis domain. In the
results, Support Vector Machine (SVM) was the most efficient and robust in both
attributes and origin prediction tasks. Both the accuracy and F1 score of the
origin prediction exceed 91%. The feature ranking approach found that the more
influential wavelengths usually appear at the lower end of the scan range, 250
nm (nanometers) to 420 nm, which is believed to be of great help for selecting
appropriate validation methods and sensors to extract wine data in future
research. The knowledge of this research provides new ideas and early solutions
for the wine industry or other beverage industries to integrate big data and
IoT in the future, which significantly promotes the development of 'Smart
Wineries'.

</details>


### [114] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: The paper introduces AMLP, an unsupervised framework to make MLP adaptive to graph aggregation, improving performance in heterophilic graphs without relying on labeled data.


<details>
  <summary>Details</summary>
Motivation: GNNs use fixed aggregators, leading to poor performance in heterophilic graphs. Existing methods rely on labeled data, which is often scarce.

Method: Proposes AMLP: (1) graph reconstruction for high-order grouping, (2) a single-layer network to encode heterophily.

Result: AMLP outperforms in node clustering and classification, showing adaptability to diverse graph scenarios.

Conclusion: AMLP offers a lightweight, unsupervised solution for improving GNN performance in heterophilic graphs.

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [115] [Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design](https://arxiv.org/abs/2507.20130)
*Yi He,Ailun Wang,Zhi Wang,Yu Liu,Xingyuan Xu,Wen Yan*

Main category: cs.LG

TL;DR: MEVO is an evolutionary framework for structure-based drug design, addressing data scarcity by combining VQ-VAE, diffusion models, and evolutionary strategies to generate high-affinity binders.


<details>
  <summary>Details</summary>
Motivation: Current generative models lack sufficient training data for structure-based drug design (SBDD). MEVO aims to bridge this gap by leveraging large-scale small molecule datasets and enhancing protein-ligand complex data.

Method: MEVO integrates a VQ-VAE for molecule representation, a diffusion model for pharmacophore-guided generation, and a pocket-aware evolutionary strategy with physics-based scoring.

Result: MEVO successfully generates high-affinity binders for protein targets, including potent inhibitors for KRASG12D, validated by free energy perturbation (FEP) methods.

Conclusion: MEVO provides a versatile, data-efficient solution for SBDD, demonstrating effectiveness in designing high-affinity ligands for challenging targets.

Abstract: Recent advances in generative models, particularly diffusion and
auto-regressive models, have revolutionized fields like computer vision and
natural language processing. However, their application to structure-based drug
design (SBDD) remains limited due to critical data constraints. To address the
limitation of training data for models targeting SBDD tasks, we propose an
evolutionary framework named MEVO, which bridges the gap between billion-scale
small molecule dataset and the scarce protein-ligand complex dataset, and
effectively increase the abundance of training data for generative SBDD models.
MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule
representation in latent space, a diffusion model for pharmacophore-guided
molecule generation, and a pocket-aware evolutionary strategy for molecule
optimization with physics-based scoring function. This framework efficiently
generate high-affinity binders for various protein targets, validated with
predicted binding affinities using free energy perturbation (FEP) methods. In
addition, we showcase the capability of MEVO in designing potent inhibitors to
KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with
similar affinity to the known highly active inhibitor evaluated by FEP
calculations. With high versatility and generalizability, MEVO offers an
effective and data-efficient model for various tasks in structure-based ligand
design.

</details>


### [116] [Awesome-OL: An Extensible Toolkit for Online Learning](https://arxiv.org/abs/2507.20144)
*Zeyi Liu,Songqiao Hu,Pengyu Han,Jiaming Liu,Xiao He*

Main category: cs.LG

TL;DR: Awesome-OL is a Python toolkit for online learning, integrating state-of-the-art algorithms, benchmark datasets, and visualization tools for reproducible research.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptive tools in online learning for streaming and non-stationary data.

Method: Built on scikit-multiflow, Awesome-OL offers a unified framework with user-friendly interactions and extensibility.

Result: Provides a publicly available toolkit (Awesome-OL) for reproducible comparisons and practical deployment.

Conclusion: Awesome-OL facilitates online learning research with flexibility, extensibility, and ease of use.

Abstract: In recent years, online learning has attracted increasing attention due to
its adaptive capability to process streaming and non-stationary data. To
facilitate algorithm development and practical deployment in this area, we
introduce Awesome-OL, an extensible Python toolkit tailored for online learning
research. Awesome-OL integrates state-of-the-art algorithm, which provides a
unified framework for reproducible comparisons, curated benchmark datasets, and
multi-modal visualization. Built upon the scikit-multiflow open-source
infrastructure, Awesome-OL emphasizes user-friendly interactions without
compromising research flexibility or extensibility. The source code is publicly
available at: https://github.com/liuzy0708/Awesome-OL.

</details>


### [117] [ASNN: Learning to Suggest Neural Architectures from Performance Distributions](https://arxiv.org/abs/2507.20164)
*Jinwook Hong*

Main category: cs.LG

TL;DR: ASNN learns the relationship between NN architecture and accuracy, suggesting improved designs, outperforming random search.


<details>
  <summary>Details</summary>
Motivation: No general function maps NN architecture to accuracy, making design heuristic or search-based. ASNN aims to automate and optimize this process.

Method: ASNN is trained on datasets of TensorFlow models with varying layers and nodes, using accuracy as input and architectural parameters as output. Iterative predictions suggest better architectures.

Result: ASNN suggested architectures outperforming original training data in 2-layer and 3-layer cases, improving mean test accuracies.

Conclusion: ASNN offers an efficient alternative to random search for NN architecture optimization, promising for automating design.

Abstract: The architecture of a neural network (NN) plays a critical role in
determining its performance. However, there is no general closed-form function
that maps between network structure and accuracy, making the process of
architecture design largely heuristic or search-based. In this study, we
propose the Architecture Suggesting Neural Network (ASNN), a model designed to
learn the relationship between NN architecture and its test accuracy, and to
suggest improved architectures accordingly. To train ASNN, we constructed
datasets using TensorFlow-based models with varying numbers of layers and
nodes. Experimental results were collected for both 2-layer and 3-layer
architectures across a grid of configurations, each evaluated with 10 repeated
trials to account for stochasticity. Accuracy values were treated as inputs,
and architectural parameters as outputs. The trained ASNN was then used
iteratively to predict architectures that yield higher performance. In both
2-layer and 3-layer cases, ASNN successfully suggested architectures that
outperformed the best results found in the original training data. Repeated
prediction and retraining cycles led to the discovery of architectures with
improved mean test accuracies, demonstrating the model's capacity to generalize
the performance-structure relationship. These results suggest that ASNN
provides an efficient alternative to random search for architecture
optimization, and offers a promising approach toward automating neural network
design. "Parts of the manuscript, including text editing and expression
refinement, were supported by OpenAI's ChatGPT. All content was reviewed and
verified by the authors."

</details>


### [118] [Partial Domain Adaptation via Importance Sampling-based Shift Correction](https://arxiv.org/abs/2507.20191)
*Cheng-Jun Guo,Chuan-Xian Ren,You-Wei Luo,Xiao-Lin Xu,Hong Yan*

Main category: cs.LG

TL;DR: The paper introduces ISÂ²C, a novel importance sampling-based method for partial domain adaptation (PDA), addressing label distribution shift and improving model generalization by sampling from a target-aligned domain.


<details>
  <summary>Details</summary>
Motivation: PDA faces challenges with label distribution shift and overfitting due to simple reweighing techniques. The goal is to better utilize labeled data and explore latent structures.

Method: Proposes ISÂ²C, which samples labeled data from a target-aligned domain, and an optimal transport-based independence criterion for conditional distribution alignment, reducing computational complexity.

Result: Theoretical guarantees show ISÂ²C dominates generalization error. Experiments on PDA benchmarks confirm its effectiveness over existing methods.

Conclusion: ISÂ²C successfully addresses PDA challenges, offering interpretability and improved performance.

Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine
learning scenarios. It aims to transfer knowledge from a labeled source domain
to a related unlabeled target domain, where the support set of the source label
distribution subsumes the target one. Previous PDA works managed to correct the
label distribution shift by weighting samples in the source domain. However,
the simple reweighing technique cannot explore the latent structure and
sufficiently use the labeled data, and then models are prone to over-fitting on
the source domain. In this work, we propose a novel importance sampling-based
shift correction (IS$^2$C) method, where new labeled data are sampled from a
built sampling domain, whose label distribution is supposed to be the same as
the target domain, to characterize the latent structure and enhance the
generalization ability of the model. We provide theoretical guarantees for
IS$^2$C by proving that the generalization error can be sufficiently dominated
by IS$^2$C. In particular, by implementing sampling with the mixture
distribution, the extent of shift between source and sampling domains can be
connected to generalization error, which provides an interpretable way to build
IS$^2$C. To improve knowledge transfer, an optimal transport-based independence
criterion is proposed for conditional distribution alignment, where the
computation of the criterion can be adjusted to reduce the complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive
experiments on PDA benchmarks validate the theoretical results and demonstrate
the effectiveness of our IS$^2$C over existing methods.

</details>


### [119] [Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading](https://arxiv.org/abs/2507.20202)
*Longfei Lu*

Main category: cs.LG

TL;DR: Classical financial indicators are special cases of neural networks with fixed weights. Technical Indicator Networks (TINs) generalize and upgrade these indicators using neural architectures.


<details>
  <summary>Details</summary>
Motivation: To bridge traditional technical analysis with modern AI by showing that classical indicators are interpretable neural networks.

Method: Propose TINs, a neural architecture that replicates and upgrades traditional indicators using n-dimensional inputs like price, volume, and sentiment.

Result: TINs modernize technical analysis by encoding domain knowledge into neural structures, enabling more advanced algorithmic trading.

Conclusion: TINs connect legacy indicators with AI, advancing algorithmic trading by leveraging neural networks.

Abstract: This work proposes that a vast majority of classical technical indicators in
financial analysis are, in essence, special cases of neural networks with fixed
and interpretable weights. It is shown that nearly all such indicators, such as
moving averages, momentum-based oscillators, volatility bands, and other
commonly used technical constructs, can be reconstructed topologically as
modular neural network components. Technical Indicator Networks (TINs) are
introduced as a general neural architecture that replicates and structurally
upgrades traditional indicators by supporting n-dimensional inputs such as
price, volume, sentiment, and order book data. By encoding domain-specific
knowledge into neural structures, TINs modernize the foundational logic of
technical analysis and propel algorithmic trading into a new era, bridging the
legacy of proven indicators with the potential of contemporary AI systems.

</details>


### [120] [Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design](https://arxiv.org/abs/2507.20243)
*Lang Yu,Zhangyang Gao,Cheng Tan,Qin Chen,Jie Zhou,Liang He*

Main category: cs.LG

TL;DR: The paper introduces Protein-SE(3), a modular benchmark for SE(3)-based protein structure design, integrating diverse generative models and evaluation metrics for fair comparison.


<details>
  <summary>Details</summary>
Motivation: The lack of a standardized benchmark for SE(3)-based protein modeling hinders comprehensive evaluation and comparison of methods.

Method: Proposes Protein-SE(3), a unified framework with scaffolding tasks, integrated models (DDPM, Score Matching, Flow Matching), and diverse metrics.

Result: Integrates advanced models (e.g., Genie1, FrameDiff, FoldFlow) and provides mathematical abstraction for future algorithm prototyping.

Conclusion: Protein-SE(3) is the first comprehensive benchmark for SE(3)-based protein design, publicly available for research.

Abstract: SE(3)-based generative models have shown great promise in protein geometry
modeling and effective structure design. However, the field currently lacks a
modularized benchmark to enable comprehensive investigation and fair comparison
of different methods. In this paper, we propose Protein-SE(3), a new benchmark
based on a unified training framework, which comprises protein scaffolding
tasks, integrated generative models, high-level mathematical abstraction, and
diverse evaluation metrics. Recent advanced generative models designed for
protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),
Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and
FrameFlow) are integrated into our framework. All integrated methods are fairly
investigated with the same training dataset and evaluation metrics.
Furthermore, we provide a high-level abstraction of the mathematical
foundations behind the generative models, enabling fast prototyping of future
algorithms without reliance on explicit protein structures. Accordingly, we
release the first comprehensive benchmark built upon unified training framework
for SE(3)-based protein structure design, which is publicly accessible at
https://github.com/BruthYU/protein-se3.

</details>


### [121] [Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining](https://arxiv.org/abs/2507.20263)
*Junjie Zhao,Chengxi Zhang,Chenkai Wang,Peng Yang*

Main category: cs.LG

TL;DR: TLRS improves RL for mining alpha factors by providing dense rewards and reducing training variance, boosting performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for mining alpha factors suffer from sparse rewards, limiting exploration and destabilizing training.

Method: Proposes Trajectory-level Reward Shaping (TLRS) with dense rewards via subsequence-level similarity and reward centering.

Result: TLRS boosts Rank Information Coefficient by 9.29% and improves computational efficiency.

Conclusion: TLRS enhances RL for alpha factor mining, offering better performance and efficiency over existing methods.

Abstract: Reinforcement learning (RL) has successfully automated the complex process of
mining formulaic alpha factors, for creating interpretable and profitable
investment strategies. However, existing methods are hampered by the sparse
rewards given the underlying Markov Decision Process. This inefficiency limits
the exploration of the vast symbolic search space and destabilizes the training
process. To address this, Trajectory-level Reward Shaping (TLRS), a novel
reward shaping method, is proposed. TLRS provides dense, intermediate rewards
by measuring the subsequence-level similarity between partially generated
expressions and a set of expert-designed formulas. Furthermore, a reward
centering mechanism is introduced to reduce training variance. Extensive
experiments on six major Chinese and U.S. stock indices show that TLRS
significantly improves the predictive power of mined factors, boosting the Rank
Information Coefficient by 9.29% over existing potential-based shaping
algorithms. Notably, TLRS achieves a major leap in computational efficiency by
reducing its time complexity with respect to the feature dimension from linear
to constant, which is a significant improvement over distance-based baselines.

</details>


### [122] [Data-Efficient Prediction-Powered Calibration via Cross-Validation](https://arxiv.org/abs/2507.20268)
*Seonghoon Yoo,Houssem Sifaou,Sangwoo Park,Joonhyuk Kang,Osvaldo Simeone*

Main category: cs.LG

TL;DR: A novel method efficiently uses limited calibration data to fine-tune a predictor and estimate synthetic label bias, ensuring rigorous coverage guarantees for AI decisions.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of calibration data for quantifying AI decision uncertainty by leveraging synthetic labels, while minimizing additional data needs.

Method: Simultaneously fine-tunes a predictor and estimates synthetic label bias using limited calibration data, ensuring rigorous coverage guarantees.

Result: Demonstrated effectiveness and performance gains in an indoor localization problem.

Conclusion: The proposed solution efficiently tackles calibration data scarcity, providing reliable uncertainty quantification for AI decisions.

Abstract: Calibration data are necessary to formally quantify the uncertainty of the
decisions produced by an existing artificial intelligence (AI) model. To
overcome the common issue of scarce calibration data, a promising approach is
to employ synthetic labels produced by a (generally different) predictive
model. However, fine-tuning the label-generating predictor on the inference
task of interest, as well as estimating the residual bias of the synthetic
labels, demand additional data, potentially exacerbating the calibration data
scarcity problem. This paper introduces a novel approach that efficiently
utilizes limited calibration data to simultaneously fine-tune a predictor and
estimate the bias of the synthetic labels. The proposed method yields
prediction sets with rigorous coverage guarantees for AI-generated decisions.
Experimental results on an indoor localization problem validate the
effectiveness and performance gains of our solution.

</details>


### [123] [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://arxiv.org/abs/2507.20272)
*Dharmesh Tailor,Alvaro H. C. Correia,Eric Nalisnick,Christos Louizos*

Main category: cs.LG

TL;DR: The paper proposes a method to construct prediction intervals for neural network regressors without held-out data by approximating full conformal prediction (full-CP) using Gauss-Newton influence and linearization, outperforming split-CP in calibration and sharpness.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in safety-critical areas require well-calibrated and sharp uncertainty estimates, but existing methods like Laplace's method and split-CP have limitations (miscalibration or reduced efficiency).

Method: The method approximates full-CP by training once and perturbing model parameters locally using Gauss-Newton influence, coupled with network linearization to efficiently compute prediction intervals.

Result: The proposed method produces locally-adaptive and often tighter prediction intervals than split-CP on standard regression benchmarks and bounding box localization tasks.

Conclusion: The approach effectively addresses the limitations of existing methods, providing a practical solution for uncertainty quantification in neural networks without held-out data.

Abstract: Uncertainty quantification is an important prerequisite for the deployment of
deep learning models in safety-critical areas. Yet, this hinges on the
uncertainty estimates being useful to the extent the prediction intervals are
well-calibrated and sharp. In the absence of inherent uncertainty estimates
(e.g. pretrained models predicting only point estimates), popular approaches
that operate post-hoc include Laplace's method and split conformal prediction
(split-CP). However, Laplace's method can be miscalibrated when the model is
misspecified and split-CP requires sample splitting, and thus comes at the
expense of statistical efficiency. In this work, we construct prediction
intervals for neural network regressors post-hoc without held-out data. This is
achieved by approximating the full conformal prediction method (full-CP).
Whilst full-CP nominally requires retraining the model for every test point and
candidate label, we propose to train just once and locally perturb model
parameters using Gauss-Newton influence to approximate the effect of
retraining. Coupled with linearization of the network, we express the absolute
residual nonconformity score as a piecewise linear function of the candidate
label allowing for an efficient procedure that avoids the exhaustive search
over the output space. On standard regression benchmarks and bounding box
localization, we show the resulting prediction intervals are locally-adaptive
and often tighter than those of split-CP.

</details>


### [124] [MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction](https://arxiv.org/abs/2507.20326)
*Jiaxi Wang,Yaosen Min,Xun Zhu,Miao Li,Ji Wu*

Main category: cs.LG

TL;DR: The paper introduces MIPS, a framework for polymer property prediction by modeling polymers as infinite monomer sequences, integrating topological and spatial information, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing polymer modeling approaches fail to capture properties accurately due to changes during polymerization, necessitating a more comprehensive method.

Method: MIPS uses multimodal pre-training, combining topological (MPM, GAM, LGA) and spatial (3D descriptors) information, with a cross-modal fusion mechanism.

Result: MIPS achieves state-of-the-art performance across eight polymer property prediction tasks.

Conclusion: MIPS effectively addresses limitations of current methods by robustly integrating topological and spatial data for polymer property prediction.

Abstract: Polymers, composed of repeating structural units called monomers, are
fundamental materials in daily life and industry. Accurate property prediction
for polymers is essential for their design, development, and application.
However, existing modeling approaches, which typically represent polymers by
the constituent monomers, struggle to capture the whole properties of polymer,
since the properties change during the polymerization process. In this study,
we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training
framework, which represents polymers as infinite sequences of monomers and
integrates both topological and spatial information for comprehensive modeling.
From the topological perspective, we generalize message passing mechanism (MPM)
and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we
demonstrate that applying MPM to infinite polymer sequences is equivalent to
applying MPM on the induced star-linking graph of monomers. For GAM, we propose
to further replace global graph attention with localized graph attention (LGA).
Moreover, we show the robustness of the "star linking" strategy through Repeat
and Shift Invariance Test (RSIT). Despite its robustness, "star linking"
strategy exhibits limitations when monomer side chains contain ring structures,
a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)
test. To overcome this issue, we propose backbone embedding to enhance the
capability of MPM and LGA on infinite polymer sequences. From the spatial
perspective, we extract 3D descriptors of repeating monomers to capture spatial
information. Finally, we design a cross-modal fusion mechanism to unify the
topological and spatial information. Experimental validation across eight
diverse polymer property prediction tasks reveals that MIPS achieves
state-of-the-art performance.

</details>


### [125] [Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning](https://arxiv.org/abs/2507.20335)
*Siyu Song,Wentao Liu,Ye Lu,Ruohua Zhang,Tao Liu,Jinze Lv,Xinyun Wang,Aimin Zhou,Fei Tan,Bo Jiang,Hao Hao*

Main category: cs.LG

TL;DR: EduAlign is a framework to align LLMs with educational principles (Helpfulness, Personalization, Creativity) using a reward model and fine-tuning, improving their effectiveness as educational assistants.


<details>
  <summary>Details</summary>
Motivation: Standard LLMs lack alignment with pedagogical principles, limiting their educational utility. EduAlign aims to bridge this gap.

Method: 1. Curate and annotate 8k educational interactions for HPC dimensions. 2. Train HPC-RM reward model. 3. Fine-tune LLM using GRPO and HPC-RM.

Result: The fine-tuned model shows significant improvement in pedagogical alignment across HPC dimensions.

Conclusion: EduAlign provides a scalable method to enhance LLMs for education, enabling more effective AI tutors.

Abstract: The integration of large language models (LLMs) into education presents
unprecedented opportunities for scalable personalized learning. However,
standard LLMs often function as generic information providers, lacking
alignment with fundamental pedagogical principles such as helpfulness,
student-centered personalization, and creativity cultivation. To bridge this
gap, we propose EduAlign, a novel framework designed to guide LLMs toward
becoming more effective and responsible educational assistants. EduAlign
consists of two main stages. In the first stage, we curate a dataset of 8k
educational interactions and annotate them-both manually and
automatically-along three key educational dimensions: Helpfulness,
Personalization, and Creativity (HPC). These annotations are used to train
HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM
outputs according to these educational principles. We further evaluate the
consistency and reliability of this reward model. In the second stage, we
leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group
Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then
assess the pre- and post-finetuning models on both educational and
general-domain benchmarks across the three HPC dimensions. Experimental results
demonstrate that the fine-tuned model exhibits significantly improved alignment
with pedagogical helpfulness, personalization, and creativity stimulation. This
study presents a scalable and effective approach to aligning LLMs with nuanced
and desirable educational traits, paving the way for the development of more
engaging, pedagogically aligned AI tutors.

</details>


### [126] [From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery](https://arxiv.org/abs/2507.20349)
*Rezaur Rashid,Gabriel Terejanu*

Main category: cs.LG

TL;DR: A novel GNN-based probabilistic framework for causal discovery outperforms traditional and recent methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional causal discovery methods struggle with scalability and capturing global structural information in large, complex datasets.

Method: Uses a GNN to encode node and edge attributes into a unified graph representation, trained on synthetic datasets with statistical measures, framing causal discovery as supervised learning.

Result: Outperforms traditional, non-GNN, and GNN-based methods in accuracy and scalability on synthetic and real-world datasets.

Conclusion: The probabilistic framework improves causal structure learning, benefiting decision-making and scientific discovery.

Abstract: Causal discovery from observational data is challenging, especially with
large datasets and complex relationships. Traditional methods often struggle
with scalability and capturing global structural information. To overcome these
limitations, we introduce a novel graph neural network (GNN)-based
probabilistic framework that learns a probability distribution over the entire
space of causal graphs, unlike methods that output a single deterministic
graph. Our framework leverages a GNN that encodes both node and edge attributes
into a unified graph representation, enabling the model to learn complex causal
structures directly from data. The GNN model is trained on a diverse set of
synthetic datasets augmented with statistical and information-theoretic
measures, such as mutual information and conditional entropy, capturing both
local and global data properties. We frame causal discovery as a supervised
learning problem, directly predicting the entire graph structure. Our approach
demonstrates superior performance, outperforming both traditional and recent
non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy
and scalability on synthetic and real-world datasets without further training.
This probabilistic framework significantly improves causal structure learning,
with broad implications for decision-making and scientific discovery across
various fields.

</details>


### [127] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL outperforms SGDL in image tasks like regression, denoising, and deblurring, with better robustness to learning rate choices and improved training stability due to eigenvalue analysis of Jacobian matrices.


<details>
  <summary>Details</summary>
Motivation: To investigate and explain the computational advantages of MGDL over SGDL in image-related tasks.

Method: Analyzed gradient descent (GD) convergence and eigenvalue distributions of Jacobian matrices for MGDL and SGDL.

Result: MGDL is more robust to learning rate choices and exhibits enhanced training stability compared to SGDL.

Conclusion: MGDL's superior performance is mathematically justified, making it a better choice for image regression, denoising, and deblurring tasks.

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [128] [Wafer Defect Root Cause Analysis with Partial Trajectory Regression](https://arxiv.org/abs/2507.20357)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi IdÃ©*

Main category: cs.LG

TL;DR: A novel framework, Partial Trajectory Regression (PTR), is introduced for wafer defect root cause analysis, addressing limitations of conventional methods by handling variable-length processing routes and using new representation learning techniques.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in identifying upstream processes causing wafer defects due to complex, variable process flows and rework operations.

Method: PTR uses partial process trajectories and counterfactual outcomes, enabled by proc2vec and route2vec representation learning, to compute process attribution scores.

Result: The framework is validated using real wafer history data from NY CREATES fab, demonstrating its effectiveness.

Conclusion: PTR provides a robust solution for root cause analysis in wafer defect scenarios with heterogeneous processes.

Abstract: Identifying upstream processes responsible for wafer defects is challenging
due to the combinatorial nature of process flows and the inherent variability
in processing routes, which arises from factors such as rework operations and
random process waiting times. This paper presents a novel framework for wafer
defect root cause analysis, called Partial Trajectory Regression (PTR). The
proposed framework is carefully designed to address the limitations of
conventional vector-based regression models, particularly in handling
variable-length processing routes that span a large number of heterogeneous
physical processes. To compute the attribution score of each process given a
detected high defect density on a specific wafer, we propose a new algorithm
that compares two counterfactual outcomes derived from partial process
trajectories. This is enabled by new representation learning methods, proc2vec
and route2vec. We demonstrate the effectiveness of the proposed framework using
real wafer history data from the NY CREATES fab in Albany.

</details>


### [129] [MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)](https://arxiv.org/abs/2507.20362)
*Hengyu Liu,Tianyi Li,Yuqiang He,Kristian Torp,Yushuai Li,Christian S. Jensen*

Main category: cs.LG

TL;DR: MH-GIN is a novel method for imputing missing values in maritime location-tracking data by capturing multi-scale dependencies among heterogeneous attributes, outperforming existing methods by 57% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Missing values in maritime tracking data hinder safety and monitoring applications, and existing imputation methods fail to account for multi-scale dependencies among attributes.

Method: MH-GIN extracts multi-scale temporal features, constructs a heterogeneous graph to model dependencies, and uses graph propagation for imputation.

Result: MH-GIN reduces imputation errors by 57% on average compared to state-of-the-art methods while remaining computationally efficient.

Conclusion: MH-GIN effectively addresses the challenge of missing data in maritime tracking by leveraging multi-scale dependencies, offering significant accuracy improvements.

Abstract: Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.

</details>


### [130] [Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis](https://arxiv.org/abs/2507.20364)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi IdÃ©*

Main category: cs.LG

TL;DR: The paper introduces Trajectory Shapley Attribution (TSA) to identify problematic upstream processes in semiconductor manufacturing by addressing limitations of Shapley values, such as ignoring process sequence and arbitrary reference points.


<details>
  <summary>Details</summary>
Motivation: The complexity of semiconductor manufacturing makes root cause analysis for wafer defects challenging, necessitating a method to trace defects back to specific upstream processes.

Method: The proposed TSA framework extends Shapley values to account for process sequence and eliminate reliance on arbitrary references, applied to diagnose wafer defects in experimental processes.

Result: TSA successfully identifies measurement items (proxies for process parameters) most relevant to abnormal defect occurrence in the tested manufacturing setup.

Conclusion: TSA provides a robust tool for root cause analysis in semiconductor manufacturing, overcoming key limitations of traditional attribution methods.

Abstract: How can we identify problematic upstream processes when a certain type of
wafer defect starts appearing at a quality checkpoint? Given the complexity of
modern semiconductor manufacturing, which involves thousands of process steps,
cross-process root cause analysis for wafer defects has been considered highly
challenging. This paper proposes a novel framework called Trajectory Shapley
Attribution (TSA), an extension of Shapley values (SV), a widely used
attribution algorithm in explainable artificial intelligence research. TSA
overcomes key limitations of standard SV, including its disregard for the
sequential nature of manufacturing processes and its reliance on an arbitrarily
chosen reference point. We applied TSA to a good-bad wafer diagnosis task in
experimental front-end-of-line processes at the NY CREATES Albany NanoTech fab,
aiming to identify measurement items (serving as proxies for process
parameters) most relevant to abnormal defect occurrence.

</details>


### [131] [Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning](https://arxiv.org/abs/2507.20369)
*Ahmed Shokry,Ayman Khalafallah*

Main category: cs.LG

TL;DR: A novel meta-learning-based clustering method eliminates parameter tuning, outperforms state-of-the-art techniques, and leverages pre-clustered samples for accurate, scalable clustering.


<details>
  <summary>Details</summary>
Motivation: Existing clustering algorithms face challenges like parameter tuning, computational complexity, and suboptimal accuracy, especially for large datasets.

Method: Uses a Prior-Data Fitted Transformer Network (PFN) to compute attention between pre-clustered and unclustered samples, inferring cluster assignments in a single forward pass.

Result: Outperforms state-of-the-art methods, generalizes well with few pre-clustered samples, and works even without them.

Conclusion: The approach is effective, scalable, and superior to existing techniques, offering a promising alternative for clustering tasks.

Abstract: Clustering is a core task in machine learning with wide-ranging applications
in data mining and pattern recognition. However, its unsupervised nature makes
it inherently challenging. Many existing clustering algorithms suffer from
critical limitations: they often require careful parameter tuning, exhibit high
computational complexity, lack interpretability, or yield suboptimal accuracy,
especially when applied to large-scale datasets. In this paper, we introduce a
novel clustering approach based on meta-learning. Our approach eliminates the
need for parameter optimization while achieving accuracy that outperforms
state-of-the-art clustering techniques. The proposed technique leverages a few
pre-clustered samples to guide the clustering process for the entire dataset in
a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted
Transformer Network (PFN) to perform clustering. The algorithm computes
attention between the pre-clustered samples and the unclustered samples,
allowing it to infer cluster assignments for the entire dataset based on the
learned relation. We theoretically and empirically demonstrate that, given just
a few pre-clustered examples, the model can generalize to accurately cluster
the rest of the dataset. Experiments on challenging benchmark datasets show
that our approach can successfully cluster well-separated data without any
pre-clustered samples, and significantly improves performance when a few
clustered samples are provided. We show that our approach is superior to the
state-of-the-art techniques. These results highlight the effectiveness and
scalability of our approach, positioning it as a promising alternative to
existing clustering techniques.

</details>


### [132] [WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks](https://arxiv.org/abs/2507.20373)
*Kiymet Kaya,Elif Ak,Sule Gunduz Oguducu*

Main category: cs.LG

TL;DR: WBHT is a novel framework combining generative modeling and attention mechanisms to detect black hole anomalies in networks, outperforming existing models with significant F1 score improvements.


<details>
  <summary>Details</summary>
Motivation: Black hole anomalies cause packet loss without notifications, disrupting connectivity and causing financial losses, necessitating better detection methods.

Method: WBHT integrates Wasserstein GANs, attention mechanisms, LSTM for long-term dependencies, and convolutional layers for local patterns, with latent space encoding for anomaly distinction.

Result: WBHT achieves F1 score improvements of 1.65% to 58.76% over existing models, detecting previously undetected anomalies efficiently.

Conclusion: WBHT is a valuable tool for proactive network monitoring and security, especially in mission-critical networks.

Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for
detecting black hole (BH) anomalies in communication networks. These anomalies
cause packet loss without failure notifications, disrupting connectivity and
leading to financial losses. WBHT combines generative modeling, sequential
learning, and attention mechanisms to improve BH anomaly detection. It
integrates a Wasserstein generative adversarial network with attention
mechanisms for stable training and accurate anomaly identification. The model
uses long-short-term memory layers to capture long-term dependencies and
convolutional layers for local temporal patterns. A latent space encoding
mechanism helps distinguish abnormal network behavior. Tested on real-world
network data, WBHT outperforms existing models, achieving significant
improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and
ability to detect previously undetected anomalies make it a valuable tool for
proactive network monitoring and security, especially in mission-critical
networks.

</details>


### [133] [Set-based Implicit Likelihood Inference of Galaxy Cluster Mass](https://arxiv.org/abs/2507.20378)
*Bonny Y. Wang,Leander Thiele*

Main category: cs.LG

TL;DR: A machine learning framework using Deep Sets and conditional normalizing flows improves galaxy cluster mass estimates by reducing scatter and providing calibrated uncertainties.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and interpretability of galaxy cluster mass predictions by leveraging positional and velocity data of member galaxies.

Method: Combines Deep Sets and conditional normalizing flows to analyze galaxy dynamics, trained on the Uchuu-UniverseMachine simulation.

Result: Significantly reduces scatter and offers well-calibrated uncertainties compared to traditional methods.

Conclusion: The framework provides a robust and interpretable solution for galaxy cluster mass estimation.

Abstract: We present a set-based machine learning framework that infers posterior
distributions of galaxy cluster masses from projected galaxy dynamics. Our
model combines Deep Sets and conditional normalizing flows to incorporate both
positional and velocity information of member galaxies to predict residual
corrections to the $M$-$\sigma$ relation for improved interpretability. Trained
on the Uchuu-UniverseMachine simulation, our approach significantly reduces
scatter and provides well-calibrated uncertainties across the full mass range
compared to traditional dynamical estimates.

</details>


### [134] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: The paper introduces DPPF, a distributed training algorithm that balances communication efficiency and model performance by leveraging flat minima in the loss landscape.


<details>
  <summary>Details</summary>
Motivation: To improve the trade-off between communication efficiency and model performance in distributed DNN training by exploring flat-minima regions.

Method: Proposes Inverse Mean Valley as a sharpness measure, integrates it as a lightweight regularizer, and develops the DPPF algorithm with pull-push dynamics.

Result: DPPF outperforms other methods in generalization and communication efficiency, locating flatter minima and providing theoretical guarantees.

Conclusion: DPPF effectively combines flat-minima seeking with distributed training, offering practical and theoretical advantages.

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [135] [ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings](https://arxiv.org/abs/2507.20426)
*Samiul Based Shuvo,Tasnia Binte Mamun,U Rajendra Acharya*

Main category: cs.LG

TL;DR: A novel deep learning framework, ResCap-DBP, combines residual learning and Capsule Networks to predict DNA-binding proteins from raw sequences, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Experimental DBP identification is costly and time-consuming, necessitating efficient computational techniques.

Method: ResCap-DBP uses residual blocks with dilated convolutions and Capsule Networks to extract features and capture hierarchical relationships. ProteinBERT embeddings and one-hot encoding were compared.

Result: The model achieved high AUC scores (up to 98.0%) on benchmark datasets, with balanced sensitivity and specificity.

Conclusion: ResCap-DBP is effective and scalable for DBP prediction, leveraging global protein representations and advanced deep learning.

Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular
processes, making their accurate identification essential for understanding
biological functions and disease mechanisms. Experimental methods for DBP
identification are time-consuming and costly, driving the need for efficient
computational prediction techniques. In this study, we propose a novel deep
learning framework, ResCap-DBP, that combines a residual learning-based encoder
with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly
from raw protein sequences. Our architecture incorporates dilated convolutions
within residual blocks to mitigate vanishing gradient issues and extract rich
sequence features, while capsule layers with dynamic routing capture
hierarchical and spatial relationships within the learned feature space. We
conducted comprehensive ablation studies comparing global and local embeddings
from ProteinBERT and conventional one-hot encoding. Results show that
ProteinBERT embeddings substantially outperform other representations on large
datasets. Although one-hot encoding showed marginal advantages on smaller
datasets, such as PDB186, it struggled to scale effectively. Extensive
evaluations on four pairs of publicly available benchmark datasets demonstrate
that our model consistently outperforms current state-of-the-art methods. It
achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On
independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%
and 83.3%, while maintaining competitive performance on larger datasets such as
PDB20000. Notably, the model maintains a well balanced sensitivity and
specificity across datasets. These results demonstrate the efficacy and
generalizability of integrating global protein representations with advanced
deep learning architectures for reliable and scalable DBP prediction in diverse
genomic contexts.

</details>


### [136] [FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning](https://arxiv.org/abs/2507.20433)
*Alessandro Capurso,Elia Piccoli,Davide Bacciu*

Main category: cs.LG

TL;DR: FAST, a framework for adaptive similarity-based transfer, improves knowledge transfer in evolving domains by leveraging visual and textual data to estimate task similarity, reducing training steps while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Transfer Learning (TL) like negative transfer, domain adaptation, and inefficiency in source policy selection, particularly in dynamic domains like game development.

Method: Uses visual frames and textual descriptions to create latent task representations, estimating environment similarity to guide policy transfer for novel tasks.

Result: FAST achieves competitive performance with fewer training steps compared to learning-from-scratch methods in racing track experiments.

Conclusion: Embedding-driven task similarity estimation is effective for efficient knowledge transfer and reduced computational costs.

Abstract: Transfer Learning (TL) offers the potential to accelerate learning by
transferring knowledge across tasks. However, it faces critical challenges such
as negative transfer, domain adaptation and inefficiency in selecting solid
source policies. These issues often represent critical problems in evolving
domains, i.e. game development, where scenarios transform and agents must
adapt. The continuous release of new agents is costly and inefficient. In this
work we challenge the key issues in TL to improve knowledge transfer, agents
performance across tasks and reduce computational costs. The proposed
methodology, called FAST - Framework for Adaptive Similarity-based Transfer,
leverages visual frames and textual descriptions to create a latent
representation of tasks dynamics, that is exploited to estimate similarity
between environments. The similarity scores guides our method in choosing
candidate policies from which transfer abilities to simplify learning of novel
tasks. Experimental results, over multiple racing tracks, demonstrate that FAST
achieves competitive final performance compared to learning-from-scratch
methods while requiring significantly less training steps. These findings
highlight the potential of embedding-driven task similarity estimations.

</details>


### [137] [BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool](https://arxiv.org/abs/2507.20440)
*Vicente Ramos,Sundous Hussein,Mohamed Abdel-Hafiz,Arunangshu Sarkar,Weixuan Liu,Katerina J. Kechris,Russell P. Bowler,Leslie Lange,Farnoush Banaei-Kashani*

Main category: cs.LG

TL;DR: BioNeuralNet is a Python framework using GNNs for multi-omics network analysis, offering modular tools for network construction, embedding, and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high dimensionality and sparsity in multi-omics data by leveraging network-based approaches for better analysis.

Method: BioNeuralNet employs Graph Neural Networks (GNNs) to create low-dimensional representations from multi-omics networks, supporting various analysis stages.

Result: The framework provides flexible, reproducible, and user-friendly tools for multi-omics network analysis, compatible with major Python packages.

Conclusion: BioNeuralNet is a valuable open-source tool for precision medicine, enhancing multi-omics research through network-based methods.

Abstract: Multi-omics data offer unprecedented insights into complex biological
systems, yet their high dimensionality, sparsity, and intricate interactions
pose significant analytical challenges. Network-based approaches have advanced
multi-omics research by effectively capturing biologically relevant
relationships among molecular entities. While these methods are powerful for
representing molecular interactions, there remains a need for tools
specifically designed to effectively utilize these network representations
across diverse downstream analyses. To fulfill this need, we introduce
BioNeuralNet, a flexible and modular Python framework tailored for end-to-end
network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural
Networks (GNNs) to learn biologically meaningful low-dimensional
representations from multi-omics networks, converting these complex molecular
networks into versatile embeddings. BioNeuralNet supports all major stages of
multi-omics network analysis, including several network construction
techniques, generation of low-dimensional representations, and a broad range of
downstream analytical tasks. Its extensive utilities, including diverse GNN
architectures, and compatibility with established Python packages (e.g.,
scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick
adoption. BioNeuralNet is an open-source, user-friendly, and extensively
documented framework designed to support flexible and reproducible multi-omics
network analysis in precision medicine.

</details>


### [138] [Provable In-Context Learning of Nonlinear Regression with Transformers](https://arxiv.org/abs/2507.20443)
*Hongbo Li,Lingjie Duan,Yingbin Liang*

Main category: cs.LG

TL;DR: The paper explores how transformers achieve in-context learning (ICL) for complex nonlinear regression tasks, analyzing attention dynamics and identifying the Lipschitz constant as a key factor in convergence.


<details>
  <summary>Details</summary>
Motivation: To advance theoretical understanding of ICL in transformers, especially for complex nonlinear tasks, beyond simpler linear cases.

Method: Analyzes stage-wise attention dynamics during training, focusing on how attention scores evolve for relevant and irrelevant features. Introduces proof techniques linking Lipschitz constants to convergence.

Result: Shows that transformers consistently attend to relevant features at convergence, with convergence time varying based on the Lipschitz constant of the task function.

Conclusion: Demonstrates transformers' ICL capability for unseen nonlinear functions, with attention dynamics and Lipschitz constants playing crucial roles.

Abstract: The transformer architecture, which processes sequences of input tokens to
produce outputs for query tokens, has revolutionized numerous areas of machine
learning. A defining feature of transformers is their ability to perform
previously unseen tasks using task-specific prompts without updating
parameters, a phenomenon known as in-context learning (ICL). Recent research
has actively explored the training dynamics behind ICL, with much of the focus
on relatively simple tasks such as linear regression and binary classification.
To advance the theoretical understanding of ICL, this paper investigates more
complex nonlinear regression tasks, aiming to uncover how transformers acquire
in-context learning capabilities in these settings. We analyze the stage-wise
dynamics of attention during training: attention scores between a query token
and its target features grow rapidly in the early phase, then gradually
converge to one, while attention to irrelevant features decays more slowly and
exhibits oscillatory behavior. Our analysis introduces new proof techniques
that explicitly characterize how the nature of general non-degenerate
L-Lipschitz task functions affects attention weights. Specifically, we identify
that the Lipschitz constant L of nonlinear function classes as a key factor
governing the convergence dynamics of transformers in ICL. Leveraging these
insights, for two distinct regimes depending on whether L is below or above a
threshold, we derive different time bounds to guarantee near-zero prediction
error. Notably, despite the convergence time depending on the underlying task
functions, we prove that query tokens consistently attend to prompt tokens with
highly relevant features at convergence, demonstrating the ICL capability of
transformers for unseen functions.

</details>


### [139] [BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering](https://arxiv.org/abs/2507.20446)
*Guanghui Zhu,Xin Fang,Lei Wang,Wenzhong Chen,Rong Gu,Chunfeng Yuan,Yihua Huang*

Main category: cs.LG

TL;DR: BOASF combines Bayesian Optimization and Adaptive Successive Filtering to automate model selection and hyperparameter optimization, outperforming existing methods in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Non-experts struggle with model selection and hyperparameter tuning due to lack of expertise. BOASF aims to automate this process efficiently.

Method: BOASF uses Bayesian Optimization for selecting promising configurations and Adaptive Successive Filtering to discard poor performers. A Softmax model allocates resources adaptively.

Result: BOASF speeds up optimization, achieves robust performance, and outperforms state-of-the-art methods under various time budgets.

Conclusion: BOASF is an effective solution for automating machine learning tasks, offering efficiency and superior performance.

Abstract: Machine learning has been making great success in many application areas.
However, for the non-expert practitioners, it is always very challenging to
address a machine learning task successfully and efficiently. Finding the
optimal machine learning model or the hyperparameter combination set from a
large number of possible alternatives usually requires considerable expert
knowledge and experience. To tackle this problem, we propose a combined
Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under
a unified multi-armed bandit framework to automate the model selection or the
hyperparameter optimization. Specifically, BOASF consists of multiple
evaluation rounds in each of which we select promising configurations for each
arm using the Bayesian optimization. Then, ASF can early discard the
poor-performed arms adaptively using a Gaussian UCB-based probabilistic model.
Furthermore, a Softmax model is employed to adaptively allocate available
resources for each promising arm that advances to the next round. The arm with
a higher probability of advancing will be allocated more resources.
Experimental results show that BOASF is effective for speeding up the model
selection and hyperparameter optimization processes while achieving robust and
better prediction performance than the existing state-of-the-art automatic
machine learning methods. Moreover, BOASF achieves better anytime performance
under various time budgets.

</details>


### [140] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: WEEP is a novel, differentiable sparse regularizer that resolves the conflict between strong sparsity and gradient-based optimization, outperforming L1-norm and other non-convex regularizers in denoising tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the dilemma of non-differentiable sparsity-inducing penalties conflicting with gradient-based optimizers, which are dominant in signal processing.

Method: The authors introduce WEEP, a weakly-convex envelope-based sparse regularizer that is fully differentiable and L-smooth, ensuring compatibility with gradient-based optimizers.

Result: WEEP demonstrates superior performance over L1-norm and other non-convex sparse regularizers in signal and image denoising tasks.

Conclusion: WEEP successfully bridges the gap between statistical performance and computational tractability in sparse regularization.

Abstract: Sparse regularization is fundamental in signal processing for efficient
signal recovery and feature extraction. However, it faces a fundamental
dilemma: the most powerful sparsity-inducing penalties are often
non-differentiable, conflicting with gradient-based optimizers that dominate
the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a
novel, fully differentiable sparse regularizer derived from the weakly-convex
envelope framework. WEEP provides strong, unbiased sparsity while maintaining
full differentiability and L-smoothness, making it natively compatible with any
gradient-based optimizer. This resolves the conflict between statistical
performance and computational tractability. We demonstrate superior performance
compared to the L1-norm and other established non-convex sparse regularizers on
challenging signal and image denoising tasks.

</details>


### [141] [Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations](https://arxiv.org/abs/2507.20453)
*Camilo Tamayo-Rousseau,Yunjia Zhao,Yiqun Zhang,Randall Balestriero*

Main category: cs.LG

TL;DR: The paper evaluates the robustness of various self-attention mechanisms (Softmax, Sigmoid, Linear, Doubly Stochastic, and Cosine) in Vision Transformers under data corruption, finding Doubly Stochastic attention most robust.


<details>
  <summary>Details</summary>
Motivation: To study the robustness of self-attention variants to noise and spurious correlations, which is under-explored despite their foundational role in Transformers.

Method: Tested Softmax, Sigmoid, Linear, Doubly Stochastic, and Cosine attention in Vision Transformers under data corruption scenarios on CIFAR-10, CIFAR-100, and Imagenette datasets.

Result: Doubly Stochastic attention demonstrated the highest robustness among the tested variants.

Conclusion: Doubly Stochastic attention is recommended for contexts with imperfect data due to its superior robustness.

Abstract: Self-attention mechanisms are foundational to Transformer architectures,
supporting their impressive success in a wide range of tasks. While there are
many self-attention variants, their robustness to noise and spurious
correlations has not been well studied. This study evaluates Softmax, Sigmoid,
Linear, Doubly Stochastic, and Cosine attention within Vision Transformers
under different data corruption scenarios. Through testing across the CIFAR-10,
CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is
the most robust. Our findings inform self-attention selection in contexts with
imperfect data.

</details>


### [142] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: The paper introduces diagonally-weighted GMM (DGMM) to address computational and storage inefficiencies in MM and GMM, offering a balanced solution for high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: The limitations of MM and GMM in high-dimensional settings due to exponential growth in complexity and storage needs motivate the development of DGMM.

Method: DGMM is proposed, focusing on diagonal weighting to balance efficiency, complexity, and stability. It avoids explicit computation of moment tensors.

Result: DGMM achieves smaller estimation errors and shorter runtime compared to MM and GMM in numerical studies.

Conclusion: DGMM provides a practical, efficient alternative for high-dimensional data analysis, validated by empirical results.

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


### [143] [Shapley-Value-Based Graph Sparsification for GNN Inference](https://arxiv.org/abs/2507.20460)
*Selahattin Akkas,Ariful Azad*

Main category: cs.LG

TL;DR: Shapley value-based graph sparsification improves GNN efficiency and interpretability by preserving influential edges and removing misleading ones, outperforming non-negative scoring methods.


<details>
  <summary>Details</summary>
Motivation: To enhance Graph Neural Networks (GNNs) efficiency and interpretability by leveraging Shapley values for graph sparsification, addressing the limitations of non-negative scoring explainability methods.

Method: Uses Shapley values to assign positive and negative importance scores to edges, enabling better pruning strategies by evaluating many graph subsets.

Result: Maintains predictive performance while significantly reducing graph complexity, improving both interpretability and inference efficiency.

Conclusion: Shapley value-based sparsification is a robust and fair method for enhancing GNN efficiency and interpretability, outperforming traditional non-negative scoring approaches.

Abstract: Graph sparsification is a key technique for improving inference efficiency in
Graph Neural Networks by removing edges with minimal impact on predictions. GNN
explainability methods generate local importance scores, which can be
aggregated into global scores for graph sparsification. However, many
explainability methods produce only non-negative scores, limiting their
applicability for sparsification. In contrast, Shapley value based methods
assign both positive and negative contributions to node predictions, offering a
theoretically robust and fair allocation of importance by evaluating many
subsets of graphs. Unlike gradient-based or perturbation-based explainers,
Shapley values enable better pruning strategies that preserve influential edges
while removing misleading or adversarial connections. Our approach shows that
Shapley value-based graph sparsification maintains predictive performance while
significantly reducing graph complexity, enhancing both interpretability and
efficiency in GNN inference.

</details>


### [144] [Conditional Diffusion Models for Global Precipitation Map Inpainting](https://arxiv.org/abs/2507.20478)
*Daiko Kishikawa,Yuka Muto,Shunji Kotsuki*

Main category: cs.LG

TL;DR: The paper proposes a machine learning method using conditional diffusion models to address incomplete satellite-based precipitation data, improving spatio-temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Incomplete precipitation data from satellites like GSMaP due to orbital gaps and poor interpolation methods hinder global monitoring.

Method: A 3D U-Net with a 3D condition encoder leverages spatio-temporal data (infrared images, grids, time) for precipitation map completion, trained on ERA5 data with pseudo-GSMaP masks.

Result: The method outperforms conventional techniques, producing more consistent inpainted precipitation maps for 2024.

Conclusion: Conditional diffusion models show promise for enhancing global precipitation monitoring.

Abstract: Incomplete satellite-based precipitation presents a significant challenge in
global monitoring. For example, the Global Satellite Mapping of Precipitation
(GSMaP) from JAXA suffers from substantial missing regions due to the orbital
characteristics of satellites that have microwave sensors, and its current
interpolation methods often result in spatial discontinuities. In this study,
we formulate the completion of the precipitation map as a video inpainting task
and propose a machine learning approach based on conditional diffusion models.
Our method employs a 3D U-Net with a 3D condition encoder to reconstruct
complete precipitation maps by leveraging spatio-temporal information from
infrared images, latitude-longitude grids, and physical time inputs. Training
was carried out on ERA5 hourly precipitation data from 2020 to 2023. We
generated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps.
Performance was evaluated for the calendar year 2024, and our approach produces
more spatio-temporally consistent inpainted precipitation maps compared to
conventional methods. These results indicate the potential to improve global
precipitation monitoring using the conditional diffusion models.

</details>


### [145] [HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization](https://arxiv.org/abs/2507.20490)
*Yanheng Hou,Xunkai Li,Zhenjun Li,Bing Zhou,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: HIAL is a novel Hypergraph Active Learning framework that outperforms existing methods by preserving high-order interactions and using a dual-perspective influence function.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Active Learning methods fail to preserve high-order structural information in hypergraphs, leading to suboptimal performance.

Method: HIAL reformulates Hypergraph Active Learning as an Influence Maximization task, using a dual-perspective influence function with a HOI-Aware propagation mechanism.

Result: HIAL significantly outperforms state-of-the-art baselines in performance, efficiency, generality, and robustness across seven datasets.

Conclusion: HIAL establishes an efficient and powerful paradigm for active learning on hypergraphs, with theoretical guarantees.

Abstract: In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense
potential in handling complex systems with high-order interactions. However,
acquiring large-scale, high-quality labeled data for these models is costly,
making Active Learning (AL) a critical technique. Existing Graph Active
Learning (GAL) methods, when applied to hypergraphs, often rely on techniques
like "clique expansion," which destroys the high-order structural information
crucial to a hypergraph's success, thereby leading to suboptimal performance.
To address this challenge, we introduce HIAL (Hypergraph Active Learning), a
native active learning framework designed specifically for hypergraphs. We
innovatively reformulate the Hypergraph Active Learning (HAL) problem as an
Influence Maximization task. The core of HIAL is a dual-perspective influence
function that, based on our novel "High-Order Interaction-Aware (HOI-Aware)"
propagation mechanism, synergistically evaluates a node's feature-space
coverage (via Magnitude of Influence, MoI) and its topological influence (via
Expected Diffusion Value, EDV). We prove that this objective function is
monotone and submodular, thus enabling the use of an efficient greedy algorithm
with a formal (1-1/e) approximation guarantee. Extensive experiments on seven
public datasets demonstrate that HIAL significantly outperforms
state-of-the-art baselines in terms of performance, efficiency, generality, and
robustness, establishing an efficient and powerful new paradigm for active
learning on hypergraphs.

</details>


### [146] [DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning](https://arxiv.org/abs/2507.20499)
*Linh Le Pham Van,Minh Hoang Nguyen,Duc Kieu,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: DmC is a novel framework for cross-domain offline RL with limited target data, using k-NN for domain proximity and a diffusion model to enhance source samples, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in cross-domain offline RL, such as dataset imbalance and partial domain overlap, with limited target data.

Method: Proposes DmC, leveraging k-NN for domain proximity and a diffusion model to generate better-aligned source samples.

Result: DmC outperforms state-of-the-art methods in MuJoCo environments, showing significant performance gains.

Conclusion: DmC effectively mitigates overfitting and enhances policy learning in cross-domain offline RL with limited target data.

Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.

</details>


### [147] [Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning](https://arxiv.org/abs/2507.20498)
*Enjun Du,Siyi Liu,Yongqi Zhang*

Main category: cs.LG

TL;DR: MoKGR is a framework for KG reasoning that personalizes path exploration using adaptive length and pruning experts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs use rigid path-exploration strategies, limiting adaptability to diverse linguistic contexts and semantic nuances.

Method: MoKGR employs a mixture-of-experts framework with length and pruning experts to personalize path exploration.

Result: MoKGR shows superior performance in transductive and inductive settings on benchmarks.

Conclusion: Personalized path exploration enhances KG reasoning, as demonstrated by MoKGR's effectiveness.

Abstract: Knowledge Graph (KG) reasoning, which aims to infer new facts from structured
knowledge repositories, plays a vital role in Natural Language Processing (NLP)
systems. Its effectiveness critically depends on constructing informative and
contextually relevant reasoning paths. However, existing graph neural networks
(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting
their ability to adapt to diverse linguistic contexts and semantic nuances. To
address these limitations, we propose \textbf{MoKGR}, a mixture-of-experts
framework that personalizes path exploration through two complementary
components: (1) a mixture of length experts that adaptively selects and weights
candidate path lengths according to query complexity, providing query-specific
reasoning depth; and (2) a mixture of pruning experts that evaluates candidate
paths from a complementary perspective, retaining the most informative paths
for each query. Through comprehensive experiments on diverse benchmark, MoKGR
demonstrates superior performance in both transductive and inductive settings,
validating the effectiveness of personalized path exploration in KGs reasoning.

</details>


### [148] [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
*Kimi Team,Yifan Bai,Yiping Bao,Guanduo Chen,Jiahao Chen,Ningxin Chen,Ruijue Chen,Yanru Chen,Yuankun Chen,Yutian Chen,Zhuofu Chen,Jialei Cui,Hao Ding,Mengnan Dong,Angang Du,Chenzhuang Du,Dikang Du,Yulun Du,Yu Fan,Yichen Feng,Kelin Fu,Bofei Gao,Hongcheng Gao,Peizhong Gao,Tong Gao,Xinran Gu,Longyu Guan,Haiqing Guo,Jianhang Guo,Hao Hu,Xiaoru Hao,Tianhong He,Weiran He,Wenyang He,Chao Hong,Yangyang Hu,Zhenxing Hu,Weixiao Huang,Zhiqi Huang,Zihao Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yongsheng Kang,Guokun Lai,Cheng Li,Fang Li,Haoyang Li,Ming Li,Wentao Li,Yanhao Li,Yiwei Li,Zhaowei Li,Zheming Li,Hongzhan Lin,Xiaohan Lin,Zongyu Lin,Chengyin Liu,Chenyu Liu,Hongzhang Liu,Jingyuan Liu,Junqi Liu,Liang Liu,Shaowei Liu,T. Y. Liu,Tianwei Liu,Weizhou Liu,Yangyang Liu,Yibo Liu,Yiping Liu,Yue Liu,Zhengying Liu,Enzhe Lu,Lijun Lu,Shengling Ma,Xinyu Ma,Yingwei Ma,Shaoguang Mao,Jie Mei,Xin Men,Yibo Miao,Siyuan Pan,Yebo Peng,Ruoyu Qin,Bowen Qu,Zeyu Shang,Lidong Shi,Shengyuan Shi,Feifan Song,Jianlin Su,Zhengyuan Su,Xinjie Sun,Flood Sung,Heyi Tang,Jiawen Tao,Qifeng Teng,Chensi Wang,Dinglu Wang,Feng Wang,Haiming Wang,Jianzhou Wang,Jiaxing Wang,Jinhong Wang,Shengjie Wang,Shuyi Wang,Yao Wang,Yejie Wang,Yiqin Wang,Yuxin Wang,Yuzhi Wang,Zhaoji Wang,Zhengtao Wang,Zhexu Wang,Chu Wei,Qianqian Wei,Wenhao Wu,Xingzhe Wu,Yuxin Wu,Chenjun Xiao,Xiaotong Xie,Weimin Xiong,Boyu Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinran Xu,Yangchuan Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Xiaofei Yang,Ying Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Xingcheng Yao,Wenjie Ye,Zhuorui Ye,Bohong Yin,Longhui Yu,Enming Yuan,Hongbang Yuan,Mengjie Yuan,Haobing Zhan,Dehao Zhang,Hao Zhang,Wanlu Zhang,Xiaobin Zhang,Yangkun Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Haotian Zhao,Yikai Zhao,Huabin Zheng,Shaojie Zheng,Jianren Zhou,Xinyu Zhou,Zaida Zhou,Zhen Zhu,Weiyu Zhuang,Xinxing Zu*

Main category: cs.LG

TL;DR: Kimi K2 is a Mixture-of-Experts (MoE) large language model with 32B activated parameters, trained using the MuonClip optimizer for stability. It excels in agentic tasks and achieves SOTA performance in benchmarks like Tau2-Bench and SWE-Bench.


<details>
  <summary>Details</summary>
Motivation: To advance open-source large language models, particularly for agentic capabilities and software engineering tasks, by addressing training instability and improving performance.

Method: Uses MuonClip optimizer with QK-clip for stable training, pre-trained on 15.5T tokens, and undergoes multi-stage post-training with agentic data synthesis and RL.

Result: Achieves top scores in benchmarks (e.g., 66.1 on Tau2-Bench, 53.7 on LiveCodeBench) and excels in coding, math, and reasoning.

Conclusion: Kimi K2 is a leading open-source model for agentic and software engineering tasks, with released checkpoints for further research.

Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32
billion activated parameters and 1 trillion total parameters. We propose the
MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to
address training instability while enjoying the advanced token efficiency of
Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero
loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a
joint reinforcement learning (RL) stage, where the model improves its
capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking
models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on
Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on
SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding,
mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,
49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without
extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering
and agentic tasks. We release our base and post-trained model checkpoints to
facilitate future research and applications of agentic intelligence.

</details>


### [149] [DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning](https://arxiv.org/abs/2507.20571)
*Shuaipeng Zhang,Lanju Kong,Yixin Zhang,Wei He,Yongqing Zheng,Han Yu,Lizhen Cui*

Main category: cs.LG

TL;DR: Proposes DAG-AFL, a blockchain-based FL framework using DAG for efficiency and accuracy, outperforming 8 methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in FL include model vulnerability, coordination, and resource consumption. Blockchain-based FL is promising but inefficient with traditional consensus.

Method: DAG-AFL uses a tip selection algorithm for temporal freshness, node reachability, and model accuracy, with DAG-based verification.

Result: Improves training efficiency by 22.7% and model accuracy by 6.5% on average.

Conclusion: DAG-AFL effectively addresses FL challenges with blockchain, enhancing efficiency and accuracy.

Abstract: Due to the distributed nature of federated learning (FL), the vulnerability
of the global model and the need for coordination among many client devices
pose significant challenges. As a promising decentralized, scalable and secure
solution, blockchain-based FL methods have attracted widespread attention in
recent years. However, traditional consensus mechanisms designed for Proof of
Work (PoW) similar to blockchain incur substantial resource consumption and
compromise the efficiency of FL, particularly when participating devices are
wireless and resource-limited. To address asynchronous client participation and
data heterogeneity in FL, while limiting the additional resource overhead
introduced by blockchain, we propose the Directed Acyclic Graph-based
Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection
algorithm that considers temporal freshness, node reachability and model
accuracy, with a DAG-based trusted verification strategy. Extensive experiments
on 3 benchmarking datasets against eight state-of-the-art approaches
demonstrate that DAG-AFL significantly improves training efficiency and model
accuracy by 22.7% and 6.5% on average, respectively.

</details>


### [150] [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
*Cheng-Fu Yang,Thanh Tran,Christos Christodoulopoulos,Weitong Ruan,Rahul Gupta,Kai-Wei Chang*

Main category: cs.LG

TL;DR: The paper proposes a method using 'precedents' to enhance the flexibility and adaptability of multi-modal guardrails for content filtering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails struggle with scalability and adaptability to varied user policies, requiring extensive retraining or suffering from limited context.

Method: Introduces a critique-revise mechanism for high-quality precedents and two strategies for robust prediction, leveraging reasoning processes of similar prior data.

Result: Outperforms previous methods in few-shot and full-dataset scenarios, with superior generalization to novel policies.

Conclusion: The precedent-based approach significantly improves guardrail flexibility and adaptability, addressing key limitations of current methods.

Abstract: A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

</details>


### [151] [Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI](https://arxiv.org/abs/2507.20714)
*Asma Sadia Khan,Fariba Tasnia Khan,Tanjim Mahmud,Salman Karim Khan,Rishita Chakma,Nahed Sharmen,Mohammad Shahadat Hossain,Karl Andersson*

Main category: cs.LG

TL;DR: An explainable AI system combining BERT and Random Forest for prostate cancer diagnosis achieves high accuracy (98%) and AUC (99%), with interpretability through SHAP analysis.


<details>
  <summary>Details</summary>
Motivation: Prostate cancer requires advanced diagnostic tools, and the study aims to provide a high-performance yet interpretable solution.

Method: Combines BERT for textual clinical notes and Random Forest for numerical lab data using a novel multimodal fusion strategy.

Result: Superior performance (98% accuracy, 99% AUC) and improved recall for intermediate stages (0.900 combined vs 0.824/0.725). SHAP analysis ensures interpretability.

Conclusion: The system offers a balance of performance, efficiency, and interpretability, addressing critical needs in prostate cancer diagnostics.

Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced
diagnostic tools. We propose an explainable AI system combining BERT (for
textual clinical notes) and Random Forest (for numerical lab data) through a
novel multimodal fusion strategy, achieving superior classification performance
on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is
established, our work demonstrates that a simple yet interpretable BERT+RF
pipeline delivers clinically significant improvements - particularly for
intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824
numerical/0.725 textual). SHAP analysis provides transparent feature importance
rankings, while ablation studies prove textual features' complementary value.
This accessible approach offers hospitals a balance of high performance
(F1=89%), computational efficiency, and clinical interpretability - addressing
critical needs in prostate cancer diagnostics.

</details>


### [152] [Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning](https://arxiv.org/abs/2507.20505)
*Binxiong Li,Yuefei Wang,Binyu Zhao,Heyang Gao,Benhan Yang,Quanzhou Luo,Xue Li,Xu Xiang,Yujie Liu,Huijie Tang*

Main category: cs.LG

TL;DR: MPCCL is a novel graph clustering model addressing gaps like long-range dependency and feature collapse via multi-scale coarsening and contrastive learning, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high-order graph features, feature diversity, and structural detail loss. MPCCL aims to overcome these limitations.

Method: Uses multi-scale coarsening for graph condensation and one-to-many contrastive learning with node embeddings, augmented views, and cluster centroids. Includes graph reconstruction loss and KL divergence for consistency.

Result: Achieves a 15.24% NMI improvement on ACM and robust gains on Citeseer, Cora, and DBLP datasets.

Conclusion: MPCCL effectively addresses key challenges in graph clustering, demonstrating superior performance and robustness.

Abstract: This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and
Contrastive Learning (MPCCL) model, a novel approach for attributed graph
clustering that effectively bridges critical gaps in existing methods,
including long-range dependency, feature collapse, and information loss.
Traditional methods often struggle to capture high-order graph features due to
their reliance on low-order attribute information, while contrastive learning
techniques face limitations in feature diversity by overemphasizing local
neighborhood structures. Similarly, conventional graph coarsening methods,
though reducing graph scale, frequently lose fine-grained structural details.
MPCCL addresses these challenges through an innovative multi-scale coarsening
strategy, which progressively condenses the graph while prioritizing the
merging of key edges based on global node similarity to preserve essential
structural information. It further introduces a one-to-many contrastive
learning paradigm, integrating node embeddings with augmented graph views and
cluster centroids to enhance feature diversity, while mitigating feature
masking issues caused by the accumulation of high-frequency node weights during
multi-scale coarsening. By incorporating a graph reconstruction loss and KL
divergence into its self-supervised learning framework, MPCCL ensures
cross-scale consistency of node representations. Experimental evaluations
reveal that MPCCL achieves a significant improvement in clustering performance,
including a remarkable 15.24% increase in NMI on the ACM dataset and notable
robust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.

</details>


### [153] [First Hallucination Tokens Are Different from Conditional Ones](https://arxiv.org/abs/2507.20836)
*Jakob Snel,Seong Joon Oh*

Main category: cs.LG

TL;DR: The paper analyzes token-level hallucination signals in foundational models, finding the first hallucinated token is more detectable than conditional tokens.


<details>
  <summary>Details</summary>
Motivation: Understanding token-level hallucination signals for real-time filtering and correction in foundational models.

Method: Leveraged the RAGTruth corpus with token-level annotations and reproduced logits to analyze hallucination signals.

Result: The first hallucinated token carries a stronger signal and is more detectable than conditional tokens.

Conclusion: The study improves understanding of token-level hallucination and releases an analysis framework for further research.

Abstract: Hallucination, the generation of untruthful content, is one of the major
concerns regarding foundational models. Detecting hallucinations at the token
level is vital for real-time filtering and targeted correction, yet the
variation of hallucination signals within token sequences is not fully
understood. Leveraging the RAGTruth corpus with token-level annotations and
reproduced logits, we analyse how these signals depend on a token's position
within hallucinated spans, contributing to an improved understanding of
token-level hallucination. Our results show that the first hallucinated token
carries a stronger signal and is more detectable than conditional tokens. We
release our analysis framework, along with code for logit reproduction and
metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.

</details>


### [154] [Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations](https://arxiv.org/abs/2507.20513)
*Shiva Sinaei,Chuanjun Zheng,Kaan AkÅit,Daisuke Iwai*

Main category: cs.LG

TL;DR: Ray2Ray uses neural representations to model optical systems efficiently, avoiding surface-by-surface computations, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional ray tracing is computationally intensive due to sequential surface-by-surface calculations.

Method: Ray2Ray employs implicit neural representations to map input-output rays in a single pass, trained on nine optical systems.

Result: Achieves positional errors of ~1Âµm and angular deviations of ~0.01 degrees.

Conclusion: Neural representations can effectively replace traditional raytracing for optical modeling.

Abstract: Ray tracing is a widely used technique for modeling optical systems,
involving sequential surface-by-surface computations, which can be
computationally intensive. We propose Ray2Ray, a novel method that leverages
implicit neural representations to model optical systems with greater
efficiency, eliminating the need for surface-by-surface computations in a
single pass end-to-end model. Ray2Ray learns the mapping between rays emitted
from a given source and their corresponding rays after passing through a given
optical system in a physically accurate manner. We train Ray2Ray on nine
off-the-shelf optical systems, achieving positional errors on the order of
1{\mu}m and angular deviations on the order 0.01 degrees in the estimated
output rays. Our work highlights the potential of neural representations as a
proxy for optical raytracer.

</details>


### [155] [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://arxiv.org/abs/2507.20853)
*Saket Tiwari,Omer Gottesman,George Konidaris*

Main category: cs.LG

TL;DR: The paper proposes a geometric approach to understand the attainable states in RL for continuous spaces, linking state space geometry to action space dimensionality, and validates this with empirical results.


<details>
  <summary>Details</summary>
Motivation: Most RL theory focuses on finite spaces, but practical applications involve continuous spaces. The paper aims to bridge this gap by analyzing the geometry of attainable states.

Method: The study uses a geometric lens to analyze the manifold of attainable states induced by policy training dynamics in a two-layer neural policy, validated via actor-critic algorithms and experiments in MuJoCo and toy environments.

Result: The dimensionality of the attainable state manifold is shown to be of the order of the action space dimensionality, empirically verified in MuJoCo and toy environments. A local manifold learning layer improves performance in high-DOF control tasks.

Conclusion: The paper establishes a theoretical link between state space geometry and action space dimensionality, offering practical improvements for RL in continuous spaces.

Abstract: Advances in reinforcement learning (RL) have led to its successful
application in complex tasks with continuous state and action spaces. Despite
these advances in practice, most theoretical work pertains to finite state and
action spaces. We propose building a theoretical understanding of continuous
state and action spaces by employing a geometric lens to understand the locally
attained set of states. The set of all parametrised policies learnt through a
semi-gradient based approach induces a set of attainable states in RL. We show
that the training dynamics of a two-layer neural policy induce a low
dimensional manifold of attainable states embedded in the high-dimensional
nominal state space trained using an actor-critic algorithm. We prove that,
under certain conditions, the dimensionality of this manifold is of the order
of the dimensionality of the action space. This is the first result of its
kind, linking the geometry of the state space to the dimensionality of the
action space. We empirically corroborate this upper bound for four MuJoCo
environments and also demonstrate the results in a toy environment with varying
dimensionality. We also show the applicability of this theoretical result by
introducing a local manifold learning layer to the policy and value function
networks to improve the performance in control environments with very high
degrees of freedom by changing one layer of the neural network to learn sparse
representations.

</details>


### [156] [Kernel Learning for Sample Constrained Black-Box Optimization](https://arxiv.org/abs/2507.20533)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: KOBO introduces a method to optimize Gaussian Process kernels using a variational autoencoder, reducing sample budgets in black box optimization.


<details>
  <summary>Details</summary>
Motivation: High-dimensional optimization with expensive sampling requires methods to reduce sample budgets by learning function structure.

Method: Proposes a continuous kernel space in a variational autoencoder's latent space, with auxiliary optimization to find the best kernel.

Result: KOBO outperforms state-of-the-art methods, achieving optimal estimates with fewer samples in synthetic and real-world applications.

Conclusion: KOBO effectively reduces sample budgets in BBO, demonstrated in applications like hearing aid personalization and generative models.

Abstract: Black box optimization (BBO) focuses on optimizing unknown functions in
high-dimensional spaces. In many applications, sampling the unknown function is
expensive, imposing a tight sample budget. Ongoing work is making progress on
reducing the sample budget by learning the shape/structure of the function,
known as kernel learning. We propose a new method to learn the kernel of a
Gaussian Process. Our idea is to create a continuous kernel space in the latent
space of a variational autoencoder, and run an auxiliary optimization to
identify the best kernel. Results show that the proposed method, Kernel
Optimized Blackbox Optimization (KOBO), outperforms state of the art by
estimating the optimal at considerably lower sample budgets. Results hold not
only across synthetic benchmark functions but also in real applications. We
show that a hearing aid may be personalized with fewer audio queries to the
user, or a generative model could converge to desirable images from limited
user ratings.

</details>


### [157] [Modeling User Behavior from Adaptive Surveys with Supplemental Context](https://arxiv.org/abs/2507.20919)
*Aman Shukla,Daniel Patrick Scantlebury,Rishabh Kumar*

Main category: cs.LG

TL;DR: LANTERN is a modular architecture for user behavior modeling, combining survey data with contextual signals to improve prediction accuracy while maintaining survey primacy.


<details>
  <summary>Details</summary>
Motivation: Surveys are limited by user fatigue and incomplete responses, making them insufficient for capturing user behavior alone. LANTERN aims to enhance survey data with contextual signals.

Method: LANTERN uses selective gating, residual connections, and late fusion via cross-attention to fuse survey responses with contextual data, treating surveys as the primary signal.

Result: LANTERN outperforms survey-only baselines in multi-label prediction and shows benefits in selective modality reliance and scalability.

Conclusion: LANTERN provides a practical, extensible blueprint for behavior modeling in survey-centric applications.

Abstract: Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.

</details>


### [158] [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
*Ansh Poonia,Maeghal Jain*

Main category: cs.LG

TL;DR: LLMs encode persona-specific info in early MLP layers, which influence reasoning. Middle MHA layers use these for output, with some heads focusing on racial/color identities.


<details>
  <summary>Details</summary>
Motivation: Understand how assigning personas affects LLM reasoning on objective tasks.

Method: Use activation patching to analyze how model components encode persona info.

Result: Early MLP layers process syntax and semantics, transforming persona tokens. Middle MHA layers shape output, with some heads attending to racial/color identities.

Conclusion: Persona assignment impacts LLM reasoning, with specific layers and heads playing key roles.

Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting
diverse personas. In this study, we examine how assigning a persona influences
a model's reasoning on an objective task. Using activation patching, we take a
first step toward understanding how key components of the model encode
persona-specific information. Our findings reveal that the early Multi-Layer
Perceptron (MLP) layers attend not only to the syntactic structure of the input
but also process its semantic content. These layers transform persona tokens
into richer representations, which are then used by the middle Multi-Head
Attention (MHA) layers to shape the model's output. Additionally, we identify
specific attention heads that disproportionately attend to racial and
color-based identities.

</details>


### [159] [Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation](https://arxiv.org/abs/2507.20542)
*Dawon Ahn,Jun-Gi Jang,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: STAFF improves group fairness in tensor decomposition by minimizing error gaps between groups while reducing overall completion error.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in existing group fairness methods for tensor decomposition.

Method: Augment tensor with additional entities to mitigate imbalance and bias, evaluated under conventional and deep learning models.

Result: STAFF achieves 36% lower MSE and 59% lower MADE than baselines, balancing fairness and accuracy.

Conclusion: STAFF effectively enhances fairness without compromising tensor completion performance.

Abstract: Group fairness is important to consider in tensor decomposition to prevent
discrimination based on social grounds such as gender or age. Although few
works have studied group fairness in tensor decomposition, they suffer from
performance degradation. To address this, we propose STAFF(Sparse Tensor
Augmentation For Fairness) to improve group fairness by minimizing the gap in
completion errors of different groups while reducing the overall tensor
completion error. Our main idea is to augment a tensor with augmented entities
including sufficient observed entries to mitigate imbalance and group bias in
the sparse tensor. We evaluate \method on tensor completion with various
datasets under conventional and deep learning-based tensor models. STAFF
consistently shows the best trade-off between completion error and group
fairness; at most, it yields 36% lower MSE and 59% lower MADE than the
second-best baseline.

</details>


### [160] [From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation](https://arxiv.org/abs/2507.20968)
*Rongyao Cai,Ming Jin,Qingsong Wen,Kexin Zhang*

Main category: cs.LG

TL;DR: DARSD is a novel UDA framework addressing domain shift in time series by decomposing representations into transferable and non-transferable components, outperforming 12 existing methods.


<details>
  <summary>Details</summary>
Motivation: Domain shift in time series causes models trained on source domains to fail in target domains. Current UDA methods ignore feature composition, limiting adaptation.

Method: DARSD decomposes representations into domain-invariant and domain-specific parts using adversarial learning, pseudo-labeling, and contrastive optimization.

Result: DARSD outperforms 12 UDA methods, achieving optimal performance in 35 out of 53 cross-domain scenarios on four datasets.

Conclusion: DARSD effectively addresses domain shift by disentangling transferable knowledge, offering a theoretically explainable and superior UDA solution.

Abstract: Domain shift poses a fundamental challenge in time series analysis, where
models trained on source domain often fail dramatically when applied in target
domain with different yet similar distributions. While current unsupervised
domain adaptation (UDA) methods attempt to align cross-domain feature
distributions, they typically treat features as indivisible entities, ignoring
their intrinsic compositions that governs domain adaptation. We introduce
DARSD, a novel UDA framework with theoretical explainability that explicitly
realizes UDA tasks from the perspective of representation space decomposition.
Our core insight is that effective domain adaptation requires not just
alignment, but principled disentanglement of transferable knowledge from mixed
representations. DARSD consists three synergistic components: (I) An
adversarial learnable common invariant basis that projects original features
into a domain-invariant subspace while preserving semantic content; (II) A
prototypical pseudo-labeling mechanism that dynamically separates target
features based on confidence, hindering error accumulation; (III) A hybrid
contrastive optimization strategy that simultaneously enforces feature
clustering and consistency while mitigating emerging distribution gaps.
Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR,
HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,
achieving optimal performance in 35 out of 53 cross-domain scenarios.

</details>


### [161] [SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment](https://arxiv.org/abs/2507.20984)
*Yixin Song,Zhenliang Xue,Dongliang Wei,Feiyang Chen,Jianxiang Gao,Junchen Liu,Hangyu Liang,Guangshuo Qin,Chengrong Tian,Bo Wen,Longyu Zhao,Xinrui Zheng,Zeyu Mi,Haibo Chen*

Main category: cs.LG

TL;DR: SmallThinker is a family of LLMs designed for local devices, optimizing for weak computational power, limited memory, and slow storage. It introduces innovations like a two-level sparse structure, pre-attention router, and hybrid sparse attention to achieve high performance on consumer CPUs.


<details>
  <summary>Details</summary>
Motivation: To enable LLM deployment on local devices with limited resources, challenging the reliance on GPU-powered cloud infrastructure.

Method: Architects SmallThinker with a deployment-aware design: two-level sparse structure (MoE + sparse feed-forward), pre-attention router for I/O efficiency, and NoPE-RoPE hybrid sparse attention for memory savings.

Result: Achieves state-of-the-art performance, outperforming larger LLMs, with models running efficiently on consumer CPUs (20+ tokens/s) using minimal memory (1GB-8GB).

Conclusion: SmallThinker demonstrates that LLMs can be optimized for local devices, eliminating the need for expensive GPU hardware while maintaining high performance.

Abstract: While frontier large language models (LLMs) continue to push capability
boundaries, their deployment remains confined to GPU-powered cloud
infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs
natively designed - not adapted - for the unique constraints of local devices:
weak computational power, limited memory, and slow storage. Unlike traditional
approaches that mainly compress existing models built for clouds, we architect
SmallThinker from the ground up to thrive within these limitations. Our
innovation lies in a deployment-aware architecture that transforms constraints
into design principles. First, We introduce a two-level sparse structure
combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward
networks, drastically reducing computational demands without sacrificing model
capacity. Second, to conquer the I/O bottleneck of slow storage, we design a
pre-attention router that enables our co-designed inference engine to prefetch
expert parameters from storage while computing attention, effectively hiding
storage latency that would otherwise cripple on-device inference. Third, for
memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to
slash KV cache requirements. We release SmallThinker-4B-A0.6B and
SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and
even outperform larger LLMs. Remarkably, our co-designed system mostly
eliminates the need for expensive GPU hardware: with Q4_0 quantization, both
models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB
and 8GB of memory respectively. SmallThinker is publicly available at
hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and
hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.

</details>


### [162] [Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy](https://arxiv.org/abs/2507.20573)
*Yaxin Xiao,Qingqing Ye,Li Hu,Huadi Zheng,Haibo Hu,Zi Liang,Haoyang Li,Yijie Jiao*

Main category: cs.LG

TL;DR: Machine unlearning algorithms fail to fully protect privacy, leaving residuals vulnerable to attacks like ReA. A dual-phase framework mitigates this risk effectively.


<details>
  <summary>Details</summary>
Motivation: To address the privacy risks posed by approximate unlearning algorithms, which leave implicit residuals of unlearned data.

Method: Proposes the Reminiscence Attack (ReA) and a dual-phase unlearning framework to eliminate residuals and ensure convergence stability.

Result: ReA outperforms prior attacks, while the framework reduces privacy risks with minimal computational overhead.

Conclusion: The dual-phase framework effectively balances unlearning efficacy and privacy protection, making it practical for real-world use.

Abstract: Machine unlearning enables the removal of specific data from ML models to
uphold the right to be forgotten. While approximate unlearning algorithms offer
efficient alternatives to full retraining, this work reveals that they fail to
adequately protect the privacy of unlearned data. In particular, these
algorithms introduce implicit residuals which facilitate privacy attacks
targeting at unlearned data. We observe that these residuals persist regardless
of model architectures, parameters, and unlearning algorithms, exposing a new
attack surface beyond conventional output-based leakage. Based on this insight,
we propose the Reminiscence Attack (ReA), which amplifies the correlation
between residuals and membership privacy through targeted fine-tuning
processes. ReA achieves up to 1.90x and 1.12x higher accuracy than prior
attacks when inferring class-wise and sample-wise membership, respectively. To
mitigate such residual-induced privacy risk, we develop a dual-phase
approximate unlearning framework that first eliminates deep-layer unlearned
data traces and then enforces convergence stability to prevent models from
"pseudo-convergence", where their outputs are similar to retrained models but
still preserve unlearned residuals. Our framework works for both classification
and generation tasks. Experimental evaluations confirm that our approach
maintains high unlearning efficacy, while reducing the adaptive privacy attack
accuracy to nearly random guess, at the computational cost of 2-12% of full
retraining from scratch.

</details>


### [163] [Personalized Treatment Effect Estimation from Unstructured Data](https://arxiv.org/abs/2507.20993)
*Henri Arno,Thomas Demeester*

Main category: cs.LG

TL;DR: The paper introduces methods for estimating personalized treatment effects using unstructured data, addressing confounding and sampling biases with theoretically grounded estimators and a regression-based correction.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on structured covariates, limiting their use with unstructured data like clinical notes or medical images, which have significant potential in fields like healthcare.

Method: The authors propose an approximate 'plug-in' method for neural representations of unstructured data, two estimators to avoid confounding bias, and a regression-based correction for sampling bias.

Result: Experiments on benchmark datasets show the plug-in method performs well across settings despite its simplicity.

Conclusion: The proposed methods effectively leverage unstructured data for causal inference while addressing biases, with practical applications in healthcare.

Abstract: Existing methods for estimating personalized treatment effects typically rely
on structured covariates, limiting their applicability to unstructured data.
Yet, leveraging unstructured data for causal inference has considerable
application potential, for instance in healthcare, where clinical notes or
medical images are abundant. To this end, we first introduce an approximate
'plug-in' method trained directly on the neural representations of unstructured
data. However, when these fail to capture all confounding information, the
method may be subject to confounding bias. We therefore introduce two
theoretically grounded estimators that leverage structured measurements of the
confounders during training, but allow estimating personalized treatment
effects purely from unstructured inputs, while avoiding confounding bias. When
these structured measurements are only available for a non-representative
subset of the data, these estimators may suffer from sampling bias. To address
this, we further introduce a regression-based correction that accounts for the
non-uniform sampling, assuming the sampling mechanism is known or can be
well-estimated. Our experiments on two benchmark datasets show that the plug-in
method, directly trainable on large unstructured datasets, achieves strong
empirical performance across all settings, despite its simplicity.

</details>


### [164] [Fusing CFD and measurement data using transfer learning](https://arxiv.org/abs/2507.20576)
*Alexander Barklage,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: A neural network-based method combines simulation and measurement data via transfer learning, outperforming traditional linear methods like proper orthogonal decomposition for aerodynamic analysis.


<details>
  <summary>Details</summary>
Motivation: Current aerodynamic analysis methods vary in accuracy and resolution, creating a need for data-driven models that combine their advantages.

Method: A neural network is trained on high-resolution simulation data first, then fine-tuned with sparse but accurate measurement data using transfer learning.

Result: The method improves accuracy near nonlinearities and provides solutions for arbitrary flow conditions, aiding design and certification.

Conclusion: The approach is generalizable to more complex architectures, offering potential for broader applications in aerodynamic analysis.

Abstract: Aerodynamic analysis during aircraft design usually involves methods of
varying accuracy and spatial resolution, which all have their advantages and
disadvantages. It is therefore desirable to create data-driven models which
effectively combine these advantages. Such data fusion methods for distributed
quantities mainly rely on proper orthogonal decomposition as of now, which is a
linear method. In this paper, we introduce a non-linear method based on neural
networks combining simulation and measurement data via transfer learning. The
network training accounts for the heterogeneity of the data, as simulation data
usually features a high spatial resolution, while measurement data is sparse
but more accurate. In a first step, the neural network is trained on simulation
data to learn spatial features of the distributed quantities. The second step
involves transfer learning on the measurement data to correct for systematic
errors between simulation and measurement by only re-training a small subset of
the entire neural network model. This approach is applied to a multilayer
perceptron architecture and shows significant improvements over the established
method based on proper orthogonal decomposition by producing more physical
solutions near nonlinearities. In addition, the neural network provides
solutions at arbitrary flow conditions, thus making the model useful for flight
mechanical design, structural sizing, and certification. As the proposed
training strategy is very general, it can also be applied to more complex
neural network architectures in the future.

</details>


### [165] [Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition](https://arxiv.org/abs/2507.20997)
*Haris Khan,Shumaila Asif,Sadia Asif*

Main category: cs.LG

TL;DR: MDM-OC is a framework for scalable, interference-free, and reversible model merging in continual learning, outperforming prior methods in accuracy and compliance.


<details>
  <summary>Details</summary>
Motivation: Addressing issues like task interference, catastrophic forgetting, and lack of reversibility in model merging and continual learning.

Method: Encodes task-specific models as deltas from a shared base, projects them into orthogonal subspaces, and merges them via gradient-based optimization.

Result: Outperforms baselines in accuracy, backward transfer, and unmerge fidelity, while being memory-efficient and computationally tractable.

Conclusion: MDM-OC provides a principled solution for modular and compliant AI system design.

Abstract: In real-world machine learning deployments, models must be continually
updated, composed, and when required, selectively undone. However, existing
approaches to model merging and continual learning often suffer from task
interference, catastrophic forgetting, or lack of reversibility. We propose
Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework
that enables scalable, interference-free, and reversible composition of
fine-tuned models. Each task-specific model is encoded as a delta from a shared
base and projected into an orthogonal subspace to eliminate conflict. These
projected deltas are then merged via gradient-based optimization to form a
unified model that retains performance across tasks. Our approach supports
continual integration of new models, structured unmerging for compliance such
as GDPR requirements, and model stability via elastic weight consolidation and
synthetic replay. Extensive experiments on vision and natural language
processing benchmarks demonstrate that MDM-OC outperforms prior baselines in
accuracy, backward transfer, and unmerge fidelity, while remaining
memory-efficient and computationally tractable. This framework offers a
principled solution for modular and compliant AI system design.

</details>


### [166] [PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation](https://arxiv.org/abs/2507.20592)
*Fei Kong,Xiaohan Shan,Yanwei Hu,Jianmin Li*

Main category: cs.LG

TL;DR: PhaseNAS is an LLM-based NAS framework with dynamic phase transitions and structured architecture templates, achieving higher accuracy and efficiency across vision tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between search space exploration and efficiency in NAS, and overcoming limitations of static search strategies and ambiguous representations in LLM-based methods.

Method: Uses dynamic phase transitions guided by real-time score thresholds and a structured architecture template language for consistent code generation.

Result: Outperforms on NAS-Bench-Macro, reduces search time by 86% for image classification, and produces better YOLOv8 variants for object detection.

Conclusion: PhaseNAS is efficient, adaptive, and generalizable for diverse vision tasks.

Abstract: Neural Architecture Search (NAS) is challenged by the trade-off between
search space exploration and efficiency, especially for complex tasks. While
recent LLM-based NAS methods have shown promise, they often suffer from static
search strategies and ambiguous architecture representations. We propose
PhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by
real-time score thresholds and a structured architecture template language for
consistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS
consistently discovers architectures with higher accuracy and better rank. For
image classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%
while maintaining or improving accuracy. In object detection, it automatically
produces YOLOv8 variants with higher mAP and lower resource cost. These results
demonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS
across diverse vision tasks.

</details>


### [167] [Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability](https://arxiv.org/abs/2507.21004)
*Fang Li*

Main category: cs.LG

TL;DR: CFNs are interpretable models using compositional mathematical functions, achieving competitive performance while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: Address the black-box nature of DNNs in high-stakes domains requiring transparency.

Method: Introduces Compositional Function Networks (CFNs) with diverse compositional patterns (sequential, parallel, conditional) and differentiable training.

Result: CFNs achieve 96.24% accuracy on CIFAR-10, outperforming interpretable models like Explainable Boosting Machines.

Conclusion: CFNs combine deep learning expressiveness with interpretability, ideal for performance-critical and accountable applications.

Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their
black-box nature limits deployment in high-stakes domains requiring
transparency. We introduce Compositional Function Networks (CFNs), a novel
framework that builds inherently interpretable models by composing elementary
mathematical functions with clear semantics. Unlike existing interpretable
approaches that are limited to simple additive structures, CFNs support diverse
compositional patterns -- sequential, parallel, and conditional -- enabling
complex feature interactions while maintaining transparency. A key innovation
is that CFNs are fully differentiable, allowing efficient training through
standard gradient descent. We demonstrate CFNs' versatility across multiple
domains, from symbolic regression to image classification with deep
hierarchical networks. Our empirical evaluation shows CFNs achieve competitive
performance against black-box models (96.24% accuracy on CIFAR-10) while
outperforming state-of-the-art interpretable models like Explainable Boosting
Machines. By combining the hierarchical expressiveness and efficient training
of deep learning with the intrinsic interpretability of well-defined
mathematical functions, CFNs offer a powerful framework for applications where
both performance and accountability are paramount.

</details>


### [168] [Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation](https://arxiv.org/abs/2507.20644)
*Julia Siekiera,Christian SchlÃ¶tterer,Stefan Kramer*

Main category: cs.LG

TL;DR: A deep generative neural network is introduced to model allele frequency trajectories in E&R experiments, outperforming classic models by capturing multivariate dependencies and enabling LD estimation in Pool-Seq data.


<details>
  <summary>Details</summary>
Motivation: Classic models like Wright-Fisher oversimplify assumptions and lack parameter certainty. Deep generative models offer better integration of dependencies and noise reduction but are underused in genomics due to data demands and interpretability issues.

Method: A deep generative neural network models allele frequency trajectories by embedding SNP observations with neighboring loci information, validated on simulated E&R experiments.

Result: The model captures allele frequency distributions and enables LD estimation in Pool-Seq data, outperforming existing methods in high-LD scenarios.

Conclusion: The deep generative model provides a powerful, interpretable tool for evolutionary studies, particularly in E&R experiments, by addressing classic model limitations and enhancing LD estimation.

Abstract: The investigation of allele frequency trajectories in populations evolving
under controlled environmental pressures has become a popular approach to study
evolutionary processes on the molecular level. Statistical models based on
well-defined evolutionary concepts can be used to validate different hypotheses
about empirical observations. Despite their popularity, classic statistical
models like the Wright-Fisher model suffer from simplified assumptions such as
the independence of selected loci along a chromosome and uncertainty about the
parameters. Deep generative neural networks offer a powerful alternative known
for the integration of multivariate dependencies and noise reduction. Due to
their high data demands and challenging interpretability they have, so far, not
been widely considered in the area of population genomics. To address the
challenges in the area of Evolve and Resequencing experiments (E&R) based on
pooled sequencing (Pool-Seq) data, we introduce a deep generative neural
network that aims to model a concept of evolution based on empirical
observations over time. The proposed model estimates the distribution of allele
frequency trajectories by embedding the observations from single nucleotide
polymorphisms (SNPs) with information from neighboring loci. Evaluation on
simulated E&R experiments demonstrates the model's ability to capture the
distribution of allele frequency trajectories and illustrates the
representational power of deep generative models on the example of linkage
disequilibrium (LD) estimation. Inspecting the internally learned
representations enables estimating pairwise LD, which is typically inaccessible
in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data
high degree of LD when compared to existing methods.

</details>


### [169] [Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference](https://arxiv.org/abs/2507.20678)
*Filip de Roos,Fabio Muratore*

Main category: cs.LG

TL;DR: The paper explores novel pivoting strategies for Cholesky decomposition, linking it to Bayesian nonparametric inference, and demonstrates improved efficiency for Gaussian processes.


<details>
  <summary>Details</summary>
Motivation: Improving numerical stability and efficiency of Cholesky decomposition, especially for applications like Gaussian processes, sparse regression, and iterative solvers.

Method: Introduces new pivoting strategies inspired by Bayesian nonparametric inference, focusing on uncertainty reduction and adaptability to observations.

Result: The proposed strategies match or outperform traditional methods with minimal additional computation.

Conclusion: The novel pivoting strategies enhance Cholesky decomposition's effectiveness, particularly in Gaussian process tasks.

Abstract: The Cholesky decomposition is a fundamental tool for solving linear systems
with symmetric and positive definite matrices which are ubiquitous in linear
algebra, optimization, and machine learning. Its numerical stability can be
improved by introducing a pivoting strategy that iteratively permutes the rows
and columns of the matrix. The order of pivoting indices determines how
accurately the intermediate decomposition can reconstruct the original matrix,
thus is decisive for the algorithm's efficiency in the case of early
termination. Standard implementations select the next pivot from the largest
value on the diagonal. In the case of Bayesian nonparametric inference, this
strategy corresponds to greedy entropy maximization, which is often used in
active learning and design of experiments. We explore this connection in detail
and deduce novel pivoting strategies for the Cholesky decomposition. The
resulting algorithms are more efficient at reducing the uncertainty over a data
set, can be updated to include information about observations, and additionally
benefit from a tailored implementation. We benchmark the effectiveness of the
new selection strategies on two tasks important to Gaussian processes: sparse
regression and inference based on preconditioned iterative solvers. Our results
show that the proposed selection strategies are either on par or, in most
cases, outperform traditional baselines while requiring a negligible amount of
additional computation.

</details>


### [170] [Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks](https://arxiv.org/abs/2507.20708)
*Valentin Lafargue,Adriana Laurindo Monteiro,Emmanuelle Claeys,Laurent Risser,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: The paper explores manipulating data samples to artificially meet fairness criteria and detecting such manipulations, offering methods and recommendations for auditors.


<details>
  <summary>Details</summary>
Motivation: Ensuring AI compliance with regulations like the EU AI Act requires fairness audits, but global metrics like Disparate Impact can be manipulated.

Method: Introduces methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, and examines detection strategies.

Result: Validated through experiments on tabular datasets, showing how data can be manipulated and how to detect it.

Conclusion: Provides tools for auditors to identify manipulated datasets and ensure genuine fairness in AI algorithms.

Abstract: Proving the compliance of AI algorithms has become an important challenge
with the growing deployment of such algorithms for real-life applications.
Inspecting possible biased behaviors is mandatory to satisfy the constraints of
the regulations of the EU Artificial Intelligence's Act. Regulation-driven
audits increasingly rely on global fairness metrics, with Disparate Impact
being the most widely used. Yet such global measures depend highly on the
distribution of the sample on which the measures are computed. We investigate
first how to manipulate data samples to artificially satisfy fairness criteria,
creating minimally perturbed datasets that remain statistically
indistinguishable from the original distribution while satisfying prescribed
fairness constraints. Then we study how to detect such manipulation. Our
analysis (i) introduces mathematically sound methods for modifying empirical
distributions under fairness constraints using entropic or optimal transport
projections, (ii) examines how an auditee could potentially circumvent fairness
inspections, and (iii) offers recommendations to help auditors detect such data
manipulations. These results are validated through experiments on classical
tabular datasets in bias detection.

</details>


### [171] [Uncertainty-driven Embedding Convolution](https://arxiv.org/abs/2507.20718)
*Sungjun Lim,Kangjun Noh,Youngjun Choi,Heeyoung Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: UEC is a method to improve text embedding ensembles by incorporating model uncertainty, enhancing performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble methods for text embeddings ignore model-specific uncertainty, limiting robustness and reliability.

Method: UEC transforms deterministic embeddings into probabilistic ones and computes adaptive ensemble weights based on uncertainty, using a Bayes-optimal solution and an uncertainty-aware similarity function.

Result: UEC consistently improves performance and robustness in retrieval, classification, and semantic similarity tasks.

Conclusion: UEC effectively leverages uncertainty modeling to enhance text embedding ensembles.

Abstract: Text embeddings are essential components in modern NLP pipelines. While
numerous embedding models have been proposed, their performance varies across
domains, and no single model consistently excels across all tasks. This
variability motivates the use of ensemble techniques to combine complementary
strengths. However, most existing ensemble methods operate on deterministic
embeddings and fail to account for model-specific uncertainty, limiting their
robustness and reliability in downstream applications. To address these
limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC
first transforms deterministic embeddings into probabilistic ones in a post-hoc
manner. It then computes adaptive ensemble weights based on embedding
uncertainty, grounded in a Bayes-optimal solution under a surrogate loss.
Additionally, UEC introduces an uncertainty-aware similarity function that
directly incorporates uncertainty into similarity scoring. Extensive
experiments on retrieval, classification, and semantic similarity benchmarks
demonstrate that UEC consistently improves both performance and robustness by
leveraging principled uncertainty modeling.

</details>


### [172] [BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network](https://arxiv.org/abs/2507.20838)
*Yongzheng Liu,Yiming Wang,Po Xu,Yingjie Xu,Yuntian Chen,Dongxiao Zhang*

Main category: cs.LG

TL;DR: A multi-building energy prediction method using spatio-temporal graph neural networks outperforms traditional methods by capturing spatial dependencies and building similarities.


<details>
  <summary>Details</summary>
Motivation: Conventional methods fail to capture spatial dependencies in building energy patterns, necessitating a more robust and interpretable approach.

Method: Proposes a spatio-temporal graph neural network with graph representation, learning, and interpretation, using building characteristics and environmental factors.

Result: Outperforms baselines like XGBoost, SVR, FCNN, GRU, and Naive on the Building Data Genome Project 2 dataset, demonstrating robustness and interpretability.

Conclusion: The method effectively captures spatial relationships and building similarities, offering superior performance and interpretability in energy load prediction.

Abstract: Due to the extensive availability of operation data, data-driven methods show
strong capabilities in predicting building energy loads. Buildings with similar
features often share energy patterns, reflected by spatial dependencies in
their operational data, which conventional prediction methods struggle to
capture. To overcome this, we propose a multi-building prediction approach
using spatio-temporal graph neural networks, comprising graph representation,
graph learning, and interpretation. First, a graph is built based on building
characteristics and environmental factors. Next, a multi-level graph
convolutional architecture with attention is developed for energy prediction.
Lastly, a method interpreting the optimized graph structure is introduced.
Experiments on the Building Data Genome Project 2 dataset confirm superior
performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,
highlighting the method's robustness, generalization, and interpretability in
capturing meaningful building similarities and spatial relationships.

</details>


### [173] [Towards Explainable Deep Clustering for Time Series Data](https://arxiv.org/abs/2507.20840)
*Udo Schlegel,Gabriel Marques Tavares,Thomas Seidl*

Main category: cs.LG

TL;DR: A survey on explainable deep clustering for time series, highlighting current methods, gaps, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Deep clustering is powerful but lacks transparency, limiting its use in safety-critical applications. This survey aims to address this gap by reviewing and analyzing existing methods.

Method: The survey collects and compares peer-reviewed and preprint papers, focusing on autoencoder and attention architectures, and evaluates their real-world applications in healthcare, finance, IoT, and climate science.

Result: Most methods lack support for streaming, irregularly sampled, or privacy-preserved data, and interpretability is often an afterthought. Six research opportunities are identified to advance the field.

Conclusion: The paper proposes making interpretability a primary design goal to develop trustworthy deep clustering for time series analytics.

Abstract: Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.

</details>


### [174] [Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait](https://arxiv.org/abs/2507.20862)
*Shomoita Jahid Mitin,Rodrigue Rizk,Maximilian Scherer,Thomas Koeglsperger,Daniel Lench,KC Santosh,Arun Singh*

Main category: cs.LG

TL;DR: A multi-modal model using EEG and clinical data detects Parkinson's gait dysfunction with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for detecting gait dysfunction in Parkinson's Disease (PD) are subjective or require specialized tools. This study aims to create an objective, data-driven solution.

Method: The study used resting-state EEG signals and clinical variables to train a Bi-cephalic Self-Attention Model (BiSAM) on 124 participants (PD patients with/without freezing of gait and healthy controls).

Result: Multi-modal models (EEG + clinical data) achieved 88% accuracy, outperforming signal-only (55%) and descriptive-only (68%) models.

Conclusion: The multi-modal BiSAM offers a scalable, efficient alternative for detecting PD-related gait dysfunction, with potential for clinical use.

Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments,
including gait dysfunction, particularly in patients with freezing of gait
(FOG). Current detection methods are either subjective or reliant on
specialized gait analysis tools. This study aims to develop an objective,
data-driven, and multi-modal classification model to detect gait dysfunction in
PD patients using resting-state EEG signals combined with demographic and
clinical variables. We utilized a dataset of 124 participants: 42 PD patients
with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy
controls. Features extracted from resting-state EEG and descriptive variables
(age, education, disease duration) were used to train a novel Bi-cephalic
Self-Attention Model (BiSAM). We tested three modalities: signal-only,
descriptive-only, and multi-modal, across different EEG channel subsets
(BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed
limited performance, achieving a maximum accuracy of 55% and 68%, respectively.
In contrast, the multi-modal models significantly outperformed both, with
BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These
results demonstrate the value of integrating EEG with objective descriptive
features for robust PDFOG+ detection. This study introduces a multi-modal,
attention-based architecture that objectively classifies PDFOG+ using minimal
EEG channels and descriptive variables. This approach offers a scalable and
efficient alternative to traditional assessments, with potential applications
in routine clinical monitoring and early diagnosis of PD-related gait
dysfunction.

</details>


### [175] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso LourenÃ§o,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: iHOMER is an online multi-label learning framework that dynamically clusters labels and adapts to concept drift, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-label data streams: evolving distributions, high-dimensional label spaces, sparse labels, and concept drift affecting label correlations.

Method: Uses incremental divisive-agglomerative clustering (Jaccard similarity) and a global tree-based learner (multivariate Bernoulli process) with drift detection at global/local levels.

Result: Outperforms 5 global baselines by 23% and 12 local baselines by 32% in experiments on 23 datasets.

Conclusion: iHOMER is robust for online multi-label classification, effectively handling dynamic label spaces and concept drift.

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


### [176] [Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction](https://arxiv.org/abs/2507.20925)
*Hongzhi Zhang,Zhonglie Liu,Kun Meng,Jiameng Chen,Jia Wu,Bo Du,Di Lin,Yan Che,Wenbin Hu*

Main category: cs.LG

TL;DR: A novel approach for zero-shot compound-protein interaction (CPI) prediction using subsequence reordering and length-variable protein augmentation improves performance, especially in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in CPI prediction, such as overlooked interdependencies in protein sequences and reliance on large datasets, to better reflect real-world drug development needs.

Method: Pretrains protein representations using subsequence reordering and applies length-variable protein augmentation for small datasets. Evaluated with baseline methods.

Result: Improves baseline model performance, excels in zero-shot scenarios, and outperforms existing pre-training models in data-scarce conditions.

Conclusion: The proposed method effectively enhances CPI prediction, particularly in challenging zero-shot and data-limited settings.

Abstract: Given the vastness of chemical space and the ongoing emergence of previously
uncharacterized proteins, zero-shot compound-protein interaction (CPI)
prediction better reflects the practical challenges and requirements of
real-world drug development. Although existing methods perform adequately
during certain CPI tasks, they still face the following challenges: (1)
Representation learning from local or complete protein sequences often
overlooks the complex interdependencies between subsequences, which are
essential for predicting spatial structures and binding properties. (2)
Dependence on large-scale or scarce multimodal protein datasets demands
significant training data and computational resources, limiting scalability and
efficiency. To address these challenges, we propose a novel approach that
pretrains protein representations for CPI prediction tasks using subsequence
reordering, explicitly capturing the dependencies between protein subsequences.
Furthermore, we apply length-variable protein augmentation to ensure excellent
pretraining performance on small training datasets. To evaluate the model's
effectiveness and zero-shot learning ability, we combine it with various
baseline methods. The results demonstrate that our approach can improve the
baseline model's performance on the CPI task, especially in the challenging
zero-shot scenario. Compared to existing pre-training models, our model
demonstrates superior performance, particularly in data-scarce scenarios where
training samples are limited. Our implementation is available at
https://github.com/Hoch-Zhang/PSRP-CPI.

</details>


### [177] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: A hybrid Fourier-neural architecture breaks the precision ceiling of PINNs for the Euler-Bernoulli beam equation, achieving ultra-low L2 errors of $1.94 \times 10^{-7}$.


<details>
  <summary>Details</summary>
Motivation: Overcome the precision limitations of PINNs (errors of $10^{-3}$-$10^{-4}$) for fourth-order PDEs, enabling broader engineering adoption.

Method: Combines a truncated Fourier series with a deep neural network for adaptive residual corrections, optimized via a two-phase strategy (Adam + L-BFGS) and harmonic tuning.

Result: Achieves a 17-fold improvement over standard PINNs and 15-500Ã better than traditional methods, with optimal performance at 10 harmonics.

Conclusion: Demonstrates that ultra-precision in scientific computing is achievable with proper design, bridging gaps in existing approaches.

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [178] [PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery](https://arxiv.org/abs/2507.20954)
*David Ye,Jan Williams,Mars Gao,Stefano Riva,Matteo Tomasetto,David Zoro,J. Nathan Kutz*

Main category: cs.LG

TL;DR: PySHRED v1.0 is a Python package implementing SHRED, a deep learning strategy for modeling high-dimensional dynamical systems, with extensions for robust sensing, reduced order modeling, and physics discovery.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling real-world dynamical systems with noisy, multi-scale, or high-dimensional data.

Method: Uses SHallow REcurrent Decoders (SHRED) and includes data preprocessors and advanced methods for handling complex data.

Result: A modular, well-documented, and easy-to-install package with extensive examples, released under MIT license.

Conclusion: PySHRED v1.0 provides a practical tool for modeling dynamical systems and is designed for future extensibility.

Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for
modeling high-dimensional dynamical systems and/or spatiotemporal data from
dynamical system snapshot observations. PySHRED is a Python package that
implements SHRED and several of its major extensions, including for robust
sensing, reduced order modeling and physics discovery. In this paper, we
introduce the version 1.0 release of PySHRED, which includes data preprocessors
and a number of cutting-edge SHRED methods specifically designed to handle
real-world data that may be noisy, multi-scale, parameterized, prohibitively
high-dimensional, and strongly nonlinear. The package is easy to install,
thoroughly-documented, supplemented with extensive code examples, and
modularly-structured to support future additions. The entire codebase is
released under the MIT license and is available at
https://github.com/pyshred-dev/pyshred.

</details>


### [179] [PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes](https://arxiv.org/abs/2507.20967)
*Tianhao Wang,Simon Klancher,Kunal Mukherjee,Josh Wiedemeier,Feng Chen,Murat Kantarcioglu,Kangkook Jee*

Main category: cs.LG

TL;DR: ProvCreator is a synthetic graph framework for complex heterogeneous graphs, using transformer-based models for realistic and privacy-aware graph generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on homogeneous graphs, lacking semantic fidelity for real-world applications with complex schemas.

Method: ProvCreator uses a graph-to-sequence encoder-decoder for lossless encoding, compression, and learnable graph generation.

Result: Validated on cybersecurity and knowledge graphs, ProvCreator captures intricate dependencies, generating realistic synthetic data.

Conclusion: ProvCreator advances synthetic graph generation for complex, heterogeneous graphs with high-dimensional attributes.

Abstract: The rise of graph-structured data has driven interest in graph learning and
synthetic data generation. While successful in text and image domains,
synthetic graph generation remains challenging -- especially for real-world
graphs with complex, heterogeneous schemas. Existing research has focused
mostly on homogeneous structures with simple attributes, limiting their
usefulness and relevance for application domains requiring semantic fidelity.
  In this research, we introduce ProvCreator, a synthetic graph framework
designed for complex heterogeneous graphs with high-dimensional node and edge
attributes. ProvCreator formulates graph synthesis as a sequence generation
task, enabling the use of transformer-based large language models. It features
a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph
structure and attributes, 2. efficiently compresses large graphs for contextual
modeling, and 3. supports end-to-end, learnable graph generation.
  To validate our research, we evaluate ProvCreator on two challenging domains:
system provenance graphs in cybersecurity and knowledge graphs from
IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate
dependencies between structure and semantics, enabling the generation of
realistic and privacy-aware synthetic datasets.

</details>


### [180] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: SAE Debias is a lightweight, model-agnostic framework using k-sparse autoencoders to mitigate gender bias in T2I models without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing gender bias in T2I diffusion models, which often generate stereotypical associations between professions and genders.

Method: Leverages a pre-trained k-sparse autoencoder to identify and suppress gender-relevant directions in the latent space during inference.

Result: Substantially reduces gender bias across multiple T2I models while maintaining generation quality.

Conclusion: SAE Debias offers an interpretable, reusable, and model-agnostic solution for fairer T2I generation.

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly
by generating stereotypical associations between professions and gendered
subjects. This paper presents SAE Debias, a lightweight and model-agnostic
framework for mitigating such bias in T2I generation. Unlike prior approaches
that rely on CLIP-based filtering or prompt engineering, which often require
model-specific adjustments and offer limited control, SAE Debias operates
directly within the feature space without retraining or architectural
modifications. By leveraging a k-sparse autoencoder pre-trained on a gender
bias dataset, the method identifies gender-relevant directions within the
sparse latent space, capturing professional stereotypes. Specifically, a biased
direction per profession is constructed from sparse latents and suppressed
during inference to steer generations toward more gender-balanced outputs.
Trained only once, the sparse autoencoder provides a reusable debiasing
direction, offering effective control and interpretable insight into biased
subspaces. Extensive evaluations across multiple T2I models, including Stable
Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially
reduces gender bias while preserving generation quality. To the best of our
knowledge, this is the first work to apply sparse autoencoders for identifying
and intervening in gender bias within T2I models. These findings contribute
toward building socially responsible generative AI, providing an interpretable
and model-agnostic tool to support fairness in text-to-image generation.

</details>


### [181] [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
*Yining Huang,Bin Li,Keke Tang,Meilian Chen*

Main category: cs.LG

TL;DR: LoRA-PAR is a dual-system LoRA framework that partitions data and parameters for System 1 (fast, intuitive) and System 2 (deliberative) tasks, using fewer parameters and achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods focus on domain adaptation or layer-wise allocation but don't tailor data and parameters to different response demands. Inspired by 'Thinking, Fast and Slow,' the paper aims to specialize LLM parameters for intuitive vs. logical tasks.

Method: Classify task data via multi-model role-playing and voting, partition parameters by importance scoring, and use a two-stage fine-tuning strategy: SFT for System 1 tasks and RL for System 2 tasks.

Result: The approach reduces active parameter usage while matching or surpassing state-of-the-art PEFT baselines.

Conclusion: LoRA-PAR effectively balances parameter efficiency and task performance by aligning parameter usage with task demands.

Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit
substantially from chain-of-thought (CoT) reasoning, yet pushing their
performance typically requires vast data, large model sizes, and full-parameter
fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,
most existing approaches primarily address domain adaptation or layer-wise
allocation rather than explicitly tailoring data and parameters to different
response demands. Inspired by "Thinking, Fast and Slow," which characterizes
two distinct modes of thought-System 1 (fast, intuitive, often automatic) and
System 2 (slower, more deliberative and analytic)-we draw an analogy that
different "subregions" of an LLM's parameters might similarly specialize for
tasks that demand quick, intuitive responses versus those requiring multi-step
logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework
that partitions both data and parameters by System 1 or System 2 demands, using
fewer yet more focused parameters for each task. Specifically, we classify task
data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training
System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and
intuition and refine System 2 tasks with reinforcement learning (RL) to
reinforce deeper logical deliberation next. Extensive experiments show that the
two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while
matching or surpassing SOTA PEFT baselines.

</details>


### [182] [Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions](https://arxiv.org/abs/2507.21016)
*Jagruti Patel,Mikkel SchÃ¶ttner,Thomas A. W. Bolton,Patric Hagmann*

Main category: cs.LG

TL;DR: The study benchmarks machine learning and deep learning models for cognitive prediction using fMRI data, finding task-based fMRI superior to resting-state. GNN combining SC and FC performed best, while TGNN showed promise for task-fMRI but struggled with RS data.


<details>
  <summary>Details</summary>
Motivation: To understand neural mechanisms of cognition and improve precision medicine and early detection of neurological/psychiatric conditions.

Method: Compared Kernel Ridge Regression (KRR), Graph Neural Networks (GNN), and Transformer-GNN (TGNN) using resting-state and task fMRI data from the Human Connectome Project.

Result: Task-based fMRI outperformed resting-state. GNN with SC and FC was best overall, though not significantly better than KRR with FC. TGNN worked well for task-fMRI but not RS data.

Conclusion: Model architecture and feature representation are crucial for leveraging neuroimaging data. Multimodal DL models and Transformer-based approaches show promise for cognitive prediction.

Abstract: Predicting cognition from neuroimaging data in healthy individuals offers
insights into the neural mechanisms underlying cognitive abilities, with
potential applications in precision medicine and early detection of
neurological and psychiatric conditions. This study systematically benchmarked
classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep
learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))
for cognitive prediction using Resting-state (RS), Working Memory, and Language
task fMRI data from the Human Connectome Project Young Adult dataset.
  Our results, based on R2 scores, Pearson correlation coefficient, and mean
absolute error, revealed that task-based fMRI, eliciting neural responses
directly tied to cognition, outperformed RS fMRI in predicting cognitive
behavior. Among the methods compared, a GNN combining structural connectivity
(SC) and functional connectivity (FC) consistently achieved the highest
performance across all fMRI modalities; however, its advantage over KRR using
FC alone was not statistically significant. The TGNN, designed to model
temporal dynamics with SC as a prior, performed competitively with FC-based
approaches for task-fMRI but struggled with RS data, where its performance
aligned with the lower-performing GNN that directly used fMRI time-series data
as node features. These findings emphasize the importance of selecting
appropriate model architectures and feature representations to fully leverage
the spatial and temporal richness of neuroimaging data.
  This study highlights the potential of multimodal graph-aware DL models to
combine SC and FC for cognitive prediction, as well as the promise of
Transformer-based approaches for capturing temporal dynamics. By providing a
comprehensive comparison of models, this work serves as a guide for advancing
brain-behavior modeling using fMRI, SC and DL.

</details>


### [183] [Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming](https://arxiv.org/abs/2507.21021)
*Zhen Zhang,Dong Sam Ha,Gota Morota,Sook Shin*

Main category: cs.LG

TL;DR: A behavior-specific filtering method improves pig behavior classification accuracy in Precision Livestock Farming, outperforming traditional uniform methods.


<details>
  <summary>Details</summary>
Motivation: To enhance behavior classification accuracy for better health management and farm efficiency in Precision Livestock Farming.

Method: Combines Wavelet Denoising with a Low Pass Filter, tailored to active and inactive pig behaviors.

Result: Achieved a peak accuracy of 94.73%, surpassing the 91.58% accuracy of traditional methods.

Conclusion: Behavior-specific filtering is effective for improving animal behavior monitoring, aiding farm management.

Abstract: This study proposes a behavior-specific filtering method to improve behavior
classification accuracy in Precision Livestock Farming. While traditional
filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%,
they apply uniform processing to all behaviors. In contrast, the proposed
behavior-specific filtering method combines Wavelet Denoising with a Low Pass
Filter, tailored to active and inactive pig behaviors, and achieved a peak
accuracy of 94.73%. These results highlight the effectiveness of
behavior-specific filtering in enhancing animal behavior monitoring, supporting
better health management and farm efficiency.

</details>


### [184] [On Using the Shapley Value for Anomaly Localization: A Statistical Investigation](https://arxiv.org/abs/2507.21023)
*Rick S. Blum,Franziska Freytag*

Main category: cs.LG

TL;DR: Using a fixed term in Shapley value calculation simplifies anomaly localization with the same error probability as full Shapley value for independent cases.


<details>
  <summary>Details</summary>
Motivation: To simplify anomaly localization in sensor data systems while maintaining accuracy.

Method: Compare a single fixed term in Shapley value calculation against full Shapley value for anomaly localization.

Result: Fixed term achieves lower complexity with same error probability for independent cases; no proof for dependent cases.

Conclusion: Simplified Shapley value method is effective for independent observations, but further research is needed for dependent cases.

Abstract: Recent publications have suggested using the Shapley value for anomaly
localization for sensor data systems. Using a reasonable mathematical anomaly
model for full control, experiments indicate that using a single fixed term in
the Shapley value calculation achieves a lower complexity anomaly localization
test, with the same probability of error, as a test using the Shapley value for
all cases tested. A proof demonstrates these conclusions must be true for all
independent observation cases. For dependent observation cases, no proof is
available.

</details>


### [185] [Optimization Performance of Factorization Machine with Annealing under Limited Training Data](https://arxiv.org/abs/2507.21024)
*Mayumi Nakano,Yuya Seki,Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: The paper proposes a sequential dataset construction method for FMA to improve optimization performance by limiting the dataset size, enhancing the impact of new data points.


<details>
  <summary>Details</summary>
Motivation: The performance of FMA stagnates as the dataset grows, diluting the impact of new data points on the surrogate model.

Method: A novel sequential dataset construction method retains only the most recent data points to boost their influence on the FM.

Result: Numerical experiments show the proposed FMA achieves better solutions with fewer evaluations than conventional FMA.

Conclusion: Limiting dataset size in FMA improves optimization efficiency by focusing on recent data.

Abstract: Black-box (BB) optimization problems aim to identify an input that minimizes
the output of a function (the BB function) whose input-output relationship is
unknown. Factorization machine with annealing (FMA) is a promising approach to
this task, employing a factorization machine (FM) as a surrogate model to
iteratively guide the solution search via an Ising machine. Although FMA has
demonstrated strong optimization performance across various applications, its
performance often stagnates as the number of optimization iterations increases.
One contributing factor to this stagnation is the growing number of data points
in the dataset used to train FM. It is hypothesized that as more data points
are accumulated, the contribution of newly added data points becomes diluted
within the entire dataset, thereby reducing their impact on improving the
prediction accuracy of FM. To address this issue, we propose a novel method for
sequential dataset construction that retains at most a specified number of the
most recently added data points. This strategy is designed to enhance the
influence of newly added data points on the surrogate model. Numerical
experiments demonstrate that the proposed FMA achieves lower-cost solutions
with fewer BB function evaluations compared to the conventional FMA.

</details>


### [186] [When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding](https://arxiv.org/abs/2507.21037)
*Jinzhou Wu,Baoping Tang,Qikang Li,Yi Wang,Cheng Li,Shujian Yu*

Main category: cs.LG

TL;DR: A novel MSDA framework using a pretrained Brain Foundation Model (BFM) for dynamic source selection and CS/CCS divergences for feature and decision-level alignment improves MI-EEG decoding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in MI-EEG decoding like inter-subject variability and limited labeled data, which cause negative transfer and high computational costs.

Method: Proposes a BFM-guided source selection and CS/CCS divergences for joint feature and decision-level alignment.

Result: Outperforms state-of-the-art baselines on benchmark datasets, with scalable and efficient BFM-guided selection.

Conclusion: The framework effectively enhances domain invariance and discriminability, reducing training time without performance loss.

Abstract: Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key
non-invasive brain-computer interface (BCI) paradigm for controlling external
systems, has been significantly advanced by deep learning. However, MI-EEG
decoding remains challenging due to substantial inter-subject variability and
limited labeled target data, which necessitate costly calibration for new
users. Many existing multi-source domain adaptation (MSDA) methods
indiscriminately incorporate all available source domains, disregarding the
large inter-subject differences in EEG signals, which leads to negative
transfer and excessive computational costs. Moreover, while many approaches
focus on feature distribution alignment, they often neglect the explicit
dependence between features and decision-level outputs, limiting their ability
to preserve discriminative structures. To address these gaps, we propose a
novel MSDA framework that leverages a pretrained large Brain Foundation Model
(BFM) for dynamic and informed source subject selection, ensuring only relevant
sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)
and Conditional CS (CCS) divergences to jointly perform feature-level and
decision-level alignment, enhancing domain invariance while maintaining class
discriminability. Extensive evaluations on two benchmark MI-EEG datasets
demonstrate that our framework outperforms a broad range of state-of-the-art
baselines. Additional experiments with a large source pool validate the
scalability and efficiency of BFM-guided selection, which significantly reduces
training time without sacrificing performance.

</details>


### [187] [Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements](https://arxiv.org/abs/2507.21040)
*Aditya Ravuri,Neil D. Lawrence*

Main category: cs.LG

TL;DR: Transformers are interpreted probabilistically as unrolled inference steps, showing initial linear dimensionality reduction and a graph Laplacian term. Modifying the attention matrix improves performance.


<details>
  <summary>Details</summary>
Motivation: To provide a probabilistic interpretation of transformers and explore their behavior at initialization and during operation.

Method: Derived from ProbDR framework, analyzing transformers as unrolled inference steps and modifying the attention matrix.

Result: Transformers initially perform linear dimensionality reduction; subtracting identity from the attention matrix improves validation performance.

Conclusion: The probabilistic interpretation and attention matrix modification enhance transformer performance, suggesting practical improvements.

Abstract: We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.

</details>


### [188] [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/abs/2507.21049)
*Zedong Wang,Siyuan Li,Dan Xu*

Main category: cs.LG

TL;DR: Rep-MTL introduces a novel approach to Multi-Task Learning by leveraging representation-level task saliency to enhance inter-task complementarity, outperforming traditional conflict-solving methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task optimization techniques focus on resolving conflicts but neglect the potential of shared representation spaces for promoting complementary task interactions.

Method: Rep-MTL quantifies task interactions using representation-level saliency, guided by entropy-based penalization and sample-wise cross-task alignment, to mitigate negative transfer and enhance complementarity.

Result: Rep-MTL achieves competitive performance on four MTL benchmarks, balancing task-specific learning and cross-task sharing effectively.

Conclusion: Rep-MTL demonstrates the importance of representation-level operations in MTO, offering a promising direction for future research.

Abstract: Despite the promise of Multi-Task Learning in leveraging complementary
knowledge across tasks, existing multi-task optimization (MTO) techniques
remain fixated on resolving conflicts via optimizer-centric loss scaling and
gradient manipulation strategies, yet fail to deliver consistent gains. In this
paper, we argue that the shared representation space, where task interactions
naturally occur, offers rich information and potential for operations
complementary to existing optimizers, especially for facilitating the
inter-task complementarity, which is rarely explored in MTO. This intuition
leads to Rep-MTL, which exploits the representation-level task saliency to
quantify interactions between task-specific optimization and shared
representation learning. By steering these saliencies through entropy-based
penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate
negative transfer by maintaining the effective training of individual tasks
instead pure conflict-solving, while explicitly promoting complementary
information sharing. Experiments are conducted on four challenging MTL
benchmarks covering both task-shift and domain-shift scenarios. The results
show that Rep-MTL, even paired with the basic equal weighting policy, achieves
competitive performance gains with favorable efficiency. Beyond standard
performance metrics, Power Law exponent analysis demonstrates Rep-MTL's
efficacy in balancing task-specific learning and cross-task sharing. The
project page is available at HERE.

</details>


### [189] [Flow Matching Policy Gradients](https://arxiv.org/abs/2507.21053)
*David McAllister,Songwei Ge,Brent Yi,Chung Min Kim,Ethan Weber,Hongsuk Choi,Haiwen Feng,Angjoo Kanazawa*

Main category: cs.LG

TL;DR: Flow Policy Optimization (FPO) integrates flow matching into policy gradient reinforcement learning, avoiding exact likelihood computation and outperforming Gaussian policies in multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage flow-based generative models for reinforcement learning without being tied to specific sampling methods or exact likelihood computations.

Method: FPO combines flow matching with policy gradients, using an advantage-weighted ratio from the conditional flow matching loss, compatible with PPO-clip.

Result: FPO successfully trains diffusion-style policies in continuous control tasks, capturing multimodal action distributions and outperforming Gaussian policies.

Conclusion: FPO is a flexible and effective method for integrating flow-based models into reinforcement learning, enhancing performance in complex tasks.

Abstract: Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [190] [Towards Multi-Agent Economies: Enhancing the A2A Protocol with Ledger-Anchored Identities and x402 Micropayments for AI Agents](https://arxiv.org/abs/2507.19550)
*Awid Vaziry,Sandro Rodriguez Garzon,Axel KÃ¼pper*

Main category: cs.MA

TL;DR: A novel architecture using DLT enhances A2A communication by solving decentralized agent discoverability and micropayments, enabling secure, scalable multi-agent economies.


<details>
  <summary>Details</summary>
Motivation: Address limitations in A2A protocols, specifically decentralized agent discoverability and micropayments, to enable trusted economic interactions among autonomous agents.

Method: Integrates DLT for tamper-proof AgentCards (smart contracts) and extends A2A with the x402 standard for blockchain-agnostic micropayments via HTTP 402.

Result: Demonstrates feasibility of DLT-based agent discovery and micropayments, enabling seamless agent interactions across boundaries.

Conclusion: The architecture advances agentic AI by providing a secure, scalable foundation for autonomous economic ecosystems.

Abstract: This research article presents a novel architecture to empower multi-agent
economies by addressing two critical limitations of the emerging Agent2Agent
(A2A) communication protocol: decentralized agent discoverability and
agent-to-agent micropayments. By integrating distributed ledger technology
(DLT), this architecture enables tamper-proof, on-chain publishing of
AgentCards as smart contracts, providing secure and verifiable agent
identities. The architecture further extends A2A with the x402 open standard,
facilitating blockchain-agnostic, HTTP-based micropayments via the HTTP 402
status code. This enables autonomous agents to seamlessly discover,
authenticate, and compensate each other across organizational boundaries. This
work further presents a comprehensive technical implementation and evaluation,
demonstrating the feasibility of DLT-based agent discovery and micropayments.
The proposed approach lays the groundwork for secure, scalable, and
economically viable multi-agent ecosystems, advancing the field of agentic AI
toward trusted, autonomous economic interactions.

</details>


### [191] [MLC-Agent: Cognitive Model based on Memory-Learning Collaboration in LLM Empowered Agent Simulation Environment](https://arxiv.org/abs/2507.20215)
*Ming Zhang,Yiling Xuan,Qun Ma,Yuwei Guo*

Main category: cs.MA

TL;DR: The paper proposes an individual agent model with a memory-learning collaboration mechanism to improve modeling accuracy in artificial societies by addressing long-term memory effects.


<details>
  <summary>Details</summary>
Motivation: Current agent models often overlook long-term memory accumulative effects, leading to deviations from real-world system characteristics.

Method: The model uses hierarchical memory modeling (individual, group, buffer pool) and a multi-indicator evaluation mechanism for dynamic memory updates and collaborative decision-making.

Result: Agents with the proposed model show better decision-making quality and adaptability compared to existing methods.

Conclusion: The memory-learning collaboration mechanism enhances individual-level modeling quality and anthropomorphic characteristics in artificial societies.

Abstract: Many real-world systems, such as transportation systems, ecological systems,
and Internet systems, are complex systems. As an important tool for studying
complex systems, computational experiments can map them into artificial society
models that are computable and reproducible within computers, thereby providing
digital and computational methods for quantitative analysis. In current
research, the construction of individual agent models often ignores the
long-term accumulative effect of memory mechanisms in the development process
of agents, which to some extent causes the constructed models to deviate from
the real characteristics of real-world systems. To address this challenge, this
paper proposes an individual agent model based on a memory-learning
collaboration mechanism, which implements hierarchical modeling of the memory
mechanism and a multi-indicator evaluation mechanism. Through hierarchical
modeling of the individual memory repository, the group memory repository, and
the memory buffer pool, memory can be effectively managed, and knowledge
sharing and dissemination between individuals and groups can be promoted. At
the same time, the multi-indicator evaluation mechanism enables dynamic
evaluation of memory information, allowing dynamic updates of information in
the memory set and promoting collaborative decision-making between memory and
learning. Experimental results show that, compared with existing memory
modeling methods, the agents constructed by the proposed model demonstrate
better decision-making quality and adaptability within the system. This
verifies the effectiveness of the individual agent model based on the
memory-learning collaboration mechanism proposed in this paper in improving the
quality of individual-level modeling in artificial society modeling and
achieving anthropomorphic characteristics.

</details>
