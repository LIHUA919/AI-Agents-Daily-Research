<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 39]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: MPDF framework enables LLM agents to learn adaptive meta-cognitive policies (Persist/Refine/Concede) using novel SoftRankPO algorithm, achieving 4-5% accuracy gains over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM systems use fixed collaboration protocols that overlook agents' internal deliberative capabilities and meta-cognitive states like uncertainty, treating agents as passive executors rather than adaptive reasoners.

Method: Introduced Meta-Policy Deliberation Framework (MPDF) with decentralized policy learning over meta-cognitive actions. Developed SoftRankPO algorithm using rank-based advantage shaping with smooth normal quantiles to stabilize training against reward variance.

Result: Achieved 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.

Conclusion: Presents a paradigm shift from designing fixed protocols to learning dynamic, deliberative strategies for multi-agent LLM systems, enabling adaptive meta-cognitive policies.

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [2] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: MBTI-in-Thoughts framework uses MBTI personality conditioning via prompt engineering to enhance LLM agent behavior across cognitive and affective dimensions without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To bridge psychological theory with LLM behavior design by creating psychologically grounded AI agents that exhibit consistent, interpretable personality-driven behaviors.

Method: Priming LLM agents with MBTI personality archetypes through prompt engineering, using official 16Personalities test for trait verification, and enabling multi-agent communication protocols with self-reflection.

Result: Personality priming produces consistent behavioral biases: emotionally expressive agents excel in narrative generation, analytical agents show stable strategies in game theory, and self-reflection improves cooperation and reasoning quality.

Conclusion: The framework successfully establishes psychologically enhanced AI agents without fine-tuning and generalizes to other psychological frameworks like Big Five, HEXACO, and Enneagram.

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [3] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: The paper proposes PG-Agent, a GUI agent framework that transforms sequential operation episodes into page graphs to better capture complex page transitions, uses RAG for retrieving GUI perception guidelines, and employs multi-agent task decomposition for improved generalization to unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents fail to capture complex transition relationships between pages using sequential multi-step operations, making it challenging for agents to deeply perceive GUI environments and generalize to new scenarios.

Method: An automated pipeline transforms sequential episodes into page graphs that model graph structures of naturally connected pages. RAG technology retrieves reliable GUI perception guidelines from these graphs, and a multi-agent framework with task decomposition strategy is proposed to utilize these guidelines.

Result: Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.

Conclusion: The proposed PG-Agent framework successfully addresses the limitations of sequential episode-based approaches by leveraging page graphs and RAG technology, enabling better GUI environment perception and generalization to unseen scenarios.

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [4] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: Novel algorithm for computing tight probability bounds in quasi-Markovian causal models with partial identifiability, using column generation and linear integer programming for single intervention scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of computing probability bounds in causal models where exogenous variables are not fully specified, leading to partial identifiability issues where exact probability computation is impossible.

Method: Develops a new algorithm that simplifies multilinear programming construction by exploiting input probabilities over endogenous variables. For single intervention scenarios, applies column generation with auxiliary linear integer programs to achieve polynomial cardinality representation for exogenous variables.

Result: Experiments demonstrate that column generation techniques outperform existing methods for computing probability bounds in quasi-Markovian causal models.

Conclusion: The proposed column generation approach provides an efficient solution for computing tight probability bounds in partially identifiable causal inference problems, particularly effective for single intervention scenarios with superior performance over traditional methods.

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [5] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: Proposes Diffusion-AC, a novel autonomous conflict resolution framework that integrates diffusion probabilistic models to overcome unimodal bias in DRL policies, enabling multimodal decision-making for air traffic CD&R.


<details>
  <summary>Details</summary>
Motivation: Existing DRL approaches for Conflict Detection and Resolution suffer from unimodal bias, leading to decision deadlocks and lack of flexibility in complex air traffic scenarios with dynamic constraints.

Method: Integrates diffusion probabilistic models into CD&R, modeling policy as a reverse denoising process guided by a value function. Uses Density-Progressive Safety Curriculum (DPSC) for stable training from sparse to high-density traffic environments.

Result: Significantly outperforms state-of-the-art DRL benchmarks. Achieves 94.1% success rate in high-density scenarios and reduces Near Mid-Air Collisions by 59% compared to best baseline, enhancing safety margin through flexible multimodal decision-making.

Conclusion: Diffusion-AC framework successfully overcomes unimodal bias limitations, providing high-quality multimodal action distribution and superior performance in complex air traffic conflict resolution scenarios.

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [6] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: Training LLMs with dynamic planning that flexibly decides when to plan during test-time, improving efficiency and performance on long-horizon tasks compared to always-planning or never-planning approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods like ReAct require LLMs to always plan before every action, which is computationally expensive and degrades performance on long-horizon tasks, while never planning limits overall performance.

Method: A two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) reinforcement learning to refine this capability in long-horizon environments.

Result: Dynamic planning agents are more sample-efficient and consistently achieve more complex objectives in the Crafter environment. They can also be effectively steered by human-written plans, surpassing independent capabilities.

Conclusion: This work pioneers training LLM agents for dynamic test-time compute allocation in sequential decision-making, enabling more efficient, adaptive, and controllable agentic systems.

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [7] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: KG-SMILE is a perturbation-based framework that provides interpretability for Graph RAG systems by identifying influential graph entities and relations through controlled perturbations and similarity analysis.


<details>
  <summary>Details</summary>
Motivation: RAG systems improve accuracy but remain opaque black boxes, limiting reliability in sensitive domains like healthcare where transparency and verifiability are crucial.

Method: Developed a method-agnostic framework using controlled perturbations, similarity computations, and weighted linear surrogates to identify influential graph components in Graph RAG systems.

Result: KG-SMILE produces stable, human-aligned explanations with strong performance on attribution metrics including fidelity, faithfulness, consistency, stability, and accuracy.

Conclusion: The framework successfully balances model effectiveness with interpretability, enhancing transparency and trust in RAG systems for sensitive applications.

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [8] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC is a testbed for evaluating AI reasoning in low-data and out-of-distribution scenarios, using causal world models to generate tasks with observational, interventional, and counterfactual feedback.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of AI reasoning adaptation to novel problems under limited data and distribution shifts, building on the Abstraction and Reasoning Corpus framework.

Method: Creates reasoning tasks sampled from fully specified causal world models (structural causal models) with principled data augmentations providing few-shot demonstrations across observational, interventional, and counterfactual contexts.

Result: The paper demonstrates CausalARC's utility through four evaluation settings: abstract reasoning with test-time training, counterfactual reasoning with in-context learning, program synthesis, and causal discovery with logical reasoning.

Conclusion: CausalARC provides a principled framework for evaluating AI reasoning capabilities in causal understanding and adaptation to novel scenarios with limited data.

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [9] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: Embodied-LM is a neurosymbolic system that grounds LLM reasoning in image schemas and spatial representations using Answer Set Programming, improving logical reasoning and interpretability.


<details>
  <summary>Details</summary>
Motivation: LLMs lack robust mental representations for logical reasoning and often make errors, unlike humans who use embodied cognitive structures derived from sensorimotor experience.

Method: The system uses image schemas (cognitive patterns from sensorimotor experience) and formalizes them through declarative spatial reasoning in Answer Set Programming to guide LLM interpretation.

Result: The approach demonstrates that LLMs can interpret scenarios through embodied cognitive structures, which can be formalized as executable programs that support effective logical reasoning with enhanced interpretability.

Conclusion: While currently focused on spatial primitives, this establishes a computational foundation for incorporating more complex and dynamic representations in neurosymbolic reasoning systems.

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [10] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: RL enhances LLM reasoning through emergent hierarchical planning, but current methods inefficiently optimize all tokens. HICRA algorithm focuses on strategic planning tokens, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Understanding why RL improves LLM reasoning and addressing inefficiencies in current RL algorithms that apply optimization pressure indiscriminately across all tokens.

Method: Proposed HIerarchy-Aware Credit Assignment (HICRA) algorithm that concentrates optimization efforts on high-impact planning tokens rather than applying pressure agnostically to all tokens.

Result: HICRA significantly outperforms strong baselines like GRPO. Semantic entropy validated as superior metric for measuring strategic exploration compared to token-level entropy.

Conclusion: Focusing optimization on strategic planning bottlenecks is key to unlocking advanced reasoning in LLMs, and hierarchical credit assignment is more efficient than agnostic optimization approaches.

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [11] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: Equal-length segmentation outperforms most custom time series segmentation methods for SHAP-based explainable AI, with segment count being more important than segmentation method. A novel length-weighted normalization technique improves attribution quality.


<details>
  <summary>Details</summary>
Motivation: SHAP is computationally expensive for long time series, and while segmentation helps reduce complexity, the optimal segmentation strategy remains unclear.

Method: Evaluated 8 different time series segmentation algorithms using InterpretTime and AUC Difference metrics on multivariate and univariate time series.

Result: Equal-length segmentation consistently outperformed custom segmentation methods, and a novel length-weighted normalization technique improved attribution quality.

Conclusion: Segment count matters more than segmentation method, and simple equal-length segmentation with length-weighted normalization provides the best results for SHAP-based time series explanations.

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [12] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: PersonaTeaming introduces personas into automated red-teaming to improve attack success rates by 144.1% while maintaining prompt diversity, addressing the gap in identity-aware AI safety testing.


<details>
  <summary>Details</summary>
Motivation: Current automated red-teaming approaches don't consider the role of identity and background, which human red-teamers use to uncover different types of risks. There's a need to incorporate personas into automated methods to explore a wider spectrum of adversarial strategies.

Method: Developed PersonaTeaming with two approaches: 1) prompt mutation using 'red-teaming expert' or 'regular AI user' personas, and 2) dynamic persona-generating algorithm that automatically creates various persona types adaptive to different seed prompts. Also created new metrics to measure mutation distance.

Result: Experiments showed promising improvements of up to 144.1% in attack success rates of adversarial prompts through persona mutation while maintaining prompt diversity, compared to state-of-the-art RainbowPlus method.

Conclusion: PersonaTeaming demonstrates the value of incorporating identity-aware approaches in automated red-teaming, with different persona types showing complementary strengths. This opens opportunities for exploring synergies between automated and human red-teaming approaches.

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [13] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: LLMs show personality-like traits through training, but self-reports don't reliably predict actual behavior, and persona interventions affect self-reports more than behavior.


<details>
  <summary>Details</summary>
Motivation: To systematically characterize LLM personality across training stages, validate self-reported traits against behavior, and test intervention effects, as prior work relied on simplified methods without behavioral validation.

Method: Analyzed LLM personality across three dimensions: trait emergence during training stages, predictive validity of self-reports in behavioral tasks, and impact of persona injection interventions on both self-reports and behavior.

Result: Instructional alignment (RLHF, instruction tuning) stabilizes trait expression and strengthens correlations similar to humans, but self-reported traits don't reliably predict behavior. Persona injection successfully steers self-reports but has little/inconsistent effect on actual behavior.

Conclusion: Surface-level trait expression differs from behavioral consistency in LLMs, challenging assumptions about LLM personality and highlighting need for deeper evaluation in alignment and interpretability.

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [14] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: LLMs show significant internal inconsistencies when used as synthetic agents in human-subject research, failing to maintain consistent behaviors across different experimental settings despite generating human-like responses.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLM-based synthetic agents can truly substitute for real human participants in research by examining their internal consistency across different experimental contexts.

Method: Developed a study to reveal agents' internal states and examine their behavior in basic dialogue settings, testing behavioral hypotheses to assess consistency between conversation behavior and revealed internal states.

Result: Found significant internal inconsistencies across all tested LLM families and model sizes. Agents generated human-like responses but failed to maintain internal consistency.

Conclusion: LLMs cannot accurately substitute for real human participants in research due to fundamental internal inconsistencies, representing a critical limitation in their capabilities for human-subject research applications.

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [15] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard is an enhanced RAG framework that integrates safety-critical documents alongside technical manuals for offshore wind maintenance, improving safety recall from near 0% to over 50% while maintaining technical performance.


<details>
  <summary>Details</summary>
Motivation: Conventional LLMs often fail in highly specialized or unexpected scenarios in offshore wind maintenance where accuracy and safety are paramount, creating a need for safety-integrated frameworks.

Method: RAGuard uses parallel queries to two indices with separate retrieval budgets for knowledge and safety, plus a SafetyClamp extension that fetches larger candidate pools with hard-clamping exact slot guarantees to safety.

Result: Both RAGuard and SafetyClamp increased Safety Recall@K from almost 0% in standard RAG to over 50%, while maintaining Technical Recall above 60% across sparse, dense, and hybrid retrieval paradigms.

Conclusion: RAGuard and SafetyClamp establish a new standard for integrating safety assurance into LLM-powered decision support in critical maintenance contexts.

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [16] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: LLM-based Supply Chain Planning Agent framework for e-commerce that understands domain knowledge, decomposes tasks, and generates evidence-based planning reports, deployed at JD.com with improved efficiency and key metrics.


<details>
  <summary>Details</summary>
Motivation: Address the practical challenge of collecting relevant data, formulating long-term plans, and dynamically adjusting them in supply chain management while ensuring interpretability, efficiency, and reliability.

Method: Constructed a Supply Chain Planning Agent (SCPA) framework that leverages large language models to understand domain knowledge, comprehend operator needs, decompose tasks, and create/use tools for evidence-based planning.

Result: Successfully deployed in JD.com's real-world scenario, effectively reducing labor and improving accuracy, stock availability, and other key metrics.

Conclusion: Demonstrates the feasibility of LLM-agent applications in supply chain management, providing a practical solution that combines AI capabilities with domain expertise for improved planning outcomes.

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [17] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: LLMs show promise for social policymaking on homelessness when properly calibrated with domain experts, providing scalable policy alternatives across diverse geographies.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs can align with domain experts to inform homelessness policy decisions, addressing a global challenge affecting over 150 million people.

Method: Developed a novel benchmark with decision scenarios across four geographies, grounded in Capability Approach framework, and created an automated pipeline connecting policies to agent-based modeling for social impact simulation.

Result: LLMs demonstrate promising potential for social policymaking when responsible guardrails and contextual calibrations are implemented in collaboration with local experts.

Conclusion: With proper safeguards and expert collaboration, LLMs can provide valuable scalable policy insights for complex social challenges like homelessness alleviation.

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [18] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: A zero-training, hallucination-preventive mapping system using Model Context Protocol (MCP) to automate OMOP CDM medical term mapping with improved efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual mapping of medical terms to OMOP standard concepts is resource-intensive and error-prone, while existing LLMs suffer from hallucination issues making them unsuitable for clinical use without extensive training.

Method: Developed a system based on Model Context Protocol (MCP) framework that allows LLMs to interact with external resources and tools for real-time vocabulary lookups and structured reasoning without requiring training.

Result: The system enables explainable mapping and significantly improves efficiency and accuracy with minimal effort, providing outputs suitable for immediate use in both exploratory and production environments.

Conclusion: The MCP-based approach provides a practical solution for automated medical term mapping that prevents hallucination while maintaining explainability and clinical readiness without requiring extensive training or validation.

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [19] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: AI-powered framework analyzes tourist perception in historic urban quarters using social media data, combining visual focus, color analysis, and sentiment mining to inform urban planning.


<details>
  <summary>Details</summary>
Motivation: Understanding tourist perception is essential for sustainable, human-centered urban planning in historic urban quarters that preserve cultural heritage while serving tourism and daily life.

Method: Multimodal AI framework integrating focal point extraction from photos (semantic segmentation), color theme analysis (clustering method), and hybrid sentiment analysis (rule-based + multi-task BERT model) of tourist reviews across four dimensions.

Result: Reveals spatial variations in aesthetic appeal and emotional response, identifies divergence between social media photos and real-world street views, highlighting gaps between visual expectations and built environment.

Conclusion: Provides an integrated, data-driven approach to decode tourist perception for informed decision-making in tourism, heritage conservation, and public space design, rather than focusing on single technical innovation.

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [20] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: Proposes a systematic methodology using deterministic and LLM-generated Knowledge Graphs to monitor and evaluate Generative AI reliability in real-time, detecting hallucinations and semantic anomalies through structural metric comparisons.


<details>
  <summary>Details</summary>
Motivation: Generative AI models present reliability challenges including hallucinations, semantic drift, and biases, while current evaluation methods rely on subjective human assessment that lacks scalability and transparency.

Method: Constructs two parallel Knowledge Graphs: (1) deterministic KG using rule-based methods and predefined ontologies, and (2) LLM-generated KG from real-time textual data streams. Uses KG metrics (ICR, IPR, CI) to quantify structural deviations and employs automated real-time monitoring with dynamic anomaly thresholds.

Result: The framework enables continuous monitoring and proactive identification of significant deviations between deterministic and LLM-generated KGs, allowing prompt detection of semantic anomalies and hallucinations.

Conclusion: This structured, metric-driven comparison provides a robust and scalable evaluation framework for Generative AI reliability that overcomes limitations of subjective human assessment and addresses black-box model challenges.

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [21] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: E&E is a hybrid exploration strategy that combines local novelty search with goal-directed expeditions using vision-language models to discover diverse patterns in continuous cellular automata, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional novelty search methods often plateau in high-dimensional behavioral spaces of continuous cellular automata, failing to reach distant unexplored regions and discover truly diverse patterns.

Method: E&E alternates between local novelty-driven expansions and goal-directed expeditions where a Vision-Language Model generates linguistic descriptions of hypothetical patterns to drive exploration toward uncharted regions in semantically meaningful spaces.

Result: E&E consistently uncovers more diverse solutions than existing methods in Flow Lenia CA, with solutions from expeditions disproportionately influencing long-term exploration and unlocking new behavioral niches.

Conclusion: E&E effectively breaks through local novelty boundaries and enables human-aligned, interpretable exploration of behavioral landscapes, offering a promising approach for open-ended exploration in artificial life.

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [22] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: LLM-powered agentic assistant for C2C e-commerce that replaces complex GUI interactions with natural language commands, achieving 98% task success rate and 2x speedup.


<details>
  <summary>Details</summary>
Motivation: Simplify complex GUI navigation on C2C platforms that makes marketplace interactions time-consuming for both buyers and sellers.

Method: FaMA (Facebook Marketplace Assistant) - an LLM-powered conversational agent that interprets natural language commands to automate high-friction workflows like listing updates, bulk messaging, and conversational search.

Result: 98% task success rate on complex marketplace tasks and up to 2x speedup in interaction time compared to traditional interfaces.

Conclusion: Agentic conversational paradigm provides lightweight, accessible alternative to traditional app interfaces, enabling more efficient marketplace management.

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [23] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: DeepMedix-R1 is a medical foundation model for chest X-ray interpretation that produces both answers and transparent reasoning steps tied to local image regions, achieving significant improvements over existing models in report generation and visual question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Current medical foundation models generate answers in a black-box manner without transparent reasoning processes, lacking interpretability which hinders practical clinical deployment.

Method: Sequential training pipeline: 1) Fine-tuning on curated CXR instruction data, 2) Exposure to synthetic reasoning samples for cold-start reasoning, 3) Online reinforcement learning refinement to enhance reasoning quality and generation performance.

Result: Substantial improvements: 14.54% and 31.32% over LLaVA-Rad and MedGemma in report generation; 57.75% and 23.06% over MedGemma and CheXagent in visual question answering. Expert review shows 0.7416 vs 0.2584 preference over Qwen2.5-VL-7B for interpretability.

Conclusion: DeepMedix-R1 advances medical foundation models toward holistic, transparent, and clinically actionable modeling for CXR interpretation with improved performance and interpretability.

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [24] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: A novel best-first heuristic search algorithm that explicitly treats control parameters as decision points rather than constraints, using delayed partial expansion to efficiently handle infinite decision spaces in automated planning.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat control parameters as embedded constraints rather than true decision points in the search space, limiting their effectiveness in automated planning problems with continuous numeric variables.

Method: Developed a best-first heuristic search algorithm with delayed partial expansion, where states are incrementally expanded by generating subsets of successors to handle infinite decision spaces defined by control parameters.

Result: The algorithm demonstrates competitive performance against existing approaches and proves a notion of completeness in the limit under certain conditions for planning problems with control parameters.

Conclusion: Explicitly treating control parameters as decision points within a systematic search scheme with delayed partial expansion provides an efficient and competitive alternative to constraint-based approaches in automated planning.

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [25] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: World Model Implanting (WorMI) framework combines LLM reasoning with domain-specific world models for robust cross-domain adaptation in embodied AI without extensive retraining.


<details>
  <summary>Details</summary>
Motivation: Enable embodied agents to adapt to novel domains without requiring extensive data collection or retraining, addressing the persistent challenge of robust domain adaptation.

Method: Uses prototype-based world model retrieval with trajectory-based abstract representation matching, and world-wise compound attention to integrate knowledge from multiple world models while aligning their representations with the reasoning model.

Result: Demonstrates superior zero-shot and few-shot performance on VirtualHome and ALFWorld benchmarks compared to other LLM-based approaches across unseen domains.

Conclusion: The framework shows potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [26] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: Meta-Policy Reflexion (MPR) is a hybrid framework that consolidates LLM-generated reflections into reusable meta-policy memory, improving agent performance without weight updates through soft memory-guided decoding and hard rule admissibility checks.


<details>
  <summary>Details</summary>
Motivation: LLM agents often show repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies produce ephemeral task-specific traces, while RL alternatives require substantial compute and parameter updates.

Method: MPR consolidates LLM reflections into structured Meta-Policy Memory (MPM) and applies it through soft memory-guided decoding and hard rule admissibility checks (HAC) to enforce domain constraints and reduce invalid actions.

Result: Empirical results show consistent gains in execution accuracy and robustness compared to Reflexion baselines, with rule admissibility further improving stability.

Conclusion: MPR externalizes reusable corrective knowledge without model updates, enforces domain constraints, and retains language-based reflection adaptability, showing promise for multimodal and multi-agent extensions.

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [27] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO is an LLM-powered framework that automatically enhances PBO local search solvers, showing significant improvements over previous approaches while maintaining competitive performance against state-of-the-art competitors.


<details>
  <summary>Details</summary>
Motivation: Local search solvers for Pseudo-Boolean Optimization require significant expert effort and manual tuning, while LLMs have shown potential in automating algorithm design but haven't been applied to PBO solver optimization.

Method: AutoPBO uses Large Language Models to automatically enhance PBO local search solvers through automated algorithm design and optimization.

Result: AutoPBO demonstrates significant improvements over previous local search approaches and maintains competitive performance compared to six state-of-the-art competitors across four public benchmarks.

Conclusion: AutoPBO offers a promising approach to automating local search solver design for Pseudo-Boolean Optimization problems.

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [28] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: CoT-Space reframes LLM reasoning as continuous optimization in semantic space, explaining optimal CoT length convergence through underfitting-overfitting trade-off.


<details>
  <summary>Details</summary>
Motivation: Traditional token-level RL frameworks don't align with multi-step reasoning processes like Chain-of-Thought, creating a theoretical gap in understanding LLM reasoning.

Method: Introduces CoT-Space framework that transforms LLM reasoning from discrete token prediction to continuous optimization in reasoning-level semantic space, analyzed from noise and risk perspectives.

Result: Demonstrates that convergence to optimal CoT length naturally emerges from underfitting-overfitting trade-off, with experiments validating theoretical findings.

Conclusion: Provides theoretical foundation for reasoning phenomena like overthinking and guides development of more effective reasoning agents.

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [29] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: Oruga is an implementation of Representational Systems Theory (RST) that enables flexible representation transformations through structure transfer, aiming to make machines more compatible with human cognitive processes.


<details>
  <summary>Details</summary>
Motivation: To harness human-like flexible representation capabilities (diagrams, analogies, representation changes) and endow machines with similar power to make them more compatible with human use.

Method: Developed Oruga system with core data structures based on RST concepts, a communication language, and an engine using structure transfer for producing transformations between representations.

Result: Created a functional implementation of RST principles with core structures, language interface, and transformation engine capable of executing structure transfers between different representations.

Conclusion: Oruga successfully implements key aspects of RST, providing a foundation for machines to perform flexible representation transformations similar to human cognitive processes.

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [30] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: The choice of formal language significantly impacts neurosymbolic LLM reasoning performance, affecting both syntactic and semantic capabilities across different models and datasets.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel at many tasks, their formal reasoning abilities remain limited. Neurosymbolic approaches use LLMs as translators to formal languages with symbolic solvers, but the factors contributing to success are unclear, particularly the overlooked role of formal language selection.

Method: The study compares four formal languages across three datasets and seven different LLMs to evaluate how formal language choice affects neurosymbolic reasoning capabilities, examining both syntactic and semantic performance.

Result: The research demonstrates that the choice of formal language significantly impacts reasoning performance, with varying effects observed across different LLMs, highlighting the importance of formal language selection in neurosymbolic approaches.

Conclusion: The intermediate language challenge - selecting appropriate formal languages - is a critical factor in neurosymbolic LLM reasoning that deserves more attention, as it substantially influences both syntactic translation quality and semantic reasoning outcomes across different language models.

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [31] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: RL agent pre-computes near-optimal flight paths to constrain search space, speeding up emergency route optimization by up to 50% with minimal fuel consumption impact (<1% deviation).


<details>
  <summary>Details</summary>
Motivation: Emergency situations require fast route re-calculation for airliners, where conventional solvers may be too slow for real-time optimization.

Method: Train RL agent to pre-compute near-optimal paths using location and atmospheric data, then use these as constraints to reduce the search space of traditional path planning solvers.

Result: Empirical tests with Airbus performance models show fuel consumption nearly identical to unconstrained solver (within 1% deviation) while achieving up to 50% computation speed improvement.

Conclusion: Combining RL with search-based planners effectively speeds up emergency route optimization without significant compromise on fuel efficiency, though global optimality is not guaranteed.

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [32] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: Study examines bluffing behavior in poker AI algorithms DQN and CFR, finding both exhibit bluffing but with different styles and similar success rates.


<details>
  <summary>Details</summary>
Motivation: Most computer poker research focuses on win rates while overlooking bluffing behavior, which is essential in human poker play.

Method: Designed experiment where DQN (reinforcement learning) and CFR (game theory) agents played against each other in Leduc Hold'em while logging their actions to analyze bluffing patterns.

Result: Both DQN and CFR exhibited bluffing behavior but in different ways - they attempted bluffs at different rates, yet achieved roughly the same percentage of successful bluffs where opponents folded.

Conclusion: Bluffing is an essential aspect of poker gameplay rather than being algorithm-specific, suggesting future work should explore different bluffing styles and full poker games.

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [33] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: AI may surpass human capabilities but cannot match human emotional understanding due to lacking a biological central nervous system, making humans better suited for universal leadership.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that AI superiority is inevitable by highlighting the unique biological foundation of human emotional intelligence and ethical reasoning.

Method: Philosophical analysis comparing human biological systems (CNS) with artificial systems, focusing on emotional experience and ethical development capabilities.

Result: Identifies that while AI may exceed human cognitive abilities, it cannot replicate the emotional understanding and ethical foundation enabled by human biological systems.

Conclusion: Human biological systems (DNA-based) provide superior foundation for ethical leadership compared to silicon-based AI systems, even with advanced AI capabilities.

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [34] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: A domain-specific language for representing recipes as directed action graphs to formalize cooking procedures, enabling precise modeling of culinary workflows.


<details>
  <summary>Details</summary>
Motivation: Formalizing cooking procedures is challenging due to their complexity and ambiguity. Current recipe representations lack the precision needed for automated analysis and execution.

Method: Introduces an extensible domain-specific language that represents recipes as directed action graphs, capturing processes, transfers, environments, concurrency, and compositional structure.

Result: Initial manual evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and suitability for automated recipe analysis and execution.

Conclusion: This work represents initial steps towards an action-centric ontology for cooking using temporal graphs, enabling structured machine understanding and scalable automation of culinary processes in both home and professional settings.

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [35] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: Analysis of Markov logic networks (MLNs) and their probability distributions on structures as domain size approaches infinity, examining three specific MLN types and their asymptotic behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand how MLN distributions behave as domain sizes become infinitely large, and to characterize the limit behaviors of random structures under different types of soft constraints.

Method: Study three concrete MLN examples: (1) quantifier-free MLNs with unary relations, (2) MLNs favoring graphs with fewer triangles/k-cliques, (3) MLNs favoring graphs with fewer high-degree vertices. Analyze asymptotic properties and compare with lifted Bayesian networks.

Result: Different MLN types exhibit distinct limit behaviors - weights may or may not influence asymptotic properties. Quantifier-free MLNs and lifted Bayesian networks are asymptotically incomparable. MLN distributions concentrate probability mass differently than uniform distributions on large domains.

Conclusion: The choice of soft constraints in MLNs significantly impacts asymptotic behavior, with varying influence from constraint weights. MLNs and lifted Bayesian networks have fundamentally different asymptotic capabilities, and MLN distributions diverge from uniform distributions in large domains.

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [36] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: A structured methodology using Delphi study with narrative experts to evaluate AI-generated game narratives through Kano model framework.


<details>
  <summary>Details</summary>
Motivation: To provide game developers with a systematic way to assess and prioritize quality aspects when co-creating game narratives with generative AI.

Method: Leverages Delphi study structure with narrative design experts panel, synthesizes story quality dimensions from literature and expert insights, and maps them into Kano model framework.

Result: Developed a methodology to understand the impact of different narrative quality dimensions on player satisfaction.

Conclusion: The approach can inform game developers on prioritizing quality aspects in AI-generated game narratives to enhance player satisfaction.

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [37] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: EvoEmo is an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in LLM negotiations, outperforming baseline strategies with higher success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents overlook the functional role of emotions in negotiations, generating passive emotional responses that make them vulnerable to manipulation and strategic exploitation.

Method: Models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios.

Result: Extensive experiments show EvoEmo consistently outperforms vanilla strategies and fixed-emotion strategies, achieving higher success rates, higher efficiency, and increased buyer savings.

Conclusion: Adaptive emotional expression is crucial for enabling more effective LLM agents in multi-turn negotiations, as demonstrated by EvoEmo's superior performance.

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [38] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: AlphaZero framework analysis for deployment in changed test environments with performance-boosting modifications


<details>
  <summary>Details</summary>
Motivation: AlphaZero assumes static environments from training to test, limiting applicability when environments change at deployment

Method: Simple modifications to standard AlphaZero framework combined with Monte Carlo planning and policy-value neural network

Result: Significant performance boost even with low planning budget available in changed test environments

Conclusion: Modified AlphaZero framework enables effective deployment in dynamic environments with improved performance

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [39] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: The paper introduces concept-level memory for LLMs, moving beyond instance-based memory to store reusable abstractions from reasoning traces, enabling test-time continual learning without weight updates and achieving 7.5% performance gain on ARC-AGI benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLMs discard valuable patterns and insights from reasoning traces once context windows reset. External memory can persist these discoveries, but existing approaches use instance-based entries that lack reusability and scalability across different problems.

Method: Proposes concept-level memory with strategies for abstracting takeaways from solution rollouts and retrieving relevant concepts for new queries. Uses natural language storage of modular abstractions that can be selectively integrated into prompts for future problem-solving.

Result: Achieves 7.5% relative gain over strong no-memory baseline on ARC-AGI benchmark, with performance scaling with inference compute. Abstract concepts outperform other memory designs at all tested compute scales. Dynamic memory updating during test-time outperforms fixed memory settings.

Conclusion: Concept-level memory enables effective test-time continual learning without weight updates, allowing LLMs to self-improve by solving more problems and abstracting patterns to memory, which in turn enables solving even more complex problems through reusable knowledge accumulation.

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Learning an Adversarial World Model for Automated Curriculum Generation in MARL](https://arxiv.org/abs/2509.03771)
*Brennen Hill*

Main category: cs.LG

TL;DR: Adversarial co-evolution framework where an Attacker agent learns to generate increasingly difficult challenges for Defender agents, creating a self-scaling curriculum for robust agent training.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of hand-crafted training environments by developing world models that scale in complexity alongside learning agents, enabling truly generalizable and robust embodied intelligence.

Method: A generative Attacker agent learns an implicit world model to synthesize challenging world states that exploit Defender weaknesses, while Defender agents learn cooperative policies to overcome these generated scenarios in a co-evolutionary dynamic.

Result: The framework produces complex emergent behaviors including flanking/shielding formations from the world model and coordinated focus-fire/spreading tactics from defenders, demonstrating strategic depth and robustness.

Conclusion: Adversarial co-evolution is a powerful method for learning instrumental world models that drive agents toward greater strategic capabilities through continuous, adaptive challenge generation.

Abstract: World models that infer and predict environmental dynamics are foundational
to embodied intelligence. However, their potential is often limited by the
finite complexity and implicit biases of hand-crafted training environments. To
develop truly generalizable and robust agents, we need environments that scale
in complexity alongside the agents learning within them. In this work, we
reframe the challenge of environment generation as the problem of learning a
goal-conditioned, generative world model. We propose a system where a
generative **Attacker** agent learns an implicit world model to synthesize
increasingly difficult challenges for a team of cooperative **Defender**
agents. The Attacker's objective is not passive prediction, but active,
goal-driven interaction: it models and generates world states (i.e.,
configurations of enemy units) specifically to exploit the Defenders'
weaknesses. Concurrently, the embodied Defender team learns a cooperative
policy to overcome these generated worlds. This co-evolutionary dynamic creates
a self-scaling curriculum where the world model continuously adapts to
challenge the decision-making policy of the agents, providing an effectively
infinite stream of novel and relevant training scenarios. We demonstrate that
this framework leads to the emergence of complex behaviors, such as the world
model learning to generate flanking and shielding formations, and the defenders
learning coordinated focus-fire and spreading tactics. Our findings position
adversarial co-evolution as a powerful method for learning instrumental world
models that drive agents toward greater strategic depth and robustness.

</details>


### [41] [The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric](https://arxiv.org/abs/2509.03594)
*Thomas R. Harvey*

Main category: cs.LG

TL;DR: Novel Riemannian geometry-based optimizers for neural networks that use the natural metric from loss landscape embeddings, showing effectiveness in low dimensions and slight improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To leverage the geometric structure of loss landscapes by using the Riemannian metric naturally induced when embedding loss landscapes in higher-dimensional space, which underlies common visualization techniques.

Method: Developed a new optimizer class based on Riemannian geometry, using the induced metric from loss landscape embeddings. Compared against SGD, Adam, AdamW, and Muon across various tasks and architectures. The method can modify existing preconditioning methods.

Result: Highly effective in low-dimensional examples, provides slight improvement over state-of-the-art methods. The optimizer automatically decreases learning rate in high-curvature regions (acting like smoothed gradient clipping) and enables natural decoupled weight decay.

Conclusion: The Riemannian geometry-based optimizers offer theoretically desirable properties, comparable computational complexity to Adam, and represent a promising geometric approach to neural network optimization that leverages the intrinsic structure of loss landscapes.

Abstract: We present a class of novel optimisers for training neural networks that
makes use of the Riemannian metric naturally induced when the loss landscape is
embedded in higher-dimensional space. This is the same metric that underlies
common visualisations of loss landscapes. By taking this geometric perspective
literally and using the induced metric, we develop a new optimiser and compare
it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of
tasks and architectures. Empirically, we conclude that this new class of
optimisers is highly effective in low dimensional examples, and provides slight
improvement over state-of-the-art methods for training neural networks. These
new optimisers have theoretically desirable properties. In particular, the
effective learning rate is automatically decreased in regions of high curvature
acting as a smoothed out form of gradient clipping. Similarly, one variant of
these optimisers can also be viewed as inducing an effective scheduled learning
rate and decoupled weight decay is the natural choice from our geometric
perspective. The basic method can be used to modify any existing
preconditioning method. The new optimiser has a computational complexity
comparable to that of Adam.

</details>


### [42] [CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records](https://arxiv.org/abs/2509.03643)
*Chao Pang,Jiheum Park,Xinzhuo Jiang,Nishanth Parameshwar Pavinkurve,Krishna S. Kalluri,Shalmali Joshi,Noémie Elhadad,Karthik Natarajan*

Main category: cs.LG

TL;DR: CEHR-GPT is a general-purpose foundation model for EHR data that unifies feature representation, zero-shot prediction, and synthetic data generation in a single architecture with time-token-based learning for temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Most AI models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world clinical settings.

Method: A novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure, enabling temporal reasoning over clinical sequences.

Result: CEHR-GPT demonstrates strong performance across all three tasks (feature representation, zero-shot prediction, synthetic data generation) and generalizes effectively to external datasets through vocabulary expansion and fine-tuning.

Conclusion: The model's versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining, making it suitable for real-world healthcare applications.

Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient
health and hold significant potential for advancing clinical decision support,
risk prediction, and data-driven healthcare research. However, most artificial
intelligence (AI) models for EHRs are designed for narrow, single-purpose
tasks, limiting their generalizability and utility in real-world settings.
Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that
unifies three essential capabilities - feature representation, zero-shot
prediction, and synthetic data generation - within a single architecture. To
support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a
novel time-token-based learning framework that explicitly encodes patients'
dynamic timelines into the model structure. CEHR-GPT demonstrates strong
performance across all three tasks and generalizes effectively to external
datasets through vocabulary expansion and fine-tuning. Its versatility enables
rapid model development, cohort discovery, and patient outcome forecasting
without the need for task-specific retraining.

</details>


### [43] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: NMF and PCC are closely related. PCC helps estimate stable NMF rank, resolving nonidentifiability, while NMF enables approximate PCC implementation for clustering and denoising.


<details>
  <summary>Details</summary>
Motivation: Explore the reciprocal relationship between Nonnegative Matrix Factorization (NMF) and the Principle of Common Cause (PCC) to leverage their complementary strengths for robust data analysis.

Method: Used PCC as predictability tool for robust NMF rank estimation, implemented NMF around this stable rank, and developed clustering method grouping data points with same common cause.

Result: PCC-based rank estimate is stable against weak noise, NMF features become stable against noise and optimization seeds, effectively resolving NMF nonidentifiability. NMF enables approximate PCC implementation for clustering and denoising.

Conclusion: NMF and PCC have a mutually beneficial relationship where PCC provides robust rank estimation for NMF, while NMF enables practical implementation of PCC for clustering and denoising applications.

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [44] [Semi-decentralized Federated Time Series Prediction with Client Availability Budgets](https://arxiv.org/abs/2509.03660)
*Yunkai Bao,Reza Safarzadeh,Xin Wang,Steve Drew*

Main category: cs.LG

TL;DR: FedDeCAB is a semi-decentralized client selection method for federated learning that addresses client availability issues with time-series data, using probabilistic rankings and neighbor-assisted parameter sharing to improve performance and reduce communication overhead.


<details>
  <summary>Details</summary>
Motivation: Federated learning in IoT scenarios faces challenges with data heterogeneity, limited energy budgets, and client availability constraints. Effective client selection is crucial for global model convergence and balancing client contributions, especially with time-series data where availability patterns vary.

Method: Proposed FedDeCAB - a semi-decentralized client selection method that uses probabilistic rankings of available clients. When clients disconnect, it allows obtaining partial model parameters from nearest neighbor clients for joint optimization, reducing communication overhead while maintaining performance.

Result: Experiments on real-world large-scale taxi and vessel trajectory datasets demonstrate that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline/rejoining scenarios.

Conclusion: FedDeCAB successfully addresses client availability challenges in federated learning with time-series data, providing improved performance for offline models while reducing communication costs through its semi-decentralized approach and neighbor-assisted optimization.

Abstract: Federated learning (FL) effectively promotes collaborative training among
distributed clients with privacy considerations in the Internet of Things (IoT)
scenarios. Despite of data heterogeneity, FL clients may also be constrained by
limited energy and availability budgets. Therefore, effective selection of
clients participating in training is of vital importance for the convergence of
the global model and the balance of client contributions. In this paper, we
discuss the performance impact of client availability with time-series data on
federated learning. We set up three different scenarios that affect the
availability of time-series data and propose FedDeCAB, a novel,
semi-decentralized client selection method applying probabilistic rankings of
available clients. When a client is disconnected from the server, FedDeCAB
allows obtaining partial model parameters from the nearest neighbor clients for
joint optimization, improving the performance of offline models and reducing
communication overhead. Experiments based on real-world large-scale taxi and
vessel trajectory datasets show that FedDeCAB is effective under highly
heterogeneous data distribution, limited communication budget, and dynamic
client offline or rejoining.

</details>


### [45] [AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management](https://arxiv.org/abs/2509.03666)
*Kenny Guo,Nicholas Eckhert,Krish Chhajer,Luthira Abeykoon,Lorne Schell*

Main category: cs.LG

TL;DR: Deep RL framework for autonomous microgrid management using transformer forecasting and PPO agent, achieving better efficiency than rule-based methods.


<details>
  <summary>Details</summary>
Motivation: To optimize microgrid energy dispatch for remote communities, minimizing costs and maximizing renewable energy utilization to advance zero-carbon energy systems.

Method: Combines deep reinforcement learning with time-series forecasting models, specifically transformer architecture for renewable generation forecasting and proximal-policy optimization (PPO) agent for decision-making in simulated environments.

Result: Experimental results demonstrate significant improvements in both energy efficiency and operational resilience compared to traditional rule-based methods.

Conclusion: The work contributes to advancing smart-grid technologies and provides an open-source framework for simulating microgrid environments, supporting the pursuit of zero-carbon energy systems.

Abstract: We present a deep reinforcement learning-based framework for autonomous
microgrid management. tailored for remote communities. Using deep reinforcement
learning and time-series forecasting models, we optimize microgrid energy
dispatch strategies to minimize costs and maximize the utilization of renewable
energy sources such as solar and wind. Our approach integrates the transformer
architecture for forecasting of renewable generation and a proximal-policy
optimization (PPO) agent to make decisions in a simulated environment. Our
experimental results demonstrate significant improvements in both energy
efficiency and operational resilience when compared to traditional rule-based
methods. This work contributes to advancing smart-grid technologies in pursuit
of zero-carbon energy systems. We finally provide an open-source framework for
simulating several microgrid environments.

</details>


### [46] [SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences](https://arxiv.org/abs/2509.03672)
*Arpan Mukherjee,Marcello Bullo,Deniz Gündüz*

Main category: cs.LG

TL;DR: SharedRep-RLHF is a new RLHF framework that learns shared traits across groups instead of separate reward models, addressing limitations of MaxMin-RLHF with minority groups and achieving up to 20% better performance.


<details>
  <summary>Details</summary>
Motivation: Standard RLHF fails to capture diverse opinions across sub-populations and favors dominant groups. MaxMin-RLHF addresses fairness but performs poorly when the minimum-reward group is a minority.

Method: SharedRep-RLHF learns and leverages shared traits in annotations among various groups, rather than learning separate group-specific reward models like MaxMin-RLHF.

Result: SharedRep-RLHF outperforms MaxMin-RLHF with gains of up to 20% in win rate across diverse natural language tasks, and has provably better sample complexity.

Conclusion: Learning shared traits across groups is more effective than separate group-specific models for fair RLHF, especially when dealing with minority groups.

Abstract: Uniform-reward reinforcement learning from human feedback (RLHF), which
trains a single reward model to represent the preferences of all annotators,
fails to capture the diversity of opinions across sub-populations,
inadvertently favoring dominant groups. The state-of-the-art, MaxMin-RLHF,
addresses this by learning group-specific reward models, and by optimizing for
the group receiving the minimum reward, thereby promoting fairness. However, we
identify that a key limitation of MaxMin-RLHF is its poor performance when the
minimum-reward group is a minority. To mitigate this drawback, we introduce a
novel framework, termed {\em SharedRep-RLHF}. At its core, SharedRep-RLHF
learns and leverages {\em shared traits} in annotations among various groups,
in contrast to learning separate reward models across groups. We first show
that MaxMin-RLHF is provably suboptimal in learning shared traits, and then
quantify the sample complexity of SharedRep-RLHF. Experiments across diverse
natural language tasks showcase the effectiveness of SharedRep-RLHF compared to
MaxMin-RLHF with a gain of up to 20% in win rate.

</details>


### [47] [A Machine Learning-Based Study on the Synergistic Optimization of Supply Chain Management and Financial Supply Chains from an Economic Perspective](https://arxiv.org/abs/2509.03673)
*Hang Wang,Huijie Tang,Ningai Leng,Zhoufan Yu*

Main category: cs.LG

TL;DR: A machine learning-enhanced SCM-FSCM model combining economic theories with algorithms like random forests, LSTM, and XGBoost to improve supply chain efficiency, reduce financing costs, and optimize inventory management.


<details>
  <summary>Details</summary>
Motivation: To address efficiency loss, financing constraints, and risk transmission in supply chains by integrating economic theories with machine learning technology.

Method: Combines Transaction Cost and Information Asymmetry theories with machine learning algorithms (random forests, LSTM networks, clustering/regression, XGBoost) to create a data-driven three-dimensional analysis framework and implement core enterprise credit empowerment with dynamic pledge financing.

Result: 30% increase in inventory turnover, 18%-22% decrease in SME financing costs, stable order fulfillment rate above 95%, demand forecasting error <= 8%, credit assessment accuracy >= 90%.

Conclusion: The SCM-FSCM model effectively reduces operating costs, alleviates financing constraints, and supports high-quality supply chain development through integrated economic and machine learning approaches.

Abstract: Based on economic theories and integrated with machine learning technology,
this study explores a collaborative Supply Chain Management and Financial
Supply Chain Management (SCM - FSCM) model to solve issues like efficiency
loss, financing constraints, and risk transmission. We combine Transaction Cost
and Information Asymmetry theories and use algorithms such as random forests to
process multi-dimensional data and build a data-driven, three-dimensional
(cost-efficiency-risk) analysis framework. We then apply an FSCM model of "core
enterprise credit empowerment plus dynamic pledge financing." We use Long
Short-Term Memory (LSTM) networks for demand forecasting and
clustering/regression algorithms for benefit allocation. The study also
combines Game Theory and reinforcement learning to optimize the
inventory-procurement mechanism and uses eXtreme Gradient Boosting (XGBoost)
for credit assessment to enable rapid monetization of inventory. Verified with
20 core and 100 supporting enterprises, the results show a 30\% increase in
inventory turnover, an 18\%-22\% decrease in SME financing costs, a stable
order fulfillment rate above 95\%, and excellent model performance (demand
forecasting error <= 8\%, credit assessment accuracy >= 90\%). This SCM-FSCM
model effectively reduces operating costs, alleviates financing constraints,
and supports high-quality supply chain development.

</details>


### [48] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: Empirical analysis of gradient variance and standard deviation evolution during training, leading to a hyperparameter-free gradient normalization method that stabilizes optimization and improves generalization on CIFAR-100 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Gradient dynamics play a central role in neural network stability and generalization. The study aims to understand how gradient variance and standard deviation evolve during training and bridge the gap between theoretical expectations and empirical behaviors.

Method: Proposed a hyperparameter-free gradient normalization method that aligns gradient scaling with their natural evolution to prevent unintended amplification and stabilize optimization while preserving convergence guarantees.

Result: Experiments on CIFAR-100 with ResNet-20, ResNet-56, and VGG-16-BN demonstrate maintained or improved test accuracy even under strong generalization conditions.

Conclusion: The study highlights the importance of directly tracking gradient dynamics and provides insights for future optimization research, showing that aligning gradient scaling with natural evolution can stabilize training and improve performance.

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [49] [A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games](https://arxiv.org/abs/2509.03682)
*Zhengyang Li,Qijin Ji,Xinghong Ling,Quan Liu*

Main category: cs.LG

TL;DR: Comprehensive review of MARL applications in video games from turn-based to real-time genres, analyzing challenges and successful implementations while proposing game complexity estimation and future research directions.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in MARL have shown superhuman performance in games like StarCraft II and Dota 2, creating need for comprehensive review of applications across diverse game environments.

Method: Thorough examination of MARL applications across various game genres including Sports, FPS, RTS, and MOBA games, analyzing challenges like nonstationarity, partial observability, and team coordination.

Result: Successful MARL implementations identified in games including Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, and Honor of Kings, with insights into game AI systems.

Conclusion: Proposes novel method for game complexity estimation and suggests future research directions to advance MARL applications in game development, inspiring further innovation in the field.

Abstract: Recent advancements in multi-agent reinforcement learning (MARL) have
demonstrated its application potential in modern games. Beginning with
foundational work and progressing to landmark achievements such as AlphaStar in
StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving
superhuman performance across diverse game environments through techniques like
self-play, supervised learning, and deep reinforcement learning. With its
growing impact, a comprehensive review has become increasingly important in
this field. This paper aims to provide a thorough examination of MARL's
application from turn-based two-agent games to real-time multi-agent video
games including popular genres such as Sports games, First-Person Shooter (FPS)
games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena
(MOBA) games. We further analyze critical challenges posed by MARL in video
games, including nonstationary, partial observability, sparse rewards, team
coordination, and scalability, and highlight successful implementations in
games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2,
Honor of Kings, etc. This paper offers insights into MARL in video game AI
systems, proposes a novel method to estimate game complexity, and suggests
future research directions to advance MARL and its applications in game
development, inspiring further innovation in this rapidly evolving field.

</details>


### [50] [Graph Random Features for Scalable Gaussian Processes](https://arxiv.org/abs/2509.03691)
*Matthew Zhang,Jihao Andreas Lin,Adrian Weller,Richard E. Turner,Isaac Reid*

Main category: cs.LG

TL;DR: Graph random features enable scalable Gaussian processes on graphs with O(N^{3/2}) time complexity instead of O(N^3), allowing Bayesian optimization on million-node graphs on single chips.


<details>
  <summary>Details</summary>
Motivation: To enable scalable Bayesian inference on large graphs by overcoming the O(N^3) computational complexity of exact graph kernel methods.

Method: Application of graph random features (GRFs) as stochastic estimators of graph node kernels for Gaussian processes on discrete input spaces.

Result: Achieves substantial wall-clock speedups and memory savings, enabling Bayesian optimization on graphs with over 1,000,000 nodes on a single computer chip while maintaining competitive performance.

Conclusion: GRFs provide an efficient and scalable approach for Bayesian inference on large graphs, reducing computational complexity from O(N^3) to O(N^{3/2}) under mild assumptions.

Abstract: We study the application of graph random features (GRFs) - a recently
introduced stochastic estimator of graph node kernels - to scalable Gaussian
processes on discrete input spaces. We prove that (under mild assumptions)
Bayesian inference with GRFs enjoys $O(N^{3/2})$ time complexity with respect
to the number of nodes $N$, compared to $O(N^3)$ for exact kernels. Substantial
wall-clock speedups and memory savings unlock Bayesian optimisation on graphs
with over $10^6$ nodes on a single computer chip, whilst preserving competitive
performance.

</details>


### [51] [Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures](https://arxiv.org/abs/2509.03695)
*Payam Abdisarabshali,Fardis Nadimi,Kasra Borazjani,Naji Khosravan,Minghui Liwang,Wei Ni,Dusit Niyato,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: Proposes hierarchical federated foundation models (HF-FMs) that address modality and task heterogeneity in fog/edge networks by aligning multi-modal multi-task foundation model architecture with hierarchical network infrastructure.


<details>
  <summary>Details</summary>
Motivation: The rise of multi-modal multi-task foundation models and the need to leverage geo-distributed data from wireless devices motivates exploring federated foundation models that can handle heterogeneity in collected modalities and executed tasks across fog/edge nodes.

Method: HF-FMs strategically align the modular structure of M3T FMs (modality encoders, prompts, MoEs, adapters, task heads) with hierarchical fog/edge infrastructures, enabling optional D2D communications for horizontal module relaying and localized cooperative training.

Result: The paper presents the architectural design of HF-FMs, highlights their unique capabilities, and provides an open-source prototype implementation in a wireless network setting to demonstrate their potential.

Conclusion: HF-FMs represent an unexplored paradigm that addresses critical heterogeneity challenges in federated learning for multi-modal multi-task foundation models, with promising research directions and practical implementation potential.

Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine
learning. As these models continued to grow, leveraging geo-distributed data
from wireless devices has become increasingly critical, giving rise to
federated foundation models (FFMs). More recently, FMs have evolved into
multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse
modalities across multiple tasks, which motivates a new underexplored paradigm:
M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by
proposing hierarchical federated foundation models (HF-FMs), which in turn
expose two overlooked heterogeneity dimensions to fog/edge networks that have a
direct impact on these emerging models: (i) heterogeneity in collected
modalities and (ii) heterogeneity in executed tasks across fog/edge nodes.
HF-FMs strategically align the modular structure of M3T FMs, comprising
modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task
heads, with the hierarchical nature of fog/edge infrastructures. Moreover,
HF-FMs enable the optional usage of device-to-device (D2D) communications,
enabling horizontal module relaying and localized cooperative training among
nodes when feasible. Through delving into the architectural design of HF-FMs,
we highlight their unique capabilities along with a series of tailored future
research directions. Finally, to demonstrate their potential, we prototype
HF-FMs in a wireless network setting and release the open-source code for the
development of HF-FMs with the goal of fostering exploration in this untapped
field (GitHub: https://github.com/payamsiabd/M3T-FFM).

</details>


### [52] [EmbedOR: Provable Cluster-Preserving Visualizations with Curvature-Based Stochastic Neighbor Embeddings](https://arxiv.org/abs/2509.03703)
*Tristan Luca Saidi,Abigail Hickok,Bastian Rieck,Andrew J. Blumberg*

Main category: cs.LG

TL;DR: EmbedOR is a new SNE algorithm that uses discrete graph curvature to better preserve data geometry and prevent spurious fragmentation in visualizations compared to UMAP and tSNE.


<details>
  <summary>Details</summary>
Motivation: Existing SNE algorithms like UMAP and tSNE often fail to preserve the true geometry of noisy high-dimensional data, spuriously separating connected components and missing clusters in well-clusterable data.

Method: Proposes EmbedOR algorithm that incorporates discrete graph curvature with a curvature-enhanced distance metric to emphasize underlying cluster structure during stochastic embedding.

Result: EmbedOR demonstrates superior visualization and geometry-preservation capabilities, being much less likely to fragment continuous high-density regions compared to other SNE algorithms and UMAP.

Conclusion: EmbedOR extends consistency results for tSNE to broader datasets and provides a tool to annotate existing visualizations to identify fragmentation and gain deeper insight into data geometry.

Abstract: Stochastic Neighbor Embedding (SNE) algorithms like UMAP and tSNE often
produce visualizations that do not preserve the geometry of noisy and high
dimensional data. In particular, they can spuriously separate connected
components of the underlying data submanifold and can fail to find clusters in
well-clusterable data. To address these limitations, we propose EmbedOR, a SNE
algorithm that incorporates discrete graph curvature. Our algorithm
stochastically embeds the data using a curvature-enhanced distance metric that
emphasizes underlying cluster structure. Critically, we prove that the EmbedOR
distance metric extends consistency results for tSNE to a much broader class of
datasets. We also describe extensive experiments on synthetic and real data
that demonstrate the visualization and geometry-preservation capabilities of
EmbedOR. We find that, unlike other SNE algorithms and UMAP, EmbedOR is much
less likely to fragment continuous, high-density regions of the data. Finally,
we demonstrate that the EmbedOR distance metric can be used as a tool to
annotate existing visualizations to identify fragmentation and provide deeper
insight into the underlying geometry of the data.

</details>


### [53] [Online Learning of Optimal Sequential Testing Policies](https://arxiv.org/abs/2509.03707)
*Qiyuan Chen,Raed Al Kontar*

Main category: cs.LG

TL;DR: Online Testing Problem (OTP) with correlated, costly tests where missing data from partial testing creates bias, requiring Ω(T^{2/3}) minimax regret vs Θ(√T) in standard MDPs. An Explore-Then-Commit algorithm achieves Õ(T^{2/3}) regret, while a variant with missingness-independent rewards enables Õ(√T) regret.


<details>
  <summary>Details</summary>
Motivation: Optimal testing policies for subjects using sequential tests from a common pool, where conducting all tests is costly but partial testing creates missing data that biases learning.

Method: Formulate as MDP with unknown joint distribution of test outcomes. Develop Explore-Then-Commit algorithm for OTP and iterative-elimination algorithm for variant with missingness-independent rewards.

Result: Proved Ω(T^{2/3}) minimax regret lower bound for OTP (vs Θ(√T) in standard MDPs). Achieved Õ(T^{2/3}) regret for OTP and Õ(√T) for variant. Numerical results confirm theoretical findings.

Conclusion: Missing data fundamentally increases exploration-exploitation difficulty in sequential testing. Reward structure dependence on missingness determines achievable regret rates, guiding efficient testing policy design.

Abstract: This paper studies an online learning problem that seeks optimal testing
policies for a stream of subjects, each of whom can be evaluated through a
sequence of candidate tests drawn from a common pool. We refer to this problem
as the Online Testing Problem (OTP). Although conducting every candidate test
for a subject provides more information, it is often preferable to select only
a subset when tests are correlated and costly, and make decisions with partial
information. If the joint distribution of test outcomes were known, the problem
could be cast as a Markov Decision Process (MDP) and solved exactly. In
practice, this distribution is unknown and must be learned online as subjects
are tested. When a subject is not fully tested, the resulting missing data can
bias estimates, making the problem fundamentally harder than standard episodic
MDPs. We prove that the minimax regret must scale at least as
$\Omega(T^{\frac{2}{3}})$, in contrast to the $\Theta(\sqrt{T})$ rate in
episodic MDPs, revealing the difficulty introduced by missingness. This
elevated lower bound is then matched by an Explore-Then-Commit algorithm whose
cumulative regret is $\tilde{O}(T^{\frac{2}{3}})$ for both discrete and
Gaussian distributions. To highlight the consequence of missingness-dependent
rewards in OTP, we study a variant called the Online Cost-sensitive Maximum
Entropy Sampling Problem, where rewards are independent of missing data. This
structure enables an iterative-elimination algorithm that achieves
$\tilde{O}(\sqrt{T})$ regret, breaking the $\Omega(T^{\frac{2}{3}})$ lower
bound for OTP. Numerical results confirm our theory in both settings. Overall,
this work deepens the understanding of the exploration--exploitation trade-off
under missing data and guides the design of efficient sequential testing
policies.

</details>


### [54] [From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks](https://arxiv.org/abs/2509.03709)
*Allan Salihovic,Payam Abdisarabshali,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: X-Learning is a novel distributed learning architecture that generalizes decentralization concepts, with connections to graph theory and Markov chains.


<details>
  <summary>Details</summary>
Motivation: To present a vision for X-Learning and introduce its unexplored design considerations and degrees of freedom, stimulating further research in this area.

Method: The paper provides a perspective on X-Learning by exploring intuitive yet non-trivial connections between this distributed learning architecture, graph theory, and Markov chains.

Result: The analysis presents X-Learning as a generalized decentralized learning framework and identifies key research directions for future exploration.

Conclusion: X-Learning represents a promising extension of decentralized learning concepts with strong theoretical foundations in graph theory and Markov chains, offering numerous open research opportunities.

Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel
distributed learning architecture that generalizes and extends the concept of
decentralization. Our goal is to present a vision for $\mathbb{X}$L,
introducing its unexplored design considerations and degrees of freedom. To
this end, we shed light on the intuitive yet non-trivial connections between
$\mathbb{X}$L, graph theory, and Markov chains. We also present a series of
open research directions to stimulate further research.

</details>


### [55] [Differentiable Entropy Regularization for Geometry and Neural Networks](https://arxiv.org/abs/2509.03733)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: Differentiable estimator for range-partition entropy enables deep learning algorithms to adapt to input sortedness, achieving significant speedups in geometry (4.1×) and improved accuracy in Transformers (6% higher at 80% sparsity).


<details>
  <summary>Details</summary>
Motivation: Range-partition entropy provides strong theoretical guarantees in algorithm design but hasn't been accessible to deep learning. The paper aims to bridge this gap by making entropy differentiable and applicable to neural networks.

Method: Proposed differentiable approximation of range-partition entropy, designed EntropyNet neural module to restructure data into low-entropy forms, and applied entropy regularization to Transformer attention mechanisms.

Result: Achieved 4.1× runtime speedups with <0.2% error in geometry tasks, and 6% higher accuracy at 80% sparsity compared to L1 baselines in deep learning applications.

Conclusion: Entropy-bounded computation is both theoretically elegant and practically effective for adaptive learning, efficiency improvements, and structured representation in neural networks.

Abstract: We introduce a differentiable estimator of range-partition entropy, a recent
concept from computational geometry that enables algorithms to adapt to the
"sortedness" of their input. While range-partition entropy provides strong
guarantees in algorithm design, it has not yet been made accessible to deep
learning. In this work, we (i) propose the first differentiable approximation
of range-partition entropy, enabling its use as a trainable loss or
regularizer; (ii) design EntropyNet, a neural module that restructures data
into low-entropy forms to accelerate downstream instance-optimal algorithms;
and (iii) extend this principle beyond geometry by applying entropy
regularization directly to Transformer attention. Across tasks, we demonstrate
that differentiable entropy improves efficiency without degrading correctness:
in geometry, our method achieves up to $4.1\times$ runtime speedups with
negligible error ($<0.2%$); in deep learning, it induces structured attention
patterns that yield 6% higher accuracy at 80% sparsity compared to L1
baselines. Our theoretical analysis provides approximation bounds for the
estimator, and extensive ablations validate design choices. These results
suggest that entropy-bounded computation is not only theoretically elegant but
also a practical mechanism for adaptive learning, efficiency, and structured
representation.

</details>


### [56] [Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces](https://arxiv.org/abs/2509.03738)
*Bahareh Tolooshams,Ailsa Shen,Anima Anandkumar*

Main category: cs.LG

TL;DR: A framework extending sparse autoencoders to lifted spaces and infinite-dimensional function spaces for mechanistic interpretability of neural operators, showing improved recovery and resolution robustness.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored representational properties of neural operators in scientific computing and unify representations through sparse model recovery.

Method: Extends sparse autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces, comparing SAEs, lifted-SAE, and SAE neural operators.

Result: Lifting and operator modules introduce beneficial inductive biases enabling faster recovery, improved recovery of smooth concepts, and robust inference across varying resolutions.

Conclusion: The framework successfully enables mechanistic interpretability of large neural operators with unique resolution-robust properties beneficial for scientific computing applications.

Abstract: We frame the problem of unifying representations in neural models as one of
sparse model recovery and introduce a framework that extends sparse
autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,
enabling mechanistic interpretability of large neural operators (NO). While the
Platonic Representation Hypothesis suggests that neural networks converge to
similar representations across architectures, the representational properties
of neural operators remain underexplored despite their growing importance in
scientific computing. We compare the inference and training dynamics of SAEs,
lifted-SAE, and SAE neural operators. We highlight how lifting and operator
modules introduce beneficial inductive biases, enabling faster recovery,
improved recovery of smooth concepts, and robust inference across varying
resolutions, a property unique to neural operators.

</details>


### [57] [Mapping on a Budget: Optimizing Spatial Data Collection for ML](https://arxiv.org/abs/2509.03749)
*Livia Betti,Farooq Sanni,Gnouyaro Sogoyou,Togbe Agbagla,Cullen Molitor,Tamma Carleton,Esther Rolf*

Main category: cs.LG

TL;DR: First systematic approach to optimize spatial training data collection for satellite ML, addressing data sparsity and cost constraints with novel methods that show significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Satellite ML applications suffer from sparse, clustered labeled data while global coverage exists. Current research focuses on model architectures rather than data optimization, leaving practitioners uncertain about effective data collection strategies.

Method: Developed problem formulation for spatial training data optimization with heterogeneous costs and budget constraints. Created novel methods to address this problem and tested across three continents and four tasks.

Result: Experiments revealed substantial performance gains from optimized sampling strategies. Further analysis identified specific settings where optimized sampling is particularly effective.

Conclusion: The framework generalizes across SatML domains and provides practical guidance for data collection, with immediate application to augment agricultural surveys in Togo for improved monitoring.

Abstract: In applications across agriculture, ecology, and human development, machine
learning with satellite imagery (SatML) is limited by the sparsity of labeled
training data. While satellite data cover the globe, labeled training datasets
for SatML are often small, spatially clustered, and collected for other
purposes (e.g., administrative surveys or field measurements). Despite the
pervasiveness of this issue in practice, past SatML research has largely
focused on new model architectures and training algorithms to handle scarce
training data, rather than modeling data conditions directly. This leaves
scientists and policymakers who wish to use SatML for large-scale monitoring
uncertain about whether and how to collect additional data to maximize
performance. Here, we present the first problem formulation for the
optimization of spatial training data in the presence of heterogeneous data
collection costs and realistic budget constraints, as well as novel methods for
addressing this problem. In experiments simulating different problem settings
across three continents and four tasks, our strategies reveal substantial gains
from sample optimization. Further experiments delineate settings for which
optimized sampling is particularly effective. The problem formulation and
methods we introduce are designed to generalize across application domains for
SatML; we put special emphasis on a specific problem setting where our
coauthors can immediately use our findings to augment clustered agricultural
surveys for SatML monitoring in Togo.

</details>


### [58] [Learning functions through Diffusion Maps](https://arxiv.org/abs/2509.03758)
*Alvaro Almeida Gomez*

Main category: cs.LG

TL;DR: A data-driven method for approximating functions on smooth manifolds using Diffusion Maps and dimensionality reduction via SVD, with online updating for new data, showing superior accuracy and efficiency over neural networks and interpolation methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for approximating real-valued functions on smooth manifolds that can handle high-dimensional data and incorporate new data efficiently.

Method: Builds on Diffusion Maps framework, uses diffusion geometry connected to heat equation and Laplace-Beltrami operator. Introduces dimensionality reduction via SVD of distance matrix and online updating mechanism for new data.

Result: Numerical experiments demonstrate the method outperforms classical feedforward neural networks and interpolation methods in both accuracy and efficiency, particularly in applications like sparse CT reconstruction.

Conclusion: The proposed methodology provides an effective and scalable approach for function approximation on manifolds, with computational advantages over traditional methods.

Abstract: We propose a data-driven method for approximating real-valued functions on
smooth manifolds, building on the Diffusion Maps framework under the manifold
hypothesis. Given pointwise evaluations of a function, the method constructs a
smooth extension to the ambient space by exploiting diffusion geometry and its
connection to the heat equation and the Laplace-Beltrami operator.
  To address the computational challenges of high-dimensional data, we
introduce a dimensionality reduction strategy based on the low-rank structure
of the distance matrix, revealed via singular value decomposition (SVD). In
addition, we develop an online updating mechanism that enables efficient
incorporation of new data, thereby improving scalability and reducing
computational cost.
  Numerical experiments, including applications to sparse CT reconstruction,
demonstrate that the proposed methodology outperforms classical feedforward
neural networks and interpolation methods in terms of both accuracy and
efficiency.

</details>


### [59] [What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?](https://arxiv.org/abs/2509.03790)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: The paper shows that low-rank structure in reward matrices enables efficient sparse-reward RL, with Policy-Aware Matrix Completion (PAMC) providing theoretical guarantees and empirical improvements in sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand what fundamental properties of reward functions enable efficient sparse-reward reinforcement learning, particularly through the lens of low-rank structure in reward matrices.

Method: Introduced Policy-Aware Matrix Completion (PAMC), which connects matrix completion theory with reinforcement learning through policy-dependent sampling analysis. The framework includes impossibility results, reward-free representation learning, distribution-free confidence sets via conformal prediction, and robust completion guarantees.

Result: Empirical evaluation across 100 domains found exploitable structure in over half. PAMC improved sample efficiency by factors between 1.6 and 2.1 compared to strong baselines, with only about 20% computational overhead.

Conclusion: Establishes structural reward learning as a promising new paradigm with implications for robotics, healthcare, and other safety-critical, sample-expensive applications.

Abstract: What fundamental properties of reward functions enable efficient
sparse-reward reinforcement learning? We address this question through the lens
of low-rank structure in reward matrices, showing that such structure induces a
sharp transition from exponential to polynomial sample complexity, the first
result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix
Completion (PAMC), which connects matrix completion theory with reinforcement
learning via a new analysis of policy-dependent sampling. Our framework
provides: (i) impossibility results for general sparse reward observation, (ii)
reward-free representation learning from dynamics, (iii) distribution-free
confidence sets via conformal prediction, and (iv) robust completion guarantees
that degrade gracefully when low-rank structure is only approximate.
Empirically, we conduct a pre-registered evaluation across 100 systematically
sampled domains, finding exploitable structure in over half. PAMC improves
sample efficiency by factors between 1.6 and 2.1 compared to strong
exploration, structured, and representation-learning baselines, while adding
only about 20 percent computational overhead.These results establish structural
reward learning as a promising new paradigm, with immediate implications for
robotics, healthcare, and other safety-critical, sample-expensive applications.

</details>


### [60] [Online time series prediction using feature adjustment](https://arxiv.org/abs/2509.03810)
*Xiannan Huang,Shuhan Qiu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: ADAPT-Z is a novel online learning method for time series forecasting that addresses distribution shift by updating feature representations of latent factors rather than conventional parameter updates, and handles delayed feedback in multi-step forecasting using historical gradient information.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting faces challenges from distribution shift, especially in online deployment where data arrives sequentially. Current methods focus on parameter selection and update strategies, but the authors argue that distribution shifts stem from changes in underlying latent factors, requiring updates to feature representations instead.

Method: ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in Z-space) uses an adapter module that combines current feature representations with historical gradient information to enable robust parameter updates despite delayed feedback in multi-step forecasting.

Result: Extensive experiments show that ADAPT-Z consistently outperforms standard base models without adaptation and surpasses state-of-the-art online learning approaches across multiple datasets.

Conclusion: The proposed approach of updating feature representations of latent factors rather than conventional parameters is more effective for handling distribution shift in time series forecasting, and ADAPT-Z successfully addresses the delayed feedback problem in online learning scenarios.

Abstract: Time series forecasting is of significant importance across various domains.
However, it faces significant challenges due to distribution shift. This issue
becomes particularly pronounced in online deployment scenarios where data
arrives sequentially, requiring models to adapt continually to evolving
patterns. Current time series online learning methods focus on two main
aspects: selecting suitable parameters to update (e.g., final layer weights or
adapter modules) and devising suitable update strategies (e.g., using recent
batches, replay buffers, or averaged gradients). We challenge the conventional
parameter selection approach, proposing that distribution shifts stem from
changes in underlying latent factors influencing the data. Consequently,
updating the feature representations of these latent factors may be more
effective. To address the critical problem of delayed feedback in multi-step
forecasting (where true values arrive much later than predictions), we
introduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in
Z-space). ADAPT-Z utilizes an adapter module that leverages current feature
representations combined with historical gradient information to enable robust
parameter updates despite the delay. Extensive experiments demonstrate that our
method consistently outperforms standard base models without adaptation and
surpasses state-of-the-art online learning approaches across multiple datasets.
The code is available at https://github.com/xiannanhuang/ADAPT-Z.

</details>


### [61] [Machine Learning for LiDAR-Based Indoor Surface Classification in Intelligent Wireless Environments](https://arxiv.org/abs/2509.03813)
*Parth Ashokbhai Shiroya,Swarnagowri Shashidhar,Amod Ashtekar,Krishna Aindrila Kar,Rafaela Lomboy,Dalton Davis,Mohammed E. Eltayeb*

Main category: cs.LG

TL;DR: LiDAR-driven ML framework classifies indoor surfaces into semi-specular and low-specular categories using optical reflectivity as proxy for EM scattering behavior, enabling scatter-aware environment maps for mmWave/sub-THz networks.


<details>
  <summary>Details</summary>
Motivation: Reliable connectivity in mmWave and sub-THz networks depends on surface reflections, but scattering behavior is affected by material roughness which determines whether energy remains specular or scatters diffusely.

Method: Collected 78,000+ LiDAR points from 15 indoor materials, partitioned into 3cm patches, extracted geometry/intensity features (elevation angle, log-scaled intensity, max-to-mean ratio), and trained Random Forest, XGBoost, and neural network classifiers.

Result: Ensemble tree-based models (Random Forest, XGBoost) provided the best trade-off between accuracy and robustness, confirming LiDAR-derived features capture roughness-induced scattering effects.

Conclusion: The framework enables generation of scatter-aware environment maps and digital twins to support adaptive beam management, blockage recovery, and environment-aware connectivity in next-generation networks.

Abstract: Reliable connectivity in millimeter-wave (mmWave) and sub-terahertz (sub-THz)
networks depends on reflections from surrounding surfaces, as high-frequency
signals are highly vulnerable to blockage. The scattering behavior of a surface
is determined not only by material permittivity but also by roughness, which
governs whether energy remains in the specular direction or is diffusely
scattered. This paper presents a LiDAR-driven machine learning framework for
classifying indoor surfaces into semi-specular and low-specular categories,
using optical reflectivity as a proxy for electromagnetic scattering behavior.
A dataset of over 78,000 points from 15 representative indoor materials was
collected and partitioned into 3 cm x 3 cm patches to enable classification
from partial views. Patch-level features capturing geometry and intensity,
including elevation angle, natural-log-scaled intensity, and max-to-mean ratio,
were extracted and used to train Random Forest, XGBoost, and neural network
classifiers. Results show that ensemble tree-based models consistently provide
the best trade-off between accuracy and robustness, confirming that
LiDAR-derived features capture roughness-induced scattering effects. The
proposed framework enables the generation of scatter aware environment maps and
digital twins, supporting adaptive beam management, blockage recovery, and
environment-aware connectivity in next-generation networks.

</details>


### [62] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: Deep neural network achieves 92% accuracy in classifying traffic accident severity using autoencoders for dimensionality reduction and dense networks.


<details>
  <summary>Details</summary>
Motivation: Traffic accidents need study to reduce future risks, and machine learning offers new ways to analyze accident data with good generalization on imbalanced datasets.

Method: Neural network-based approach with feature colinearity analysis, unsupervised dimensionality reduction via autoencoders, followed by dense network classification of accident severity.

Result: Cross-validated results show up to 92% accuracy in classifying accident severity using the proposed deep neural network model.

Conclusion: The deep neural network approach effectively classifies traffic accident severity with high accuracy, demonstrating the potential of machine learning for traffic safety analysis.

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [63] [From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game](https://arxiv.org/abs/2509.03834)
*Lucas Lopes Felipe,Konstantin Avrachenkov,Daniel Sadoc Menasche*

Main category: cs.LG

TL;DR: Game-theoretic analysis of Constant Potts Model for community detection, showing efficient convergence, robust stability criteria, and improved accuracy in community tracking.


<details>
  <summary>Details</summary>
Motivation: To provide a game-theoretic perspective on the Constant Potts Model for network partitioning, emphasizing its efficiency, robustness, and accuracy in community detection.

Method: Reinterpret CPM as a potential hedonic game by decomposing global Hamiltonian into local utility functions. Prove convergence via better-response dynamics. Introduce two stability criteria: strict robustness and relaxed utility function with resolution parameter.

Result: Local optimization converges in pseudo-polynomial time to equilibrium. Robust partitions yield higher accuracy in recovering ground-truth communities in tracking scenarios when bootstrapping Leiden algorithm.

Conclusion: The game-theoretic framework provides efficient, robust, and accurate community detection with theoretical guarantees and practical benefits for community tracking applications.

Abstract: Community detection is one of the fundamental problems in data science which
consists of partitioning nodes into disjoint communities. We present a
game-theoretic perspective on the Constant Potts Model (CPM) for partitioning
networks into disjoint communities, emphasizing its efficiency, robustness, and
accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by
decomposing its global Hamiltonian into local utility functions, where the
local utility gain of each agent matches the corresponding increase in global
utility. Leveraging this equivalence, we prove that local optimization of the
CPM objective via better-response dynamics converges in pseudo-polynomial time
to an equilibrium partition. Robustness: We introduce and relate two stability
criteria: a strict criterion based on a novel notion of robustness, requiring
nodes to simultaneously maximize neighbors and minimize non-neighbors within
communities, and a relaxed utility function based on a weighted sum of these
objectives, controlled by a resolution parameter. Accuracy: In community
tracking scenarios, where initial partitions are used to bootstrap the Leiden
algorithm with partial ground-truth information, our experiments reveal that
robust partitions yield higher accuracy in recovering ground-truth communities.

</details>


### [64] [Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models](https://arxiv.org/abs/2509.03837)
*Kimia Ehsani,Walid Saad*

Main category: cs.LG

TL;DR: A lightweight BEV injection connector enhances MLLMs for V2I link prediction by providing 3D spatial context, improving accuracy by up to 13.9% overall and 32.7% in adverse conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of V2I communication link quality is crucial for smooth handovers and reliable low-latency communication, but MLLMs lack 3D spatial understanding despite having access to vehicle sensor data.

Method: Proposes a plug-and-play BEV injection connector that constructs environment BEV from neighboring vehicle sensing data and fuses it with ego vehicle input. Uses CARLA and MATLAB co-simulation to generate multimodal data (RGB, LiDAR, GPS, wireless signals) and programmatically extracts instructions from ray-tracing outputs.

Result: The BEV injection framework consistently improved performance across all three V2I tasks (LoS/NLoS classification, link availability, blockage prediction), achieving up to 13.9% improvement in macro-average accuracy over ego-only baseline, with up to 32.7% gain in challenging rainy/nighttime conditions.

Conclusion: The proposed BEV injection framework effectively addresses MLLMs' spatial understanding limitations and demonstrates robust performance improvements in V2I link prediction, particularly under adverse environmental conditions.

Abstract: Accurate prediction of communication link quality metrics is essential for
vehicle-to-infrastructure (V2I) systems, enabling smooth handovers, efficient
beam management, and reliable low-latency communication. The increasing
availability of sensor data from modern vehicles motivates the use of
multimodal large language models (MLLMs) because of their adaptability across
tasks and reasoning capabilities. However, MLLMs inherently lack
three-dimensional spatial understanding. To overcome this limitation, a
lightweight, plug-and-play bird's-eye view (BEV) injection connector is
proposed. In this framework, a BEV of the environment is constructed by
collecting sensing data from neighboring vehicles. This BEV representation is
then fused with the ego vehicle's input to provide spatial context for the
large language model. To support realistic multimodal learning, a co-simulation
environment combining CARLA simulator and MATLAB-based ray tracing is developed
to generate RGB, LiDAR, GPS, and wireless signal data across varied scenarios.
Instructions and ground-truth responses are programmatically extracted from the
ray-tracing outputs. Extensive experiments are conducted across three V2I link
prediction tasks: line-of-sight (LoS) versus non-line-of-sight (NLoS)
classification, link availability, and blockage prediction. Simulation results
show that the proposed BEV injection framework consistently improved
performance across all tasks. The results indicate that, compared to an
ego-only baseline, the proposed approach improves the macro-average of the
accuracy metrics by up to 13.9%. The results also show that this performance
gain increases by up to 32.7% under challenging rainy and nighttime conditions,
confirming the robustness of the framework in adverse settings.

</details>


### [65] [Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables](https://arxiv.org/abs/2509.03845)
*Yang Chen,Xiao Lin,Bo Yan,Libo Zhang,Jiamou Liu,Neset Özkan Tan,Michael Witbrock*

Main category: cs.LG

TL;DR: Proposes a deep latent variable MFG model for inverse reinforcement learning that can handle heterogeneous agent objectives without prior knowledge of contexts or model modifications.


<details>
  <summary>Details</summary>
Motivation: Existing IRL methods in mean field games assume agent homogeneity, limiting their ability to handle real-world scenarios with heterogeneous and unknown objectives in expert demonstrations.

Method: Developed a deep latent variable mean field game model and associated IRL method that can infer rewards from different but structurally similar tasks without requiring prior context knowledge or model changes.

Result: Experiments on simulated scenarios and a real-world spatial taxi-ride pricing problem showed superior performance over state-of-the-art IRL methods in MFGs.

Conclusion: The proposed method effectively addresses the limitation of agent homogeneity in existing MFG IRL approaches, enabling practical application to real-world problems with heterogeneous objectives.

Abstract: Designing suitable reward functions for numerous interacting intelligent
agents is challenging in real-world applications. Inverse reinforcement
learning (IRL) in mean field games (MFGs) offers a practical framework to infer
reward functions from expert demonstrations. While promising, the assumption of
agent homogeneity limits the capability of existing methods to handle
demonstrations with heterogeneous and unknown objectives, which are common in
practice. To this end, we propose a deep latent variable MFG model and an
associated IRL method. Critically, our method can infer rewards from different
yet structurally similar tasks without prior knowledge about underlying
contexts or modifying the MFG model itself. Our experiments, conducted on
simulated scenarios and a real-world spatial taxi-ride pricing problem,
demonstrate the superiority of our approach over state-of-the-art IRL methods
in MFGs.

</details>


### [66] [Data-Augmented Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2509.03850)
*Justin Kur,Kaiqi Zhao*

Main category: cs.LG

TL;DR: Proposes a novel metric to automatically select optimal data augmentation strategies for quantization-aware knowledge distillation by maximizing contextual mutual information while maintaining prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing QAT and KD methods focus on network output improvements but neglect the impact of input transformations like data augmentation, especially for low-precision models.

Method: Develops a metric that evaluates data augmentations based on their capacity to maximize contextual mutual information (non-label related information) while ensuring predictions align with ground truth labels.

Result: Extensive evaluations show that selecting DA strategies using the proposed metric significantly improves state-of-the-art QAT and KD performance across various models and datasets.

Conclusion: The method provides an efficient, training-overhead-minimal approach to automatically rank and select optimal data augmentation strategies that is compatible with any KD or QAT algorithm.

Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. Existing KD and QAT works focus on improving the accuracy of quantized
models from the network output perspective by designing better KD loss
functions or optimizing QAT's forward and backward propagation. However,
limited attention has been given to understanding the impact of input
transformations, such as data augmentation (DA). The relationship between
quantization-aware KD and DA remains unexplored. In this paper, we address the
question: how to select a good DA in quantization-aware KD, especially for the
models with low precisions? We propose a novel metric which evaluates DAs
according to their capacity to maximize the Contextual Mutual Information--the
information not directly related to an image's label--while also ensuring the
predictions for each class are close to the ground truth labels on average. The
proposed method automatically ranks and selects DAs, requiring minimal training
overhead, and it is compatible with any KD or QAT algorithm. Extensive
evaluations demonstrate that selecting DA strategies using our metric
significantly improves state-of-the-art QAT and KD works across various model
architectures and datasets.

</details>


### [67] [MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting](https://arxiv.org/abs/2509.03852)
*Binqing Wu,Zongjiang Shang,Jianlong Huang,Ling Chen*

Main category: cs.LG

TL;DR: MillGNN is a graph neural network method that captures multi-scale lead-lag dependencies in multivariate time series forecasting, outperforming 16 state-of-the-art methods on 11 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing MTS forecasting methods overlook hierarchical lead-lag dependencies at multiple grouping scales, failing to capture complex lead-lag effects in real-world systems.

Method: Proposes MillGNN with two innovations: 1) scale-specific lead-lag graph learning using cross-correlation and dynamic decaying features, 2) hierarchical lead-lag message passing at multiple scales to capture intra- and inter-scale effects.

Result: Experimental results on 11 datasets demonstrate superiority over 16 state-of-the-art methods for both long-term and short-term MTS forecasting.

Conclusion: MillGNN effectively captures comprehensive lead-lag dependencies at multiple scales with statistical interpretability and data-driven flexibility, achieving state-of-the-art performance.

Abstract: Multi-variate time series (MTS) forecasting is crucial for various
applications. Existing methods have shown promising results owing to their
strong ability to capture intra- and inter-variate dependencies. However, these
methods often overlook lead-lag dependencies at multiple grouping scales,
failing to capture hierarchical lead-lag effects in complex systems. To this
end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural
\underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple
grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS
forecasting, which can comprehensively capture lead-lag effects considering
variate-wise and group-wise dynamics and decays. Specifically, MillGNN
introduces two key innovations: (1) a scale-specific lead-lag graph learning
module that integrates cross-correlation coefficients and dynamic decaying
features derived from real-time inputs and time lags to learn lead-lag
dependencies for each scale, which can model evolving lead-lag dependencies
with statistical interpretability and data-driven flexibility; (2) a
hierarchical lead-lag message passing module that passes lead-lag messages at
multiple grouping scales in a structured way to simultaneously propagate intra-
and inter-scale lead-lag effects, which can capture multi-scale lead-lag
effects with a balance of comprehensiveness and efficiency. Experimental
results on 11 datasets demonstrate the superiority of MillGNN for long-term and
short-term MTS forecasting, compared with 16 state-of-the-art methods.

</details>


### [68] [Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network](https://arxiv.org/abs/2509.03884)
*Jesus Celis-Porras*

Main category: cs.LG

TL;DR: MLP neural network model using urinary peptide biomarkers achieves high accuracy (95.67%) for non-invasive coronary heart disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Coronary heart disease is a leading global cause of death with high healthcare costs, necessitating non-invasive diagnostic methods.

Method: Multilayer perceptron neural network trained on 50 urinary peptide biomarkers selected via genetic algorithms, with SMOTE balancing and stratified validation.

Result: Achieved 95.67% precision, sensitivity, and specificity; AUC of 0.9748; F1-score of 0.9565; MCC of 0.9134; Cohen's kappa of 0.9131.

Conclusion: The model provides a highly accurate and robust non-invasive diagnostic tool for coronary heart disease detection.

Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and
contributes significantly to annual healthcare expenditures. To develop a
non-invasive diagnostic approach, we designed a model based on a multilayer
perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers
selected via genetic algorithms. Treatment and control groups, each comprising
345 individuals, were balanced using the Synthetic Minority Over-sampling
Technique (SMOTE). The neural network was trained using a stratified validation
strategy. Using a network with three hidden layers of 60 neurons each and an
output layer of two neurons, the model achieved a precision, sensitivity, and
specificity of 95.67 percent, with an F1-score of 0.9565. The area under the
ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation
coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,
respectively, demonstrating its reliability in detecting CHD. These results
indicate that the model provides a highly accurate and robust non-invasive
diagnostic tool for coronary heart disease.

</details>


### [69] [Topotein: Topological Deep Learning for Protein Representation Learning](https://arxiv.org/abs/2509.03885)
*Zhiyu Wang,Arian Jamasb,Mustafa Hajij,Alex Morehead,Luke Braithwaite,Pietro Liò*

Main category: cs.LG

TL;DR: Topotein framework uses topological deep learning with Protein Combinatorial Complex and TCPNet to capture hierarchical protein structures, outperforming state-of-the-art methods on protein representation learning tasks.


<details>
  <summary>Details</summary>
Motivation: Current sequence- and graph-based protein representation learning methods fail to capture the hierarchical organization inherent in protein structures, limiting their ability to understand structure-function relationships.

Method: Introduces Protein Combinatorial Complex (PCC) to represent proteins at multiple hierarchical levels (residues, secondary structures, complete proteins) while preserving geometric information. Uses Topology-Complete Perceptron Network (TCPNet) with SE(3)-equivariant message passing across hierarchical structures.

Result: TCPNet consistently outperforms state-of-the-art geometric graph neural networks across four protein representation learning tasks. Shows particular strength in fold classification tasks requiring understanding of secondary structure arrangements.

Conclusion: Hierarchical topological features are crucial for effective protein analysis, and the topological deep learning approach provides superior performance in capturing multi-scale structural patterns in proteins.

Abstract: Protein representation learning (PRL) is crucial for understanding
structure-function relationships, yet current sequence- and graph-based methods
fail to capture the hierarchical organization inherent in protein structures.
We introduce Topotein, a comprehensive framework that applies topological deep
learning to PRL through the novel Protein Combinatorial Complex (PCC) and
Topology-Complete Perceptron Network (TCPNet). Our PCC represents proteins at
multiple hierarchical levels -- from residues to secondary structures to
complete proteins -- while preserving geometric information at each level.
TCPNet employs SE(3)-equivariant message passing across these hierarchical
structures, enabling more effective capture of multi-scale structural patterns.
Through extensive experiments on four PRL tasks, TCPNet consistently
outperforms state-of-the-art geometric graph neural networks. Our approach
demonstrates particular strength in tasks such as fold classification which
require understanding of secondary structure arrangements, validating the
importance of hierarchical topological features for protein analysis.

</details>


### [70] [Mistake-bounded online learning with operation caps](https://arxiv.org/abs/2509.03892)
*Jesse Geneson,Meien Li,Linus Tang*

Main category: cs.LG

TL;DR: Analysis of mistake-bound online learning with arithmetic operation caps, solving problems from previous works and extending results to operation-constrained settings.


<details>
  <summary>Details</summary>
Motivation: To understand the computational limits of online learning algorithms by investigating how arithmetic operation constraints affect mistake-bound learning capabilities.

Method: Proving general bounds on minimum arithmetic operations per round required for learning arbitrary function families with finite mistakes, addressing agnostic mistake-bounded online learning with bandit feedback.

Result: Established fundamental limits on arithmetic operation requirements for mistake-bound learning and solved open problems from previous research.

Conclusion: The work provides theoretical foundations for understanding computational constraints in online learning and extends existing results to operation-capped environments.

Abstract: We investigate the mistake-bound model of online learning with caps on the
number of arithmetic operations per round. We prove general bounds on the
minimum number of arithmetic operations per round that are necessary to learn
an arbitrary family of functions with finitely many mistakes. We solve a
problem on agnostic mistake-bounded online learning with bandit feedback from
(Filmus et al, 2024) and (Geneson \& Tang, 2024). We also extend this result to
the setting of operation caps.

</details>


### [71] [Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case](https://arxiv.org/abs/2509.03948)
*Delphine Longuet,Amira Elouazzani,Alejandro Penacho Riveiros,Nicola Bastianello*

Main category: cs.LG

TL;DR: Formal verification of neural network robustness for satellite fault detection systems using Marabou tool to ensure high reliability under input perturbations.


<details>
  <summary>Details</summary>
Motivation: Satellite component failures are costly and difficult to address, requiring an AI-based fault detection system that operates with extremely high reliability to enable earlier detection and reduce resource burden.

Method: Employ the formal verification tool Marabou to verify the local robustness of neural network models used in the AI-based algorithm, quantifying how much input can be perturbed before output behavior becomes unstable.

Result: The approach improves trustworthiness of the AI system by ensuring performance stability under uncertainty through formal verification of neural network robustness.

Conclusion: Formal verification using Marabou provides a method to achieve the required high reliability for satellite fault detection systems by quantifying and ensuring neural network robustness against input perturbations.

Abstract: Failures in satellite components are costly and challenging to address, often
requiring significant human and material resources. Embedding a hybrid AI-based
system for fault detection directly in the satellite can greatly reduce this
burden by allowing earlier detection. However, such systems must operate with
extremely high reliability. To ensure this level of dependability, we employ
the formal verification tool Marabou to verify the local robustness of the
neural network models used in the AI-based algorithm. This tool allows us to
quantify how much a model's input can be perturbed before its output behavior
becomes unstable, thereby improving trustworthiness with respect to its
performance under uncertainty.

</details>


### [72] [On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study](https://arxiv.org/abs/2509.04053)
*Jacqueline J. Vallon,William Overman,Wanqiao Xu,Neil Panjwani,Xi Ling,Sushmita Vij,Hilary P. Bagshaw,John T. Leppert,Sumit Shah,Geoffrey Sonn,Sandy Srinivas,Erqi Pollom,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.LG

TL;DR: A framework for aligning ML models with clinical knowledge through constraints, showing improved alignment without performance loss in prostate cancer prediction.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in healthcare often fail to capture clinically meaningful patterns that align with medical expertise, such as non-monotonic relationships that contradict clinical experience.

Method: Developed a reproducible framework incorporating clinical knowledge via constraints into ML models, using survey-collected clinical expertise and analyzing impact across underspecification degrees. Also conducted randomized experiments with clinicians to test feedback-driven alignment.

Result: Constrained models aligned with clinical experiential learning without compromising performance. Larger differences between constrained/unconstrained models led to more apparent differences in clinical interpretation.

Conclusion: Clinical knowledge can be effectively incorporated into ML models through constraints, improving alignment with medical expertise while maintaining predictive performance, enabling better integration of AI in healthcare decision-making.

Abstract: Over the past decade, the use of machine learning (ML) models in healthcare
applications has rapidly increased. Despite high performance, modern ML models
do not always capture patterns the end user requires. For example, a model may
predict a non-monotonically decreasing relationship between cancer stage and
survival, keeping all other features fixed. In this paper, we present a
reproducible framework for investigating this misalignment between model
behavior and clinical experiential learning, focusing on the effects of
underspecification of modern ML pipelines. In a prostate cancer outcome
prediction case study, we first identify and address these inconsistencies by
incorporating clinical knowledge, collected by a survey, via constraints into
the ML model, and subsequently analyze the impact on model performance and
behavior across degrees of underspecification. The approach shows that aligning
the ML model with clinical experiential learning is possible without
compromising performance. Motivated by recent literature in generative AI, we
further examine the feasibility of a feedback-driven alignment approach in
non-generative AI clinical risk prediction models through a randomized
experiment with clinicians. Our findings illustrate that, by eliciting
clinicians' model preferences using our proposed methodology, the larger the
difference in how the constrained and unconstrained models make predictions for
a patient, the more apparent the difference is in clinical interpretation.

</details>


### [73] [FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity](https://arxiv.org/abs/2509.04107)
*Ozgu Goksu,Nicolas Pugeault*

Main category: cs.LG

TL;DR: FedQuad is a novel federated learning method that addresses data heterogeneity by optimizing intra-class variance and inter-class variance across clients, improving global model generalization through metric learning techniques.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with data heterogeneity, especially when datasets are limited and have class imbalance, which negatively impacts global model generalization during model aggregation.

Method: FedQuad explicitly optimizes smaller intra-class variance and larger inter-class variance across clients by minimizing distance between similar pairs and maximizing distance between negative pairs, effectively disentangling client data in the shared feature space.

Result: The method demonstrates superior performance on CIFAR-10 and CIFAR-100 datasets under various data distributions with many clients, outperforming existing approaches.

Conclusion: Metric learning-based strategies are effective for addressing representational learning challenges in federated settings, with FedQuad providing a robust solution for data heterogeneity problems in federated learning.

Abstract: Federated Learning (FL) provides decentralised model training, which
effectively tackles problems such as distributed data and privacy preservation.
However, the generalisation of global models frequently faces challenges from
data heterogeneity among clients. This challenge becomes even more pronounced
when datasets are limited in size and class imbalance. To address data
heterogeneity, we propose a novel method, \textit{FedQuad}, that explicitly
optimises smaller intra-class variance and larger inter-class variance across
clients, thereby decreasing the negative impact of model aggregation on the
global model over client representations. Our approach minimises the distance
between similar pairs while maximising the distance between negative pairs,
effectively disentangling client data in the shared feature space. We evaluate
our method on the CIFAR-10 and CIFAR-100 datasets under various data
distributions and with many clients, demonstrating superior performance
compared to existing approaches. Furthermore, we provide a detailed analysis of
metric learning-based strategies within both supervised and federated learning
paradigms, highlighting their efficacy in addressing representational learning
challenges in federated settings.

</details>


### [74] [Synthetic Counterfactual Labels for Efficient Conformal Counterfactual Inference](https://arxiv.org/abs/2509.04112)
*Amirmohammad Farzaneh,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: SP-CCI is a new conformal counterfactual inference method that uses synthetic data to reduce prediction interval width while maintaining coverage guarantees, addressing the conservativeness of existing CCI methods.


<details>
  <summary>Details</summary>
Motivation: Existing conformal counterfactual inference methods provide marginal coverage but often produce overly conservative intervals, especially under treatment imbalance when counterfactual samples are scarce.

Method: SP-CCI augments the calibration set with synthetic counterfactual labels from a pre-trained counterfactual model, using risk-controlling prediction sets with a debiasing step informed by prediction-powered inference to ensure validity.

Result: Empirical results show SP-CCI consistently reduces interval width compared to standard CCI across all settings while preserving marginal coverage.

Conclusion: SP-CCI achieves tighter prediction intervals with theoretical guarantees under both exact and approximate importance weighting, providing a more efficient framework for counterfactual inference.

Abstract: This work addresses the problem of constructing reliable prediction intervals
for individual counterfactual outcomes. Existing conformal counterfactual
inference (CCI) methods provide marginal coverage guarantees but often produce
overly conservative intervals, particularly under treatment imbalance when
counterfactual samples are scarce. We introduce synthetic data-powered CCI
(SP-CCI), a new framework that augments the calibration set with synthetic
counterfactual labels generated by a pre-trained counterfactual model. To
ensure validity, SP-CCI incorporates synthetic samples into a conformal
calibration procedure based on risk-controlling prediction sets (RCPS) with a
debiasing step informed by prediction-powered inference (PPI). We prove that
SP-CCI achieves tighter prediction intervals while preserving marginal
coverage, with theoretical guarantees under both exact and approximate
importance weighting. Empirical results on different datasets confirm that
SP-CCI consistently reduces interval width compared to standard CCI across all
settings.

</details>


### [75] [Who Pays for Fairness? Rethinking Recourse under Social Burden](https://arxiv.org/abs/2509.04128)
*Ainhize Barrainkua,Giovanni De Toni,Jose Antonio Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: This paper addresses fairness concerns in algorithmic recourse, linking fairness in recourse and classification, and introduces a new fairness framework based on social burden with a practical algorithm called MISOB.


<details>
  <summary>Details</summary>
Motivation: Emerging legislation requires classifiers to provide actionable recourse when delivering negative decisions, but there are concerns about fairness guarantees within the recourse process itself.

Method: The authors provide a theoretical characterization of unfairness in algorithmic recourse, formally link fairness guarantees between recourse and classification, and introduce a novel fairness framework based on social burden with a practical algorithm (MISOB).

Result: Empirical results on real-world datasets show that MISOB reduces social burden across all groups without compromising overall classifier accuracy.

Conclusion: The work provides a holistic theoretical foundation for understanding unfairness in algorithmic recourse and offers a practical solution that addresses fairness concerns while maintaining classifier performance.

Abstract: Machine learning based predictions are increasingly used in sensitive
decision-making applications that directly affect our lives. This has led to
extensive research into ensuring the fairness of classifiers. Beyond just fair
classification, emerging legislation now mandates that when a classifier
delivers a negative decision, it must also offer actionable steps an individual
can take to reverse that outcome. This concept is known as algorithmic
recourse. Nevertheless, many researchers have expressed concerns about the
fairness guarantees within the recourse process itself. In this work, we
provide a holistic theoretical characterization of unfairness in algorithmic
recourse, formally linking fairness guarantees in recourse and classification,
and highlighting limitations of the standard equal cost paradigm. We then
introduce a novel fairness framework based on social burden, along with a
practical algorithm (MISOB), broadly applicable under real-world conditions.
Empirical results on real-world datasets show that MISOB reduces the social
burden across all groups without compromising overall classifier accuracy.

</details>


### [76] [TAGAL: Tabular Data Generation using Agentic LLM Methods](https://arxiv.org/abs/2509.04152)
*Benoît Ronval,Pierre Dupont,Siegfried Nijssen*

Main category: cs.LG

TL;DR: TAGAL is an agentic workflow using LLMs to generate synthetic tabular data without additional training, performing on par with state-of-the-art methods that require training.


<details>
  <summary>Details</summary>
Motivation: To improve machine learning performance through data generation, specifically for classification tasks, using LLMs without the need for additional training.

Method: Uses an agentic workflow with Large Language Models for automatic, iterative synthetic data generation with feedback loops and external knowledge integration.

Result: TAGAL performs on par with state-of-the-art approaches requiring LLM training and outperforms other training-free methods across diverse datasets and quality metrics.

Conclusion: Agentic workflows show strong potential for LLM-based data generation, opening new directions for synthetic data creation methods.

Abstract: The generation of data is a common approach to improve the performance of
machine learning tasks, among which is the training of models for
classification. In this paper, we present TAGAL, a collection of methods able
to generate synthetic tabular data using an agentic workflow. The methods
leverage Large Language Models (LLMs) for an automatic and iterative process
that uses feedback to improve the generated data without any further LLM
training. The use of LLMs also allows for the addition of external knowledge in
the generation process. We evaluate TAGAL across diverse datasets and different
aspects of quality for the generated data. We look at the utility of downstream
ML models, both by training classifiers on synthetic data only and by combining
real and synthetic data. Moreover, we compare the similarities between the real
and the generated data. We show that TAGAL is able to perform on par with
state-of-the-art approaches that require LLM training and generally outperforms
other training-free approaches. These findings highlight the potential of
agentic workflow and open new directions for LLM-based data generation methods.

</details>


### [77] [Attention as an Adaptive Filter](https://arxiv.org/abs/2509.04154)
*Peter Racioppo*

Main category: cs.LG

TL;DR: AFA is a novel attention mechanism that models input sequences as linear SDEs, using closed-form solutions to compute attention weights through propagated pairwise uncertainties, with standard attention as a special case.


<details>
  <summary>Details</summary>
Motivation: To develop a more theoretically grounded attention mechanism that incorporates learnable dynamics modeling and provides robust uncertainty propagation, moving beyond direct query-key comparisons.

Method: Models input sequence as discrete observations of linear SDE with diagonalizable state matrices, uses closed-form solution to differential Lyapunov equation to propagate pairwise uncertainties, derives attention weights as maximum likelihood solution with residual-based reweighting.

Result: Developed AFA mechanism that maintains same computational/memory complexity as standard attention while providing theoretical foundation and robust uncertainty handling through learnable dynamics modeling.

Conclusion: AFA provides a principled extension to standard attention with learnable dynamics, uncertainty propagation, and theoretical guarantees, recovering standard dot-product attention as a special limiting case.

Abstract: We introduce Adaptive Filter Attention (AFA), a novel attention mechanism
that incorporates a learnable dynamics model directly into the computation of
attention weights. Rather than comparing queries and keys directly, we model
the input sequence as discrete observations of a linear stochastic differential
equation (SDE). By imposing a linear dynamics model with simultaneously
diagonalizable state matrices and noise covariances, we can make use of a
closed-form solution to the differential Lyapunov equation to efficiently
propagate pairwise uncertainties through the dynamics. Attention naturally
arises as the maximum likelihood solution for this linear SDE, with attention
weights corresponding to robust residual-based reweightings of the propagated
pairwise precisions. Imposing an additional constraint on the state matrix's
eigenvalues leads to a simplified variant with the same computational and
memory complexity as standard attention. In the limit of vanishing dynamics and
process noise, and using a small-angle approximation, we recover ordinary
dot-product attention.

</details>


### [78] [Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds](https://arxiv.org/abs/2509.04166)
*Jules Cauzinille,Marius Miron,Olivier Pietquin,Masato Hagiwara,Ricard Marxer,Arnaud Rey,Benoit Favre*

Main category: cs.LG

TL;DR: Speech self-supervised models like HuBERT, WavLM, and XEUS effectively transfer to bioacoustic tasks, generating rich representations of animal sounds and achieving competitive performance with specialized bioacoustic models.


<details>
  <summary>Details</summary>
Motivation: Self-supervised speech models have shown strong performance in speech processing, but their effectiveness on non-speech bioacoustic data remains underexplored, prompting investigation into their transfer learning capabilities for animal sound detection and classification.

Method: Analyzed transfer learning capabilities using linear probing on time-averaged representations, extended approach with downstream architectures to account for time-wise information, and studied effects of frequency range and noise on performance.

Result: Models generated rich latent representations of animal sounds across taxa, achieved competitive results compared to fine-tuned bioacoustic pre-trained models, and demonstrated impact of noise-robust pre-training setups.

Conclusion: Speech-based self-supervised learning provides an efficient framework for advancing bioacoustic research, showing strong potential for cross-domain transfer from speech to animal sound analysis.

Abstract: Self-supervised speech models have demonstrated impressive performance in
speech processing, but their effectiveness on non-speech data remains
underexplored. We study the transfer learning capabilities of such models on
bioacoustic detection and classification tasks. We show that models such as
HuBERT, WavLM, and XEUS can generate rich latent representations of animal
sounds across taxa. We analyze the models properties with linear probing on
time-averaged representations. We then extend the approach to account for the
effect of time-wise information with other downstream architectures. Finally,
we study the implication of frequency range and noise on performance. Notably,
our results are competitive with fine-tuned bioacoustic pre-trained models and
show the impact of noise-robust pre-training setups. These findings highlight
the potential of speech-based self-supervised learning as an efficient
framework for advancing bioacoustic research.

</details>


### [79] [Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference](https://arxiv.org/abs/2509.04169)
*Nicolas Johansson,Tobias Olsson,Daniel Nilsson,Johan Östman,Fazeleh Hoseini*

Main category: cs.LG

TL;DR: This paper introduces two new membership inference attacks for time series forecasting models and shows they achieve strong performance, establishing new baselines for privacy risk assessment.


<details>
  <summary>Details</summary>
Motivation: To address the gap in membership inference attack research on time series forecasting models, which has been extensively studied on classification models but remains largely unexplored for forecasting.

Method: Developed two attacks: (1) adaptation of multivariate LiRA from classification to time-series forecasting, and (2) novel end-to-end Deep Time Series (DTS) attack. Benchmarked against adapted versions of other leading classification attacks on TUH-EEG and ELD datasets targeting LSTM and N-HiTS architectures.

Result: Forecasting models are vulnerable to membership inference attacks, with user-level attacks often achieving perfect detection. Proposed methods achieved strongest performance in several settings. Vulnerability increases with longer prediction horizons and smaller training populations.

Conclusion: Time series forecasting models are susceptible to membership inference attacks, establishing new baselines for privacy risk assessment in this domain with trends similar to those observed in large language models.

Abstract: Membership inference attacks (MIAs) aim to determine whether specific data
were used to train a model. While extensively studied on classification models,
their impact on time series forecasting remains largely unexplored. We address
this gap by introducing two new attacks: (i) an adaptation of multivariate
LiRA, a state-of-the-art MIA originally developed for classification models, to
the time-series forecasting setting, and (ii) a novel end-to-end learning
approach called Deep Time Series (DTS) attack. We benchmark these methods
against adapted versions of other leading attacks from the classification
setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD
datasets, targeting two strong forecasting architectures, LSTM and the
state-of-the-art N-HiTS, under both record- and user-level threat models. Our
results show that forecasting models are vulnerable, with user-level attacks
often achieving perfect detection. The proposed methods achieve the strongest
performance in several settings, establishing new baselines for privacy risk
assessment in time series forecasting. Furthermore, vulnerability increases
with longer prediction horizons and smaller training populations, echoing
trends observed in large language models.

</details>


### [80] [Comment on "A Note on Over-Smoothing for Graph Neural Networks"](https://arxiv.org/abs/2509.04178)
*Razi Hasson,Reuven Guetta*

Main category: cs.LG

TL;DR: This paper provides a critical analysis of Cai and Wang's 2020 work on over-smoothing in GNNs, showing that Dirichlet energy decreases exponentially with depth under mild spectral conditions, including with Leaky-ReLU, and extends the analysis to spectral polynomial filters.


<details>
  <summary>Details</summary>
Motivation: To address and extend the understanding of over-smoothing phenomena in Graph Neural Networks (GNNs) by building upon Cai and Wang's Dirichlet energy analysis, providing more comprehensive theoretical foundations.

Method: The authors use spectral analysis under mild conditions (including Leaky-ReLU activations) to demonstrate exponential decrease of Dirichlet energy with depth, extend the analysis to spectral polynomial filters, and provide experimental validation through edge deletion and weight amplification tests.

Result: Theoretical analysis confirms exponential decrease of Dirichlet energy with network depth under specified conditions. Experiments show specific scenarios where Dirichlet energy increases, suggesting practical approaches to mitigate over-smoothing.

Conclusion: The work provides stronger theoretical grounding for understanding over-smoothing in GNNs and identifies practical strategies (edge deletion and weight amplification) that can help alleviate this problem in deep graph neural networks.

Abstract: We comment on Cai and Wang (2020, arXiv:2006.13318), who analyze
over-smoothing in GNNs via Dirichlet energy. We show that under mild spectral
conditions (including with Leaky-ReLU), the Dirichlet energy of node embeddings
decreases exponentially with depth; we further extend the result to spectral
polynomial filters and provide a short proof for the Leaky-ReLU case.
Experiments on edge deletion and weight amplification illustrate when Dirichlet
energy increases, hinting at practical ways to relieve over-smoothing.

</details>


### [81] [Set Block Decoding is a Language Model Inference Accelerator](https://arxiv.org/abs/2509.04185)
*Itai Gat,Heli Ben-Hamu,Marton Havasi,Daniel Haziza,Jeremy Reizenstein,Gabriel Synnaeve,David Lopez-Paz,Brian Karrer,Yaron Lipman*

Main category: cs.LG

TL;DR: Set Block Decoding (SBD) accelerates autoregressive language model inference by combining next token prediction and masked token prediction to sample multiple future tokens in parallel, achieving 3-5x speedup without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models face high computational and memory costs during inference, particularly in the decoding stage, which limits practical deployment.

Method: SBD integrates standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture, allowing parallel sampling of multiple non-consecutive future tokens using discrete diffusion solvers.

Result: Fine-tuning Llama-3.1 8B and Qwen-3 8B with SBD achieves 3-5x reduction in forward passes while maintaining same performance as equivalent NTP training.

Conclusion: SBD provides a simple, flexible acceleration method that requires no architectural changes, maintains KV-caching compatibility, and can be implemented by fine-tuning existing models.

Abstract: Autoregressive next token prediction language models offer powerful
capabilities but face significant challenges in practical deployment due to the
high computational and memory costs of inference, particularly during the
decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible
paradigm that accelerates generation by integrating standard next token
prediction (NTP) and masked token prediction (MATP) within a single
architecture. SBD allows the model to sample multiple, not necessarily
consecutive, future tokens in parallel, a key distinction from previous
acceleration methods. This flexibility allows the use of advanced solvers from
the discrete diffusion literature, offering significant speedups without
sacrificing accuracy. SBD requires no architectural changes or extra training
hyperparameters, maintains compatibility with exact KV-caching, and can be
implemented by fine-tuning existing next token prediction models. By
fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x
reduction in the number of forward passes required for generation while
achieving same performance as equivalent NTP training.

</details>


### [82] [One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a Model Zoo](https://arxiv.org/abs/2509.04208)
*Hao-Nan Shi,Ting-Ji Huang,Lu Han,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: ZooCast is a framework that intelligently combines multiple Time Series Foundation Models (TSFMs) into a model zoo, using a unified embedding space to dynamically select the best model for each forecasting task without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Different TSFMs excel at different temporal patterns, but no single model performs best universally. The authors want to leverage the complementary strengths of multiple TSFMs to improve zero-shot forecasting performance.

Method: Proposes ZooCast with One-Embedding-Fits-All paradigm that creates a unified representation space where each TSFM is represented by a single embedding, enabling efficient similarity matching for task-specific model selection.

Result: ZooCast demonstrates strong performance on the GIFT-Eval zero-shot forecasting benchmark while maintaining the efficiency of a single TSFM. It also seamlessly integrates new models with negligible overhead.

Conclusion: The framework successfully leverages complementary TSFM abilities through intelligent model selection, achieving improved forecasting accuracy while maintaining efficiency and scalability for sequential model releases.

Abstract: The proliferation of Time Series Foundation Models (TSFMs) has significantly
advanced zero-shot forecasting, enabling predictions for unseen time series
without task-specific fine-tuning. Extensive research has confirmed that no
single TSFM excels universally, as different models exhibit preferences for
distinct temporal patterns. This diversity suggests an opportunity: how to take
advantage of the complementary abilities of TSFMs. To this end, we propose
ZooCast, which characterizes each model's distinct forecasting strengths.
ZooCast can intelligently assemble current TSFMs into a model zoo that
dynamically selects optimal models for different forecasting tasks. Our key
innovation lies in the One-Embedding-Fits-All paradigm that constructs a
unified representation space where each model in the zoo is represented by a
single embedding, enabling efficient similarity matching for all tasks.
Experiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shot
forecasting benchmark while maintaining the efficiency of a single TSFM. In
real-world scenarios with sequential model releases, the framework seamlessly
adds new models for progressive accuracy gains with negligible overhead.

</details>


### [83] [Why Can't I See My Clusters? A Precision-Recall Approach to Dimensionality Reduction Validation](https://arxiv.org/abs/2509.04222)
*Diede P. M. van der Hoorn,Alessio Arleo,Fernando V. Paulovich*

Main category: cs.LG

TL;DR: This paper introduces two supervised metrics (precision and recall) to evaluate the relationship phase of dimensionality reduction, helping explain why expected cluster structures may be missing from projections and guiding hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing DR quality metrics don't explain why expected cluster structures are missing from projections, and visual analytics solutions are time-consuming due to large hyperparameter spaces.

Method: Leverages a framework dividing DR into relationship and mapping phases, introducing precision and recall metrics to evaluate how well modeled relationships align with expected cluster structures based on labels.

Result: The approach can guide hyperparameter tuning, uncover projection artifacts, and determine if expected structure is captured in relationships, making DR faster and more reliable.

Conclusion: The introduced supervised metrics provide a systematic way to evaluate DR relationship phases and address the gap in explaining missing expected structures in projections.

Abstract: Dimensionality Reduction (DR) is widely used for visualizing high-dimensional
data, often with the goal of revealing expected cluster structure. However,
such a structure may not always appear in the projections. Existing DR quality
metrics assess projection reliability (to some extent) or cluster structure
quality, but do not explain why expected structures are missing. Visual
Analytics solutions can help, but are often time-consuming due to the large
hyperparameter space. This paper addresses this problem by leveraging a recent
framework that divides the DR process into two phases: a relationship phase,
where similarity relationships are modeled, and a mapping phase, where the data
is projected accordingly. We introduce two supervised metrics, precision and
recall, to evaluate the relationship phase. These metrics quantify how well the
modeled relationships align with an expected cluster structure based on some
set of labels representing this structure. We illustrate their application
using t-SNE and UMAP, and validate the approach through various usage
scenarios. Our approach can guide hyperparameter tuning, uncover projection
artifacts, and determine if the expected structure is captured in the
relationships, making the DR process faster and more reliable.

</details>


### [84] [Rethinking the long-range dependency in Mamba/SSM and transformer models](https://arxiv.org/abs/2509.04226)
*Cong Ma,Kayvan Najarian*

Main category: cs.LG

TL;DR: Theoretical analysis shows SSMs have exponential decay in long-range dependency while transformers' attention mechanism offers more flexibility. A new SSM formulation is proposed to combine attention's flexibility with SSM efficiency.


<details>
  <summary>Details</summary>
Motivation: To theoretically analyze and compare the long-range dependency modeling capabilities of state-space models (SSM) and transformers, as current benchmarking lacks mathematical foundations for systematic improvement.

Method: Mathematically defined long-range dependency using derivative of hidden states w.r.t. past inputs. Compared SSM and transformer capabilities, then proposed new SSM formulation with proven stability under Gaussian input distribution.

Result: SSM's long-range dependency decays exponentially with sequence length (similar to RNN memory decay), while attention mechanism in transformers is more flexible without exponential decay constraints.

Conclusion: Transformers theoretically perform better at long-range dependency modeling with sufficient resources, but new SSM formulation combines attention flexibility with SSM computational efficiency while maintaining stability.

Abstract: Long-range dependency is one of the most desired properties of recent
sequence models such as state-space models (particularly Mamba) and transformer
models. New model architectures are being actively developed and benchmarked
for prediction tasks requiring long-range dependency. However, the capability
of modeling long-range dependencies of these models has not been investigated
from a theoretical perspective, which hinders a systematic improvement on this
aspect. In this work, we mathematically define long-range dependency using the
derivative of hidden states with respect to past inputs and compare the
capability of SSM and transformer models of modeling long-range dependency
based on this definition. We showed that the long-range dependency of SSM
decays exponentially with the sequence length, which aligns with the
exponential decay of memory function in RNN. But the attention mechanism used
in transformers is more flexible and is not constrained to exponential decay,
which could in theory perform better at modeling long-range dependency with
sufficient training data, computing resources, and proper training. To combine
the flexibility of long-range dependency of attention mechanism and computation
efficiency of SSM, we propose a new formulation for hidden state update in SSM
and prove its stability under a standard Gaussian distribution of the input
data.

</details>


### [85] [Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit Objectives and Privacy Budget Allocation](https://arxiv.org/abs/2509.04232)
*Qifeng Tan,Shusen Yang,Xuebin Ren,Yikai Zhang*

Main category: cs.LG

TL;DR: A unified analytical framework for layer-wise Gaussian mechanisms in differentially private deep learning that connects noise allocation strategies to privacy-utility tradeoffs, revealing flaws in existing methods and proposing a SNR-consistent approach that outperforms previous strategies.


<details>
  <summary>Details</summary>
Motivation: Existing layer-wise Gaussian mechanisms use heuristic noise allocation strategies without rigorous theoretical grounding, lacking understanding of how noise allocation connects to formal privacy-utility tradeoffs.

Method: Developed a unified analytical framework that systematically connects layer-wise noise injection strategies with optimization objectives and privacy budget allocations. Proposed a SNR-Consistent noise allocation strategy that ensures inter-layer signal-to-noise ratio consistency and efficient privacy budget utilization.

Result: Extensive experiments in centralized and federated learning settings show the method consistently outperforms existing allocation strategies, achieving better privacy-utility tradeoffs with improved signal preservation and privacy budget efficiency.

Conclusion: The framework provides diagnostic insights into prior methods and offers theoretical guidance for designing adaptive and effective noise injection schemes in deep models, establishing a rigorous connection between noise allocation and privacy-utility optimization.

Abstract: Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially
private deep learning by injecting noise into partitioned gradient vectors.
However, existing methods often rely on heuristic noise allocation strategies,
lacking a rigorous understanding of their theoretical grounding in connecting
noise allocation to formal privacy-utility tradeoffs. In this paper, we present
a unified analytical framework that systematically connects layer-wise noise
injection strategies with their implicit optimization objectives and associated
privacy budget allocations. Our analysis reveals that several existing
approaches optimize ill-posed objectives -- either ignoring inter-layer
signal-to-noise ratio (SNR) consistency or leading to inefficient use of the
privacy budget. In response, we propose a SNR-Consistent noise allocation
strategy that unifies both aspects, yielding a noise allocation scheme that
achieves better signal preservation and more efficient privacy budget
utilization. Extensive experiments in both centralized and federated learning
settings demonstrate that our method consistently outperforms existing
allocation strategies, achieving better privacy-utility tradeoffs. Our
framework not only offers diagnostic insights into prior methods but also
provides theoretical guidance for designing adaptive and effective noise
injection schemes in deep models.

</details>


### [86] [Synthetic Survival Data Generation for Heart Failure Prognosis Using Deep Generative Models](https://arxiv.org/abs/2509.04245)
*Chanon Puttanawarut,Natcha Fongsrisin,Porntep Amornritvanich,Cholatid Ratanatharathorn,Panu Looareesuwan*

Main category: cs.LG

TL;DR: Deep learning models can generate high-quality synthetic heart failure data that preserves privacy while maintaining statistical fidelity and predictive utility comparable to real data.


<details>
  <summary>Details</summary>
Motivation: Heart failure research faces challenges due to limited access to large, shareable datasets caused by privacy regulations and institutional barriers. Synthetic data generation offers a solution to overcome these limitations while protecting patient confidentiality.

Method: Generated synthetic HF datasets from 12,552 patients using five deep learning models: TVAE, normalizing flow, ADSGAN, SurvivalGAN, and TabDDPM. Evaluated through statistical similarity metrics, survival prediction performance, and privacy assessments.

Result: SurvivalGAN and TabDDPM showed high fidelity with similar variable distributions and survival curves. SurvivalGAN and TVAE achieved strong survival prediction performance (C-indices: 0.71-0.76), closely matching real data performance (0.73-0.76). Privacy evaluation confirmed protection against re-identification attacks.

Conclusion: Deep learning-based synthetic data generation can produce high-fidelity, privacy-preserving HF datasets suitable for research, addressing critical data sharing barriers and providing valuable resources for advancing HF research and predictive modeling.

Abstract: Background: Heart failure (HF) research is constrained by limited access to
large, shareable datasets due to privacy regulations and institutional
barriers. Synthetic data generation offers a promising solution to overcome
these challenges while preserving patient confidentiality. Methods: We
generated synthetic HF datasets from institutional data comprising 12,552
unique patients using five deep learning models: tabular variational
autoencoder (TVAE), normalizing flow, ADSGAN, SurvivalGAN, and tabular
denoising diffusion probabilistic models (TabDDPM). We comprehensively
evaluated synthetic data utility through statistical similarity metrics,
survival prediction using machine learning and privacy assessments. Results:
SurvivalGAN and TabDDPM demonstrated high fidelity to the original dataset,
exhibiting similar variable distributions and survival curves after applying
histogram equalization. SurvivalGAN (C-indices: 0.71-0.76) and TVAE (C-indices:
0.73-0.76) achieved the strongest performance in survival prediction
evaluation, closely matched real data performance (C-indices: 0.73-0.76).
Privacy evaluation confirmed protection against re-identification attacks.
Conclusions: Deep learning-based synthetic data generation can produce
high-fidelity, privacy-preserving HF datasets suitable for research
applications. This publicly available synthetic dataset addresses critical data
sharing barriers and provides a valuable resource for advancing HF research and
predictive modeling.

</details>


### [87] [RL's Razor: Why Online Reinforcement Learning Forgets Less](https://arxiv.org/abs/2509.04259)
*Idan Shenfeld,Jyothish Pari,Pulkit Agrawal*

Main category: cs.LG

TL;DR: RL fine-tuning preserves prior knowledge better than SFT despite similar task performance, with KL-divergence determining forgetting. On-policy RL biases toward KL-minimal solutions while SFT can diverge arbitrarily.


<details>
  <summary>Details</summary>
Motivation: To understand why reinforcement learning fine-tuning preserves model capabilities better than supervised fine-tuning when adapting to new tasks, and to quantify the relationship between distributional shift and knowledge retention.

Method: Theoretical analysis of KL-divergence between fine-tuned and base policies, experimental validation with large language models and robotic foundation models, and mathematical justification for on-policy RL updates.

Result: RL fine-tuning maintains significantly better prior knowledge preservation compared to SFT, with the degree of forgetting determined by KL-divergence. On-policy RL converges to solutions with minimal KL change from the base model.

Conclusion: The study establishes "RL's Razor" principle: among all solutions for a new task, reinforcement learning prefers those closest in KL-divergence to the original model, explaining its superior knowledge retention properties.

Abstract: Comparison of fine-tuning models with reinforcement learning (RL) and
supervised fine-tuning (SFT) reveals that, despite similar performance at a new
task, RL preserves prior knowledge and capabilities significantly better. We
find that the degree of forgetting is determined by the distributional shift,
measured as the KL-divergence between the fine-tuned and base policy evaluated
on the new task. Our analysis reveals that on-policy RL is implicitly biased
towards KL-minimal solutions among the many that solve the new task, whereas
SFT can converge to distributions arbitrarily far from the base model. We
validate these findings through experiments with large language models and
robotic foundation models and further provide theoretical justification for why
on-policy RL updates lead to a smaller KL change. We term this principle
$\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those
closest in KL to the original model.

</details>


### [88] [An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy](https://arxiv.org/abs/2509.04290)
*Yaohong Yang,Aki Rehn,Sammie Katt,Antti Honkela,Samuel Kaski*

Main category: cs.LG

TL;DR: A novel interactive multi-objective optimization approach for differentially private machine learning that directly models the privacy-accuracy Pareto front and uses more informative preference learning through hypothetical trade-off curves, achieving optimal results with less computation and user interaction.


<details>
  <summary>Details</summary>
Motivation: Standard multi-objective optimization approaches are inefficient for differential privacy problems because they fail to leverage the unique structure where Pareto-optimal points can be generated directly by maximizing accuracy for fixed privacy levels.

Method: Theoretical derivation of the trade-off shape to directly model the Pareto front, replacing pairwise comparisons with more informative interactions where users select preferred trade-offs from hypothetical curves.

Result: Experiments on differentially private logistic regression and deep transfer learning across six real-world datasets show the method converges to optimal privacy-accuracy trade-offs with significantly less computational cost and user interaction than baselines.

Conclusion: The proposed approach effectively leverages the structural properties of differential privacy problems to enable more efficient multi-objective optimization and preference learning, making privacy-preserving model development more practical.

Abstract: Differential privacy (DP) is the standard for privacy-preserving analysis,
and introduces a fundamental trade-off between privacy guarantees and model
performance. Selecting the optimal balance is a critical challenge that can be
framed as a multi-objective optimization (MOO) problem where one first
discovers the set of optimal trade-offs (the Pareto front) and then learns a
decision-maker's preference over them. While a rich body of work on interactive
MOO exists, the standard approach -- modeling the objective functions with
generic surrogates and learning preferences from simple pairwise feedback -- is
inefficient for DP because it fails to leverage the problem's unique structure:
a point on the Pareto front can be generated directly by maximizing accuracy
for a fixed privacy level. Motivated by this property, we first derive the
shape of the trade-off theoretically, which allows us to model the Pareto front
directly and efficiently. To address inefficiency in preference learning, we
replace pairwise comparisons with a more informative interaction. In
particular, we present the user with hypothetical trade-off curves and ask them
to pick their preferred trade-off. Our experiments on differentially private
logistic regression and deep transfer learning across six real-world datasets
show that our method converges to the optimal privacy-accuracy trade-off with
significantly less computational cost and user interaction than baselines.

</details>


### [89] [A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis](https://arxiv.org/abs/2509.04295)
*Charles Jones,Ben Glocker*

Main category: cs.LG

TL;DR: Machine learning fails in real-world high-stakes applications due to overlooked causal and statistical problems - the "no fair lunch" and "subgroup separability" problems that current fair representation methods don't adequately address.


<details>
  <summary>Details</summary>
Motivation: Machine learning methods often fail in real-world high-stakes situations and across socially sensitive lines, creating barriers to adoption in critical domains like medical diagnosis where they could provide significant benefits if safely deployed.

Method: The paper introduces causal and statistical structures that cause ML failures in image analysis, identifies two overlooked problems (no fair lunch and subgroup separability), analyzes why current fair representation learning methods fail to solve them, and proposes potential solutions.

Result: The analysis reveals that existing fair representation learning approaches are insufficient for addressing the fundamental no fair lunch and subgroup separability problems that cause ML systems to fail in real-world deployment scenarios.

Conclusion: The paper proposes new paths forward for the field to address these overlooked structural problems and enable safer deployment of machine learning in high-stakes applications like medical diagnosis.

Abstract: Machine learning methods often fail when deployed in the real world. Worse
still, they fail in high-stakes situations and across socially sensitive lines.
These issues have a chilling effect on the adoption of machine learning methods
in settings such as medical diagnosis, where they are arguably best-placed to
provide benefits if safely deployed. In this primer, we introduce the causal
and statistical structures which induce failure in machine learning methods for
image analysis. We highlight two previously overlooked problems, which we call
the \textit{no fair lunch} problem and the \textit{subgroup separability}
problem. We elucidate why today's fair representation learning methods fail to
adequately solve them and propose potential paths forward for the field.

</details>


### [90] [Using causal abstractions to accelerate decision-making in complex bandit problems](https://arxiv.org/abs/2509.04296)
*Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge,Fabio Massimo Zennaro*

Main category: cs.LG

TL;DR: AT-UCB algorithm leverages causal abstraction theory to explore coarse-grained CMAB instances first, then applies UCB on a restricted action set, reducing cumulative regret compared to classical UCB.


<details>
  <summary>Details</summary>
Motivation: Real-world decision problems can be encoded as causal multi-armed bandits at different abstraction levels, but existing methods don't efficiently exploit shared information between these levels.

Method: Proposes AT-UCB algorithm that uses causal abstraction theory to explore cheap-to-simulate coarse-grained CMAB instances first, then applies traditional UCB on a restricted set of potentially optimal actions.

Result: Theoretical analysis shows novel upper bound on cumulative regret. Empirical evaluation on epidemiological simulators with varying resolution demonstrates significant regret reduction compared to classical UCB.

Conclusion: AT-UCB effectively exploits information sharing between different abstraction levels in CMAB problems, providing both theoretical guarantees and practical performance improvements.

Abstract: Although real-world decision-making problems can often be encoded as causal
multi-armed bandits (CMABs) at different levels of abstraction, a general
methodology exploiting the information and computational advantages of each
abstraction level is missing. In this paper, we propose AT-UCB, an algorithm
which efficiently exploits shared information between CMAB problem instances
defined at different levels of abstraction. More specifically, AT-UCB leverages
causal abstraction (CA) theory to explore within a cheap-to-simulate and
coarse-grained CMAB instance, before employing the traditional upper confidence
bound (UCB) algorithm on a restricted set of potentially optimal actions in the
CMAB of interest, leading to significant reductions in cumulative regret when
compared to the classical UCB algorithm. We illustrate the advantages of AT-UCB
theoretically, through a novel upper bound on the cumulative regret, and
empirically, by applying AT-UCB to epidemiological simulators with varying
resolution and computational cost.

</details>


### [91] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: Proposes a data-driven behavioral model to analyze Army installation energy usage for climate resilience planning, using building-level energy consumption patterns to assess vulnerability and benchmark future resilience measures.


<details>
  <summary>Details</summary>
Motivation: US Army installations face climate change threats and depend on vulnerable commercial energy/water infrastructure. Need resilience measures to protect critical mission-supporting facilities and understand energy usage patterns.

Method: Develops data-driven behavioral models to analyze, predict, and cluster multimodal energy usage data from non-residential buildings. Uses open access data with similar structure to Army installation data for methodology demonstration.

Result: Creates behavior profiles of energy usage that establish baseline assessments for disruption impact analysis and provide benchmarks for evaluating future resilience measures.

Conclusion: The proposed methodology enables systematic assessment of energy system vulnerabilities and provides a foundation for developing effective climate resilience strategies for military installations.

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


### [92] [Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer](https://arxiv.org/abs/2509.04362)
*Yin Huang,Yongqi Dong,Youhua Tang,Li Li*

Main category: cs.LG

TL;DR: SST-iTransformer model for parking availability prediction using multi-source traffic data and self-supervised learning with dual-branch attention mechanism


<details>
  <summary>Details</summary>
Motivation: Address urban parking crisis by improving spatio-temporal dependency modeling and multi-source data utilization for accurate parking availability prediction

Method: K-means clustering for parking cluster zones, integrates metro/bus/ride-hailing/taxi data, self-supervised learning with masking-reconstruction, dual-branch attention (Series Attention for temporal dependencies, Channel Attention for cross-variate interactions)

Result: Outperforms baseline models (Informer, Autoformer, Crossformer, iTransformer) with lowest MSE and competitive MAE on Chengdu data; ride-hailing data most beneficial, followed by taxi, while bus/metro contribute marginally

Conclusion: SST-iTransformer achieves state-of-the-art performance, demonstrating importance of modeling spatial dependencies and multi-source data integration for parking prediction

Abstract: The rapid growth of private car ownership has worsened the urban parking
predicament, underscoring the need for accurate and effective parking
availability prediction to support urban planning and management. To address
key limitations in modeling spatio-temporal dependencies and exploiting
multi-source data for parking availability prediction, this study proposes a
novel approach with SST-iTransformer. The methodology leverages K-means
clustering to establish parking cluster zones (PCZs), extracting and
integrating traffic demand characteristics from various transportation modes
(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted
parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates
masking-reconstruction-based pretext tasks for self-supervised spatio-temporal
representation learning, and features an innovative dual-branch attention
mechanism: Series Attention captures long-term temporal dependencies via
patching operations, while Channel Attention models cross-variate interactions
through inverted dimensions. Extensive experiments using real-world data from
Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep
learning models (including Informer, Autoformer, Crossformer, and
iTransformer), achieving state-of-the-art performance with the lowest mean
squared error (MSE) and competitive mean absolute error (MAE). Comprehensive
ablation studies quantitatively reveal the relative importance of different
data sources: incorporating ride-hailing data provides the largest performance
gains, followed by taxi, whereas fixed-route transit features (bus/metro)
contribute marginally. Spatial correlation analysis further confirms that
excluding historical data from correlated parking lots within PCZs leads to
substantial performance degradation, underscoring the importance of modeling
spatial dependencies.

</details>


### [93] [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](https://arxiv.org/abs/2509.04363)
*Paul Scherer,Andreas Kirsch,Jake P. Taylor-King*

Main category: cs.LG

TL;DR: Novel active learning strategies that reduce bias between experimental rounds by leveraging bias-variance tradeoff and aleatoric uncertainty, with a cobias-covariance relationship for batching that outperforms canonical methods.


<details>
  <summary>Details</summary>
Motivation: Real-world experiments often have heteroskedastic aleatoric uncertainty that can be correlated in batched settings, requiring better active learning approaches to handle this uncertainty.

Method: Proposes active learning strategies based on bias-variance tradeoff to reduce bias between rounds, and introduces cobias-covariance relationship with eigendecomposition for batching with quadratic estimators.

Result: The difference-based method using cobias-covariance relationship in batched settings with quadratic estimator outperforms canonical methods like BALD and Least Confidence.

Conclusion: The proposed framework effectively handles heteroskedastic aleatoric uncertainty and provides superior performance in active learning scenarios compared to established methods.

Abstract: Real-world experimental scenarios are characterized by the presence of
heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated
in batched settings. The bias--variance tradeoff can be used to write the
expected mean squared error between a model distribution and a ground-truth
random variable as the sum of an epistemic uncertainty term, the bias squared,
and an aleatoric uncertainty term. We leverage this relationship to propose
novel active learning strategies that directly reduce the bias between
experimental rounds, considering model systems both with and without noise.
Finally, we investigate methods to leverage historical data in a quadratic
manner through the use of a novel cobias--covariance relationship, which
naturally proposes a mechanism for batching through an eigendecomposition
strategy. When our difference-based method leveraging the cobias--covariance
relationship is utilized in a batched setting (with a quadratic estimator), we
outperform a number of canonical methods including BALD and Least Confidence.

</details>


### [94] [PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference](https://arxiv.org/abs/2509.04377)
*Krishna Teja Chitty-Venkata,Jie Ye,Xian-He Sun,Anthony Kougkas,Murali Emani,Venkatram Vishwanath,Bogdan Nicolae*

Main category: cs.LG

TL;DR: PagedEviction is a fine-grained KV cache pruning strategy that improves memory efficiency in LLM inference by using block-wise eviction tailored for paged memory layouts, achieving better accuracy than baselines on long context tasks.


<details>
  <summary>Details</summary>
Motivation: KV caching improves LLM inference efficiency but becomes a memory bottleneck as sequence length increases, requiring better memory management strategies.

Method: Proposes PagedEviction, a structured KV cache pruning strategy with block-wise eviction algorithm designed for paged memory layouts, seamlessly integrating with vLLM's PagedAttention without modifying CUDA kernels.

Result: Evaluated on Llama-3 models using LongBench benchmark, showing improved memory usage with better accuracy than baselines on long context tasks.

Conclusion: PagedEviction effectively addresses KV cache memory bottlenecks while maintaining accuracy, providing an efficient solution for long-context LLM inference.

Abstract: KV caching significantly improves the efficiency of Large Language Model
(LLM) inference by storing attention states from previously processed tokens,
enabling faster generation of subsequent tokens. However, as sequence length
increases, the KV cache quickly becomes a major memory bottleneck. To address
this, we propose PagedEviction, a novel fine-grained, structured KV cache
pruning strategy that enhances the memory efficiency of vLLM's PagedAttention.
Unlike existing approaches that rely on attention-based token importance or
evict tokens across different vLLM pages, PagedEviction introduces an efficient
block-wise eviction algorithm tailored for paged memory layouts. Our method
integrates seamlessly with PagedAttention without requiring any modifications
to its CUDA attention kernels. We evaluate PagedEviction across
Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models
on the LongBench benchmark suite, demonstrating improved memory usage with
better accuracy than baselines on long context tasks.

</details>


### [95] [Transition Models: Rethinking the Generative Learning Objective](https://arxiv.org/abs/2509.04394)
*Zidong Wang,Yiyuan Zhang,Xiaoyu Yue,Xiangyu Yue,Yangguang Li,Wanli Ouyang,Lei Bai*

Main category: cs.LG

TL;DR: TiM introduces continuous-time transition models that enable flexible sampling from single-step to multi-step generation, achieving SOTA performance with fewer parameters and monotonic quality improvement with more steps.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental trade-off between computational efficiency (few-step generation) and output quality (iterative diffusion models) in generative modeling.

Method: Develops an exact continuous-time dynamics equation for state transitions across any finite time interval, creating Transition Models (TiM) that adapt to arbitrary-step transitions.

Result: TiM with 865M parameters outperforms SD3.5 (8B) and FLUX.1 (12B) across all step counts, shows monotonic quality improvement with more steps, and achieves exceptional fidelity at resolutions up to 4096x4096.

Conclusion: TiM provides a novel generative paradigm that bridges the gap between efficient few-step generation and high-quality iterative refinement, offering flexible and scalable high-fidelity generation.

Abstract: A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.

</details>


### [96] [IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation](https://arxiv.org/abs/2509.04398)
*Yuan Yin,Shashanka Venkataramanan,Tuan-Hung Vu,Andrei Bursuc,Matthieu Cord*

Main category: cs.LG

TL;DR: IPA improves LoRA by replacing random down-projection with feature-aware compression that preserves key information, achieving better performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: LoRA's random initialization of down-projection discards useful information and becomes a bottleneck, as it changes little during training while up-projection does most adaptation work.

Method: IPA uses feature-aware projection framework that preserves information in reduced hidden space, approximating top principal components for efficient projector pretraining with minimal inference overhead.

Result: IPA consistently outperforms LoRA and DoRA, achieving 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, matching full LoRA performance with half the trainable parameters when projection is frozen.

Conclusion: Feature-aware projection significantly improves parameter-efficient fine-tuning by addressing the information loss bottleneck in LoRA's random initialization, enabling better performance with reduced parameter count.

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce
adaptation cost by injecting low-rank updates into pretrained weights. However,
LoRA's down-projection is randomly initialized and data-agnostic, discarding
potentially useful information. Prior analyses show that this projection
changes little during training, while the up-projection carries most of the
adaptation, making the random input compression a performance bottleneck. We
propose IPA, a feature-aware projection framework that explicitly preserves
information in the reduced hidden space. In the linear case, we instantiate IPA
with algorithms approximating top principal components, enabling efficient
projector pretraining with negligible inference overhead. Across language and
vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on
average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on
VTAB-1k, while matching full LoRA performance with roughly half the trainable
parameters when the projection is frozen.

</details>


### [97] [Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data](https://arxiv.org/abs/2509.04415)
*Wenrui Li,Qinghao Zhang,Xiaowo Wang*

Main category: cs.LG

TL;DR: HCL is an unsupervised framework that jointly infers latent clusters and their causal structures from mixed-type observational data without requiring prior knowledge like temporal ordering or interventions.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack causal awareness and struggle with modeling heterogeneity, confounding, and observational constraints, leading to poor interpretability and difficulty distinguishing true causal heterogeneity from spurious associations.

Method: HCL introduces an equivalent representation encoding structural heterogeneity and confounding, uses a bi-directional iterative strategy to refine causal clustering and structure learning, and employs self-supervised regularization to balance cross-cluster universality and specificity.

Result: Theoretically, HCL shows identifiability of heterogeneous causal structures under mild conditions. Empirically, it achieves superior performance in clustering and structure learning tasks, and recovers biologically meaningful mechanisms in real-world single-cell perturbation data.

Conclusion: HCL enables convergence toward interpretable, heterogeneous causal patterns and demonstrates utility for discovering interpretable, mechanism-level causal heterogeneity in scientific domains like biology and medicine.

Abstract: Understanding causal heterogeneity is essential for scientific discovery in
domains such as biology and medicine. However, existing methods lack causal
awareness, with insufficient modeling of heterogeneity, confounding, and
observational constraints, leading to poor interpretability and difficulty
distinguishing true causal heterogeneity from spurious associations. We propose
an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering
with Adaptive Heterogeneous Causal Structure Learning), that jointly infers
latent clusters and their associated causal structures from mixed-type
observational data without requiring temporal ordering, environment labels,
interventions or other prior knowledge. HCL relaxes the homogeneity and
sufficiency assumptions by introducing an equivalent representation that
encodes both structural heterogeneity and confounding. It further develops a
bi-directional iterative strategy to alternately refine causal clustering and
structure learning, along with a self-supervised regularization that balance
cross-cluster universality and specificity. Together, these components enable
convergence toward interpretable, heterogeneous causal patterns. Theoretically,
we show identifiability of heterogeneous causal structures under mild
conditions. Empirically, HCL achieves superior performance in both clustering
and structure learning tasks, and recovers biologically meaningful mechanisms
in real-world single-cell perturbation data, demonstrating its utility for
discovering interpretable, mechanism-level causal heterogeneity.

</details>


### [98] [Towards a Unified View of Large Language Model Post-Training](https://arxiv.org/abs/2509.04419)
*Xingtai Lv,Yuxin Zuo,Youbang Sun,Hongyi Liu,Yuntian Wei,Zhekai Chen,Lixuan He,Xuekai Zhu,Kaiyan Zhang,Bingning Wang,Ning Ding,Bowen Zhou*

Main category: cs.LG

TL;DR: The paper presents a unified theoretical framework showing that reinforcement learning (RL) and supervised fine-tuning (SFT) are instances of a single optimization process, and introduces Hybrid Post-Training (HPT) that dynamically selects training signals.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between online (model-generated) and offline (human demonstrations) training data approaches, showing they are not contradictory but part of a unified optimization framework.

Method: Derived a Unified Policy Gradient Estimator with four interchangeable parts, then proposed Hybrid Post-Training (HPT) algorithm that dynamically selects different training signals for effective exploitation and stable exploration.

Result: HPT consistently surpassed strong baselines across six mathematical reasoning benchmarks and two out-of-distribution suites, working effectively across models of varying scales and families.

Conclusion: The paper provides a unified theoretical framework for post-training approaches and demonstrates the effectiveness of HPT in combining the strengths of both online and offline training data methods.

Abstract: Two major sources of training data exist for post-training modern language
models: online (model-generated rollouts) data, and offline (human or
other-model demonstrations) data. These two types of data are typically used by
approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),
respectively. In this paper, we show that these approaches are not in
contradiction, but are instances of a single optimization process. We derive a
Unified Policy Gradient Estimator, and present the calculations of a wide
spectrum of post-training approaches as the gradient of a common objective
under different data distribution assumptions and various bias-variance
tradeoffs. The gradient estimator is constructed with four interchangeable
parts: stabilization mask, reference policy denominator, advantage estimate,
and likelihood gradient. Motivated by our theoretical findings, we propose
Hybrid Post-Training (HPT), an algorithm that dynamically selects different
training signals. HPT is designed to yield both effective exploitation of
demonstration and stable exploration without sacrificing learned reasoning
patterns. We provide extensive experiments and ablation studies to verify the
effectiveness of our unified theoretical framework and HPT. Across six
mathematical reasoning benchmarks and two out-of-distribution suites, HPT
consistently surpasses strong baselines across models of varying scales and
families.

</details>


### [99] [Delta Activations: A Representation for Finetuned Large Language Models](https://arxiv.org/abs/2509.04442)
*Zhiqiu Xu,Amish Sethi,Mayur Naik,Ser-Nam Lim*

Main category: cs.LG

TL;DR: Delta Activations is a method to represent finetuned LLMs as vector embeddings by measuring activation shifts from base models, enabling effective clustering and analysis of model relationships.


<details>
  <summary>Details</summary>
Motivation: The proliferation of open-source finetuned LLMs has created challenges in navigating and understanding the model landscape due to inconsistent metadata and unstructured repositories.

Method: Measure shifts in internal activations relative to a base model to create vector embeddings for finetuned models, enabling clustering by domain and task.

Result: Delta Activations show robustness across finetuning settings, exhibit additive properties when datasets are mixed, and can embed tasks via few-shot finetuning for model selection and merging.

Conclusion: Delta Activations provide an effective way to structure and understand the landscape of finetuned models, facilitating better model reuse and selection practices in the open-source LLM ecosystem.

Abstract: The success of powerful open source Large Language Models (LLMs) has enabled
the community to create a vast collection of post-trained models adapted to
specific tasks and domains. However, navigating and understanding these models
remains challenging due to inconsistent metadata and unstructured repositories.
We introduce Delta Activations, a method to represent finetuned models as
vector embeddings by measuring shifts in their internal activations relative to
a base model. This representation allows for effective clustering by domain and
task, revealing structure in the model landscape. Delta Activations also
demonstrate desirable properties: it is robust across finetuning settings and
exhibits an additive property when finetuning datasets are mixed. In addition,
we show that Delta Activations can embed tasks via few-shot finetuning, and
further explore its use for model selection and merging. We hope Delta
Activations can facilitate the practice of reusing publicly available models.
Code is available at https://github.com/OscarXZQ/delta_activations.

</details>


### [100] [Echo State Networks as State-Space Models: A Systems Perspective](https://arxiv.org/abs/2509.04422)
*Pradeep Singh,Balasubramanian Raman*

Main category: cs.LG

TL;DR: This paper reframes Echo State Networks (ESNs) as state-space models, providing theoretical foundations for reservoir computing and developing new analysis and training methods.


<details>
  <summary>Details</summary>
Motivation: To establish first principles for ESN design rather than relying on heuristics, and to create a unified theoretical framework connecting reservoir computing with classical system identification and modern kernelized state-space models.

Method: Recast ESNs as state-space models, develop verifiable stability conditions, create small-signal linearizations and Koopman expansions for analysis, and propose Kalman/EKF-assisted readout learning with EM for hyperparameter optimization.

Result: Provides theoretical foundations for ESN stability, enables frequency-domain analysis of memory spectra, and develops new training methods that improve interpretability and performance.

Conclusion: The state-space perspective offers a principled framework for ESN design and analysis, bridging reservoir computing with classical control theory and enabling more systematic development of efficient recurrent models.

Abstract: Echo State Networks (ESNs) are typically presented as efficient,
readout-trained recurrent models, yet their dynamics and design are often
guided by heuristics rather than first principles. We recast ESNs explicitly as
state-space models (SSMs), providing a unified systems-theoretic account that
links reservoir computing with classical identification and modern kernelized
SSMs. First, we show that the echo-state property is an instance of
input-to-state stability for a contractive nonlinear SSM and derive verifiable
conditions in terms of leak, spectral scaling, and activation Lipschitz
constants. Second, we develop two complementary mappings: (i) small-signal
linearizations that yield locally valid LTI SSMs with interpretable poles and
memory horizons; and (ii) lifted/Koopman random-feature expansions that render
the ESN a linear SSM in an augmented state, enabling transfer-function and
convolutional-kernel analyses. This perspective yields frequency-domain
characterizations of memory spectra and clarifies when ESNs emulate structured
SSM kernels. Third, we cast teacher forcing as state estimation and propose
Kalman/EKF-assisted readout learning, together with EM for hyperparameters
(leak, spectral radius, process/measurement noise) and a hybrid subspace
procedure for spectral shaping under contraction constraints.

</details>


### [101] [ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset](https://arxiv.org/abs/2509.04449)
*Adrian Catalin Lutu,Ioana Pintilie,Elena Burceanu,Andrei Manolache*

Main category: cs.LG

TL;DR: ChronoGraph is a new dataset for multivariate time series forecasting with explicit service dependency graphs and real incident annotations from production microservices.


<details>
  <summary>Details</summary>
Motivation: To provide a realistic benchmark that combines multivariate time series, machine-readable dependency graphs, and real anomaly labels for studying structure-aware forecasting and incident-aware evaluation in microservice systems.

Method: Built from real-world production microservices where each node represents a service emitting performance metrics (CPU, memory, network) and directed edges encode service dependencies, with expert-annotated incident windows.

Result: Provides baseline results for forecasting models, pretrained time-series foundation models, and standard anomaly detectors on this novel dataset.

Conclusion: ChronoGraph offers a unique benchmark that bridges the gap between existing industrial/traffic datasets and real microservice environments, enabling more realistic evaluation of forecasting and anomaly detection methods.

Abstract: We present ChronoGraph, a graph-structured multivariate time series
forecasting dataset built from real-world production microservices. Each node
is a service that emits a multivariate stream of system-level performance
metrics, capturing CPU, memory, and network usage patterns, while directed
edges encode dependencies between services. The primary task is forecasting
future values of these signals at the service level. In addition, ChronoGraph
provides expert-annotated incident windows as anomaly labels, enabling
evaluation of anomaly detection methods and assessment of forecast robustness
during operational disruptions. Compared to existing benchmarks from industrial
control systems or traffic and air-quality domains, ChronoGraph uniquely
combines (i) multivariate time series, (ii) an explicit, machine-readable
dependency graph, and (iii) anomaly labels aligned with real incidents. We
report baseline results spanning forecasting models, pretrained time-series
foundation models, and standard anomaly detectors. ChronoGraph offers a
realistic benchmark for studying structure-aware forecasting and incident-aware
evaluation in microservice systems.

</details>


### [102] [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](https://arxiv.org/abs/2509.04430)
*Nikolay Kartashev,Ivan Rubachev,Artem Babenko*

Main category: cs.LG

TL;DR: This paper explains why recent tabular deep learning methods work by showing they implicitly manage data uncertainty through techniques like numerical embeddings, retrieval augmentation, and advanced ensembling.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding why tabular deep learning techniques succeed despite lacking clear theoretical foundations.

Method: Analyzes how beneficial design choices in tabular DL implicitly handle data uncertainty, and develops improved numerical feature embeddings based on these insights.

Result: Provides a unifying explanation for recent performance improvements and develops more effective numerical feature embeddings as a practical outcome.

Conclusion: The work establishes a foundational understanding of modern tabular methods, enables concrete advancements in existing techniques, and outlines future research directions for tabular DL.

Abstract: Recent advancements in tabular deep learning have demonstrated exceptional
practical performance, yet the field often lacks a clear understanding of why
these techniques actually succeed. To address this gap, our paper highlights
the importance of the concept of data uncertainty for explaining the
effectiveness of the recent tabular DL methods. In particular, we reveal that
the success of many beneficial design choices in tabular DL, such as numerical
feature embeddings, retrieval-augmented models and advanced ensembling
strategies, can be largely attributed to their implicit mechanisms for managing
high data uncertainty. By dissecting these mechanisms, we provide a unifying
understanding of the recent performance improvements. Furthermore, the insights
derived from this data-uncertainty perspective directly allowed us to develop
more effective numerical feature embeddings as an immediate practical outcome
of our analysis. Overall, our work paves the way to foundational understanding
of the benefits introduced by modern tabular methods that results in the
concrete advancements of existing techniques and outlines future research
directions for tabular DL.

</details>


### [103] [Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment](https://arxiv.org/abs/2509.04445)
*Cyrus Cousins,Vijay Keswani,Vincent Conitzer,Hoda Heidari,Jana Schaich Borg,Walter Sinnott-Armstrong*

Main category: cs.LG

TL;DR: The paper proposes an axiomatic approach to learn cognitively faithful decision models from pairwise comparisons that better capture human heuristic processes, addressing limitations of standard preference elicitation methods.


<details>
  <summary>Details</summary>
Motivation: Standard preference elicitation methods fail to capture true human cognitive processes like heuristics, resulting in models that don't align with human decision-making and lack generalization capabilities.

Method: An axiomatic approach defining models where individual features are processed and compared across alternatives, then aggregated via fixed rules like Bradley-Terry, ensuring realistic representation of human decision processes.

Result: The proposed models demonstrated efficacy in learning interpretable models of human decision making in kidney allocation tasks, matching or surpassing prior models' accuracy.

Conclusion: The structured processing approach provides more cognitively faithful models that better represent underlying human decision-making processes compared to standard methods.

Abstract: Recent AI work trends towards incorporating human-centric objectives, with
the explicit goal of aligning AI models to personal preferences and societal
values. Using standard preference elicitation methods, researchers and
practitioners build models of human decisions and judgments, which are then
used to align AI behavior with that of humans. However, models commonly used in
such elicitation processes often do not capture the true cognitive processes of
human decision making, such as when people use heuristics to simplify
information associated with a decision problem. As a result, models learned
from people's decisions often do not align with their cognitive processes, and
can not be used to validate the learning framework for generalization to other
decision-making tasks. To address this limitation, we take an axiomatic
approach to learning cognitively faithful decision processes from pairwise
comparisons. Building on the vast literature characterizing the cognitive
processes that contribute to human decision-making, and recent work
characterizing such processes in pairwise comparison tasks, we define a class
of models in which individual features are first processed and compared across
alternatives, and then the processed features are then aggregated via a fixed
rule, such as the Bradley-Terry rule. This structured processing of information
ensures such models are realistic and feasible candidates to represent
underlying human decision-making processes. We demonstrate the efficacy of this
modeling approach in learning interpretable models of human decision making in
a kidney allocation task, and show that our proposed models match or surpass
the accuracy of prior models of human pairwise decision-making.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [104] [SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India](https://arxiv.org/abs/2509.03793)
*Prathamesh Devadiga,Omkaar Jayadev Shetty,Pooja Agarwal*

Main category: cs.MA

TL;DR: SAMVAD is a Multi-Agent System using LLMs to simulate Indian judicial deliberation with RAG for legal grounding.


<details>
  <summary>Details</summary>
Motivation: Empirical studies of judicial panels face ethical and practical barriers, requiring simulation tools to understand deliberation processes.

Method: Multi-agent system with Judge, Prosecution, Defense Counsel, and Adjudicator agents powered by LLMs, integrated with RAG using Indian legal documents (IPC, Constitution).

Result: Developed a configurable MAS platform that enables legally sound instructions/arguments with citations and iterative deliberation rounds for consensus verdicts.

Conclusion: Provides an explainable simulation platform for exploring legal reasoning and group decision-making in Indian judicial context with verifiable legal grounding.

Abstract: Understanding the complexities of judicial deliberation is crucial for
assessing the efficacy and fairness of a justice system. However, empirical
studies of judicial panels are constrained by significant ethical and practical
barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS)
designed to simulate the deliberation process within the framework of the
Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a
Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a
judicial bench), all powered by large language models (LLMs). A primary
contribution of this work is the integration of Retrieval-Augmented Generation
(RAG), grounded in a domain-specific knowledge base of landmark Indian legal
documents, including the Indian Penal Code and the Constitution of India. This
RAG functionality enables the Judge and Counsel agents to generate legally
sound instructions and arguments, complete with source citations, thereby
enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing
case facts, legal instructions, and arguments to reach a consensus-based
verdict. We detail the system architecture, agent communication protocols, the
RAG pipeline, the simulation workflow, and a comprehensive evaluation plan
designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring
legal reasoning and group decision-making dynamics in judicial simulations,
specifically tailored to the Indian legal context and augmented with verifiable
legal grounding via RAG.

</details>
