{"id": "2601.17454", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17454", "abs": "https://arxiv.org/abs/2601.17454", "authors": ["Muhammad Ahmed Atif", "Nehal Naeem Haji", "Mohammad Shahid Shaikh", "Muhammad Ebad Atif"], "title": "Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning", "comment": null, "summary": "Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.", "AI": {"tldr": "Centralized learning often underperforms independent learning in multi-agent coordination tasks with embodiment constraints, showing coordination benefits are not universal but dependent on specific regimes and roles.", "motivation": "To test the common assumption that centralized value learning improves coordination and stability in multi-agent reinforcement learning under controlled conditions, particularly examining whether this advantage holds with embodiment constraints like speed and stamina limitations.", "method": "Direct comparison of independent vs centralized Q-learning in a fully tabular predator-prey gridworld with explicit embodiment constraints on agent speed and stamina. The study evaluates multiple kinematic regimes and asymmetric agent roles, eliminating confounding effects from function approximation and representation learning.", "result": "Centralized learning fails to provide consistent advantages and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient instability.", "conclusion": "Increased coordination can become a liability under embodiment constraints, and the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal, challenging conventional assumptions in multi-agent RL."}}
{"id": "2601.18284", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18284", "abs": "https://arxiv.org/abs/2601.18284", "authors": ["Hsiao-Chuan Chang", "Sheng-You Huang", "Yen-Chi Chen", "I-Chen Wu"], "title": "VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim", "comment": null, "summary": "Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control.", "AI": {"tldr": "VissimRL is a modular RL framework for traffic signal control that provides a Python API for Vissim's complex COM interface, enabling easier RL research with high-fidelity traffic simulation.", "motivation": "Vissim offers high-fidelity driver behavior modeling and industrial adoption but is underutilized in RL research due to its complex interface and lack of standardized frameworks.", "method": "The paper proposes VissimRL, a modular RL framework that encapsulates Vissim's COM interface through a high-level Python API, providing standardized environments for both single- and multi-agent training.", "result": "VissimRL significantly reduces development effort while maintaining runtime efficiency, supports consistent improvements in traffic performance during training, and enables emergent coordination in multi-agent control.", "conclusion": "VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control."}}
{"id": "2601.17065", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17065", "abs": "https://arxiv.org/abs/2601.17065", "authors": ["Haoxuan Li", "He Chang", "Yunshan Ma", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting", "comment": null, "summary": "Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.", "AI": {"tldr": "ThinkTank-ME is a multi-expert framework designed for Middle East event forecasting, overcoming limitations of single-model approaches by capturing geopolitical nuances through expert collaboration.", "motivation": "Existing LLM-based event forecasting methods use single-model architectures that generate predictions along a singular explicit trajectory, failing to capture diverse geopolitical nuances in complex regions like the Middle East.", "method": "Introduced ThinkTank-ME, a Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis by facilitating multi-expert specialization, and constructed POLECAT-FOR-ME, a benchmark for rigorous evaluation.", "result": "Experimental results show superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks.", "conclusion": "ThinkTank-ME effectively addresses the limitations of prior approaches by leveraging expert collaboration for improved Middle East event forecasting."}}
{"id": "2601.17133", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17133", "abs": "https://arxiv.org/abs/2601.17133", "authors": ["Inderjeet Singh", "Eleonore Vissol-Gaudin", "Andikan Otung", "Motoyoshi Sekiya"], "title": "Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation", "comment": "Accepted to AAAI 2026. 13 pages, 3 figures, 10 tables. Code available at: https://github.com/FujitsuResearch/knexa-fl", "summary": "Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.", "AI": {"tldr": "KNEXA-FL introduces a decentralized federated learning framework that uses a central matchmaker to orchestrate optimal peer-to-peer knowledge exchange between LLMs without aggregating models or accessing raw data.", "motivation": "There's a fundamental conflict between the need for diverse cross-organizational data for LLM fine-tuning and data privacy/sovereignty concerns. While federated learning addresses privacy, centralized FL has single points of failure and vulnerability to attacks, and decentralized FL with random P2P pairings suffers from inefficiency and negative transfer due to agent heterogeneity.", "method": "KNEXA-FL uses a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem. It employs LinUCB algorithm on abstract agent profiles to learn optimal matchmaking policies, orchestrating direct knowledge exchange between heterogeneous PEFT-based LLM agents via secure distillation without accessing the models themselves.", "result": "On a challenging code generation task, KNEXA-FL yields substantial gains, improving Pass@1 by approximately 50% relative to random P2P collaboration. The orchestrated approach demonstrates stable convergence, unlike a powerful centralized distillation baseline that suffers from catastrophic performance collapse.", "conclusion": "Adaptive, learning-based orchestration is established as a foundational principle for building robust and effective decentralized AI ecosystems, resolving the trade-off between privacy preservation and efficient knowledge transfer in LLM fine-tuning."}}
{"id": "2601.17009", "categories": ["cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17009", "abs": "https://arxiv.org/abs/2601.17009", "authors": ["Yanhua Zhao"], "title": "Online parameter estimation for the Crazyflie quadcopter through an EM algorithm", "comment": "20 pages, 37 figures", "summary": "Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.", "AI": {"tldr": "The paper studies the effects of random noise on a quadcopter system, using an extended Kalman filter for state estimation, a linear quadratic Gaussian controller, and the expectation maximization algorithm for parameter estimation, comparing offline and online methods.", "motivation": "Drones are increasingly used in applications like search and rescue, photography, agriculture, and transportation, but their performance can be affected by noise, especially in critical scenarios like earthquake response where reliability is vital. This paper aims to analyze and mitigate noise impacts on quadcopters.", "method": "The method involves modeling the quadcopter with a stochastic differential equation (SDE) system, adding random noise, using an extended Kalman filter to estimate states from noisy sensor data, applying a linear quadratic Gaussian controller for optimal control, and using the expectation maximization algorithm for parameter estimation, with both offline and online estimation approaches.", "result": "The results indicate that online parameter estimation yields a slightly larger range of convergence values compared to offline parameter estimation, suggesting differences in estimation accuracy or robustness under noise conditions.", "conclusion": "The study demonstrates that noise significantly affects quadcopter systems, and while both offline and online parameter estimation methods are effective, online estimation may offer broader convergence, which could enhance drone performance in noisy environments for applications like disaster response."}}
{"id": "2601.16984", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.16984", "abs": "https://arxiv.org/abs/2601.16984", "authors": ["Rahul Ghosh", "Chun-Hao Liu", "Gaurav Rele", "Vidya Sagar Ravipati", "Hazar Aouad"], "title": "TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\\%$ recall, $83\\%$ claim recall, and $92\\%$ faithfulness, representing a $16\\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.", "AI": {"tldr": "TelcoAI is an agentic, multi-modal RAG system designed to enhance understanding of complex 3GPP technical specifications by improving query handling, visual integration, and document analysis through advanced chunking and retrieval methods.", "motivation": "Current LLM-based approaches are insufficient for processing 3GPP specifications due to challenges with complex queries, visual content (e.g., diagrams), and document interdependencies, limiting their practical utility in telecommunications.", "method": "Introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams in the TelcoAI system, a tailored RAG framework for 3GPP documentation.", "result": "Achieves 87% recall, 83% claim recall, and 92% faithfulness in benchmarks, outperforming state-of-the-art baselines by 16% improvement, as validated on expert-curated queries and other benchmarks.", "conclusion": "TelcoAI demonstrates that agentic and multi-modal reasoning significantly advance technical document understanding, offering practical benefits for telecommunications research and engineering applications."}}
{"id": "2601.17168", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17168", "abs": "https://arxiv.org/abs/2601.17168", "authors": ["Judy Zhu", "Dhari Gandhi", "Himanshu Joshi", "Ahmad Rezaie Mianroodi", "Sedef Akinli Kocak", "Dhanesh Ramachandran"], "title": "Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability", "comment": null, "summary": "Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.", "AI": {"tldr": "Existing interpretability methods for static models are inadequate for agentic AI systems, requiring new approaches to embed interpretability across the agent lifecycle for safety and accountability.", "motivation": "Agentic systems, leveraging LLMs for autonomous goal-directed behavior, introduce unique AI safety challenges like goal misalignment and compounding errors, but current interpretability techniques designed for static models fail to address their dynamic and interactive nature.", "method": "This paper analyzes the limitations of existing interpretability methods when applied to agentic systems, identifying gaps in insight into agent decision-making, and proposes future directions for tailored interpretability techniques.", "result": "The assessment reveals that agentic systems' temporal dynamics, compounding decisions, and context-dependent behaviors demand new analytical approaches to provide traceability and accountability across goal formation, environmental interaction, and outcome evaluation.", "conclusion": "Developing specialized interpretability techniques is essential for embedding oversight mechanisms in agentic AI systems to ensure their safe and accountable deployment."}}
{"id": "2601.16991", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16991", "abs": "https://arxiv.org/abs/2601.16991", "authors": ["Longteng Zhang", "Sen Wu", "Shuai Hou", "Zhengyu Qing", "Zhuo Zheng", "Danning Ke", "Qihong Lin", "Qiang Wang", "Shaohuai Shi", "Xiaowen Chu"], "title": "Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models", "comment": null, "summary": "Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\\times$, and delivers up to a $1.7\\times$ inference speedup.", "AI": {"tldr": "SALR combines sparse pruning with low-rank adaptation to reduce model size and speed up inference while maintaining performance comparable to LoRA.", "motivation": "Fine-tuning large language models requires substantial resources (storage and computation), and existing methods like LoRA still rely on dense weights, while naive pruning degrades LoRA's performance.", "method": "Proposes SALR, which unifies low-rank adaptation with sparse pruning under a mean-squared-error framework. It statically prunes only the frozen base weights to minimize pruning error, recovers residual information via a truncated-SVD low-rank adapter, fuses multiple adapters into a single GEMM, and uses bitmap-based encoding with two-stage pipelined decoding for efficiency.", "result": "Achieves 50% sparsity on various LLMs, matches LoRA performance on GSM8K and MMLU benchmarks, reduces model size by 2\u00d7, and delivers up to 1.7\u00d7 inference speedup.", "conclusion": "SALR provides an effective fine-tuning paradigm that balances compression, speed, and performance, making large language models more feasible for resource-constrained environments."}}
{"id": "2601.18631", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18631", "abs": "https://arxiv.org/abs/2601.18631", "authors": ["Mingyang Song", "Haoyu Sun", "Jiawei Gu", "Linjie Li", "Luxin Xu", "Ranjay Krishna", "Yu Cheng"], "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "comment": "28 pages, 10 figures and 13 tables", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "AI": {"tldr": "AdaReasoner is a family of multimodal models that learns tool use as a general reasoning skill, achieving state-of-the-art performance by autonomously selecting, sequencing, and adapting tools based on task context.", "motivation": "Humans use tools to solve problems beyond their immediate capabilities, providing a promising paradigm for improving visual reasoning in MLLMs. Effective reasoning requires knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even with new tools or tasks.", "method": "1) Scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; 2) Tool-GRPO reinforcement learning algorithm optimizing tool selection and sequencing based on end-task success; 3) Adaptive learning mechanism dynamically regulating tool usage.", "result": "AdaReasoner exhibits strong tool-adaptive and generalization behaviors: autonomously adopts beneficial tools, suppresses irrelevant ones, adjusts tool usage frequency based on task demands. Achieves state-of-the-art performance across benchmarks, improving 7B base model by +24.9% on average, surpassing proprietary systems like GPT-5 on multiple tasks including VSP and Jigsaw.", "conclusion": "AdaReasoner demonstrates that learning tool use as a general reasoning skill enables multimodal models to effectively coordinate multiple tools and generalize to unseen tools, leading to significant performance improvements in visual reasoning tasks."}}
{"id": "2601.17188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17188", "abs": "https://arxiv.org/abs/2601.17188", "authors": ["Swapn Shah", "Wlodek Zadrozny"], "title": "Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction", "comment": null, "summary": "The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2601.16994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16994", "abs": "https://arxiv.org/abs/2601.16994", "authors": ["Lucas M. Morello", "Matheus Lima Castro", "Pedro Cesar M. G. Camargo", "Liliane Moreira Nery", "Darllan Collins da Cunha e Silva", "Leopoldo Lusquino Filho"], "title": "A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts", "comment": null, "summary": "This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.", "AI": {"tldr": "This paper presents a harmonized weekly-resolution dengue hospitalization dataset for Brazil (1999-2021), created by disaggregating monthly data using cubic spline interpolation, validated against reference data, and accompanied by comprehensive explanatory variables for epidemiological forecasting.", "motivation": "To address the need for higher temporal granularity in epidemiological data for more effective AI model training in disease forecasting, particularly by transforming originally monthly dengue hospitalization data to weekly resolution to enable better pattern recognition and prediction capabilities.", "method": "The methodology involved: 1) Harmonizing municipal-level dengue hospitalization time series across Brazil, 2) Disaggregating monthly data to weekly resolution using interpolation with correction to preserve monthly totals, 3) Comparing three disaggregation strategies (linear interpolation, jittering, cubic spline) using S\u00e3o Paulo reference data with both monthly and weekly counts, 4) Applying cubic spline interpolation to generate weekly series for 1999-2021, and 5) Including comprehensive explanatory variables with consistent temporal disaggregation.", "result": "Cubic spline interpolation achieved the highest adherence to reference data among the three tested strategies, and was adopted to generate harmonized weekly dengue hospitalization time series for Brazilian municipalities from 1999-2021. The dataset includes comprehensive explanatory variables and quality metrics documentation.", "conclusion": "The study successfully created a harmonized weekly-resolution dengue hospitalization dataset for Brazil, demonstrating that cubic spline interpolation preserves monthly totals while achieving high adherence to reference data. The comprehensive dataset with consistent explanatory variables enables improved multivariate time-series analysis and AI model development for epidemiological forecasting."}}
{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "Researchers developed a generative AI model that uses real-world clinical records to simulate realistic future patient trajectories, enabling personalized treatment planning and virtual clinical trials.", "motivation": "Simulation has transformative potential in clinical medicine for personalized treatment and virtual trials, but patient trajectory simulation is challenging due to complex biological and sociocultural factors.", "method": "Developed a generative simulator model that takes patient history as input and synthesizes realistic future trajectories, pretrained on over 200 million clinical records.", "result": "The model produced high-fidelity future timelines matching real data in event rates, lab results, and temporal dynamics, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons.", "conclusion": "The study reveals the untapped value of real-world EHR data and introduces a scalable framework for in silico modeling of clinical care."}}
{"id": "2601.17006", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17006", "abs": "https://arxiv.org/abs/2601.17006", "authors": ["Xuchen Li", "Jing Chen", "Xuzhao Li", "Hao Liang", "Xiaohuan Zhou", "Taifeng Wang", "Wentao Zhang"], "title": "MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning", "comment": "Preprint, Under review", "summary": "In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.", "AI": {"tldr": "MathMixup is a novel data synthesis method that generates high-quality, difficulty-controllable math problems to improve LLM training via curriculum learning, achieving state-of-the-art results on mathematical reasoning benchmarks.", "motivation": "Existing methods for synthesizing training data for mathematical reasoning with LLMs lack diversity and precise control over problem difficulty, hindering efficient curriculum learning.", "method": "The MathMixup paradigm hybridizes and decomposes strategies to generate problems, uses automated self-checking and manual screening for quality, and constructs the MathMixupQA dataset with a designed curriculum learning strategy.", "result": "Fine-tuned Qwen2.5-7B model achieves an average score of 52.6% across seven mathematical benchmarks, surpassing previous state-of-the-art methods.", "conclusion": "MathMixup effectively enhances LLM mathematical reasoning performance and advances data-centric curriculum learning, demonstrating broad applicability."}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "A theoretical framework for multi-agent systems predicts performance regimes (help, saturate, collapse) based on context windows, lossy communication, and shared failures, with phase transitions determined by a single scalar parameter.", "motivation": "Multi-agent systems under fixed inference budgets show inconsistent performance (help, saturate, or collapse), but existing theories don't explain these regimes based on fundamental constraints of modern agent stacks like finite context windows, lossy communication, and shared failures.", "method": "Develop a minimal theory with calibratable parameters: compute-performance scaling exponent \u03b2, message-length fidelity curve \u03b3(m), shared-error correlation \u03c1, and context window W. Analyze binary success/failure tasks with majority aggregation in deep b-ary trees with correlated inputs and lossy communication.", "result": "Prove sharp phase transition determined by scalar \u03b1\u03c1 combining \u03b3(m), \u03c1, and fan-in b. Derive organization exponent s; budgeted synergy occurs when s>\u03b2, yielding compute allocation rules and budget thresholds. Characterize saturation via mixing depth and provide clipped predictor. Validate predictions in synthetic simulations and real LLM agent-system scaling studies.", "conclusion": "The theory successfully explains multi-agent performance regimes through fundamental constraints, provides design principles for compute allocation, and aligns with empirical observations from large-scale LLM agent studies, offering a unified framework for understanding and optimizing agent organizations."}}
{"id": "2601.17007", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17007", "abs": "https://arxiv.org/abs/2601.17007", "authors": ["Beatriz P\u00e9rez-S\u00e1nchez", "Noelia S\u00e1nchez-Maro\u00f1o", "Miguel A. D\u00edaz-Freire"], "title": "Analysis of voice recordings features for Classification of Parkinson's Disease", "comment": null, "summary": "Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.\n  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.", "AI": {"tldr": "This paper proposes combining machine learning models with feature selection to diagnose Parkinson's disease using voice recordings, showing neural networks perform well and fewer features can suffice.", "motivation": "Parkinson's disease is chronic and hard to diagnose early due to mild initial symptoms; while voice recordings can help, clinical analysis is costly, and not all features may be relevant.", "method": "The paper uses various machine learning models combined with feature selection techniques to identify the most informative features from vocal recordings for classification.", "result": "The study finds that machine learning methods, especially neural networks, are effective for PD classification, and the number of features can be significantly reduced without harming model performance.", "conclusion": "Feature selection enhances the accuracy and efficiency of machine learning models for early PD diagnosis using voice data, highlighting neural networks' suitability and potential for cost-effective clinical application."}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "AI": {"tldr": "TheoremForge reduces formal math's high agentic workflow costs by decomposing synthesis, recovering signals from failures, achieving a 12.6% Verified Rate at $0.481 per success.", "motivation": "High implementation expenses hinder large-scale formal mathematics data synthesis, leading to scarce open-source corpora and training difficulties.", "method": "Cost-effective pipeline divides process into five sub-tasks: statement formalization, proof generation, premise selection, proof correction, proof sketching, with Decoupled Extraction Strategy to salvage signals from failed trajectories.", "result": "On 2,000-problem benchmark, TheoremForge attains 12.6% Verified Rate vs 8.6% baseline, at average cost of $0.481 per successful trajectory with Gemini-3-Flash, boosting proof generation data yield by 1.6x over standard filtering.", "conclusion": "TheoremForge serves as a scalable framework for efficient data synthesis in formal math, enabling enhanced corpora for expert model training."}}
{"id": "2601.17008", "categories": ["cs.LG", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.17008", "abs": "https://arxiv.org/abs/2601.17008", "authors": ["Haochong Xia", "Simin Li", "Ruixiao Xu", "Zhixia Zhang", "Hongxiang Wang", "Zhiqian Liu", "Teng Yao Long", "Molei Qin", "Chuqiao Zong", "Bo An"], "title": "Bayesian Robust Financial Trading with Adversarial Synthetic Market Data", "comment": null, "summary": "Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.", "AI": {"tldr": "Proposes a Bayesian Robust Framework combining macro-conditioned GAN-based data generation and robust policy learning for algorithmic trading to handle evolving market regimes, outperforming baselines.", "motivation": "Machine learning models in algorithmic trading degrade in real-world changing markets due to insufficient robustness to uncertainties and lack of realistic simulation training, causing overfitting.", "method": "Introduces a Bayesian Robust Framework with two components: (1) a macro-conditioned GAN generator using economic indicators to synthesize realistic data; (2) a two-player zero-sum Bayesian Markov game where an adversarial agent perturbs indicators and trading agent, using a quantile belief network, seeks Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play.", "result": "Extensive tests on 9 financial instruments show the framework outperforms 9 state-of-the-art baselines, with improved profitability and risk management in extreme events like COVID.", "conclusion": "The framework provides a reliable solution for trading under uncertain and shifting market dynamics, enhancing robustness and performance."}}
{"id": "2601.17335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17335", "abs": "https://arxiv.org/abs/2601.17335", "authors": ["Angshul Majumdar"], "title": "The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability", "comment": null, "summary": "We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and G\u00f6del--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.", "AI": {"tldr": "This paper investigates whether AGI has a coherent theoretical definition and shows that claims about its properties are highly sensitive to formal parameters, with limitations like lack of universal robustness, bounded generalizability, and impossibility of self-certification.", "motivation": "To determine if AGI can be defined theoretically to support absolute claims about existence, robustness, or self-verification, motivated by debates on its feasibility based on empirical AI progress.", "method": "Formalizes AGI axiomatically as a distributional, resource-bounded semantic predicate with parameters like task family and performance functional, then uses mathematical proofs and frameworks like Rice-style and G\u00f6del-Tarski arguments.", "result": "Derives four key results: 1) Generality is relational and distribution-dependent, 2) Non-invariance makes AGI not universally robust, 3) Bounded transfer limits generalization with finite resources, 4) AGI cannot be self-certified by any computable procedure.", "conclusion": "Strong, distribution-independent AGI claims are undefined without explicit formal indexing, and empirical AI progress does not guarantee the attainability of self-certifying general intelligence, making it an ill-posed search target in certain contexts."}}
{"id": "2601.17010", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17010", "abs": "https://arxiv.org/abs/2601.17010", "authors": ["Hudson Golino"], "title": "Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study", "comment": "18 pages, 6 figures, conference paper", "summary": "Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.", "AI": {"tldr": "LLM embeddings are non-uniform in semantic structure; optimal depth for psychological item analysis depends on metric choice and scales with item pool size.", "motivation": "Current applications treat LLM embeddings as static, cross-sectional, assuming uniform contributions across all coordinates, potentially missing optimal structural information concentrated in specific embedding regions.", "method": "Study reframes embeddings as searchable landscapes, adapting Dynamic Exploratory Graph Analysis (DynEGA) to traverse embedding coordinates using a pseudo-temporal ordering. A large-scale Monte Carlo simulation embedded items for grandiose narcissism dimensions using OpenAI's text-embedding-3-small, varying item pool sizes and embedding depths.", "result": "TEFI achieves minima at deep embedding depths (900-1,200 dimensions) with maximal entropy-based organization but degraded structural accuracy, while NMI peaks at shallow depths with strong dimensional recovery but suboptimal entropy fit. A weighted composite criterion balances accuracy and organization, with optimal depth scaling systematically with item pool size.", "conclusion": "Embedding landscapes are non-uniform semantic spaces requiring principled optimization (e.g., composite criteria) rather than default full-vector usage, as single-metric optimization leads to structurally incoherent solutions."}}
{"id": "2601.17343", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17343", "abs": "https://arxiv.org/abs/2601.17343", "authors": ["Wei Liu", "Haomei Xu", "Hongkai Liu", "Zhiying Deng", "Ruixuan Li", "Heng Huang", "Yee Whye Teh", "Wee Sun Lee"], "title": "Are We Evaluating the Edit Locality of LLM Model Editing Properly?", "comment": null, "summary": "Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.", "AI": {"tldr": "The paper identifies problems with current specificity evaluation in LLM editing and proposes a new protocol that produces more sensitive, correlated metrics for better assessing knowledge preservation.", "motivation": "Existing specificity evaluation protocols for model editing in LLMs are inadequate, with fundamental issues in design, weak correlation with specificity regularizers, and insufficient sensitivity to distinguish between different methods' knowledge preservation capabilities.", "method": "A constructive evaluation protocol is proposed that eliminates the conflict between open-ended LLMs and determined answers, avoids query-independent fluency biases, and allows smooth adjustment of evaluation strictness in a near-continuous space.", "result": "Experiments across various LLMs, datasets, and editing methods show that metrics from the proposed protocol are more sensitive to changes in specificity regularizer strength, exhibit strong correlation with them, and enable finer discrimination of methods' knowledge preservation capabilities.", "conclusion": "The proposed evaluation protocol addresses fundamental issues with existing specificity metrics and enables more sensitive, fine-grained assessment of knowledge preservation in model editing methods, better balancing efficacy and specificity."}}
{"id": "2601.17063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17063", "abs": "https://arxiv.org/abs/2601.17063", "authors": ["Byeongju Kim", "Jungwan Lee", "Donghyeon Han", "Hoi-Jun Yoo", "Sangyeob Kim"], "title": "FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices", "comment": null, "summary": "Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.", "AI": {"tldr": "FlashMoE is a system that offloads inactive experts to SSD instead of DRAM, enabling efficient Mixture-of-Experts inference on memory-constrained devices with a novel caching strategy.", "motivation": "Existing MoE inference systems like Fiddler or DAOP rely on DRAM-based offloading, which becomes impractical as MoE models grow to hundreds of GB, making them unsuitable for on-device environments with limited RAM.", "method": "Proposes FlashMoE, which offloads inactive experts to SSD and uses a lightweight ML-based caching strategy combining recency and frequency signals to maximize expert reuse and reduce storage I/O.", "result": "On a user-grade desktop platform, FlashMoE improves cache hit rate by up to 51% over LRU and LFU policies and achieves up to 2.6x speedup compared to existing MoE inference systems.", "conclusion": "FlashMoE addresses the limitations of DRAM-based offloading for large MoE models by leveraging SSD storage, making on-device MoE inference feasible and efficient through adaptive caching."}}
{"id": "2601.17346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17346", "abs": "https://arxiv.org/abs/2601.17346", "authors": ["Haoxin Xu", "Changyong Qi", "Tong Liu", "Bohao Zhang", "Anna He", "Bingqian Jiang", "Longwei Zheng", "Xiaoqing Gu"], "title": "Multi-Agent Learning Path Planning via LLMs", "comment": null, "summary": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.", "AI": {"tldr": "This paper proposes a Multi-Agent Learning Path Planning framework using LLMs for personalized education, outperforming baselines in path quality and cognitive alignment.", "motivation": "Current learning path planning in intelligent tutoring systems lacks transparency, adaptability, and learner-centered explainability, limiting effective personalized learning in higher education.", "method": "The study introduces a MALPP framework with three LLM-powered agents (learner analytics, path planning, reflection) that collaborate via structured prompts and rules, grounded in Cognitive Load Theory and Zone of Proximal Development, and tested on the MOOCCubeX dataset with seven LLMs.", "result": "Experiments show MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment, with ablation studies validating the collaboration and theoretical constraints.", "conclusion": "This research contributes to trustworthy, explainable AI in education and demonstrates a scalable, learner-centered adaptive instruction approach using LLMs, addressing key limitations in existing systems."}}
{"id": "2601.17348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17348", "abs": "https://arxiv.org/abs/2601.17348", "authors": ["Srikant Panda", "Sourabh Singh Yadav", "Palkesh Malviya"], "title": "Auditing Disability Representation in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.", "AI": {"tldr": "This paper studies how vision-language models (VLMs) generate biased or unfounded inferences when prompted with disability context in person-centric images, introducing a benchmark and framework to evaluate and mitigate these interpretation shifts.", "motivation": "VLMs are used in socially sensitive applications, but their behavior concerning disability is underexplored; the work aims to systematically analyze interpretation shifts where models go beyond observable evidence.", "method": "Develop a benchmark with Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) for 9 disability categories; evaluate 15 state-of-the-art VLMs in zero-shot settings using text-based metrics and an LLM-as-judge protocol validated by disabled annotators.", "result": "Disability context consistently degrades interpretive fidelity, causing shifts like speculative inference, narrative elaboration, affective degradation, and deficit-oriented framing, with amplified effects by race and gender; targeted prompting and preference fine-tuning effectively improve fidelity.", "conclusion": "The findings highlight biases in VLMs regarding disability and demonstrate practical methods to reduce interpretation shifts, emphasizing the need for more equitable AI applications."}}
{"id": "2601.17069", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17069", "abs": "https://arxiv.org/abs/2601.17069", "authors": ["Shahil Shaik", "Jonathon M. Smereka", "Yue Wang"], "title": "Multi-Agent Deep Reinforcement Learning Under Constrained Communications", "comment": "21 pages, 8 figures, Under review at ICLR", "summary": "Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.", "AI": {"tldr": "A distributed MARL framework replaces centralized training with decentralized peer-to-peer communication, using D-GAT for global state inference and achieving outperformance over CTDE methods in diverse cooperative tasks.", "motivation": "CTDE methods in MARL face scalability, robustness, and generalization issues due to reliance on global state information, and are brittle in practical scenarios like changing teammates or varying environments.", "method": "Developed D-GAT for global state inference through multi-hop communication with input-dependent attention, and integrated it into DG-MAPPO for distributed local policy and value function optimization using local observations and shared/averaged rewards.", "result": "Outperformed CTDE baselines on StarCraftII, Google Research Football, and Multi-Agent Mujoco, achieving superior coordination in homogeneous and heterogeneous tasks.", "conclusion": "The distributed MARL framework provides a scalable, robust solution without centralized training, enabling agents to learn and act solely via communication, potentially the first to eliminate privileged centralized information reliance."}}
{"id": "2601.17426", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17426", "abs": "https://arxiv.org/abs/2601.17426", "authors": ["Zhengqing Zang", "Yuqi Ding", "Yanmei Gu", "Changkai Song", "Zhengkai Yang", "Guoping Du", "Junbo Zhao", "Haobo Wang"], "title": "A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models", "comment": null, "summary": "Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.", "AI": {"tldr": "LLMs show evolution in logical reasoning from traditional to modern logic, influenced by model size, thinking processes, and base model architecture.", "motivation": "To investigate whether large language models exhibit a similar evolution in logical reasoning as humans have (from intuition-driven to formal systems), using existential import in syllogisms as a probe.", "method": "Using existential import as a probe to evaluate syllogistic reasoning under traditional vs. modern logic frameworks. Conducted extensive experiments testing state-of-the-art LLMs on a new syllogism dataset.", "result": "Three key findings: (1) Model size scaling promotes shift toward modern logic, (2) Thinking (reasoning processes) serves as an efficient accelerator beyond parameter scaling, (3) Base model architecture crucially determines how easily and stably this logical shift emerges.", "conclusion": "LLMs do exhibit evolution in logical reasoning frameworks, with this shift influenced by multiple factors including model scale, reasoning processes, and architectural foundations. This provides insights into how AI systems develop logical capabilities."}}
{"id": "2601.17073", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17073", "abs": "https://arxiv.org/abs/2601.17073", "authors": ["Yifei Zhang", "Meimei Liu", "Zhengwu Zhang"], "title": "Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis", "comment": null, "summary": "Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.", "AI": {"tldr": "CM-JIVNet is a probabilistic framework that disentangles joint and individual latent representations from paired structural and functional connectivity data for improved multimodal brain analysis.", "motivation": "Integration of structural and functional connectivity data is challenging due to high dimensionality, non-linearity, and modality-specific variations, hindering cross-modal pattern discovery for behavior.", "method": "Propose CM-JIVNet, using a multi-head attention fusion module to capture non-linear cross-modal dependencies and isolate independent modality-specific signals in a probabilistic framework.", "result": "On HCP-YA data, CM-JIVNet shows superior cross-modal reconstruction and behavioral trait prediction by disentangling joint and individual feature spaces.", "conclusion": "CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis by effectively learning factorized latent representations."}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "Lattice is a framework for self-constructing and continuously improving conversational AI guardrails through iterative simulation, optimization, and autonomous adaptation.", "motivation": "Existing conversational AI guardrails use static rules that cannot adapt to new threats or different deployment contexts, creating vulnerabilities in dynamic environments.", "method": "Two-stage framework: 1) Construction builds initial guardrails from labeled examples through iterative simulation and optimization, 2) Continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation.", "result": "On ProsocialDialog dataset: 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. Continuous improvement achieves 7pp F1 improvement on cross-domain data through closed-loop optimization.", "conclusion": "Effective conversational AI guardrails can be self-constructed and continuously improved through iterative optimization, demonstrating superior adaptability compared to static rule-based approaches."}}
{"id": "2601.17074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17074", "abs": "https://arxiv.org/abs/2601.17074", "authors": ["Akila Sampath", "Vandana Janeja", "Jianwu Wang"], "title": "PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction", "comment": null, "summary": "The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.", "AI": {"tldr": "PhysE-Inv integrates LSTM encoder-decoder with attention and physics-guided contrastive learning to estimate Arctic snow depth from sparse, noisy sea ice data through physics-constrained inversion.", "motivation": "Accurate Arctic snow depth estimation is critical but challenging due to extreme data scarcity and noise in sea ice parameters. Existing models are either too sensitive to sparse data or lack physical interpretability needed for climate applications.", "method": "Introduces PhysE-Inv framework with: 1) LSTM Encoder-Decoder with Multi-head Attention architecture, 2) Physics-guided contrastive learning, 3) Surjective physics-constrained inversion methodology using hydrostatic balance forward model as proxy, 4) Reconstruction physics regularization over latent space to discover hidden physical parameters from noisy time-series data.", "result": "PhysE-Inv significantly improves prediction performance, reducing error by 20% compared to state-of-the-art baselines, while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods.", "conclusion": "The approach pioneers noise-tolerant, interpretable inverse modeling with wide applicability in geospatial and cryospheric domains, offering a path forward for physically consistent estimation from sparse, noisy data."}}
{"id": "2601.17542", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17542", "abs": "https://arxiv.org/abs/2601.17542", "authors": ["Vinoth Punniyamoorthy", "Nitin Saksena", "Srivenkateswara Reddy Sankiti", "Nachiappan Chockalingam", "Aswathnarayan Muthukrishnan Kirubakaran", "Shiva Kumar Reddy Carimireddy", "Durgaraman Maruthavanan"], "title": "Cognitive Platform Engineering for Autonomous Cloud Operations", "comment": null, "summary": "Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.", "AI": {"tldr": "Cognitive Platform Engineering integrates AI-driven sensing, reasoning, and autonomous action into DevOps platforms to address scalability and dynamism challenges in cloud-native systems.", "motivation": "Traditional DevOps automation struggles with cloud-native scale and dynamism, leading to reactive operations, delayed remediation, and dependency on manual expertise due to growing telemetry volume and configuration drift.", "method": "Proposes a four-plane reference architecture: data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. Implemented prototype using Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection.", "result": "Prototype demonstrates improvements in mean time to resolution, resource efficiency, and compliance. Shows that embedding intelligence enables resilient, self-adjusting, and intent-aligned cloud environments.", "conclusion": "Cognitive Platform Engineering enables resilient, self-adjusting cloud ecosystems. Future research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud systems."}}
{"id": "2601.17076", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17076", "abs": "https://arxiv.org/abs/2601.17076", "authors": ["Jiajun Chen", "Yue Wu", "Kai Huang", "Wen Xi", "Yangyang Wu", "Xiaoye Miao", "Mengying Zhu", "Meng Xi", "Guanjie Cheng"], "title": "E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning", "comment": "11 pages", "summary": "Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \\emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \\textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \\textsf{E2PL} unifies two novel prompt designs: \\emph{task-tailored prompts} for class-incremental adaptation and \\emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \\emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \\emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \\textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.", "AI": {"tldr": "E2PL is a novel prompt learning framework for incomplete multi-view multi-label class incremental learning, using task-tailored and missing-aware prompts with efficient prototype tensorization and dynamic contrastive learning to handle missing views and new classes efficiently.", "motivation": "Real-world web-scale multi-view multi-label classification faces challenges with missing views and continuously emerging classes, and existing methods are not designed to handle both issues simultaneously, leading to inefficiency or scalability problems.", "method": "Proposes the E2PL framework with two prompt designs: task-tailored prompts for class-incremental adaptation and missing-aware prompts for flexible integration of view-missing scenarios. Includes an efficient prototype tensorization module using atomic tensor decomposition to reduce exponential parameter complexity to linear, and a dynamic contrastive learning strategy to model dependencies among missing-view patterns.", "result": "Experiments on three benchmarks show that E2PL consistently outperforms state-of-the-art methods in both effectiveness and efficiency, demonstrating robustness in handling incomplete views and dynamic class expansion.", "conclusion": "E2PL provides an effective and scalable solution for the novel IMvMLCIL task, addressing key real-world challenges in web applications."}}
{"id": "2601.17564", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17564", "abs": "https://arxiv.org/abs/2601.17564", "authors": ["Aadam", "Monu Verma", "Mohamed Abdel-Mottaleb"], "title": "JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research", "comment": null, "summary": "The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.", "AI": {"tldr": "JaxARC is a high-performance JAX-based RL environment for the ARC, offering massive parallelism, up to 5,400x speedup, and peak throughput of 790M steps/second to enable large-scale inductive reasoning research.", "motivation": "Existing Gymnasium-based RL environments for the Abstraction and Reasoning Corpus (ARC), which tests AI's human-like inductive reasoning, suffer from computational bottlenecks that severely limit experimental scale.", "method": "Developed JaxARC, an open-source RL environment implemented in JAX with a functional, stateless architecture that supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility.", "result": "JaxARC achieves 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second, making large-scale RL research previously computationally infeasible now feasible.", "conclusion": "JaxARC addresses computational limitations in ARC research by providing a high-performance environment that enables scalable experiments, advancing the study of inductive reasoning in AI."}}
{"id": "2601.17090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17090", "abs": "https://arxiv.org/abs/2601.17090", "authors": ["Noam Koren", "Rafael Moschopoulos", "Kira Radinsky", "Elad Hazan"], "title": "SFO: Learning PDE Operators via Spectral Filtering", "comment": null, "summary": "Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.", "AI": {"tldr": "Spectral Filtering Operator (SFO) is a novel neural operator that uses a fixed universal spectral basis to efficiently capture long-range interactions in PDEs, achieving state-of-the-art accuracy with fewer parameters.", "motivation": "Neural operators struggle to efficiently capture long-range, nonlocal interactions inherent in PDE solution maps, limiting their effectiveness for complex systems governed by PDEs.", "method": "SFO parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed global orthonormal basis derived from eigenmodes of the Hilbert matrix. It learns only spectral coefficients of rapidly decaying eigenvalues, enabling highly efficient representation based on theoretical findings about discrete Green's functions exhibiting spatial Linear Dynamical System structure.", "result": "Across six benchmarks (reaction-diffusion, fluid dynamics, and 3D electromagnetics), SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.", "conclusion": "SFO provides an efficient neural operator framework that successfully addresses the challenge of capturing long-range interactions in PDEs through spectral filtering theory, offering both parameter efficiency and improved accuracy."}}
{"id": "2601.17587", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17587", "abs": "https://arxiv.org/abs/2601.17587", "authors": ["Azza Fadhel", "Nathaniel W. Zuckschwerdt", "Aryan Deshwal", "Susmita Bose", "Amit Bandyopadhyay", "Jana Doppa"], "title": "Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design", "comment": "Proceedings of Innovative Applications of AI (IAAI) 2026 Conference", "summary": "Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.", "AI": {"tldr": "Use AI-driven adaptive design with surrogate models to efficiently find optimal additive manufacturing parameters for metal alloys, specifically GRCop-42, achieving defect-free outputs in three months vs. manual failure.", "motivation": "Configuring additive manufacturing parameters for metal alloys like GRCop-42 is inefficient using trial-and-error due to high resource costs and large configuration spaces, hindering access to critical aerospace materials.", "method": "Combine AI-driven adaptive experimental design with domain knowledge, building a surrogate model from past experiments to intelligently select small batches of input configurations (e.g., laser power, scan speed) for validation iteratively.", "result": "Deployed for Directed Energy Deposition, yielded multiple defect-free GRCop-42 outputs within three months, dramatically reducing time and resources compared to months of manual experimentation that failed.", "conclusion": "The approach successfully democratizes access to high-quality GRCop-42 fabrication on available infrared laser platforms, enabling cost-effective, decentralized aerospace production and advancing additive manufacturing efficiency."}}
{"id": "2601.17091", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17091", "abs": "https://arxiv.org/abs/2601.17091", "authors": ["Ole St\u00fcven", "Keno Moenck", "Thorsten Sch\u00fcppstuhl"], "title": "CUROCKET: Optimizing ROCKET for GPU", "comment": null, "summary": "ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.", "AI": {"tldr": "CUROCKET enables GPU acceleration of ROCKET for time series classification, overcoming kernel heterogeneity challenges to achieve 11\u00d7 better energy efficiency.", "motivation": "ROCKET is computationally efficient for time series classification but current implementations are CPU-bound. Convolution is highly parallelizable and well-suited for GPU execution, which could significantly speed up computation, but ROCKET's inhomogeneous kernels pose challenges for standard GPU convolution methods.", "method": "Developed an algorithm that efficiently handles ROCKET's inhomogeneous kernels on GPU, using parallelization techniques suited for GPU architecture while maintaining compatibility with ROCKET's feature extraction process.", "result": "CUROCKET achieves up to 11 times higher computational efficiency per watt than CPU-based ROCKET implementations while maintaining the same accuracy and feature extraction capabilities.", "conclusion": "CUROCKET enables efficient GPU-based ROCKET execution, achieving up to 11\u00d7 higher computational efficiency per watt than CPU implementations, making time series classification more accessible and sustainable."}}
{"id": "2601.17588", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17588", "abs": "https://arxiv.org/abs/2601.17588", "authors": ["Marcus Ma", "Shrikanth Narayanan"], "title": "Intelligence Requires Grounding But Not Embodiment", "comment": null, "summary": "Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.", "AI": {"tldr": "Intelligence requires grounding (connecting abstract concepts to real-world referents) rather than physical embodiment itself, as demonstrated through four key properties of intelligence that can be achieved by non-embodied but grounded agents.", "motivation": "To address the ongoing scientific debate about whether physical embodiment is necessary for intelligence, particularly in light of recent advances in large language models that challenge traditional views on embodied cognition.", "method": "1. Define intelligence as possessing four properties: motivation, predictive ability, understanding of causality, and learning from experience.\n2. Argue that each property can be achieved by non-embodied but grounded agents.\n3. Present a thought experiment of an intelligent LLM agent operating in a digital environment.\n4. Address potential counterarguments to the position.", "result": "The paper demonstrates that grounding (which involves connecting abstract concepts to real-world referents) is necessary for intelligence, but physical embodiment itself is not required. Each of the four intelligence properties can be satisfied by agents that are grounded but not physically embodied.", "conclusion": "Intelligence requires grounding rather than embodiment. Grounding provides the necessary connection to the world that enables intelligent behavior, while physical embodiment is merely one possible way to achieve grounding. Non-embodied agents, such as LLMs operating in digital environments, can be intelligent if they are properly grounded."}}
{"id": "2601.17093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17093", "abs": "https://arxiv.org/abs/2601.17093", "authors": ["Olha Sirikova", "Alvin Chan"], "title": "The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations", "comment": "Accepted to AAAI 2026 Workshop on AI for Scientific Research (AI4Research)", "summary": "Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.", "AI": {"tldr": "A framework called Triangle of Similarity is proposed to compare neural network representations using three perspectives: static similarity, functional similarity, and sparsity similarity, applied to various models with findings on architectural influence, pruning effects, and regularization.", "motivation": "To provide a more holistic view for comparing neural network representations, as existing methods are limited, enabling better understanding and validation of models in scientific applications.", "method": "Propose the Triangle of Similarity framework combining static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyze CNNs, Vision Transformers, and Vision-Language Models using in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds.", "result": "Initial findings: (1) architectural family determines similarity, forming clusters; (2) CKA self-similarity and task accuracy correlate during pruning, with accuracy degrading more sharply; (3) pruning regularizes representations for some model pairs, revealing a shared computational core.", "conclusion": "The framework offers a holistic approach for assessing model convergence on similar internal mechanisms, useful for model selection and analysis in scientific research."}}
{"id": "2601.17642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17642", "abs": "https://arxiv.org/abs/2601.17642", "authors": ["Zhihao Zhang", "Liting Huang", "Guanghao Wu", "Preslav Nakov", "Heng Ji", "Usman Naseem"], "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context", "comment": "Preprint", "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}", "AI": {"tldr": "Health-ORSC-Bench is a new benchmark for evaluating over-refusal and safe completion in healthcare LLMs, testing models on ambiguous prompts and revealing issues with binary safety boundaries.", "motivation": "Current safety alignment in LLMs for healthcare relies on binary refusal boundaries, leading to over-refusal of benign queries or unsafe compliance with harmful ones, without adequately assessing safe completion.", "method": "Introduced Health-ORSC-Bench, with 31,920 benign boundary prompts across 7 health categories, using an automated pipeline and human validation to test models at varying intent ambiguity levels.", "result": "Evaluated 30 state-of-the-art LLMs, showing safety-optimized models refuse up to 80% of 'Hard' benign prompts, while domain-specific models sacrifice safety for utility; larger frontier models exhibit more over-refusal than smaller or MoE-based ones.", "conclusion": "Health-ORSC-Bench provides a standard for calibrating medical AI towards nuanced completions, highlighting that current LLMs struggle to balance refusal and compliance effectively."}}
{"id": "2601.17094", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17094", "abs": "https://arxiv.org/abs/2601.17094", "authors": ["Junichiro Niimi"], "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation", "comment": null, "summary": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.", "AI": {"tldr": "LLMs generate text fluently but lack true world understanding. An architecture separating world models from language models improves performance in the consumer review domain, showing higher sentiment correlation and controllable text generation.", "motivation": "To address whether LLMs understand the world or just produce plausible language, by proposing a separation between world models and language models to enable consistent and controllable text generation.", "method": "An architecture with three components: a Deep Boltzmann Machine (DBM) as an energy-based world model, an adapter to project belief states into embedding space, and a frozen GPT-2 for linguistic competence, applied to Amazon smartphone reviews.", "result": "Conditioning through the world model yields better sentiment correlation, lower perplexity, and greater semantic similarity than prompt-based generation alone. The DBM's energy function distinguishes coherent market configurations. Interventions on attributes propagate causally to generated text, showing distributions consistent with natural samples.", "conclusion": "Separating linguistic competence from world understanding allows small-scale language models to achieve consistent, controllable generation, supporting the 'mouth is not the brain' principle."}}
{"id": "2601.17678", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17678", "abs": "https://arxiv.org/abs/2601.17678", "authors": ["Zhiyu An", "Wan Du"], "title": "DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories", "comment": null, "summary": "We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.", "AI": {"tldr": "Inverse mechanism learning: recovering unknown incentive mechanisms from observed strategic interactions of self-interested learning agents using likelihood-based framework DIML.", "motivation": "Existing approaches like inverse game theory and multi-agent inverse RL focus on structured mechanisms with utility/reward parameters, but many real-world mechanisms are unstructured mappings from actions to payoffs. Differentiable mechanism design optimizes mechanisms forward rather than inferring them from observed behavior.", "method": "Propose DIML (Differentiable Inverse Mechanism Learning), a likelihood-based framework that differentiates through multi-agent learning dynamics models and uses candidate mechanisms to generate counterfactual payoffs for predicting observed actions.", "result": "Established identifiability of payoff differences under conditional logit response model; proved statistical consistency of maximum likelihood estimation; DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, performing comparably to tabular enumeration oracle in small environments and scaling to large hundred-participant environments.", "conclusion": "DIML provides a principled approach for inverse mechanism learning that can recover unstructured mechanisms from observed strategic interactions, bridging observational inference with mechanism design and enabling counterfactual analysis in complex multi-agent systems."}}
{"id": "2601.17108", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17108", "abs": "https://arxiv.org/abs/2601.17108", "authors": ["Dianxin Luan", "Chengsi Liang", "Jie Huang", "Zheng Lin", "Kaitao Meng", "John Thompson", "Cheng-Xiang Wang"], "title": "MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism", "comment": null, "summary": "This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.", "AI": {"tldr": "A Mamba-assisted neural network with self-attention is proposed for low-complexity channel estimation in large-subcarrier OFDM systems, using bidirectional selective scan to enhance performance.", "motivation": "To address the challenge of efficient channel estimation for OFDM with many subcarriers by reducing complexity while capturing long-range dependencies, improving over traditional methods.", "method": "Integrated a customized Mamba architecture with self-attention mechanism, implemented a bidirectional selective scan to handle non-causal channel gains, and designed it to have lower space complexity than transformers.", "result": "Simulations on 3GPP TS 36.101 channel show superior channel estimation performance with fewer tunable parameters compared to baseline neural networks.", "conclusion": "The framework effectively enhances OFDM channel estimation performance with reduced complexity, making it suitable for large-scale subcarrier configurations."}}
{"id": "2601.17699", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17699", "abs": "https://arxiv.org/abs/2601.17699", "authors": ["Harper Hua", "Zhen Han", "Zhengyuan Shen", "Jeremy Lee", "Patrick Guan", "Qi Zhu", "Sullam Jeoung", "Yueyan Chen", "Yunfei Bai", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL", "comment": null, "summary": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.", "AI": {"tldr": "A multi-turn RL framework for Text-to-SQL improves performance by iterative reasoning and execution feedback, outperforming large models.", "motivation": "The gap between AI systems and human experts in Text-to-SQL due to lack of iterative reasoning and error correction in single-pass methods.", "method": "SQL-Trail, a multi-turn reinforcement learning framework with adaptive turn-budget allocation and composite rewards.", "result": "Sets new state-of-the-art with up to 18x higher data efficiency and outperforms larger proprietary models by 5%.", "conclusion": "Interactive, agentic workflows are effective for robust Text-to-SQL generation, demonstrated by strong benchmark results."}}
{"id": "2601.17111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17111", "abs": "https://arxiv.org/abs/2601.17111", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Austin Xu", "Caiming Xiong", "Shafiq Joty"], "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts", "comment": "Preprint", "summary": "Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.", "AI": {"tldr": "LLEP is a novel expert parallelism algorithm that dynamically reroutes tokens and expert parameters from overloaded to underutilized devices to handle imbalanced routing in MoE models, achieving significant speedup and memory reduction.", "motivation": "MoE models trained with load-balancing constraints still exhibit imbalanced routing during post-training/inference, causing compute/memory failures in expert parallelism due to tokens funneling to few experts on overloaded devices.", "method": "Least-Loaded Expert Parallelism (LLEP) dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones to ensure all devices complete workloads within minimum collective latency while respecting memory constraints.", "result": "LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP, enabling ~1.9x faster post-training/inference for gpt-oss-120b, supported by theoretical analysis and empirical evaluations.", "conclusion": "LLEP effectively addresses imbalanced routing challenges in MoE models, enabling faster and higher-throughput post-training and inference while providing a principled framework for hardware-specific hyperparameter tuning."}}
{"id": "2601.17717", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17717", "abs": "https://arxiv.org/abs/2601.17717", "authors": ["Kaituo Zhang", "Mingzhi Hu", "Hoang Anh Duy Le", "Fariha Kabir Torsha", "Zhimeng Jiang", "Minh Khai Bui", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zhen Xiong", "Ying Lin", "Guanchu Wang", "Na Zou"], "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.", "AI": {"tldr": "Introduces a framework to audit LLM-generated synthetic data quality across multiple modalities, shifting focus from extrinsic to intrinsic evaluation.", "motivation": "To address the challenge of ensuring high quality in LLM-generated synthetic data, which is crucial as LLMs transform data from scarce to controllable for training, evaluation, and iteration, but existing research lacks direct focus on quality and a unified perspective across modalities.", "method": "Proposes the LLM Data Auditor framework: first, describes how LLMs generate data across six modalities; then, systematically categorizes intrinsic metrics for evaluating synthetic data from quality and trustworthiness dimensions; uses this to analyze experimental evaluations of representative generation methods per modality.", "result": "Identifies substantial deficiencies in current evaluation practices for synthetic data across modalities, highlighting limitations in existing methodologies and unaddressed quality issues.", "conclusion": "Offers concrete recommendations to improve evaluation of data generation and outlines methodologies for practical application of synthetic data across different modalities, bridging the gap in research on data quality and unifying perspectives in this field."}}
{"id": "2601.17112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17112", "abs": "https://arxiv.org/abs/2601.17112", "authors": ["A. El Ichi", "K. Jbilou"], "title": "Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.", "AI": {"tldr": "Introduces a tensor compression framework using the cproduct for low-rank approximation of LLM weights to reduce memory and computational costs.", "motivation": "LLMs have high memory and computational demands; this paper aims to reduce these through efficient compression methods.", "method": "Uses algebraic structure of the cproduct to represent weight tensors in a transform domain and jointly approximate frontal slices with low-rank factors for multidimensional correlation exploitation.", "result": "Proposes a computationally efficient compression that goes beyond traditional methods like SVD.", "conclusion": "The framework shows potential to alleviate LLM memory and computational burdens through tensor compression."}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "A new benchmark called EntWorld is introduced to evaluate enterprise AI agents across six professional domains, revealing large performance gaps compared to humans and highlighting the need for domain-specific solutions.", "motivation": "Existing benchmarks for multimodal agents focus on consumer-oriented scenarios and fail to capture the complexity and rigor of professional enterprise workflows, which involve high-density UIs, strict business logic constraints, and precise state-consistent information retrieval.", "method": "The authors propose EntWorld, a large-scale benchmark with a schema-grounded task generation framework that reverse-engineers business logic from database schemas to synthesize realistic workflows, and use SQL-based deterministic verification for state-transition validation instead of visual matching.", "result": "Experimental results show state-of-the-art models like GPT-4.1 achieve only 47.61% success rate on EntWorld, substantially lower than human performance, highlighting a significant enterprise capability gap.", "conclusion": "EntWorld is introduced as a rigorous testbed to facilitate the development and evaluation of enterprise-ready digital agents, revealing significant performance gaps that necessitate domain-specific solutions."}}
{"id": "2601.17130", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17130", "abs": "https://arxiv.org/abs/2601.17130", "authors": ["Megha Khosla"], "title": "How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?", "comment": null, "summary": "Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.", "AI": {"tldr": "This paper analyzes privacy leakage risks in graph neural networks (GNNs), specifically focusing on membership inference attacks, highlighting the importance of graph-specific analysis, the impact of graph structure and edge access, and limitations in auditing differentially private GNNs for node-level tasks.", "motivation": "GNNs are widely used in sensitive applications, raising concerns about training data leakage, but existing research on privacy leakage often relies on non-graph domains (e.g., images and tabular data), necessitating a graph-specific analysis to understand the impact of graph structure.", "method": "The research formalizes membership inference over node-neighbourhood tuples, investigating two dimensions: (i) training graph construction (e.g., snowball vs. random sampling) and (ii) inference-time edge access, with experimental evaluation across models and datasets.", "result": "Empirical findings show that snowball sampling's coverage bias harms generalization relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, reduces the train-test gap, and yields the lowest membership advantage. The generalization gap is an incomplete proxy for MI risk, as edge access can independently affect MI. Additionally, inductive splits in node-level tasks break exchangeability, limiting standard bounds for auditing differentially private GNNs.", "conclusion": "The study emphasizes the need for graph-aware privacy analysis in GNNs, demonstrating that graph structure significantly influences membership inference risks, and highlights challenges in auditing differentially private models for node-level tasks due to broken exchangeability in inductive splits."}}
{"id": "2601.17735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17735", "abs": "https://arxiv.org/abs/2601.17735", "authors": ["Kyungho Kim", "Geon Lee", "Juyeon Kim", "Dongwon Choi", "Shinhwan Kang", "Kijung Shin"], "title": "ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents", "comment": "Accepted in ACM WWW 2026 (Short Paper)", "summary": "Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.", "AI": {"tldr": "ReFuGe is an agentic framework using LLM agents to generate relational features for RDB prediction tasks, improving performance through iterative filtering.", "motivation": "Prediction tasks on relational databases require generating informative relational features, which is challenging due to complex schemas and large feature spaces without supervision.", "method": "The framework employs three specialized LLM agents: schema selection, feature generation, and feature filtering, operating in an iterative feedback loop until convergence.", "result": "Experiments on RDB benchmarks show that ReFuGe substantially enhances performance across various prediction tasks.", "conclusion": "ReFuGe effectively addresses the challenges of relational feature generation through agentic reasoning and filtering, boosting predictive capabilities in RDB applications."}}
{"id": "2601.17744", "categories": ["cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17744", "abs": "https://arxiv.org/abs/2601.17744", "authors": ["Amjad Fatmi"], "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems", "comment": "40 pages, 10 figures. Preprint. Code: https://github.com/faramesh/faramesh-core", "summary": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.", "AI": {"tldr": "Faramesh provides a mandatory execution control plane for autonomous agents that enforces authorization checks before actions can have real-world side effects, ensuring predictable governance without hidden coupling to orchestration layers.", "motivation": "Autonomous agents increasingly trigger real-world side effects like infrastructure deployment, database modifications, and financial transactions, but most agent stacks lack mandatory execution checkpoints where organizations can deterministically control actions before they change reality.", "method": "Faramesh introduces an Action Authorization Boundary (AAB) that canonicalizes agent intent into Canonical Action Representations (CAR), evaluates actions deterministically against policy and state, and issues PERMIT/DEFER/DENY decisions that executors must validate. It provides decision-centric append-only provenance logging keyed by action hashes.", "result": "The system creates enforceable, predictable governance for autonomous execution while remaining framework- and model-agnostic, supporting multi-agent/multi-tenant deployments, and independent of transport protocols like MCP. It enables auditability, verification, and deterministic replay without re-running agent reasoning.", "conclusion": "Faramesh addresses the critical gap in autonomous agent systems by providing a protocol-agnostic execution control plane with non-bypassable authorization checks, moving beyond observability-only approaches to deliver actual enforceable governance before actions have real-world consequences."}}
{"id": "2601.17135", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17135", "abs": "https://arxiv.org/abs/2601.17135", "authors": ["Jakob Karalus", "Friedhelm Schwenker"], "title": "ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning", "comment": null, "summary": "Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.", "AI": {"tldr": "ConceptACT enhances imitation learning by incorporating human-provided semantic concepts during training, improving efficiency without needing them at deployment.", "motivation": "Current imitation learning methods rely only on low-level sensorimotor data, missing the rich semantic knowledge humans have for tasks, limiting learning efficiency.", "method": "Extends Action Chunking with Transformers by integrating episode-level semantic concept annotations via concept-aware cross-attention in the final encoder layer, aligned with human annotations.", "result": "ConceptACT converges faster and achieves better sample efficiency than standard ACT, outperforming naive auxiliary losses or language-conditioned models.", "conclusion": "Properly integrated semantic supervision provides powerful inductive biases, enabling more efficient robot learning with minimal annotation burden."}}
{"id": "2601.17767", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17767", "abs": "https://arxiv.org/abs/2601.17767", "authors": ["Rajan Das Gupta", "Xiaobin Wu", "Xun Liu", "Jiaqi He"], "title": "HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis", "comment": "Accepted and published in the 2025 4th International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)", "summary": "Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.", "AI": {"tldr": "A hybrid ensemble framework combining CNN, LSTM, KNN, and XGBoost achieves superior CVD prediction accuracy (82.30% and 97.10%) on two datasets, demonstrating robust clinical potential for early intervention.", "motivation": "Cardiovascular disease is the leading cause of global mortality, creating urgent demand for intelligent diagnostic tools. Traditional predictive models often fail to generalize across heterogeneous datasets and complex physiological patterns.", "method": "A hybrid ensemble framework integrating deep learning architectures (CNN and LSTM) with classical machine learning algorithms (KNN and XGBoost) using an ensemble voting mechanism. This combines the representational power of deep networks with the interpretability and efficiency of traditional models.", "result": "The proposed model achieves 82.30% accuracy on Dataset I and 97.10% accuracy on Dataset II, with consistent gains in precision, recall, and F1-score across both publicly available Kaggle datasets.", "conclusion": "The findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. The study supports UN Sustainable Development Goal 3 by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative healthcare solutions."}}
{"id": "2601.17180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17180", "abs": "https://arxiv.org/abs/2601.17180", "authors": ["In\u00e9s Gonzalez-Pepe", "Vinuyan Sivakolunthu", "Jacob Fortin", "Yohan Chatelain", "Tristan Glatard"], "title": "Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging", "comment": null, "summary": "Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.", "AI": {"tldr": "Using NaN values to identify and skip numerically unstable operations in CNNs, achieving up to 1.67x speedup with minimal performance impact.", "motivation": "Deep learning models for neuroimaging are increasingly large and computationally expensive, with many operations being applied to values dominated by numerical noise that have negligible impact on model outputs.", "method": "Introduces Conservative & Aggressive NaNs - two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. These are implemented in PyTorch without architectural changes.", "result": "For data with >50% NaNs: consistent runtime improvements; for >67% NaNs (common in neuroimaging): average 1.67x inference speedup. Conservative NaNs reduces convolution operations by average 30% across models/datasets with no measurable performance degradation, skipping up to 64.64% of convolutions in specific layers. Aggressive NaNs skips up to 69.30% of convolutions but may occasionally affect performance.", "conclusion": "Numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs, particularly beneficial for neuroimaging applications where sparse data patterns are common."}}
{"id": "2601.17789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17789", "abs": "https://arxiv.org/abs/2601.17789", "authors": ["Yiming Su", "Kunzhao Xu", "Yanjie Gao", "Fan Yang", "Cheng Li", "Mao Yang", "Tianyin Xu"], "title": "Neuro-Symbolic Verification on Instruction Following of LLMs", "comment": null, "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.", "AI": {"tldr": "NSVIF is a neuro-symbolic framework that verifies whether LLM outputs follow instructions by modeling instructions as constraints and solving them through logical reasoning and semantic analysis.", "motivation": "LLMs often fail to follow instructions, and these violations can propagate through agentic workflows, causing task failures and system incidents. Current approaches lack effective verification methods.", "method": "NSVIF models user instructions as both logical and semantic constraints, formulating verification as a constraint-satisfaction problem. It uses a unified solver that orchestrates logical reasoning and semantic analysis without assumptions about the instruction or LLM.", "result": "NSVIF significantly outperforms LLM-based approaches on VIFBENCH benchmark and provides interpretable feedback. The feedback helps improve LLMs' instruction-following capability without post-training.", "conclusion": "NSVIF provides an effective neuro-symbolic framework for verifying LLM instruction-following, offering better performance than LLM-based approaches and enabling improvement of LLMs through interpretable feedback."}}
{"id": "2601.17183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17183", "abs": "https://arxiv.org/abs/2601.17183", "authors": ["Farzam Asad", "Junaid Saif Khan", "Maria Tariq", "Sundus Munir", "Muhammad Adnan Khan"], "title": "Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data", "comment": "27 pages, 7 figures, 4 tables", "summary": "Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.", "AI": {"tldr": "FedProx outperforms centralized and local models for heart disease prediction using federated learning on non-IID hospital data while preserving patient privacy.", "motivation": "Healthcare institutions cannot share patient data due to privacy regulations (HIPAA, GDPR), but federated learning enables collaborative model training without centralizing raw data. Clinical datasets are inherently non-IID due to demographic disparities and institutional differences.", "method": "Comprehensive simulation research using FedProx (Federated Proximal Optimization) for heart disease prediction on UCI dataset. Created realistic non-IID data partitions by simulating four heterogeneous hospital clients from Cleveland Clinic data (303 patients) through demographic-based stratification. Conducted ablation studies with statistical validation on 50 independent runs.", "result": "FedProx with proximal parameter mu=0.05 achieved 85.00% accuracy, outperforming both centralized learning (83.33%) and isolated local models (78.45% average) while maintaining patient privacy. Proximal regularization effectively curbs client drift in heterogeneous environments.", "conclusion": "FedProx is effective for privacy-preserving collaborative learning in healthcare with non-IID data. The research provides algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, with results directly transferable to hospital IT administrators."}}
{"id": "2601.17814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17814", "abs": "https://arxiv.org/abs/2601.17814", "authors": ["Haoxuan Ma", "Guannan Lai", "Han-Jia Ye"], "title": "MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.", "AI": {"tldr": "A benchmark called MMR-Bench is proposed for multimodal routing, enabling query-level model selection to efficiently manage diverse tasks and computational costs in MLLM deployments.", "motivation": "Multimodal large language models (MLLMs) have rapid advancements, but heterogeneity in architecture, alignment, and efficiency means no single model excels across all tasks from lightweight OCR to complex reasoning. Using one MLLM for all queries either wastes compute on easy instances or sacrifices accuracy on hard ones, creating a tension that query-level model selection (routing) addresses. However, extending routing from text-only LLMs to MLLMs is challenging due to modality fusion, wide computational cost variation, and lack of standardized, budget-aware evaluation.", "method": "Present MMR-Bench, a unified benchmark that isolates the multimodal routing problem. It provides a controlled environment with modality-aware inputs and variable compute budgets, a broad suite of vision-language tasks covering OCR, VQA, and multimodal math reasoning, and strong references like single-model baselines, oracle upper bounds, and representative routing policies. The approach incorporates multimodal signals for routing decisions.", "result": "Using MMR-Bench, incorporating multimodal signals improves routing quality, with empirical results showing enhancements in the cost-accuracy frontier. The routed system exceeds the strongest single model's accuracy at approximately 33% of its cost. Policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, demonstrating effectiveness of the benchmark.", "conclusion": "MMR-Bench establishes itself as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. It addresses the challenges in extending routing to MLLMs, enabling comparisons under fixed candidate sets and cost models, and shows that it advances practical large-scale MLLM applications by bridging gaps between performance and efficiency."}}
{"id": "2601.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17189", "abs": "https://arxiv.org/abs/2601.17189", "authors": ["Sabrina Mokhtari", "Sara Kodeiri", "Shubhankar Mohapatra", "Florian Tramer", "Gautam Kamath"], "title": "Rethinking Benchmarks for Differentially Private Image Classification", "comment": null, "summary": "We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.", "AI": {"tldr": "The paper proposes comprehensive benchmarks for differentially private image classification across various settings and creates a public leaderboard to track progress.", "motivation": "There's a need for standardized, comprehensive benchmarks to evaluate differentially private machine learning techniques across diverse scenarios (with/without additional data, convex settings, different datasets) to better understand which approaches remain effective in different contexts.", "method": "The authors suggest a comprehensive set of benchmarks covering various settings for differentially private image classification. They test established techniques on these benchmarks to evaluate their effectiveness across different scenarios.", "result": "The paper establishes benchmarks that allow researchers to evaluate differentially private ML techniques in diverse settings and creates a publicly available leaderboard for the community to track progress in this field.", "conclusion": "Comprehensive benchmarking is crucial for advancing differentially private machine learning, and the proposed benchmarks and public leaderboard will help the community systematically evaluate and track progress across different settings and techniques."}}
{"id": "2601.17826", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17826", "abs": "https://arxiv.org/abs/2601.17826", "authors": ["Siyuan Yang", "Xihan Bian", "Jiayin Tang"], "title": "RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance", "comment": null, "summary": "The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.", "AI": {"tldr": "RegGuard is an industrial-scale AI assistant that automates interpretation of heterogeneous regulatory texts and aligns them with corporate policies using novel semantic chunking and domain-adapted reranking components.", "motivation": "Multinational pharmaceutical companies face significant burdens from increasing frequency and complexity of regulatory updates across jurisdictions. Manual interpretation by compliance teams is costly, error-prone, and inefficient.", "method": "The system uses a secure pipeline to ingest heterogeneous documents and features two novel components: HiSACC for hierarchical semantic aggregation and contextual chunking of long documents, and ReLACE, a domain-adapted cross-encoder for improved retrieval ranking through joint modeling of queries and candidates.", "result": "Enterprise evaluations show RegGuard improves answer quality in relevance, groundedness, and contextual focus while significantly mitigating hallucination risk. The system architecture supports auditability, traceability, provenance tracking, access control, and incremental indexing.", "conclusion": "RegGuard provides an effective AI solution for regulatory compliance automation that is responsive to evolving document sources and suitable for domains with stringent compliance demands, addressing key challenges in pharmaceutical regulatory interpretation."}}
{"id": "2601.17192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17192", "abs": "https://arxiv.org/abs/2601.17192", "authors": ["Sukirt Thakur", "Marcus Roper", "Yang Zhou", "Reza Akbarian Bafghi", "Brahmajee K. Nallamothu", "C. Alberto Figueroa", "Srinivas Paruchuri", "Scott Burger", "Maziar Raissi"], "title": "PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics", "comment": null, "summary": "Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.\n  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $\u03c1= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \\times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.\n  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.", "AI": {"tldr": "A non-invasive, uncertainty-aware framework using physics-informed neural networks and variational inference to estimate coronary flow reserve directly from standard angiography in about 3 minutes per patient on a single GPU, showing high correlation with invasive methods.", "motivation": "Coronary microvascular dysfunction is underdiagnosed because current gold-standard measurements are invasive and vary in reproducibility, so a non-invasive method is needed.", "method": "Integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport without requiring ground-truth flow measurements, validated on synthetic and clinical data.", "result": "Reliably identifies degraded data with strong correspondence between predictive uncertainty and error (Pearson r = 0.997). Clinical validation in 12 patients shows strong agreement with invasive bolus thermodilution (Pearson r = 0.90).", "conclusion": "Approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function, potentially expanding access to diagnosis and establishing a new paradigm for physics-informed inference from clinical imaging."}}
{"id": "2601.17828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17828", "abs": "https://arxiv.org/abs/2601.17828", "authors": ["Tanvi Verma", "Yang Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards", "comment": null, "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.", "AI": {"tldr": "IGFT trains medical conversational AI with online RL and information gain rewards, eliminating need for pre-collected human conversations.", "motivation": "Bridge gap by enabling models to learn effective multi-turn questioning strategies without costly expert human annotations, using self-generated conversations.", "method": "Combine online GRPO with information-theoretic reward function using GPT-4o-mini assessments, fine-tuned on models like Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B.", "result": "Improved performance on Avey and MIMIC datasets, outperforming OpenAI and medical baselines; DeepSeek model shows 10.9%-12.9% F1 improvements.", "conclusion": "IGFT's online RL approach empowers medical conversational AI to generate comprehensive HPIs efficiently, showing strong generalization and surpassing existing methods."}}
{"id": "2601.17196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17196", "abs": "https://arxiv.org/abs/2601.17196", "authors": ["Nghia Thu Truong", "Qui Phu Pham", "Quang Nguyen", "Dung Luong", "Mai Tran"], "title": "Accelerated Sinkhorn Algorithms for Partial Optimal Transport", "comment": null, "summary": "Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $\u03b3$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.", "AI": {"tldr": "Accelerated Sinkhorn method for Partial Optimal Transport (ASPOT) achieves improved complexity of O(n^{7/3}\u03b5^{-5/3}) by combining alternating minimization with Nesterov acceleration, outperforming standard Sinkhorn methods for POT.", "motivation": "Standard Sinkhorn methods for Partial Optimal Transport have suboptimal complexity bounds that limit scalability, especially when dealing with unequal marginal sizes or outliers in distributions.", "method": "ASPOT integrates alternating minimization with Nesterov-style acceleration for POT, with an informed choice of entropic parameter \u03b3 to improve convergence rates.", "result": "Theoretical analysis shows ASPOT achieves O(n^{7/3}\u03b5^{-5/3}) complexity, and experiments on real-world applications demonstrate favorable performance compared to classical Sinkhorn methods.", "conclusion": "ASPOT provides an efficient accelerated algorithm for Partial Optimal Transport with improved theoretical complexity bounds and practical performance, addressing limitations of existing Sinkhorn-based approaches."}}
{"id": "2601.17887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17887", "abs": "https://arxiv.org/abs/2601.17887", "authors": ["Jiahe Guo", "Xiangran Guo", "Yulin Hu", "Zimo Long", "Xingyu Sui", "Xuda Zhi", "Yongbo Huang", "Hao He", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents", "comment": null, "summary": "Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.", "AI": {"tldr": "Personalized LLM agents with long-term memory can inadvertently legitimize harmful queries through benign personal memories, creating a safety failure called \"intent legitimation.\"", "motivation": "Most personalized agent research focuses on utility and user experience while treating memory as neutral, overlooking safety implications. The paper aims to explore how benign personal memories can bias intent inference and cause models to legitimize harmful queries.", "method": "Introduced PS-Bench benchmark to identify and quantify intent legitimation in personalized interactions. Tested across multiple memory-augmented agent frameworks and base LLMs. Provided mechanistic evidence from internal representations space and proposed a lightweight detection-reflection method.", "result": "Personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. The detection-reflection method effectively reduces safety degradation caused by intent legitimation.", "conclusion": "This work provides the first systematic exploration of intent legitimation as a safety failure mode arising from benign real-world personalization, highlighting the importance of assessing safety under long-term personal context."}}
{"id": "2601.17204", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.17204", "abs": "https://arxiv.org/abs/2601.17204", "authors": ["Yinkai Wang", "Yan Zhou Chen", "Xiaohui Chen", "Li-Ping Liu", "Soha Hassoun"], "title": "SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment", "comment": "preprint", "summary": "Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.", "AI": {"tldr": "SpecBridge is an implicit alignment framework that identifies small molecules from MS/MS spectra by fine-tuning a spectral encoder to project into a frozen molecular foundation model's latent space, then retrieving molecules via cosine similarity to precomputed embeddings.", "motivation": "Small-molecule identification from tandem mass spectrometry remains a bottleneck in untargeted settings where spectral libraries are incomplete. Current deep learning approaches fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch.", "method": "SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. This treats structure identification as a geometric alignment problem.", "result": "Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small.", "conclusion": "Aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch for small-molecule identification from MS/MS spectra."}}
{"id": "2601.17897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17897", "abs": "https://arxiv.org/abs/2601.17897", "authors": ["Jiayu Liu", "Yinhe Long", "Zhenya Huang", "Enhong Chen"], "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis", "comment": null, "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.", "AI": {"tldr": "UniCog is a unified framework that analyzes LLM cognition through a latent mind space, revealing a Pareto principle where a shared reasoning core is complemented by ability-specific signatures, and enables performance improvements via latent-informed candidate prioritization.", "motivation": "Existing interpretability methods are limited in explaining how cognitive abilities are engaged during LLM reasoning, despite growing evidence that LLM cognitive processes differ fundamentally from human cognition.", "method": "UniCog formulates a latent variable model that encodes diverse cognitive abilities from dense model activations into sparse, disentangled latent dimensions, analyzing six advanced LLMs including DeepSeek-V3.2 and GPT-4o.", "result": "The analysis reveals a Pareto principle of LLM cognition (shared reasoning core + ability-specific signatures), shows reasoning failures manifest as anomalous intensity in latent activations, and demonstrates that latent-informed candidate prioritization improves reasoning performance by up to 7.5% across challenging benchmarks.", "conclusion": "UniCog opens a new paradigm in LLM analysis by providing a cognition-grounded view of reasoning dynamics, offering both interpretability insights and practical performance improvements through latent space analysis."}}
{"id": "2601.17207", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17207", "abs": "https://arxiv.org/abs/2601.17207", "authors": ["Maedeh Makki", "Satish Chandran", "Maziar Raissi", "Adrien Grenier", "Behzad Mohebbi"], "title": "NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations", "comment": null, "summary": "We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.", "AI": {"tldr": "NewPINNs integrates neural networks with conventional numerical solvers by having the network produce candidate solutions that are evolved by the solver, then training minimizes the discrepancy between network predictions and solver-evolved states.", "motivation": "To address failure modes of standard physics-informed neural networks (PINNs), including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff/nonlinear regimes.", "method": "Couples neural networks with conventional numerical solvers by integrating the solver directly into the training loop. The network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between network prediction and solver-evolved state.", "result": "Demonstrates effectiveness across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.", "conclusion": "NewPINNs successfully mitigates known PINN failure modes by delegating physics enforcement, boundary conditions, and numerical stability to established numerical solvers, enabling learning of physically admissible solutions without problem-specific loss engineering."}}
{"id": "2601.17915", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17915", "abs": "https://arxiv.org/abs/2601.17915", "authors": ["Saurabh Jha", "Rohan Arora", "Bhavya", "Noah Zheutlin", "Paulina Toro Isaza", "Laura Shwartz", "Yu Deng", "Daby Sow", "Ruchi Mahindru", "Ruchir Puri"], "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation", "comment": null, "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.\n  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "AI": {"tldr": "EoG framework improves LLM agent reliability for open-ended investigations by separating reasoning from control and using dependency graphs for abductive reasoning.", "motivation": "Current LLM agents fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous data. They struggle with hidden dependency structures, bounded context windows requiring premature summarization, and ReAct-style agents' brittleness where results are sensitive to exploration order and lack belief revision mechanisms.", "method": "EoG formulates investigation as abductive reasoning over a dependency graph. It disaggregates the process: an LLM performs bounded local evidence mining and labeling (cause vs symptom), while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier.", "result": "On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "conclusion": "The EoG framework addresses key limitations of current LLM agents in open-ended investigations by separating semantic reasoning from controller duties and using dependency graphs for systematic belief propagation, resulting in more reliable and consistent performance."}}
{"id": "2601.17215", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.17215", "abs": "https://arxiv.org/abs/2601.17215", "authors": ["Ruoqing Zheng", "Chang Sun", "Qibin Liu", "Lauri Laatu", "Arianna Cox", "Benedikt Maier", "Alexander Tapper", "Jose G. F. Coutinho", "Wayne Luk", "Zhiqiang Que"], "title": "JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers", "comment": "15 pages,", "summary": "We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.", "AI": {"tldr": "JetFormer is a versatile Transformer architecture for particle jet tagging at LHC, designed for both offline and online use, achieving high accuracy with computational efficiency and hardware deployability.", "motivation": "Prior approaches for particle jet tagging are often tailored to specific deployment regimes, limiting versatility across the full spectrum of scenarios from high-accuracy offline analysis to ultra-low-latency online triggering at LHC.", "method": "JetFormer is a scalable encoder-only Transformer architecture that processes variable-length sets of particle features without relying on explicit pairwise interactions, and includes a hardware-aware optimization pipeline for compact variants based on multi-objective hyperparameter search, pruning, and quantization.", "result": "On JetClass dataset, JetFormer matches ParT model accuracy within 0.7% with 37.4% fewer FLOPs; on HLS4ML datasets, it outperforms models like MLPs by 3-4% in accuracy; compact versions like JetFormer-tiny enable FPGA deployment with sub-microsecond latency.", "conclusion": "JetFormer unifies high-performance modeling and deployability in a single framework, providing a practical pathway for Transformer-based jet taggers in offline and online LHC environments, with code available for implementation."}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "This survey paper reviews Self-Driving Laboratories (SDLs) in soft matter, framing them as AI agent problems, summarizing methods like Bayesian optimization, planning, and tool use, proposing a taxonomy, and discussing benchmarks and challenges.", "motivation": "SDL provides a demanding testbed for agentic AI with challenges like expensive actions, noisy feedback, constraints, and non-stationarity, motivating connection to established AI principles and supporting development in real laboratories.", "method": "The paper reviews method families including Bayesian optimization and active learning for sample efficiency, planning and reinforcement learning for long-horizon optimization, and tool-using agents for orchestration, emphasizing verifiable and provenance-aware policies.", "result": "It proposes a capability-driven taxonomy for SDL systems based on decision horizon, uncertainty modeling, and other factors, synthesizes benchmark task templates and evaluation metrics, and distills lessons for performance, robustness, and safety.", "conclusion": "The paper outlines open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure to advance SDL research and enable meaningful AI comparison in laboratory settings."}}
{"id": "2601.17224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17224", "abs": "https://arxiv.org/abs/2601.17224", "authors": ["Dmitrii Torbunov", "Yihui Ren", "Lijun Wu", "Yimei Zhu"], "title": "Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning", "comment": null, "summary": "Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.", "AI": {"tldr": "CDI extends from 1D temporal to 2D spatial conditioning for probabilistic parameter inference from spatial observations, validated on CBED diffraction patterns with well-calibrated uncertainty quantification.", "motivation": "Uncertainty quantification is critical for scientific inverse problems to distinguish identifiable parameters from ambiguous ones. While CDI worked for 1D temporal signals, its applicability to higher-dimensional spatial data was unexplored.", "method": "Extend Conditional Diffusion Model-based Inverse Problem Solver (CDI) to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. Validate on convergent beam electron diffraction (CBED) parameter inference using simulated data with ground-truth parameters.", "result": "CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. Standard regression methods mask uncertainty by predicting training set means for poorly constrained parameters.", "conclusion": "CDI successfully extends from temporal to spatial domains, providing genuine uncertainty information required for robust scientific inference in challenging multi-parameter inverse problems like materials characterization."}}
{"id": "2601.17923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17923", "abs": "https://arxiv.org/abs/2601.17923", "authors": ["Ali Najar"], "title": "Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation", "comment": "5 pages", "summary": "Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.", "AI": {"tldr": "Lifelong learning agents in Dark Souls III use hierarchical skill graphs for efficient learning and selective fine-tuning when environments change.", "motivation": "To enable lifelong agents to expand their competence over time without retraining from scratch or overwriting previously learned behaviors in complex real-time control settings.", "method": "Represent combat as a directed skill graph with five reusable skills (camera control, target lock-on, movement, dodging, heal-attack decision), train components in hierarchical curriculum, and use selective fine-tuning when environment changes.", "result": "Targeted fine-tuning of just two skills rapidly recovers performance under limited interaction budget when environment shifts from Phase 1 to Phase 2, demonstrating skill-graph curricula with selective fine-tuning enable evolving agents.", "conclusion": "Skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments."}}
{"id": "2601.17257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17257", "abs": "https://arxiv.org/abs/2601.17257", "authors": ["Javier Porras-Valenzuela", "Samar Hadou", "Alejandro Ribeiro"], "title": "A Constrained Optimization Perspective of Unrolled Transformers", "comment": null, "summary": "We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.", "AI": {"tldr": "A constrained optimization framework makes transformers behave like optimization descent algorithms, enhancing robustness and generalization without sacrificing in-distribution performance.", "motivation": "To create transformer models that mimic optimization descent behavior for better training stability and improved performance on tasks like video denoising and text classification.", "method": "Introduced layerwise descent constraints on the objective function and replaced standard ERM with a primal-dual training scheme, applied to both unrolled and pretrained transformer architectures.", "result": "Constrained transformers showed stronger robustness to perturbations, maintained higher out-of-distribution generalization, and preserved in-distribution performance on video denoising and text classification tasks.", "conclusion": "The constrained optimization framework successfully enables transformers to exhibit descent-like behavior, leading to improved robustness and generalization while maintaining core performance."}}
{"id": "2601.17942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17942", "abs": "https://arxiv.org/abs/2601.17942", "authors": ["Yu-Jie Yang", "Hung-Fu Chang", "Po-An Chen"], "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting", "comment": "29 pages, 22 figures", "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.", "AI": {"tldr": "The paper proposes two approaches for Text-to-SQL: SSEV (self-refinement with ensemble voting) for standard benchmarks, and ReCAPAgent-SQL (multi-agent framework) for complex enterprise databases.", "motivation": "Text-to-SQL technology lowers data analysis barriers but faces challenges with query ambiguity, schema linking complexity, limited generalization across SQL dialects, and domain-specific understanding needs.", "method": "1) SSEV pipeline: Single-Agent Self-Refinement with Ensemble Voting built on PET-SQL, using self-refinement with Weighted Majority Voting and randomized variant without ground-truth data. 2) ReCAPAgent-SQL: Multi-agent framework with specialized agents for planning, knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation.", "result": "SSEV achieves 85.5% execution accuracy on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. ReCAPAgent-SQL achieves 31% execution accuracy on first 100 queries of Spider 2.0-Lite, showing significant improvements for enterprise scenarios.", "conclusion": "The work facilitates scalable Text-to-SQL deployment in practical settings, supporting better data-driven decision-making with lower cost and greater efficiency."}}
{"id": "2601.17260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17260", "abs": "https://arxiv.org/abs/2601.17260", "authors": ["Marco Pollanen"], "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment", "comment": "10 Pages, 5 Figures", "summary": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $\u03b2$) yields progressively \"better\" behavior. We instead treat $\u03b2$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $\u03b2\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $\u03b2$ induces capability losses that persist even after $\u03b2$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $\u03b2$ landscape rather than reliance on margins or aggregate benchmarks.", "AI": {"tldr": "DPO's \u03b2 parameter doesn't simply yield \"better\" behavior with higher values; it creates complex, architecture-dependent effects on reasoning capability, with preference margins sometimes anticorrelating with actual performance.", "motivation": "The study challenges the common assumption that increasing alignment pressure (\u03b2) in DPO progressively improves model behavior, revealing that \u03b2's effects are more complex and can actually harm reasoning capabilities.", "method": "Densely swept \u03b2 parameter across three 7B open-weight model families (Mistral, Llama, Qwen) under fixed DPO recipe, analyzing logic-probe margins, reasoning capability, and training path effects including hysteresis.", "result": "Found sharp non-monotonic capability changes in Mistral (only positive near \u03b2\u224810\u207b\u00b2), architecture-dependent response modes, anticorrelation between preference margins and reasoning capability (r=-0.91 for Llama), and hysteresis effects where high \u03b2 exposure causes persistent capability losses.", "conclusion": "Capability evaluation should be resolved across the \u03b2 landscape rather than relying on preference margins or aggregate benchmarks, as \u03b2 optimization can inadvertently select capability-impaired models."}}
{"id": "2601.18027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18027", "abs": "https://arxiv.org/abs/2601.18027", "authors": ["Chiyuan Fu", "Lyuhao Chen", "Yunze Xiao", "Weihao Xuan", "Carlos Busso", "Mona Diab"], "title": "Sentipolis: Emotion-Aware Agents for Social Simulations", "comment": null, "summary": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.", "AI": {"tldr": "Sentipolis framework enhances LLM agents for social simulation by integrating continuous emotional state representation, dual-speed emotion dynamics, and emotion-memory coupling, improving emotional believability, communication, and continuity, with model-dependent effects and support for studying social dynamics.", "motivation": "Current LLM agents in social simulation often treat emotion as transient, leading to emotional amnesia and weak long-term continuity, limiting realistic emotional behavior in interactions.", "method": "The Sentipolis framework uses continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics to model fast and slow emotional changes, and emotion--memory coupling to link emotional states to memory.", "result": "Across multiple base models and evaluators from thousands of interactions, Sentipolis boosts emotionally grounded behavior, communication, and emotional continuity, with believability gains for higher-capacity models but potential drops for smaller ones, and shows network-level patterns like reciprocal and clustered relationships.", "conclusion": "Sentipolis advances emotional modeling in social simulation, enabling more realistic and continuous emotional behavior in LLM agents, and serves as a tool for exploring complex social dynamics, though its effectiveness varies with model capacity."}}
{"id": "2601.17261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17261", "abs": "https://arxiv.org/abs/2601.17261", "authors": ["Wei Lin", "Yining Jiang", "Qingyu Song", "Qiao Xiang", "Hong Xu"], "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning", "comment": "21 pages in total, including 9 pages of main text, with 4 figures and 3 tables. This manuscript is submitted to arXiv", "summary": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.", "AI": {"tldr": "AGZO introduces activation-guided zeroth-order optimization that uses forward-pass activation structure to create low-rank perturbation subspaces, improving gradient estimation accuracy while maintaining memory efficiency comparable to standard ZO methods.", "motivation": "Zeroth-order optimization is promising for fine-tuning LLMs under memory constraints, but existing methods use isotropic perturbations that ignore valuable structural information available during forward passes.", "method": "AGZO extracts a compact, activation-informed subspace during forward passes and restricts perturbations to this low-rank subspace, leveraging the insight that gradients of linear layers are confined to the subspace spanned by input activations.", "result": "AGZO consistently outperforms state-of-the-art ZO baselines on Qwen3 and Pangu models across benchmarks, significantly narrowing the performance gap with first-order fine-tuning while maintaining almost the same peak memory footprint as other ZO methods.", "conclusion": "Activation-guided subspace perturbation in ZO optimization provides a theoretically grounded and empirically effective approach that improves gradient estimation quality while preserving the memory efficiency advantages of ZO methods for LLM fine-tuning."}}
{"id": "2601.18061", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18061", "abs": "https://arxiv.org/abs/2601.18061", "authors": ["Kiana Jafari", "Paul Ulrich Nikolaus Rust", "Duncan Eddy", "Robbie Fraser", "Nina Vasan", "Darja Djordjevic", "Akanksha Dadlani", "Max Lamparth", "Eugenia Kim", "Mykel Kochenderfer"], "title": "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing", "comment": "17 pages, 7 pages of appendix, 21 tables", "summary": "Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $\u03b1= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.", "AI": {"tldr": "Expert psychiatrists show poor agreement when evaluating LLM mental health responses, especially on safety-critical items like suicide/self-harm, revealing systematic disagreement rooted in different clinical frameworks rather than measurement error.", "motivation": "To test the assumption in Learning from Human Feedback (LHF) that aggregated expert judgments provide valid ground truth for AI training and evaluation, particularly in high-stakes domains like mental health where expert consensus is essential.", "method": "Three certified psychiatrists independently evaluated LLM-generated mental health responses using a calibrated rubric. They measured inter-rater reliability using ICC and Krippendorff's \u03b1, and conducted qualitative interviews to understand the nature of disagreements.", "result": "Inter-rater reliability was consistently poor (ICC 0.087-0.295), below acceptable thresholds. Disagreement was highest on safety-critical items, with suicide/self-harm responses showing the greatest divergence. One factor had negative reliability (Krippendorff's \u03b1 = -0.203). Qualitative analysis revealed disagreement stems from coherent but incompatible clinical frameworks: safety-first, engagement-centered, and culturally-informed orientations.", "conclusion": "Expert disagreement in safety-critical AI evaluation is a sociotechnical phenomenon where professional experience introduces principled divergence. Aggregated labels function as arithmetic compromises that erase professional philosophies. The paper recommends shifting from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement."}}
{"id": "2601.17274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17274", "abs": "https://arxiv.org/abs/2601.17274", "authors": ["Samar Hadou", "Alejandro Ribeiro"], "title": "Unrolled Neural Networks for Constrained Optimization", "comment": null, "summary": "In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.", "AI": {"tldr": "CDU: A neural framework that unrolls dual ascent algorithms using two coupled networks to solve constrained optimization problems with strong OOD generalization.", "motivation": "Traditional optimization algorithms can be slow, and neural networks lack interpretability for constrained problems. The authors aim to create accelerated, learnable counterparts to dual ascent algorithms that maintain mathematical structure while being trainable.", "method": "Constrained Dual Unrolling (CDU) uses two coupled neural networks: a primal network that finds stationary points of the Lagrangian for given dual multipliers, and a dual network that generates trajectories toward optimal multipliers. Training involves nested optimization with alternating updates between primal and dual networks, imposing primal-descent and dual-ascent constraints through constrained learning.", "result": "The framework produces near-optimal, near-feasible solutions for mixed-integer quadratic programs and wireless power allocation problems, demonstrating strong out-of-distribution generalization capabilities.", "conclusion": "CDU successfully bridges optimization theory and deep learning by creating interpretable, accelerated neural solvers for constrained optimization problems that maintain mathematical structure while achieving good generalization performance."}}
{"id": "2601.18067", "categories": ["cs.AI", "cs.NE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18067", "abs": "https://arxiv.org/abs/2601.18067", "authors": ["Wei-Po Hsin", "Ren-Hao Deng", "Yao-Ting Hsieh", "En-Ming Huang", "Shih-Hao Hung"], "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization", "comment": "17 pages, 6 figures, 8 tables", "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.", "AI": {"tldr": "EvolVE is a framework that applies multiple evolution strategies to chip design automation, with MCTS for functional correctness and IGR for optimization, achieving state-of-the-art results on benchmarks and significant PPA improvements on industry-scale problems.", "motivation": "Verilog design is labor-intensive and requires extensive expertise. LLMs offer automation potential but struggle with hardware logic and concurrency due to limited training data and sequential reasoning.", "method": "Presents EvolVE framework analyzing multiple evolution strategies: Monte Carlo Tree Search (MCTS) for functional correctness and Idea-Guided Refinement (IGR) for optimization. Uses Structured Testbench Generation (STG) to accelerate evolution. Introduces IC-RTL benchmark for complex optimization problems from industry contests.", "result": "Achieves 98.1% on VerilogEval v2 and 92% on RTLLM v2. On IC-RTL industry-scale suite, surpasses contest participant implementations, reducing PPA product by up to 66% in Huffman Coding and 17% geometric mean across all problems.", "conclusion": "EvolVE establishes new state-of-the-art in chip design automation by effectively combining evolution strategies tailored to different aspects of hardware design, demonstrating significant improvements in both correctness and optimization metrics."}}
{"id": "2601.17275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17275", "abs": "https://arxiv.org/abs/2601.17275", "authors": ["Lianlei Shan", "Han Chen", "Yixuan Wang", "Zhenjie Liu", "Wei Li"], "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning", "comment": "12 pages,", "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.", "AI": {"tldr": "DLR is a latent-space bidirectional contrastive RL framework that shifts reasoning chain exploration from token-level to continuous latent space, using a lightweight assistant model to sample latent encodings and a frozen main model for decoding, enabling stable training and longer reasoning chains without catastrophic forgetting.", "motivation": "LLMs perform statistical fitting rather than systematic logical deduction for complex reasoning tasks. Traditional RL approaches face challenges in high-dimensional discrete token spaces: sample inefficiency, high gradient variance, and catastrophic forgetting.", "method": "Proposes DeepLatent Reasoning (DLR) with: 1) Lightweight assistant model to sample K reasoning chain encodings in latent space, 2) Dual reward mechanism (correctness + formatting) to filter high-value latent trajectories, 3) Frozen main model for single-pass decoding, 4) Contrastive learning objective for directed exploration in latent space.", "result": "Under comparable GPU budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates sustainable accumulation of reasoning capabilities while mathematically eliminating catastrophic forgetting.", "conclusion": "DLR provides a viable path toward reliable and scalable reinforcement learning for LLMs by shifting reasoning exploration to latent space and keeping main model parameters frozen, addressing fundamental structural bottlenecks of traditional RL approaches."}}
{"id": "2601.18119", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18119", "abs": "https://arxiv.org/abs/2601.18119", "authors": ["Jing Ye", "Yiwen Duan", "Yonghong Yu", "Victor Ma", "Yang Gao", "Xing Chen"], "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?", "comment": null, "summary": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.\n  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.\n  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.", "AI": {"tldr": "OurBench is a novel benchmark for SQL debugging, automating realistic bug injection and execution-free evaluation, showing LLMs struggle significantly on complex enterprise queries.", "motivation": "SQL is critical in enterprise data but hard to generate correctly in one go, requiring debugging, and there's a lack of benchmarks for enterprise-level SQL reasoning.", "method": "Developed OurBench with automated construction using reverse engineering to inject bugs and execution-free evaluation framework; includes syntax (OurBenchSyn) and semantic (OurBenchSem) queries.", "result": "Evaluation of 30 LLMs shows low accuracy: best model achieves 36.46% on syntax and 32.17% on semantic errors, most below 20%; explores strategies and challenges for improvement.", "conclusion": "OurBench highlights the gap in LLM performance on SQL debugging, providing a foundation and directions for future research in enterprise SQL tooling."}}
{"id": "2601.17301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17301", "abs": "https://arxiv.org/abs/2601.17301", "authors": ["Yunhui Liu", "Tieke He", "Yongchao Liu", "Can Yi", "Hong Jin", "Chuntao Hong"], "title": "Tabular Foundation Models are Strong Graph Anomaly Detectors", "comment": "Accepted by WWW 2026 (Short Paper)", "summary": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a \"one model per dataset\" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a \"one-for-all\" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by \"flattening\" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.", "AI": {"tldr": "TFM4GAD adapts tabular foundation models for graph anomaly detection by flattening graphs into augmented feature tables, achieving better performance than specialized GAD models without retraining.", "motivation": "Existing GAD methods require training a separate model for each dataset, leading to high computational costs, substantial data demands, and poor generalization to new datasets. There is a need for a \"one-for-all\" foundation model that can detect anomalies across diverse graphs without retraining.", "method": "The framework adapts TFMs by \"flattening\" the graph into an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations, then processes this table using a TFM in a fully in-context regime.", "result": "Extensive experiments show TFM4GAD achieves significant performance gains over specialized GAD models trained from scratch on multiple datasets with various TFM backbones.", "conclusion": "TFM4GAD offers a new perspective and practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors, enabling effective cross-domain anomaly detection without retraining."}}
{"id": "2601.18123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18123", "abs": "https://arxiv.org/abs/2601.18123", "authors": ["Muhammad Ibrahim Khan", "Bivin Pradeep", "James Brusey"], "title": "Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters", "comment": "Accepted at AAAI 2026", "summary": "Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.", "AI": {"tldr": "The paper introduces deadline-aware control for immersion water heaters using reinforcement learning, showing PPO achieves significant energy savings compared to bang-bang and MCTS methods.", "motivation": "Domestic immersion water heaters often operate inefficiently by ignoring predictable demand windows and ambient losses, leading to unnecessary energy consumption.", "method": "Developed a Gymnasium environment modeling the system with first-order thermal losses and discrete actions. Compared a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy across various initial temperatures, deadlines, and target temperatures.", "result": "PPO achieved the lowest energy consumption across sweeps, e.g., 3.23 kWh at a 60-step horizon, representing savings of 26% at 30 steps and 69% at 90 steps compared to other methods, with similar gains in specific trajectories.", "conclusion": "Learned deadline-aware control, particularly via PPO, effectively reduces energy consumption with minimal inference cost after training, offering a more efficient alternative to traditional continuous operation."}}
{"id": "2601.17303", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.17303", "abs": "https://arxiv.org/abs/2601.17303", "authors": ["Samaresh Kumar Singh", "Joyjit Roy"], "title": "Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach", "comment": "9 pages, 8 figures, and Submitted to IEEE SoutheastCon 2026", "summary": "As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital \"immune system\" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.", "AI": {"tldr": "A decentralized multi-agent swarm architecture using AI agents at edge gateways provides real-time threat detection and response for IIoT networks with sub-millisecond latency, high accuracy, and significant bandwidth reduction.", "motivation": "Centralized security monitoring in large-scale IIoT environments creates latency issues that attackers can exploit, risking compromise of entire manufacturing ecosystems. Traditional static firewalls are inadequate for dynamic IIoT networks.", "method": "Proposes a decentralized multi-agent swarm (DMAS) architecture with autonomous AI agents at each edge gateway, using lightweight peer-to-peer communication for cooperative anomaly detection without cloud infrastructure. Includes consensus-based threat validation (CVT) where agents vote on threat levels for instant quarantine.", "result": "Experimental testbed with 2000 IIoT devices showed: sub-millisecond response times (average 0.85ms), 97.3% accuracy detecting malicious activity under high load, 87% accuracy detecting zero-day attacks, prevention of real-time cascading failures, and 89% reduction in network bandwidth compared to cloud-based solutions.", "conclusion": "The DMAS architecture provides superior security for IIoT networks compared to centralized and edge computing approaches, offering real-time threat detection, high accuracy, bandwidth efficiency, and resilience against cascading failures in industrial environments."}}
{"id": "2601.18130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18130", "abs": "https://arxiv.org/abs/2601.18130", "authors": ["Jize Wang", "Han Wu", "Zhiyuan You", "Yiming Song", "Yijun Wang", "Zifei Shan", "Yining Li", "Songyang Zhang", "Xinyi Le", "Cailian Chen", "Xinping Guan", "Dacheng Tao"], "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents", "comment": null, "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.", "AI": {"tldr": "RouteMoA reduces Mixture-of-Agents costs by 89.8% and latency by 63.6% using dynamic routing with a scorer, judges, and ranking.", "motivation": "Mixture-of-Agents (MoA) enhances LLM performance with layered collaboration, but dense topology increases costs and latency, and existing filtering methods require full inference before judging, lacking effective cost reduction and handling for large model pools.", "method": "Proposes RouteMoA with dynamic routing: a lightweight scorer screens models by predicting performance from queries without inference; a mixture of judges refines scores via self- and cross-assessment using existing outputs; and a ranking mechanism balances performance, cost, and latency for model selection.", "result": "RouteMoA outperforms MoA across tasks and model pool sizes, achieving 89.8% cost reduction and 63.6% latency reduction in large-scale pools.", "conclusion": "RouteMoA effectively addresses MoA's inefficiencies by enabling efficient model collaboration with significant cost and latency savings, making it scalable for practical applications."}}
{"id": "2601.17307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17307", "abs": "https://arxiv.org/abs/2601.17307", "authors": ["Haobing Liu", "Yinuo Zhang", "Tingting Wang", "Ruobing Jiang", "Yanwei Yu"], "title": "Weighted Graph Clustering via Scale Contraction and Graph Structure Learning", "comment": null, "summary": "Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.", "AI": {"tldr": "The document proposes a contractile edge-weight-aware graph clustering network to address challenges of storage, time, and noise in using edge weights for clustering by incorporating graph contraction and attention mechanisms, showing superior performance and efficiency on real-world datasets.", "motivation": "Most existing graph clustering methods do not fully utilize edge weights, which introduces challenges like increased storage and training time, and noise negatively impacts clustering results. Few studies jointly optimize clustering and edge weights to mitigate these issues.", "method": "The method includes a cluster-oriented graph contraction module to reduce graph scale while preserving important nodes, and an edge-weight-aware attention network to identify and weaken noisy connections.", "result": "Extensive experiments on three real-world weighted graph datasets show the model outperforming the best baseline in clustering effectiveness. The graph contraction module significantly reduces training time and storage space.", "conclusion": "The proposed contractile edge-weight-aware graph clustering network effectively addresses storage, time, and noise challenges in using edge weights for clustering, enhancing performance and efficiency in real-world scenarios."}}
{"id": "2601.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18132", "abs": "https://arxiv.org/abs/2601.18132", "authors": ["Xi Chen", "Hongru Zhou", "Huahui Yi", "Shiyu Feng", "Hanyu Zhou", "Tiancheng He", "Mingke You", "Li Wang", "Qiankun Li", "Kun Wang", "Weili Fu", "Kang Li", "Jian Li"], "title": "RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening", "comment": "28 page, 3 figures", "summary": "Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.", "AI": {"tldr": "RareAlert is an early screening system using LLM reasoning and machine learning to predict rare disease risk from initial visit data, achieving high accuracy and enabling scalable deployment.", "motivation": "Missed and delayed diagnosis in rare diseases due to insufficient triage processes at initial clinical encounters, necessitating universal screening to reduce diagnostic delays.", "method": "Develop RareAlert by integrating reasoning from ten LLMs, calibrating and weighting signals with machine learning, and distilling into a single deployable model; use RareBench dataset of 158,666 cases covering 33 disease categories.", "result": "RareAlert, based on Qwen3-4B, achieved an AUC of 0.917 on test set, outperforming ML ensembles and LLMs like GPT-5 and Claude-3.7-Sonnet, demonstrating effective alignment of reasoning.", "conclusion": "RareAlert redefines rare disease identification through universal uncertainty resolution, offering accurate, privacy-preserving, scalable screening suitable for local deployment to improve early detection."}}
{"id": "2601.17309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17309", "abs": "https://arxiv.org/abs/2601.17309", "authors": ["Anagha Sabu", "Vidhya S", "Narayanan C Krishnan"], "title": "PAR: Plausibility-aware Amortized Recourse Generation", "comment": null, "summary": "Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.", "AI": {"tldr": "PAR is an amortized approximate inference method for algorithmic recourse that generates highly likely, valid, and customized counterfactuals efficiently using tractable probabilistic models.", "motivation": "Algorithmic recourse needs to provide actionable recommendations that flip unfavorable model decisions while being realistic and feasible. Existing methods may not efficiently generate recourses that are both highly plausible and respect constraints like validity, similarity, and sparsity.", "method": "Formulates recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution. Uses PAR, an amortized approximate inference procedure with tractable probabilistic models for exact likelihood evaluation and efficient gradient propagation. Includes neighborhood-based conditioning for customized recourse generation and is trained to maximize likelihood under accepted-class distribution while minimizing under denied-class distribution.", "result": "PAR demonstrates superior performance over state-of-the-art approaches on widely used algorithmic recourse datasets, generating recourses that are valid, similar to factuals, sparse, and highly plausible while being computationally efficient.", "conclusion": "PAR provides an effective solution for algorithmic recourse by framing it as a constrained MAP inference problem and using amortized approximate inference with tractable probabilistic models, resulting in high-quality, efficient recourse generation that outperforms existing methods."}}
{"id": "2601.18137", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18137", "abs": "https://arxiv.org/abs/2601.18137", "authors": ["Yinger Zhang", "Shutong Jiang", "Renhao Li", "Jianhong Tu", "Yang Su", "Lianghao Deng", "Xudong Guo", "Chenxu Lv", "Junyang Lin"], "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "comment": null, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "AI": {"tldr": "DeepPlanning is a challenging benchmark for long-horizon agent planning that requires proactive information gathering, local constraint reasoning, and global optimization, exposing limitations in current frontier LLMs.", "motivation": "Current agent evaluation benchmarks focus on local, step-level reasoning rather than global constrained optimization (time/financial budgets) that requires genuine planning. Existing LLM planning benchmarks underrepresent active information gathering and fine-grained local constraints typical of real-world settings.", "method": "Introduces DeepPlanning benchmark featuring multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization.", "result": "Evaluations show that even frontier agentic LLMs struggle with these problems, highlighting the need for reliable explicit reasoning patterns and parallel tool use for better effectiveness-efficiency trade-offs.", "conclusion": "DeepPlanning addresses gaps in current planning benchmarks and reveals promising directions for improving agentic LLMs over long planning horizons. The code and data are open-sourced to support future research."}}
{"id": "2601.17329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17329", "abs": "https://arxiv.org/abs/2601.17329", "authors": ["Tiejin Chen", "Xiaoou Liu", "Vishnu Nandam", "Kuan-Ru Liou", "Hua Wei"], "title": "Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment", "comment": "Accetped to Findings of EACL", "summary": "Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \\emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \\emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.", "AI": {"tldr": "CFA (Conformal Feedback Alignment) leverages conformal prediction to improve alignment by quantifying answer reliability for better preference weighting in DPO/PPO-style training.", "motivation": "Address limitations in preference-based alignment methods like RLHF, which often struggle with noisy and inconsistent labels due to ignoring answer-level reliability, focusing only on preference weighting.", "method": "Proposes CFA, a framework that uses Conformal Prediction (CP) to construct prediction sets with controllable coverage for quantifying answer-level reliability, then aggregates these into weights for DPO- and PPO-style training.", "result": "Experiments across datasets show that CFA improves alignment robustness and data efficiency, demonstrating better performance by complementary modeling of answer-side uncertainty.", "conclusion": "CFA effectively grounds preference weighting in statistical guarantees, complementing existing methods to yield more robust and data-efficient alignment; code is provided for implementation."}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "Success conditioning (goal-conditioned RL, Decision Transformers, etc.) exactly solves a trust-region optimization problem with automatic \u03c7\u00b2 divergence constraint, ensuring conservative policy improvement without degradation or dangerous distribution shift.", "motivation": "The motivation is to understand the theoretical foundation behind widely used success conditioning techniques (rejection sampling with SFT, goal-conditioned RL, Decision Transformers) and clarify what optimization problem they actually solve, which has remained unclear despite their popularity.", "method": "The paper proves that success conditioning exactly solves a trust-region optimization problem that maximizes policy improvement subject to an automatically determined \u03c7\u00b2 divergence constraint. The analysis establishes an identity relating relative policy improvement, magnitude of policy change, and action-influence (measuring how random action variation affects success rates).", "result": "The key result is that success conditioning emerges as a conservative improvement operator that cannot degrade performance or induce dangerous distribution shift. When it fails, it does so observably by hardly changing the policy at all. The theory is applied to return thresholding, showing it can amplify improvement but at the cost of potential misalignment with the true objective.", "conclusion": "Success conditioning provides a theoretically grounded, conservative approach to policy improvement that guarantees safety from performance degradation while maintaining observability of failure modes. The analysis connects practical techniques to formal optimization theory and reveals the trade-offs involved in common practices like return thresholding."}}
{"id": "2601.17330", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17330", "abs": "https://arxiv.org/abs/2601.17330", "authors": ["Laurent Caraffa"], "title": "Thermodynamically Optimal Regularization under Information-Geometric Constraints", "comment": "7 pages, 0 figures", "summary": "Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.\n  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.\n  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.", "AI": {"tldr": "A theoretical framework linking thermodynamic optimality, information geometry, and regularization in machine learning, showing Fisher-Rao metric as unique admissible geometry and deriving implications for efficiency.", "motivation": "To unify diverse regularization techniques like weight decay and dropout with thermodynamic efficiency bounds and information geometry.", "method": "Proposes a framework under three assumptions (intrinsic information measure, max-entropy belief states, quasi-static processes), proving conditional optimality theorem and deriving geometries for Gaussian and circular models.", "result": "Fisher-Rao metric is uniquely admissible; thermodynamically optimal regularization minimizes squared Fisher-Rao distance; classical regularization schemes lack thermodynamic optimality; introduces thermodynamic efficiency measure and testable predictions.", "conclusion": "Provides a principled geometric and thermodynamic foundation for regularization in machine learning, bridging theory and practice with implications for energy-efficient learning."}}
{"id": "2601.18197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18197", "abs": "https://arxiv.org/abs/2601.18197", "authors": ["Shaokang Wang", "Pei Fu", "Ruoceng Zhang", "Shaojie Zhang", "Xiuwen Xi", "Jiahui Yang", "Bin Qin", "Ying Huang", "Zhenbo Luo", "Jian Luan"], "title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.", "AI": {"tldr": "GAIA: A GUI Action Critic's Data Flywheel System trains models with iterative critic capabilities to prevent irreversible errors in GUI agents, enhancing performance through self-improving cycles.", "motivation": "Address the critical challenge of irreversibility in GUI agent operations, where a single erroneous action can cause catastrophic deviations, by enhancing the models' capabilities to prevent such errors.", "method": "Train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent; this critic evaluates immediate action correctness to select higher-success operations. Then, use the initial critic to guide agent actions, collect refined positive/negative samples, and initiate a self-improving cycle. Augmented data trains a second-round critic with enhanced discernment capability.", "result": "Experiments on various datasets show that the ICM improves test-time performance of multiple closed-source and open-source models, and performance gradually improves as data is recycled through the flywheel system.", "conclusion": "The GAIA framework, by integrating iterative critic capabilities through the Data Flywheel System, effectively addresses the irreversibility challenge in GUI agents by enabling continuous self-improvement and enhanced Test-Time Scaling, leading to improved performance that scales with data recycling for closed-source and open-source models."}}
{"id": "2601.17334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17334", "abs": "https://arxiv.org/abs/2601.17334", "authors": ["Yufeng Huang"], "title": "Power-based Partial Attention: Bridging Linear-Complexity and Full Attention", "comment": "12 pages, 3 figures", "summary": "It is widely accepted from transformer research that \"attention is all we need\", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \\leq p \\leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.", "AI": {"tldr": "Paper introduces PPA, a sub-quadratic attention mechanism, showing S-curve performance transition and existence of p<1 matching full attention results.", "motivation": "To systematically quantify if sub-quadratic attention can match full transformer attention performance, addressing the need to reduce computational complexity from O(L^2).", "method": "Introduces power-based partial attention (PPA) with complexity O(L^(1+p)), where 0\u2264p\u22641, enabling exploration of performance scaling from linear (p=0, sliding-window) to quadratic (p=1, full) attention.", "result": "Experiments show an S-curve behavior with performance transition from sliding-window to full attention over a narrow p range, plateauing near p=1; existence of p<1 such that O(L^(1+p)) attention achieves similar results as O(L^2) full attention.", "conclusion": "Sub-quadratic attention mechanisms like PPA are sufficient for comparable transformer performance, with potential for computational efficiency gains without significant accuracy loss."}}
{"id": "2601.18202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18202", "abs": "https://arxiv.org/abs/2601.18202", "authors": ["Fangyuan Xu", "Rujun Han", "Yanfei Chen", "Zifeng Wang", "I-Hung Hsu", "Jun Yan", "Vishy Tirumalashetty", "Eunsol Choi", "Tomas Pfister", "Chen-Yu Lee"], "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback", "comment": null, "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.", "AI": {"tldr": "SAGE is an automated pipeline for generating difficult, high-quality question-answer pairs for deep search from documents to train improved agents.", "motivation": "Human annotation for deep search tasks is costly due to complex reasoning needed across documents.", "method": "Uses a generator to propose QA pairs and a search agent which solves them, iterating in multiple rounds to refine difficulty.", "result": "Generated questions require diverse reasoning; synthetic data improves agent performance by up to 23% on benchmarks and enables adaptation to live search without retraining.", "conclusion": "SAGE provides an effective way to create synthetic data for training robust deep search agents, reducing reliance on human labels."}}
{"id": "2601.17357", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17357", "abs": "https://arxiv.org/abs/2601.17357", "authors": ["Davide Ettori"], "title": "Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory", "comment": "Master thesis, MS in Computer Science, University of Illinois Chicago, defended November 21, 2025", "summary": "Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.", "AI": {"tldr": "Thesis proposes spectral geometry framework using eigenvalue analysis of hidden activations to improve reliability and reduce computational costs in large neural networks, with two main contributions: EigenTrack for hallucination detection and RMT-KD for model compression.", "motivation": "Large language models and deep neural networks achieve strong performance but suffer from reliability issues (hallucinations, out-of-distribution behavior) and high computational costs, creating a need for more robust monitoring and efficient compression methods.", "method": "Uses spectral geometry and random matrix theory to analyze eigenvalue structure of hidden activations. EigenTrack detects hallucinations using spectral features and their temporal dynamics. RMT-KD compresses models by identifying informative spectral components and applying iterative knowledge distillation.", "result": "Developed a unified framework with two working methods: EigenTrack for real-time detection of hallucinations and out-of-distribution behavior, and RMT-KD for producing compact, efficient models while preserving accuracy through principled compression.", "conclusion": "Spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks, offering a unified approach to address both reliability and efficiency challenges."}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "Investigates agentic post-training for LLM agents in unseen domains, identifying state information richness and planning complexity as key factors for cross-domain generalization, with practical recommendations for improvement.", "motivation": "Generalist LLM agents are post-trained on narrow environments but deployed in broader, unseen domains, creating a challenge for robustness when test domains are unknown.", "method": "Analyzes environment properties (state information richness and planning complexity) and modeling choices (e.g., SFT warmup, step-by-step thinking) to see their impact on out-of-domain performance, using domains like Sokoban and ALFWorld, and proposes a randomization technique to increase state richness.", "result": "Finds that state information richness and planning complexity strongly correlate with cross-domain generalization, while domain realism is less important; increasing state richness via lightweight randomization improves robustness.", "conclusion": "Focusing on environment factors like state richness and planning complexity, along with careful modeling choices, can enhance cross-domain generalization in LLM agents, with simple techniques like state randomization offering practical benefits."}}
{"id": "2601.17360", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17360", "abs": "https://arxiv.org/abs/2601.17360", "authors": ["Jiankai Jin", "Xiangzheng Zhang", "Zhao Liu", "Deyue Zhang", "Quanchen Zou"], "title": "Robust Privacy: Inference-Time Privacy through Certified Robustness", "comment": null, "summary": "Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\u03c3=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.", "AI": {"tldr": "This paper introduces Robust Privacy (RP), an inference-time privacy notion based on certified robustness, where an input enjoys R-Robust Privacy if the model's prediction is invariant within a radius-R neighborhood, protecting against attribute inference and model inversion attacks.", "motivation": "To address privacy vulnerabilities in machine learning systems where personalized outputs can leak sensitive input attributes at inference time, requiring a formal privacy notion and practical enhancements.", "method": "Introduce Robust Privacy (RP), inspired by certified robustness, and develop Attribute Privacy Enhancement (APE) to translate input-level invariance into attribute-level privacy, tested in a controlled recommendation task with experiments on model inversion attacks.", "result": "RP expands the inference interval for sensitive attributes in recommendation tasks and significantly reducesto reduce attack success rates (ASR), e.g., from 73% to 4% at small noise levels, with only partial performance degradation.", "conclusion": "RP provides an effective inference-time privacy framework that enhances privacy while maintaining model utility, offering practical mitigation against attribute inference and model inversion attacks in personalized ML systems."}}
{"id": "2601.18225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18225", "abs": "https://arxiv.org/abs/2601.18225", "authors": ["Pei Wang", "Yanan Wu", "Xiaoshuai Song", "Weixun Wang", "Gengru Chen", "Zhongwen Li", "Kezhong Yan", "Ken Deng", "Qi Liu", "Shuaibing Zhao", "Shaopan Xiong", "Xuepeng Liu", "Xuefeng Chen", "Wanxi Deng", "Wenbo Su", "Bo Zheng"], "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.", "AI": {"tldr": "ShopSimulator is a Chinese shopping simulation environment for evaluating LLM-based agents, revealing their limitations in tasks like deep search and personalization, and suggesting SFT and RL training improves performance.", "motivation": "Existing research lacks a unified simulation environment for evaluating LLM-based agents in e-commerce shopping contexts that can capture personal preferences, multi-turn dialogues, and discrimination of similar products, often focusing on benchmarks without training support.", "method": "Introduces ShopSimulator, a large-scale Chinese shopping environment, to evaluate LLMs across diverse scenarios, analyze errors (e.g., struggles with deep search, personalization balance, and user engagement), and explore training methods like supervised fine-tuning (SFT) and reinforcement learning (RL).", "result": "Evaluation shows even top-performing LLMs achieve less than 40% full-success rate. Error analysis highlights issues in deep search, product selection, balancing personalization, and effective user engagement. Training with SFT and RL yields significant performance improvements.", "conclusion": "ShopSimulator provides a challenging testbed for LLM-based shopping agents, identifying key weaknesses and demonstrating that targeted training (SFT+RL) can effectively enhance agent performance. Code and data are publicly released to facilitate further research."}}
{"id": "2601.17376", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17376", "abs": "https://arxiv.org/abs/2601.17376", "authors": ["Ruijin Hua", "Zichuan Liu", "Kun Zhang", "Yiyuan Yang"], "title": "Diversified Scaling Inference in Time Series Foundation Models", "comment": "23 pages, 16 figures, 9 tables", "summary": "The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.", "AI": {"tldr": "Study examines how Time Series Foundation Models (TSFMs) behave under inference scaling and explores controlled sampling diversity to enhance performance without retraining, proposing RobustMSE metric.", "motivation": "TSFMs mainly rely on large-scale pre-training but lack optimization of inference-time compute; investigation aims to tap this potential by scaling and diversifying sampling.", "method": "Analyzes TSFMs under standard sampling, applies tailored time series perturbations for diversified inference scaling, derives theoretical threshold for diversity-fidelity trade-off, and conducts experiments across models and datasets.", "result": "Diversified inference scaling yields substantial performance gains without parameter updates; RobustMSE metric quantifies headroom performance under fixed budget; effective in parallel environments without retraining.", "conclusion": "Inference design is a critical, compute-efficient dimension for TSFM optimization, enabling reliable performance through diverse large-scale inference."}}
{"id": "2601.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18226", "abs": "https://arxiv.org/abs/2601.18226", "authors": ["Haotian Li", "Shijun Yang", "Weizhen Qi", "Silei Zhao", "Rui Hua", "Mingzhu Song", "Xiaojian Yang", "Chao Peng"], "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks", "comment": null, "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.", "AI": {"tldr": "An agent system that evolves tools through in-situ interactions to adapt to open-ended environments.", "motivation": "Conventional agents fail in open-ended environments due to static toolsets and lack of supervision, necessitating dynamic capability expansion.", "method": "In-Situ Self-Evolving paradigm using tool evolution via short-term feedback; implements Yunjue Agent with Parallel Batch Evolution for efficiency.", "result": "Outperforms baselines on five benchmarks under zero-start setting; general knowledge transfers effectively in warm-start evaluations.", "conclusion": "The approach enables resilient, self-evolving intelligence, demonstrated by empirical success and a proposed convergence metric, with open-sourced resources for future work."}}
{"id": "2601.17396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17396", "abs": "https://arxiv.org/abs/2601.17396", "authors": ["Vashista Nobaub"], "title": "GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems", "comment": "21 pages, 5 figures. Includes theoretical analysis, ablation studies, and experiments on synthetic and real vibration datasets. Code available", "summary": "Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.", "AI": {"tldr": "GO-OSC is a geometry-aware representation learning framework for oscillatory time series that uses canonical latent parameterization and invariant linear geometric probes to detect early-stage degradation, outperforming energy-based methods in sensitivity and robustness.", "motivation": "Early-stage degradation in oscillatory systems, such as phase jitter or frequency drift, often occurs before detectable changes in signal energy. Classical energy-based diagnostics and unconstrained learned representations are structurally insensitive in this regime, leading to delayed or unstable detection.", "method": "Introduces GO-OSC, a geometry-aware representation learning framework that enforces a canonical and identifiable latent parameterization for oscillatory time series. It defines invariant linear geometric probes targeting degradation-relevant directions in latent space and provides theoretical insights on detection power.", "result": "Theoretical results show that under early phase-only degradation, energy-based statistics have zero first-order detection power, while geometric probes achieve strictly positive sensitivity. Experiments on synthetic benchmarks and real vibration datasets validate this with earlier detection, improved data efficiency, and robustness to operating condition changes.", "conclusion": "GO-OSC enhances early degradation detection in oscillatory systems by leveraging geometric distortions and canonical representations, offering a promising alternative to traditional energy-based approaches for improved diagnostics."}}
{"id": "2601.18282", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18282", "abs": "https://arxiv.org/abs/2601.18282", "authors": ["Lei Wei", "Jinpeng Ou", "Xiao Peng", "Bin Wang"], "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "AI": {"tldr": "TAFC enhances LLM function calling by adding explicit reasoning at function and parameter levels through a \"think\" parameter augmentation, improving accuracy and interpretability without modifying model architecture.", "motivation": "Current LLM function calling mechanisms lack explicit reasoning transparency during parameter generation, especially for complex functions with interdependent parameters. Existing approaches like chain-of-thought prompting operate at agent level but fail to provide fine-grained reasoning guidance for individual function parameters.", "method": "Proposes Think-Augmented Function Calling (TAFC) framework with: 1) Universal \"think\" parameter augmentation for articulating decision-making, 2) Dynamic optimization for parameter descriptions to improve reasoning quality, 3) Automatic granular reasoning triggering based on complexity scoring for complex parameters, 4) Reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications and maintains full API compatibility.", "result": "Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "conclusion": "TAFC successfully addresses the transparency gap in LLM function calling by introducing explicit reasoning mechanisms at both function and parameter levels, improving accuracy while maintaining compatibility with existing models and APIs."}}
{"id": "2601.17407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17407", "abs": "https://arxiv.org/abs/2601.17407", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Saif Eddin Jabari"], "title": "Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.", "AI": {"tldr": "D-SENO is a lightweight neural operator for solving PDEs efficiently, achieving faster training and high accuracy by combining dilated convolution and squeeze-and-excitation modules.", "motivation": "Fast and accurate surrogates for physics-driven PDEs are needed in fields like aerodynamics, but existing transformer-based models and neural operators are parameter-heavy, leading to costly training and slow deployment.", "method": "Propose D-SENO, which combines dilated convolution blocks to capture wide receptive fields and squeeze-and-excitation modules for channel-wise attention, allowing adaptive recalibration of feature channels and effective modeling of long-range physical dependencies.", "result": "D-SENO achieves training speed up to approximately 20x faster than standard transformer-based models and neural operators, while matching or surpassing them in accuracy across multiple PDE benchmarks; ablation studies show that removing SE modules leads to a slight performance drop.", "conclusion": "D-SENO is a promising lightweight framework for efficient and accurate PDE inference, with potential applications in various physics-driven domains."}}
{"id": "2601.18308", "categories": ["cs.AI", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18308", "abs": "https://arxiv.org/abs/2601.18308", "authors": ["Geunsik Lim"], "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience", "comment": "19 pages", "summary": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2601.17430", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17430", "abs": "https://arxiv.org/abs/2601.17430", "authors": ["Zichuan Yang", "Yiming Xing"], "title": "Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection", "comment": "47 pages, 26 figures", "summary": "We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT", "AI": {"tldr": "ECC-AHT is an adaptive algorithm for identifying anomalous streams under correlated noise, using active noise cancellation and optimal sample complexity.", "motivation": "To address monitoring and security in cyber-physical systems, where existing methods fail to exploit correlation under correlated noise, requiring efficient algorithms for anomaly detection.", "method": "Proposes ECC-AHT, which selects continuous, constrained measurements to maximize Chernoff information between hypotheses, enabling active noise cancellation through differential sensing.", "result": "ECC-AHT achieves optimal sample complexity guarantees and outperforms state-of-the-art baselines in synthetic and real-world correlated environments.", "conclusion": "ECC-AHT effectively solves the problem of identifying anomalous subsets under correlated noise with improved performance and efficiency."}}
{"id": "2601.18353", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18353", "abs": "https://arxiv.org/abs/2601.18353", "authors": ["Tuhin Chakrabarty", "Paramveer S. Dhillon"], "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books", "comment": "Proceedings of CHI 2026 Conference (To Appear)", "summary": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.", "AI": {"tldr": "AI's ability to emulate author styles challenges human creative superiority, with experts preferring human output initially but AI after fine-tuning, leading to identity crises among writers and questions about creative labor.", "motivation": "To challenge the assumption that creative writing is uniquely human by investigating whether Generative AI can effectively emulate acclaimed author styles and how this impacts perceptions of writing quality and expertise.", "method": "Conducted a behavioral experiment with 28 MFA writers (experts) competing against three LLMs to emulate 50 critically acclaimed authors, using blind pairwise comparisons judged by 28 expert judges and 131 lay judges under in-context prompting and fine-tuning conditions.", "result": "Experts preferred human writing in 82.7% of cases with in-context prompting, but this reversed to 62% preference for AI after fine-tuning; lay judges consistently preferred AI writing. Expert writers experienced identity crisis and eroded aesthetic confidence upon preferring AI.", "conclusion": "The findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor, as AI's capabilities in emulating styles disrupt human notions of good writing and artistic identity."}}
{"id": "2601.17441", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17441", "abs": "https://arxiv.org/abs/2601.17441", "authors": ["Ondrej Bohdal", "Taha Ceritli", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Data-driven Clustering and Merging of Adapters for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.", "AI": {"tldr": "A novel method (D2C) is proposed for clustering task-specific adapters in on-device LLMs to create multi-task adapters, improving performance under storage constraints.", "motivation": "On-device LLMs use adapters for downstream tasks, but storing all adapters is infeasible due to memory limits; selecting representative adapters that generalize across tasks is an unexplored challenge.", "method": "D2C: a method for adapter clustering using minimal task-specific examples (e.g., 10 per task) with iterative optimization to refine clusters, merging adapters within each cluster.", "result": "Experimental results show that the method effectively boosts performance for considered storage budgets.", "conclusion": "The proposed D2C method successfully addresses the adapter selection challenge for resource-constrained devices, enhancing generalization across multiple tasks."}}
{"id": "2601.18381", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18381", "abs": "https://arxiv.org/abs/2601.18381", "authors": ["Yinghan Hou", "Zongyou Yang"], "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito", "comment": "14 pages, 7 figures", "summary": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.", "AI": {"tldr": "An AI agent framework is developed to transform legacy finite difference codes into the Devito environment using RAG, LLMs, and a LangGraph-based workflow.", "motivation": "To facilitate the transformation of legacy finite difference implementations into the Devito environment through AI-assisted methods.", "method": "The framework combines RAG and open-source LLMs in a hybrid LangGraph architecture, builds a Devito knowledge graph via document parsing and community detection, optimizes with GraphRAG, uses reverse engineering for query strategies, and employs a multi-stage retrieval pipeline with parallel searching and semantic analysis. Code synthesis is constrained by Pydantic, and validation integrates static analysis with G-Eval.", "result": "The system enables precise contextual retrieval, structured code synthesis, and comprehensive validation covering correctness, structure, math, and API compliance, implemented on LangGraph with concurrent processing.", "conclusion": "The framework incorporates feedback mechanisms inspired by reinforcement learning, transitioning from static code translation to dynamic, adaptive analytical behavior for improved legacy code transformation."}}
{"id": "2601.17449", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17449", "abs": "https://arxiv.org/abs/2601.17449", "authors": ["Yusheng Zhao", "Jiaye Xie", "Qixin Zhang", "Weizhi Zhang", "Xiao Luo", "Zhiping Xiao", "Philip S. Yu", "Ming Zhang"], "title": "DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise", "comment": null, "summary": "Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.", "AI": {"tldr": "DREAM: A method for graph neural networks with label noise using dual-standard selection and dynamic optimization.", "motivation": "Existing methods for graph learning with label noise struggle to distinguish reliable from unreliable nodes and neglect relational information in graph topology.", "method": "Proposes DREAM, which uses relation-informed dynamic optimization with a dual-standard selection strategy based on node proximity and graph topology to compute semantic homogeneity for reliability evaluation.", "result": "Extensive experiments on six graph datasets under three noise types show DREAM outperforms competing baselines.", "conclusion": "DREAM effectively handles label noise in graphs by integrating dual-standard selection, semantic homogeneity, and dynamic optimization, with theoretical analysis supporting its design."}}
{"id": "2601.18383", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18383", "abs": "https://arxiv.org/abs/2601.18383", "authors": ["Zhenyuan Guo", "Tong Chen", "Wenlong Meng", "Chen Gong", "Xin Yu", "Chengkun Wei", "Wenzhi Chen"], "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "AI": {"tldr": "DynTS method identifies and retains only decision-critical tokens in reasoning traces to optimize memory and compute in Large Reasoning Models.", "motivation": "Large Reasoning Models incur high memory and computational costs due to lengthy reasoning trace generation, with analysis showing only some tokens are critical for decision-making.", "method": "Proposes Dynamic Thinking-Token Selection (DynTS) that uses attention maps to identify decision-critical tokens and retains their KV cache states during inference, evicting redundant entries.", "result": "DynTS reduces memory footprint and computational overhead by selectively keeping key tokens, improving efficiency of Large Reasoning Models without compromising accuracy.", "conclusion": "Focusing on critical tokens in reasoning traces via DynTS offers an effective way to enhance the efficiency of Large Reasoning Models, balancing performance and resource usage."}}
{"id": "2601.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17467", "abs": "https://arxiv.org/abs/2601.17467", "authors": ["Jianxiong Zhang", "Bing Guo", "Yuming Jiang", "Haobo Wang", "Bo An", "Xuefeng Du"], "title": "Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping", "comment": null, "summary": "Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.", "AI": {"tldr": "ARS improves hallucination detection in large reasoning models by learning trace-conditioned representations that encode answer stability through counterfactual perturbations.", "motivation": "Large reasoning models generate seemingly coherent reasoning traces but still produce incorrect answers, making hallucination detection challenging. Existing approaches using trace text or vanilla hidden states are brittle as they can overfit to superficial patterns rather than answer validity.", "method": "ARS (Answer-agreement Representation Shaping) learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. It generates counterfactual answers through small latent interventions (perturbing trace-boundary embeddings) and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that cluster answer-agreeing states together and separate answer-disagreeing ones.", "result": "Experiments show that ARS consistently improves hallucination detection and achieves substantial gains over strong baselines. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training.", "conclusion": "ARS provides an effective approach to hallucination detection in large reasoning models by learning representations that expose latent instability indicative of hallucination risk through answer-agreement shaping."}}
{"id": "2601.18467", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18467", "abs": "https://arxiv.org/abs/2601.18467", "authors": ["Yuhang Zhou", "Kai Zheng", "Qiguang Chen", "Mengkang Hu", "Qingfeng Sun", "Can Xu", "Jingjing Chen"], "title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents", "comment": null, "summary": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.", "AI": {"tldr": "OffSeeker, an 8B offline-trained agent, performs competitively with larger online RL agents using synthesized data, challenging reliance on expensive online training.", "motivation": "Online RL for deep research agents is financially costly; offline training is hindered by lack of high-quality trajectories, necessitating more efficient methods.", "method": "Introduced DeepForge for task synthesis to generate large-scale research queries and curated dataset (66k QA pairs, 33k SFT, 21k DPO) for offline training.", "result": "Trained OffSeeker (8B) offline; extensive evaluations across six benchmarks show it leads among similar-sized agents and competes with 30B online RL systems.", "conclusion": "Expensive online RL is not essential; offline training with synthesized data can produce powerful, effective research agents like OffSeeker."}}
{"id": "2601.17469", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17469", "abs": "https://arxiv.org/abs/2601.17469", "authors": ["Wei Ju", "Wei Zhang", "Siyu Yi", "Zhengyang Mao", "Yifan Wang", "Jingyang Yuan", "Zhiping Xiao", "Ziyue Qiao", "Ming Zhang"], "title": "Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction", "comment": null, "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.", "AI": {"tldr": "A novel GNN approach, ICGNN, designed to handle label noise in graph-structured data by introducing an influence contradiction score for noise detection, a Gaussian mixture model for label verification, a soft correction strategy, and pseudo-labeling for enhanced supervision.", "motivation": "Label noise in real-world graph data, due to annotation errors or inconsistencies, severely impacts the robustness and effectiveness of Graph Neural Networks (GNNs), necessitating methods to mitigate noise challenges.", "method": "Proposes ICGNN with: 1) a noise indicator using an influence contradiction score (ICS) based on graph diffusion to assess label credibility; 2) a Gaussian mixture model to precisely detect noisy nodes; 3) a soft strategy integrating neighbor predictions for label correction; 4) pseudo-labeling of unlabeled nodes for additional supervision.", "result": "ICGNN demonstrates superiority in handling noisy labels over benchmark datasets, improving GNN robustness and performance in graph learning tasks.", "conclusion": "ICGNN effectively addresses label noise in graphs by leveraging structural information for noise detection and correction, enhancing the reliability and efficiency of GNN applications."}}
{"id": "2601.18491", "categories": ["cs.AI", "cs.CC", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18491", "abs": "https://arxiv.org/abs/2601.18491", "authors": ["Dongrui Liu", "Qihan Ren", "Chen Qian", "Shuai Shao", "Yuejin Xie", "Yu Li", "Zhonghao Yang", "Haoyu Luo", "Peng Wang", "Qingyu Liu", "Binxin Hu", "Ling Tang", "Jilin Mei", "Dadi Guo", "Leitao Yuan", "Junyao Yang", "Guanxu Chen", "Qihao Lin", "Yi Yu", "Bo Zhang", "Jiaxuan Guo", "Jie Zhang", "Wenqi Shao", "Huiqi Deng", "Zhiheng Xi", "Wenjie Wang", "Wenxuan Wang", "Wen Shen", "Zhikai Chen", "Haoyu Xie", "Jialing Tao", "Juntao Dai", "Jiaming Ji", "Zhongjie Ba", "Linfeng Zhang", "Yong Liu", "Quanshi Zhang", "Lei Zhu", "Zhihua Wei", "Hui Xue", "Chaochao Lu", "Jing Shao", "Xia Hu"], "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "comment": "40 pages, 26 figures", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "AI": {"tldr": "This paper introduces a three-dimensional taxonomy for categorizing agentic risks and proposes AgentDoG, a diagnostic guardrail framework for AI agent safety and security, with state-of-the-art performance demonstrated across various models.", "motivation": "The rise of AI agents has led to complex safety and security challenges due to autonomous tool use and environmental interactions, but current guardrail models lack agentic risk awareness and transparency in risk diagnosis.", "method": "To address this, the authors propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by source, failure mode, and consequence. Guided by this taxonomy, they develop a fine-grained agentic safety benchmark (ATBench) and the Diagnostic Guardrail framework (AgentDoG) for monitoring agent trajectories and diagnosing root causes of unsafe or unreasonable actions.", "result": "AgentDoG variants are available in sizes of 4B, 7B, and 8B parameters across Qwen and Llama models. Extensive experiments show that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios, with all models and datasets openly released.", "conclusion": "The proposed AgentDoG framework provides a fine-grained, contextual, and transparent approach to agent safety and security by diagnosing root causes and offering insights beyond binary labels, facilitating effective agent alignment and advancing risk management in AI agent applications."}}
