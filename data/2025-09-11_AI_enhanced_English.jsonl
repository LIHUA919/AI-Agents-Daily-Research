{"id": "2509.07993", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.07993", "abs": "https://arxiv.org/abs/2509.07993", "authors": ["Federico Fontana", "Anxhelo Diko", "Romeo Lanzino", "Marco Raoul Marini", "Bachir Kaddar", "Gian Luca Foresti", "Luigi Cinque"], "title": "Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization", "comment": null, "summary": "The rapid evolution of deepfake generation technologies poses critical\nchallenges for detection systems, as non-continual learning methods demand\nfrequent and expensive retraining. We reframe deepfake detection (DFD) as a\nContinual Learning (CL) problem, proposing an efficient framework that\nincrementally adapts to emerging visual manipulation techniques while retaining\nknowledge of past generators. Our framework, unlike prior approaches that rely\non unreal simulation sequences, simulates the real-world chronological\nevolution of deepfake technologies in extended periods across 7 years.\nSimultaneously, our framework builds upon lightweight visual backbones to allow\nfor the real-time performance of DFD systems. Additionally, we contribute two\nnovel metrics: Continual AUC (C-AUC) for historical performance and Forward\nTransfer AUC (FWT-AUC) for future generalization. Through extensive\nexperimentation (over 600 simulations), we empirically demonstrate that while\nefficient adaptation (+155 times faster than full retraining) and robust\nretention of historical knowledge is possible, the generalization of current\napproaches to future generators without additional training remains near-random\n(FWT-AUC $\\approx$ 0.5) due to the unique imprint characterizing each existing\ngenerator. Such observations are the foundation of our newly proposed\nNon-Universal Deepfake Distribution Hypothesis.\n  \\textbf{Code will be released upon acceptance.}", "AI": {"tldr": "This paper reframes deepfake detection as a continual learning problem, proposing an efficient framework that adapts to new manipulation techniques while retaining knowledge of past generators, with significant speed improvements but limited future generalization.", "motivation": "The rapid evolution of deepfake generation technologies requires frequent and expensive retraining of detection systems, making continual learning approaches necessary to handle emerging manipulation techniques efficiently.", "method": "Proposes a continual learning framework for deepfake detection that simulates real-world chronological evolution of deepfake technologies over 7 years, using lightweight visual backbones for real-time performance and introducing two novel metrics (C-AUC and FWT-AUC).", "result": "Achieves 155x faster adaptation than full retraining with robust retention of historical knowledge, but generalization to future generators without additional training remains near-random (FWT-AUC \u2248 0.5) due to unique generator imprints.", "conclusion": "Current approaches can efficiently adapt to new deepfake generators but cannot generalize to unseen future generators, leading to the proposed Non-Universal Deepfake Distribution Hypothesis that each generator leaves a unique imprint."}}
{"id": "2509.08058", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08058", "abs": "https://arxiv.org/abs/2509.08058", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "How Far Are We from True Unlearnability?", "comment": "This paper has been accepted by ICLR 2025", "summary": "High-quality data plays an indispensable role in the era of large models, but\nthe use of unauthorized data for model training greatly damages the interests\nof data owners. To overcome this threat, several unlearnable methods have been\nproposed, which generate unlearnable examples (UEs) by compromising the\ntraining availability of data. Clearly, due to unknown training purposes and\nthe powerful representation learning capabilities of existing models, these\ndata are expected to be unlearnable for models across multiple tasks, i.e.,\nthey will not help improve the model's performance. However, unexpectedly, we\nfind that on the multi-task dataset Taskonomy, UEs still perform well in tasks\nsuch as semantic segmentation, failing to exhibit cross-task unlearnability.\nThis phenomenon leads us to question: How far are we from attaining truly\nunlearnable examples? We attempt to answer this question from the perspective\nof model optimization. To this end, we observe the difference in the\nconvergence process between clean and poisoned models using a simple model\narchitecture. Subsequently, from the loss landscape we find that only a part of\nthe critical parameter optimization paths show significant differences,\nimplying a close relationship between the loss landscape and unlearnability.\nConsequently, we employ the loss landscape to explain the underlying reasons\nfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify the\nunlearnability of parameters based on this explanation. Furthermore, we propose\nan Unlearnable Distance (UD) to measure the unlearnability of data based on the\nSAL distribution of parameters in clean and poisoned models. Finally, we\nconduct benchmark tests on mainstream unlearnable methods using the proposed\nUD, aiming to promote community awareness of the capability boundaries of\nexisting unlearnable methods.", "AI": {"tldr": "Current unlearnable examples fail to maintain cross-task unlearnability, particularly in multi-task scenarios like semantic segmentation. The study analyzes this through model optimization perspectives, proposes new metrics (SAL and UD) to quantify unlearnability, and benchmarks existing methods.", "motivation": "To address the limitation that current unlearnable examples don't maintain effectiveness across multiple tasks despite models' powerful representation learning capabilities, and to understand why they fail to achieve true cross-task unlearnability.", "method": "Analyzed convergence differences between clean and poisoned models using simple architectures, examined loss landscapes, proposed Sharpness-Aware Learnability (SAL) to quantify parameter unlearnability, and developed Unlearnable Distance (UD) to measure data unlearnability based on SAL distributions.", "result": "Found that only part of critical parameter optimization paths show significant differences between clean and poisoned models, revealing a close relationship between loss landscape and unlearnability. Proposed metrics successfully quantify unlearnability boundaries.", "conclusion": "Current unlearnable methods have significant limitations in achieving cross-task unlearnability. The proposed SAL and UD metrics provide effective ways to measure and understand these limitations, helping the community better assess the capabilities of existing unlearnable techniques."}}
{"id": "2509.08086", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08086", "abs": "https://arxiv.org/abs/2509.08086", "authors": ["Michael Kishelev", "Pranab Bhadani", "Wanying Ding", "Vinay Chaudhri"], "title": "JEL: A Novel Model Linking Knowledge Graph entities to News Mentions", "comment": null, "summary": "We present JEL, a novel computationally efficient end-to-end multi-neural\nnetwork based entity linking model, which beats current state-of-art model.\nKnowledge Graphs have emerged as a compelling abstraction for capturing\ncritical relationships among the entities of interest and integrating data from\nmultiple heterogeneous sources. A core problem in leveraging a knowledge graph\nis linking its entities to the mentions (e.g., people, company names) that are\nencountered in textual sources (e.g., news, blogs., etc) correctly, since there\nare thousands of entities to consider for each mention. This task of linking\nmentions and entities is referred as Entity Linking (EL). It is a fundamental\ntask in natural language processing and is beneficial in various uses cases,\nsuch as building a New Analytics platform. News Analytics, in JPMorgan, is an\nessential task that benefits multiple groups across the firm. According to a\nsurvey conducted by the Innovation Digital team 1 , around 25 teams across the\nfirm are actively looking for news analytics solutions, and more than \\$2\nmillion is being spent annually on external vendor costs. Entity linking is\ncritical for bridging unstructured news text with knowledge graphs, enabling\nusers access to vast amounts of curated data in a knowledge graph and\ndramatically facilitating their daily work.", "AI": {"tldr": "JEL is a computationally efficient end-to-end multi-neural network entity linking model that outperforms current state-of-the-art models.", "motivation": "Entity linking is crucial for connecting unstructured text with knowledge graphs, particularly in financial news analytics where JPMorgan spends over $2M annually on external vendor solutions. Accurate entity linking enables access to curated knowledge graph data and improves daily workflow efficiency.", "method": "Novel end-to-end multi-neural network architecture designed for computational efficiency in entity linking tasks.", "result": "JEL beats current state-of-the-art entity linking models in performance.", "conclusion": "The proposed JEL model provides superior entity linking capabilities that can significantly benefit financial news analytics and reduce reliance on expensive external vendor solutions."}}
{"id": "2509.08087", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08087", "abs": "https://arxiv.org/abs/2509.08087", "authors": ["Victor Garcia", "Mariia Sidulova", "Aldo Badano"], "title": "Performance Assessment Strategies for Generative AI Applications in Healthcare", "comment": null, "summary": "Generative artificial intelligence (GenAI) represent an emerging paradigm\nwithin artificial intelligence, with applications throughout the medical\nenterprise. Assessing GenAI applications necessitates a comprehensive\nunderstanding of the clinical task and awareness of the variability in\nperformance when implemented in actual clinical environments. Presently, a\nprevalent method for evaluating the performance of generative models relies on\nquantitative benchmarks. Such benchmarks have limitations and may suffer from\ntrain-to-the-test overfitting, optimizing performance for a specified test set\nat the cost of generalizability across other task and data distributions.\nEvaluation strategies leveraging human expertise and utilizing cost-effective\ncomputational models as evaluators are gaining interest. We discuss current\nstate-of-the-art methodologies for assessing the performance of GenAI\napplications in healthcare and medical devices.", "AI": {"tldr": "Evaluation challenges and methodologies for generative AI in healthcare, focusing on limitations of quantitative benchmarks and emerging human-expert evaluation approaches.", "motivation": "GenAI applications in healthcare require robust evaluation methods that go beyond traditional quantitative benchmarks to ensure clinical validity and generalizability.", "method": "Discusses current state-of-the-art evaluation methodologies including human expertise-based assessment and cost-effective computational models as evaluators.", "result": "Identifies limitations of train-to-the-test overfitting in quantitative benchmarks and highlights the need for evaluation strategies that maintain generalizability across diverse clinical tasks and data distributions.", "conclusion": "Comprehensive evaluation of GenAI in healthcare requires combining quantitative metrics with human expertise and adaptable computational models to ensure clinical relevance and broad applicability."}}
{"id": "2509.08811", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.08811", "abs": "https://arxiv.org/abs/2509.08811", "authors": ["Andrew Jun Lee", "Grace Qiyuan Miao", "Rick Dale", "Alexia Galati", "Hongjing Lu"], "title": "Teamwork as Linear Interpersonal Dynamics", "comment": null, "summary": "Successful teamwork depends on interpersonal dynamics, the ways in which\nindividuals coordinate, influence, and adapt to one another over time. Existing\nmeasures of interpersonal dynamics, such as CRQA, correlation, Granger\ncausality, and transfer entropy, typically capture only a single dimension:\neither the synchrony/coordination or the direction of influence between\nindividuals. What is missing is a psychologically meaningful representation\nthat unifies these dimensions and varies systematically with behavior. We\npropose the context matrix as one such representation. The context matrix is\nthe transition matrix in a linear dynamical system, with entries specifying how\nmuch each individual's current behavior is attributable to their own versus\nevery other group member's past behaviors. Its values can be distilled into\npsychologically interpretable summary features of synchrony and directional\ninfluence. Evidence for the context matrix as psychologically meaningful is\nprovided in two steps. First, we develop a sequential Bayesian model that\ninfers context matrices from timeseries data and show that it accurately\nrecovers them in noisy simulations. Second, applying the model to human\neyetracking data, we show that summary features of the inferred context\nmatrices capture expected task-based differences in interpersonal dynamics (or\nlack thereof), predict task accuracy in psychologically reasonable ways, and\nshow some correspondence with existing measures (CRQA and Granger causality).\nWe conclude by situating the context matrix within a broader agenda for\nmodeling interpersonal dynamics.", "AI": {"tldr": "The paper proposes a context matrix as a unified representation of interpersonal dynamics that captures both synchrony/coordination and directional influence between individuals, validated through simulations and human eye-tracking data.", "motivation": "Existing measures of interpersonal dynamics (CRQA, correlation, Granger causality, transfer entropy) only capture single dimensions - either synchrony or direction of influence, but lack a psychologically meaningful unified representation.", "method": "Developed a sequential Bayesian model to infer context matrices from timeseries data, validated through noisy simulations, then applied to human eye-tracking data to extract psychologically interpretable summary features of synchrony and directional influence.", "result": "The context matrix accurately recovered ground truth in simulations. In human data, its summary features captured task-based differences in interpersonal dynamics, predicted task accuracy in psychologically reasonable ways, and showed correspondence with existing measures (CRQA and Granger causality).", "conclusion": "The context matrix provides a psychologically meaningful unified representation of interpersonal dynamics that bridges synchrony and directional influence, offering a foundation for broader modeling of interpersonal coordination and influence."}}
{"id": "2509.07997", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07997", "abs": "https://arxiv.org/abs/2509.07997", "authors": ["Abigail Breitfeld", "Alberto Candela", "Juan Delfa", "Akseli Kangaslahti", "Itai Zilberstein", "Steve Chien", "David Wettergreen"], "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites", "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data.", "AI": {"tldr": "Learning-based dynamic targeting for Earth observation satellites using reinforcement and imitation learning outperforms traditional heuristic methods by 10-13.7% in data collection efficiency.", "motivation": "Earth observing satellites have limitations in orbital flexibility, sensor field of view, and resource allocation. They need to optimize data collection by focusing on the most informative measurements through intelligent targeting.", "method": "Two learning-based approaches: reinforcement learning and imitation learning, building on a dynamic programming solution to plan sampling location sequences. Both methods are trained with relatively small datasets.", "result": "Imitation learning performs 10.0% better than the best heuristic method, while reinforcement learning performs 13.7% better on average. Both methods show effective training with small data amounts.", "conclusion": "Learning-based approaches significantly improve dynamic targeting performance for satellite data collection compared to conventional heuristic methods, demonstrating the value of AI techniques in space observation optimization."}}
{"id": "2509.08089", "categories": ["cs.LG", "cs.CR", "68T99"], "pdf": "https://arxiv.org/pdf/2509.08089", "abs": "https://arxiv.org/abs/2509.08089", "authors": ["Lucas Fenaux", "Zheng Wang", "Jacob Yan", "Nathan Chung", "Florian Kerschbaum"], "title": "Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning", "comment": null, "summary": "Federated Learning is a distributed learning technique in which multiple\nclients cooperate to train a machine learning model. Distributed settings\nfacilitate backdoor attacks by malicious clients, who can embed malicious\nbehaviors into the model during their participation in the training process.\nThese malicious behaviors are activated during inference by a specific trigger.\nNo defense against backdoor attacks has stood the test of time, especially\nagainst adaptive attackers, a powerful but not fully explored category of\nattackers. In this work, we first devise a new adaptive adversary that\nsurpasses existing adversaries in capabilities, yielding attacks that only\nrequire one or two malicious clients out of 20 to break existing\nstate-of-the-art defenses. Then, we present Hammer and Anvil, a principled\ndefense approach that combines two defenses orthogonal in their underlying\nprinciple to produce a combined defense that, given the right set of\nparameters, must succeed against any attack. We show that our best combined\ndefense, Krum+, is successful against our new adaptive adversary and\nstate-of-the-art attacks.", "AI": {"tldr": "New adaptive backdoor attack breaks existing FL defenses with just 1-2 malicious clients, while proposed Hammer and Anvil defense with Krum+ successfully counters these attacks.", "motivation": "Federated Learning is vulnerable to backdoor attacks from malicious clients, and existing defenses fail against adaptive attackers who can bypass current protection mechanisms.", "method": "Developed a new adaptive adversary that outperforms existing attackers, then created Hammer and Anvil - a combined defense approach using two orthogonal defense principles to create robust protection.", "result": "The new adaptive attack breaks state-of-the-art defenses with only 1-2 malicious clients out of 20. The proposed Krum+ defense successfully counters both the new adaptive adversary and existing state-of-the-art attacks.", "conclusion": "The paper demonstrates the vulnerability of current FL defenses to sophisticated adaptive attacks and provides a principled defense approach that can withstand such attacks through combined orthogonal defense mechanisms."}}
{"id": "2509.08088", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.08088", "abs": "https://arxiv.org/abs/2509.08088", "authors": ["Linyao Chen", "Zimian Peng", "Yingxuan Yang", "Yikun Wang", "Wenzheng Tom Tang", "Hiroki H. Kobayashi", "Weinan Zhang"], "title": "EnvX: Agentize Everything with Agentic AI", "comment": null, "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem.", "AI": {"tldr": "EnvX is a framework that transforms GitHub repositories into intelligent AI agents using a three-phase process, enabling natural language interaction and inter-agent collaboration to automate software reuse.", "motivation": "Manual software reuse from open-source repositories is time-consuming, error-prone, and disconnected, creating significant barriers for developers who need to navigate documentation, understand APIs, and write integration code.", "method": "EnvX uses Agentic AI to agentize repositories through: (1) TODO-guided environment initialization, (2) human-aligned agentic automation for autonomous task performance, and (3) Agent-to-Agent (A2A) protocol for multi-agent collaboration, combining LLMs with structured tool integration.", "result": "On the GitTaskBench benchmark with 18 repositories across various domains, EnvX achieved 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks and demonstrating effective multi-repository collaboration.", "conclusion": "EnvX represents a paradigm shift from treating repositories as passive code resources to intelligent, interactive agents, significantly improving accessibility and collaboration in the open-source ecosystem."}}
{"id": "2509.08116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08116", "abs": "https://arxiv.org/abs/2509.08116", "authors": ["Nooshin Maghsoodi", "Sarah Nassar", "Paul F R Wilson", "Minh Nguyen Nhat To", "Sophia Mannina", "Shamel Addas", "Stephanie Sibley", "David Maslove", "Purang Abolmaesumi", "Parvin Mousavi"], "title": "Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography", "comment": null, "summary": "Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart\nconditions; however, the effectiveness of artificial intelligence (AI)-based\nECG analysis is often hindered by the limited availability of labeled data.\nSelf-supervised learning (SSL) can address this by leveraging large-scale\nunlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning\nRepresentation for ECG), a physiology-aware contrastive learning framework that\nincorporates domain-specific priors to enhance the generalizability and\nclinical relevance of ECG-based arrhythmia classification. Methods: During\npretraining, PhysioCLR learns to bring together embeddings of samples that\nshare similar clinically relevant features while pushing apart those that are\ndissimilar. Unlike existing methods, our method integrates ECG physiological\nsimilarity cues into contrastive learning, promoting the learning of clinically\nmeaningful representations. Additionally, we introduce ECG- specific\naugmentations that preserve the ECG category post augmentation and propose a\nhybrid loss function to further refine the quality of learned representations.\nResults: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,\nfor multilabel ECG diagnoses, as well as a private ICU dataset labeled for\nbinary classification. Across the Chapman, Georgia, and private cohorts,\nPhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,\nunderscoring its robust cross-dataset generalization. Conclusion: By embedding\nphysiological knowledge into contrastive learning, PhysioCLR enables the model\nto learn clinically meaningful and transferable ECG eatures. Significance:\nPhysioCLR demonstrates the potential of physiology-informed SSL to offer a\npromising path toward more effective and label-efficient ECG diagnostics.", "AI": {"tldr": "PhysioCLR is a physiology-aware contrastive learning framework that incorporates ECG domain knowledge to improve arrhythmia classification with limited labeled data, achieving 12% AUROC improvement over baselines.", "motivation": "AI-based ECG analysis is limited by scarce labeled data. Self-supervised learning can leverage unlabeled data, but existing methods lack integration of physiological knowledge for clinically meaningful representations.", "method": "PhysioCLR integrates ECG physiological similarity cues into contrastive learning, uses ECG-specific augmentations that preserve category information, and employs a hybrid loss function to refine learned representations.", "result": "Evaluated on Chapman, Georgia, and private ICU datasets, PhysioCLR achieved 12% mean AUROC improvement relative to the strongest baseline, demonstrating robust cross-dataset generalization.", "conclusion": "Embedding physiological knowledge into contrastive learning enables learning clinically meaningful and transferable ECG features, offering a promising path for label-efficient ECG diagnostics."}}
{"id": "2509.08721", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.08721", "abs": "https://arxiv.org/abs/2509.08721", "authors": ["Jeffrey Amico", "Gabriel Passamani Andrade", "John Donaghy", "Ben Fielding", "Tristin Forbus", "Harry Grieve", "Semih Kara", "Jari Kolehmainen", "Yihua Lou", "Christopher Nies", "Edward Phillip Flores Nu\u00f1o", "Diogo Ortega", "Shikhar Rastogi", "Austin Virts", "Matthew J. Wright"], "title": "Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing", "comment": "14 pages, 6 figures", "summary": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.", "AI": {"tldr": "SAPO is a decentralized asynchronous RL algorithm for post-training language models that enables scalable reinforcement learning across heterogeneous compute networks without central bottlenecks.", "motivation": "Traditional RL post-training for language models requires significant parallelization with high technical challenges and costs. Current approaches face bottlenecks in scaling due to latency, memory, and reliability issues.", "method": "Swarm sAmpling Policy Optimization (SAPO) uses a fully decentralized network where each node manages its own policy models and shares rollouts with others. No assumptions about latency, model homogeneity, or hardware are required.", "result": "SAPO achieved cumulative reward gains of up to 94% in controlled experiments. Successfully tested on a network with thousands of nodes running on diverse hardware and models.", "conclusion": "SAPO provides an effective decentralized solution for scaling RL post-training, avoiding common bottlenecks while enabling knowledge propagation across heterogeneous networks."}}
{"id": "2509.08151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08151", "abs": "https://arxiv.org/abs/2509.08151", "authors": ["Botao Zhu", "Jeslyn Wang", "Dusit Niyato", "Xianbin Wang"], "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI", "comment": null, "summary": "Accurate trustworthiness evaluation of potential collaborating devices is\nessential for the effective execution of complex computing tasks. This\nevaluation process involves collecting diverse trust-related data from\npotential collaborators, including historical performance and available\nresources, for collaborator selection. However, when each task owner\nindependently assesses all collaborators' trustworthiness, frequent data\nexchange, complex reasoning, and dynamic situation changes can result in\nsignificant overhead and deteriorated trust evaluation. To overcome these\nchallenges, we propose a task-specific trust semantics distillation (2TSD)\nmodel based on a large AI model (LAM)-driven teacher-student agent\narchitecture. The teacher agent is deployed on a server with powerful\ncomputational capabilities and an augmented memory module dedicated to\nmultidimensional trust-related data collection, task-specific trust semantics\nextraction, and task-collaborator matching analysis. Upon receiving\ntask-specific requests from device-side student agents, the teacher agent\ntransfers the trust semantics of potential collaborators to the student agents,\nenabling rapid and accurate collaborator selection. Experimental results\ndemonstrate that the proposed 2TSD model can reduce collaborator evaluation\ntime, decrease device resource consumption, and improve the accuracy of\ncollaborator selection.", "AI": {"tldr": "Proposes 2TSD model using LAM-driven teacher-student architecture for efficient trust evaluation in collaborative computing tasks", "motivation": "Independent trust evaluation by each task owner causes significant overhead from frequent data exchange, complex reasoning, and dynamic changes", "method": "Teacher agent on server collects trust data and extracts task-specific trust semantics, transfers to student agents for rapid collaborator selection", "result": "Reduces evaluation time, decreases device resource consumption, and improves selection accuracy", "conclusion": "2TSD model effectively overcomes trust evaluation challenges in collaborative computing through semantics distillation"}}
{"id": "2509.08120", "categories": ["cs.LG", "math.OC", "G.4; D.2; G.m; G.3; I.2"], "pdf": "https://arxiv.org/pdf/2509.08120", "abs": "https://arxiv.org/abs/2509.08120", "authors": ["Konstantin Burlachenko"], "title": "Optimization Methods and Software for Federated Learning", "comment": "A dissertation by Konstantin Burlachenko submitted in partial\n  fulfillment of the requirements for the degree of Doctor of Philosophy", "summary": "Federated Learning (FL) is a novel, multidisciplinary Machine Learning\nparadigm where multiple clients, such as mobile devices, collaborate to solve\nmachine learning problems. Initially introduced in Kone{\\v{c}}n{\\'y} et al.\n(2016a,b); McMahan et al. (2017), FL has gained further attention through its\ninclusion in the National AI Research and Development Strategic Plan (2023\nUpdate) of the United States (Science and on Artificial Intelligence, 2023).\nThe FL training process is inherently decentralized and often takes place in\nless controlled settings compared to data centers, posing unique challenges\ndistinct from those in fully controlled environments. In this thesis, we\nidentify five key challenges in Federated Learning and propose novel approaches\nto address them. These challenges arise from the heterogeneity of data and\ndevices, communication issues, and privacy concerns for clients in FL training.\nMoreover, even well-established theoretical advances in FL require diverse\nforms of practical implementation to enhance their real-world applicability.\nOur contributions advance FL algorithms and systems, bridging theoretical\nadvancements and practical implementations. More broadly, our work serves as a\nguide for researchers navigating the complexities of translating theoretical\nmethods into efficient real-world implementations and software. Additionally,\nit offers insights into the reverse process of adapting practical\nimplementation aspects back into theoretical algorithm design. This reverse\nprocess is particularly intriguing, as the practical perspective compels us to\nexamine the underlying mechanics and flexibilities of algorithms more deeply,\noften uncovering new dimensions of the algorithms under study.", "AI": {"tldr": "This thesis addresses five key challenges in Federated Learning (FL) - data/device heterogeneity, communication issues, and privacy concerns - by proposing novel approaches that bridge theoretical advancements with practical implementations.", "motivation": "FL operates in decentralized, less controlled environments compared to data centers, creating unique challenges that require solutions beyond traditional machine learning approaches. The inclusion of FL in the US National AI Research and Development Strategic Plan highlights its growing importance.", "method": "The thesis identifies five core FL challenges and develops novel approaches to address them, focusing on translating theoretical methods into practical implementations while also adapting practical insights back into theoretical algorithm design.", "result": "The work advances FL algorithms and systems by creating a bidirectional bridge between theory and practice, enabling more efficient real-world implementations while also informing theoretical developments through practical perspectives.", "conclusion": "This research provides a comprehensive guide for translating FL theoretical methods into practical implementations and demonstrates the value of reverse-engineering practical insights to enhance theoretical algorithm design, uncovering new dimensions of FL algorithms."}}
{"id": "2509.08785", "categories": ["cs.AI", "cs.MA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.08785", "abs": "https://arxiv.org/abs/2509.08785", "authors": ["Anup Tuladhar", "Araz Minhas", "Adam Kirton", "Eli Kinney-Lang"], "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making", "comment": "Extended Abstract for RLDM 2025", "summary": "We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.", "AI": {"tldr": "Experimental platform combining reinforcement learning with language model reasoning to study how narrative frameworks influence AI decision-making.", "motivation": "Bridge the gap between AI's decision-making capabilities and narrative reasoning, which have mostly been studied separately, to explore how narrative elements shape reward-based learning.", "method": "Dual-system architecture with RL policy for action suggestions and language model processing through narrative frameworks. Implemented in configurable gridworld environment with modular design for controlled testing.", "result": "Preliminary implementation provides foundation for studying narrative framework effects on reward-based decisions and exploring interactions between optimization-based learning and symbolic reasoning.", "conclusion": "The platform enables initial experimentation with narrative elements while maintaining consistent environment and reward structures, facilitating future research on narrative-guided AI decision-making."}}
{"id": "2509.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08222", "abs": "https://arxiv.org/abs/2509.08222", "authors": ["Minjong Yoo", "Jinwoo Jang", "Wei-jin Park", "Honguk Woo"], "title": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following", "comment": "21 pages. NeurIPS 2024", "summary": "This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)\nframework, designed to tackle continual instruction following tasks of embodied\nagents in dynamic, non-stationary environments. The framework enhances Large\nLanguage Models' (LLMs) embodied reasoning capabilities by efficiently\nexploring the physical environment and establishing the environmental context\nmemory, thereby effectively grounding the task planning process in time-varying\nenvironment contexts. In ExRAP, given multiple continual instruction following\ntasks, each instruction is decomposed into queries on the environmental context\nmemory and task executions conditioned on the query results. To efficiently\nhandle these multiple tasks that are performed continuously and simultaneously,\nwe implement an exploration-integrated task planning scheme by incorporating\nthe {information-based exploration} into the LLM-based planning process.\nCombined with memory-augmented query evaluation, this integrated scheme not\nonly allows for a better balance between the validity of the environmental\ncontext memory and the load of environment exploration, but also improves\noverall task performance. Furthermore, we devise a {temporal consistency\nrefinement} scheme for query evaluation to address the inherent decay of\nknowledge in the memory. Through experiments with VirtualHome, ALFRED, and\nCARLA, our approach demonstrates robustness against a variety of embodied\ninstruction following scenarios involving different instruction scales and\ntypes, and non-stationarity degrees, and it consistently outperforms other\nstate-of-the-art LLM-based task planning approaches in terms of both goal\nsuccess rate and execution efficiency.", "AI": {"tldr": "ExRAP framework enhances LLMs' embodied reasoning for continual instruction following in dynamic environments through exploration-integrated planning and memory-augmented query evaluation.", "motivation": "To address the challenge of embodied agents performing continual instruction following tasks in dynamic, non-stationary environments where environmental contexts change over time.", "method": "Proposes Exploratory Retrieval-Augmented Planning (ExRAP) framework that combines information-based exploration with LLM-based planning, uses environmental context memory, and implements temporal consistency refinement for query evaluation.", "result": "Demonstrates robustness across various embodied instruction following scenarios (VirtualHome, ALFRED, CARLA) with different instruction scales/types and non-stationarity degrees, consistently outperforming state-of-the-art LLM-based approaches in goal success rate and execution efficiency.", "conclusion": "ExRAP effectively grounds task planning in time-varying environmental contexts, balances environmental context memory validity with exploration load, and addresses knowledge decay through temporal consistency refinement, making it suitable for dynamic embodied AI applications."}}
{"id": "2509.08122", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.08122", "abs": "https://arxiv.org/abs/2509.08122", "authors": ["Kishan Padayachy", "Ronald Richman", "Salvatore Scognamiglio", "Mario V. W\u00fcthrich"], "title": "In-Context Learning Enhanced Credibility Transformer", "comment": null, "summary": "The starting point of our network architecture is the Credibility Transformer\nwhich extends the classical Transformer architecture by a credibility mechanism\nto improve model learning and predictive performance. This Credibility\nTransformer learns credibilitized CLS tokens that serve as learned\nrepresentations of the original input features. In this paper we present a new\nparadigm that augments this architecture by an in-context learning mechanism,\ni.e., we increase the information set by a context batch consisting of similar\ninstances. This allows the model to enhance the CLS token representations of\nthe instances by additional in-context information and fine-tuning. We\nempirically verify that this in-context learning enhances predictive accuracy\nby adapting to similar risk patterns. Moreover, this in-context learning also\nallows the model to generalize to new instances which, e.g., have feature\nlevels in the categorical covariates that have not been present when the model\nwas trained -- for a relevant example, think of a new vehicle model which has\njust been developed by a car manufacturer.", "AI": {"tldr": "Extends Credibility Transformer with in-context learning using similar instances to enhance CLS token representations and improve predictive accuracy, especially for unseen categorical features.", "motivation": "To improve model learning and predictive performance by incorporating additional context from similar instances, enabling better adaptation to risk patterns and generalization to new categorical feature levels not seen during training.", "method": "Augments the Credibility Transformer architecture with an in-context learning mechanism that uses a context batch of similar instances to enhance CLS token representations through additional information and fine-tuning.", "result": "Empirical verification shows enhanced predictive accuracy through adaptation to similar risk patterns and improved generalization to new instances with previously unseen categorical feature levels.", "conclusion": "In-context learning effectively enhances the Credibility Transformer's performance by leveraging similar instances, providing better predictive accuracy and generalization capabilities for novel categorical features."}}
{"id": "2509.08282", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08282", "abs": "https://arxiv.org/abs/2509.08282", "authors": ["Seonghyeon Go"], "title": "Real-world Music Plagiarism Detection With Music Segment Transcription System", "comment": "Accepted in APSIPA 2025 but not published yet(will be published in 2\n  month..), Arxiv preprint ready for references in future-works", "summary": "As a result of continuous advances in Music Information Retrieval (MIR)\ntechnology, generating and distributing music has become more diverse and\naccessible. In this context, interest in music intellectual property protection\nis increasing to safeguard individual music copyrights. In this work, we\npropose a system for detecting music plagiarism by combining various MIR\ntechnologies. We developed a music segment transcription system that extracts\nmusically meaningful segments from audio recordings to detect plagiarism across\ndifferent musical formats. With this system, we compute similarity scores based\non multiple musical features that can be evaluated through comprehensive\nmusical analysis. Our approach demonstrated promising results in music\nplagiarism detection experiments, and the proposed method can be applied to\nreal-world music scenarios. We also collected a Similar Music Pair (SMP)\ndataset for musical similarity research using real-world cases. The dataset are\npublicly available.", "AI": {"tldr": "A music plagiarism detection system using MIR technologies to extract meaningful segments and compute similarity scores across different musical formats, with promising experimental results and a publicly available dataset.", "motivation": "Growing interest in music intellectual property protection due to advances in Music Information Retrieval technology and increased accessibility of music generation/distribution.", "method": "Developed a music segment transcription system that extracts musically meaningful segments from audio recordings and computes similarity scores based on multiple musical features for comprehensive analysis.", "result": "Demonstrated promising results in music plagiarism detection experiments, showing applicability to real-world music scenarios. Created and publicly released a Similar Music Pair (SMP) dataset for musical similarity research.", "conclusion": "The proposed system effectively combines various MIR technologies for music plagiarism detection and provides a valuable dataset for future research in musical similarity analysis."}}
{"id": "2509.08129", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08129", "abs": "https://arxiv.org/abs/2509.08129", "authors": ["Francisco M. Castro-Mac\u00edas", "Francisco J. S\u00e1ez-Maldonado", "Pablo Morales-\u00c1lvarez", "Rafael Molina"], "title": "torchmil: A PyTorch-based library for deep Multiple Instance Learning", "comment": null, "summary": "Multiple Instance Learning (MIL) is a powerful framework for weakly\nsupervised learning, particularly useful when fine-grained annotations are\nunavailable. Despite growing interest in deep MIL methods, the field lacks\nstandardized tools for model development, evaluation, and comparison, which\nhinders reproducibility and accessibility. To address this, we present\ntorchmil, an open-source Python library built on PyTorch. torchmil offers a\nunified, modular, and extensible framework, featuring basic building blocks for\nMIL models, a standardized data format, and a curated collection of benchmark\ndatasets and models. The library includes comprehensive documentation and\ntutorials to support both practitioners and researchers. torchmil aims to\naccelerate progress in MIL and lower the entry barrier for new users. Available\nat https://torchmil.readthedocs.io.", "AI": {"tldr": "torchmil is an open-source PyTorch library that provides standardized tools for Multiple Instance Learning (MIL), addressing the lack of reproducibility and accessibility in deep MIL methods.", "motivation": "The field of deep Multiple Instance Learning lacks standardized tools for model development, evaluation, and comparison, which hinders reproducibility and accessibility.", "method": "Developed torchmil, an open-source Python library built on PyTorch that offers a unified, modular framework with basic building blocks for MIL models, standardized data format, benchmark datasets, and comprehensive documentation.", "result": "Created a complete library with standardized tools, data formats, benchmark datasets, and tutorials to support both practitioners and researchers in MIL.", "conclusion": "torchmil aims to accelerate progress in Multiple Instance Learning and lower the entry barrier for new users by providing accessible, reproducible tools and resources."}}
{"id": "2509.08312", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08312", "abs": "https://arxiv.org/abs/2509.08312", "authors": ["Binghan Wu", "Shoufeng Wang", "Yunxin Liu", "Ya-Qin Zhang", "Joseph Sifakis", "Ye Ouyang"], "title": "Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies", "comment": "7 pages, 5 figures. This manuscript is a preprint", "summary": "The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a\nstrategic inflection point in telecommunications, where networks must transcend\nreactive automation to achieve genuine cognitive capabilities--fulfilling TM\nForum's vision of self-configuring, self-healing, and self-optimizing systems\nthat deliver zero-wait, zero-touch, and zero-fault services. This work bridges\nthe gap between architectural theory and operational reality by implementing\nJoseph Sifakis's AN Agent reference architecture in a functional cognitive\nsystem, deploying coordinated proactive-reactive runtimes driven by hybrid\nknowledge representation. Through an empirical case study of a Radio Access\nNetwork (RAN) Link Adaptation (LA) Agent, we validate this framework's\ntransformative potential: demonstrating sub-10 ms real-time control in 5G NR\nsub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link\nAdaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for\nultra-reliable services through dynamic Modulation and Coding Scheme (MCS)\noptimization. These improvements confirm the architecture's viability in\novercoming traditional autonomy barriers and advancing critical L4-enabling\ncapabilities toward next-generation objectives.", "AI": {"tldr": "Implementation of a cognitive autonomous network agent architecture achieving real-time 5G control with 6% throughput improvement and 67% BLER reduction through dynamic MCS optimization.", "motivation": "Bridging the gap between architectural theory and operational reality for Level 4 Autonomous Networks, moving beyond reactive automation to achieve genuine cognitive capabilities for self-configuring, self-healing networks.", "method": "Implemented Joseph Sifakis's AN Agent reference architecture with coordinated proactive-reactive runtimes driven by hybrid knowledge representation, tested through an empirical case study of a RAN Link Adaptation Agent.", "result": "Achieved sub-10 ms real-time control in 5G NR sub-6 GHz, 6% higher downlink throughput than OLLA algorithms, and 67% Block Error Rate reduction for ultra-reliable services.", "conclusion": "The framework demonstrates transformative potential in overcoming traditional autonomy barriers and advancing critical L4-enabling capabilities toward next-generation network objectives."}}
{"id": "2509.08140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08140", "abs": "https://arxiv.org/abs/2509.08140", "authors": ["Mihir Kumar", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Afriyie Kwesi Samuel", "Fuat Alican", "Yigit Ihlamur"], "title": "From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital", "comment": "6 pages, 3 figures", "summary": "This paper presents a framework for predicting rare, high-impact outcomes by\nintegrating large language models (LLMs) with a multi-model machine learning\n(ML) architecture. The approach combines the predictive strength of black-box\nmodels with the interpretability required for reliable decision-making. We use\nLLM-powered feature engineering to extract and synthesize complex signals from\nunstructured data, which are then processed within a layered ensemble of models\nincluding XGBoost, Random Forest, and Linear Regression. The ensemble first\nproduces a continuous estimate of success likelihood, which is then thresholded\nto produce a binary rare-event prediction. We apply this framework to the\ndomain of Venture Capital (VC), where investors must evaluate startups with\nlimited and noisy early-stage data. The empirical results show strong\nperformance: the model achieves precision between 9.8X and 11.1X the random\nclassifier baseline in three independent test subsets. Feature sensitivity\nanalysis further reveals interpretable success drivers: the startup's category\nlist accounts for 15.6% of predictive influence, followed by the number of\nfounders, while education level and domain expertise contribute smaller yet\nconsistent effects.", "AI": {"tldr": "A framework combining LLMs with multi-model ML architecture for predicting rare high-impact outcomes, achieving 9.8X-11.1X precision over random baseline in VC startup evaluation.", "motivation": "To address the challenge of predicting rare, high-impact outcomes in domains like Venture Capital where investors must evaluate startups with limited and noisy early-stage data, requiring both predictive strength and interpretability.", "method": "Integrates LLM-powered feature engineering to extract signals from unstructured data, processed through a layered ensemble of models (XGBoost, Random Forest, Linear Regression) that first produces continuous success likelihood estimates, then thresholds for binary rare-event prediction.", "result": "Strong performance with precision between 9.8X and 11.1X the random classifier baseline across three independent test subsets. Feature analysis shows startup category (15.6% influence) and number of founders as key predictors, with education level and domain expertise having smaller consistent effects.", "conclusion": "The framework successfully combines predictive power with interpretability for rare-event prediction, demonstrating practical value in VC decision-making through both high performance and actionable insights into success drivers."}}
{"id": "2509.08380", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08380", "abs": "https://arxiv.org/abs/2509.08380", "authors": ["Prathamesh Vasudeo Naik", "Naresh Kumar Dintakurthi", "Zhanghao Hu", "Yue Wang", "Robby Qiu"], "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives", "comment": null, "summary": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a\nhigh-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.\nWhile large language models (LLMs) offer promising fluency, they suffer from\nfactual hallucination, limited crime typology alignment, and poor\nexplainability -- posing unacceptable risks in compliance-critical domains.\nThis paper introduces Co-Investigator AI, an agentic framework optimized to\nproduce Suspicious Activity Reports (SARs) significantly faster and with\ngreater accuracy than traditional methods. Drawing inspiration from recent\nadvances in autonomous agent architectures, such as the AI Co-Scientist, our\napproach integrates specialized agents for planning, crime type detection,\nexternal intelligence gathering, and compliance validation. The system features\ndynamic memory management, an AI-Privacy Guard layer for sensitive data\nhandling, and a real-time validation agent employing the Agent-as-a-Judge\nparadigm to ensure continuous narrative quality assurance. Human investigators\nremain firmly in the loop, empowered to review and refine drafts in a\ncollaborative workflow that blends AI efficiency with domain expertise. We\ndemonstrate the versatility of Co-Investigator AI across a range of complex\nfinancial crime scenarios, highlighting its ability to streamline SAR drafting,\nalign narratives with regulatory expectations, and enable compliance teams to\nfocus on higher-order analytical work. This approach marks the beginning of a\nnew era in compliance reporting -- bringing the transformative benefits of AI\nagents to the core of regulatory processes and paving the way for scalable,\nreliable, and transparent SAR generation.", "AI": {"tldr": "Co-Investigator AI is an agentic framework that automates Suspicious Activity Report generation with specialized agents for planning, crime detection, intelligence gathering, and compliance validation, significantly improving speed and accuracy while keeping human investigators in the loop.", "motivation": "Traditional SAR generation is high-cost, low-scalability, and current LLMs suffer from factual hallucination, poor crime typology alignment, and limited explainability - posing unacceptable risks in compliance-critical AML workflows.", "method": "Agentic framework with specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. Features dynamic memory management, AI-Privacy Guard for sensitive data, and real-time validation using Agent-as-a-Judge paradigm.", "result": "The system demonstrates versatility across complex financial crime scenarios, streamlining SAR drafting, aligning narratives with regulatory expectations, and enabling compliance teams to focus on higher-order analytical work.", "conclusion": "This approach marks a new era in compliance reporting, bringing AI agent benefits to regulatory processes and enabling scalable, reliable, and transparent SAR generation while maintaining human oversight."}}
{"id": "2509.08156", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08156", "abs": "https://arxiv.org/abs/2509.08156", "authors": ["Swati Swati", "Arjun Roy", "Emmanouil Panagiotou", "Eirini Ntoutsi"], "title": "MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs", "comment": "Accepted to be published in the Proceedings of the 34th ACM\n  International Conference on Information and Knowledge Management, November\n  10--14, 2025, Seoul, Republic of Korea", "summary": "Fairness-aware classification requires balancing performance and fairness,\noften intensified by intersectional biases. Conflicting fairness definitions\nfurther complicate the task, making it difficult to identify universally fair\nsolutions. Despite growing regulatory and societal demands for equitable AI,\npopular toolkits offer limited support for exploring multi-dimensional fairness\nand related trade-offs. To address this, we present mmm-fair, an open-source\ntoolkit leveraging boosting-based ensemble approaches that dynamically\noptimizes model weights to jointly minimize classification errors and diverse\nfairness violations, enabling flexible multi-objective optimization. The system\nempowers users to deploy models that align with their context-specific needs\nwhile reliably uncovering intersectional biases often missed by\nstate-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth\nmulti-attribute fairness, multi-objective optimization, a no-code, chat-based\ninterface, LLM-powered explanations, interactive Pareto exploration for model\nselection, custom fairness constraint definition, and deployment-ready models\nin a single open-source toolkit, a combination rarely found in existing\nfairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.", "AI": {"tldr": "mmm-fair is an open-source toolkit that addresses multi-dimensional fairness in classification through boosting-based ensemble methods, enabling dynamic optimization of model weights to balance performance and diverse fairness constraints.", "motivation": "Addressing the challenge of balancing performance and fairness in classification, particularly with intersectional biases and conflicting fairness definitions, while meeting growing regulatory demands for equitable AI.", "method": "Uses boosting-based ensemble approaches that dynamically optimize model weights to jointly minimize classification errors and diverse fairness violations, supporting flexible multi-objective optimization.", "result": "Enables deployment of context-specific fair models and reliably uncovers intersectional biases often missed by state-of-the-art methods.", "conclusion": "mmm-fair provides a comprehensive open-source solution combining multi-attribute fairness, multi-objective optimization, no-code interface, LLM explanations, and deployment-ready models - a unique combination not found in existing fairness tools."}}
{"id": "2509.08500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08500", "abs": "https://arxiv.org/abs/2509.08500", "authors": ["Kechen Jiao", "Zhirui Fang", "Jiahao Liu", "Bei Li", "Qifan Wang", "Xinyu Liu", "Junhao Ruan", "Zhongjian Qiao", "Yifan Zhu", "Yaxin Xu", "Jingang Wang", "Xiu Li"], "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making", "comment": null, "summary": "Using effective generalization capabilities of vision language models (VLMs)\nin context-specific dynamic tasks for embodied artificial intelligence remains\na significant challenge. Although supervised fine-tuned models can better align\nwith the real physical world, they still exhibit sluggish responses and\nhallucination issues in dynamically changing environments, necessitating\nfurther alignment. Existing post-SFT methods, reliant on reinforcement learning\nand chain-of-thought (CoT) approaches, are constrained by sparse rewards and\naction-only optimization, resulting in low sample efficiency, poor consistency,\nand model degradation. To address these issues, this paper proposes\nThought-Centric Preference Optimization (TCPO) for effective embodied\ndecision-making. Specifically, TCPO introduces a stepwise preference-based\noptimization approach, transforming sparse reward signals into richer step\nsample pairs. It emphasizes the alignment of the model's intermediate reasoning\nprocess, mitigating the problem of model degradation. Moreover, by\nincorporating Action Policy Consistency Constraint (APC), it further imposes\nconsistency constraints on the model output. Experiments in the ALFWorld\nenvironment demonstrate an average success rate of 26.67%, achieving a 6%\nimprovement over RL4VLM and validating the effectiveness of our approach in\nmitigating model degradation after fine-tuning. These results highlight the\npotential of integrating preference-based learning techniques with CoT\nprocesses to enhance the decision-making capabilities of vision-language models\nin embodied agents.", "AI": {"tldr": "TCPO method improves embodied AI decision-making by aligning intermediate reasoning processes and using stepwise preference optimization to address model degradation and sparse reward issues in vision language models.", "motivation": "Vision language models struggle with dynamic embodied tasks due to sluggish responses, hallucinations, and poor sample efficiency from existing reinforcement learning and chain-of-thought methods that suffer from sparse rewards and action-only optimization.", "method": "Proposes Thought-Centric Preference Optimization (TCPO) with stepwise preference-based optimization to transform sparse rewards into richer step samples, plus Action Policy Consistency Constraint (APC) to enforce output consistency and align intermediate reasoning.", "result": "Achieves 26.67% average success rate in ALFWorld environment, showing 6% improvement over RL4VLM and effectively mitigates model degradation after fine-tuning.", "conclusion": "TCPO demonstrates the potential of integrating preference-based learning with chain-of-thought processes to enhance vision-language models' decision-making capabilities in embodied agents."}}
{"id": "2509.08163", "categories": ["cs.LG", "q-fin.RM", "stat.AP", "stat.ML", "90B50, 62P05, 62H20, 68T07"], "pdf": "https://arxiv.org/pdf/2509.08163", "abs": "https://arxiv.org/abs/2509.08163", "authors": ["Ho Ming Lee", "Katrien Antonio", "Benjamin Avanzi", "Lorenzo Marchi", "Rui Zhou"], "title": "Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation", "comment": null, "summary": "Ensuring equitable treatment (fairness) across protected attributes (such as\ngender or ethnicity) is a critical issue in machine learning. Most existing\nliterature focuses on binary classification, but achieving fairness in\nregression tasks-such as insurance pricing or hiring score assessments-is\nequally important. Moreover, anti-discrimination laws also apply to continuous\nattributes, such as age, for which many existing methods are not applicable. In\npractice, multiple protected attributes can exist simultaneously; however,\nmethods targeting fairness across several attributes often overlook so-called\n\"fairness gerrymandering\", thereby ignoring disparities among intersectional\nsubgroups (e.g., African-American women or Hispanic men). In this paper, we\npropose a distance covariance regularisation framework that mitigates the\nassociation between model predictions and protected attributes, in line with\nthe fairness definition of demographic parity, and that captures both linear\nand nonlinear dependencies. To enhance applicability in the presence of\nmultiple protected attributes, we extend our framework by incorporating two\nmultivariate dependence measures based on distance covariance: the previously\nproposed joint distance covariance (JdCov) and our novel concatenated distance\ncovariance (CCdCov), which effectively address fairness gerrymandering in both\nregression and classification tasks involving protected attributes of various\ntypes. We discuss and illustrate how to calibrate regularisation strength,\nincluding a method based on Jensen-Shannon divergence, which quantifies\ndissimilarities in prediction distributions across groups. We apply our\nframework to the COMPAS recidivism dataset and a large motor insurance claims\ndataset.", "AI": {"tldr": "A distance covariance regularization framework for fairness in machine learning that handles multiple protected attributes and prevents fairness gerrymandering in both regression and classification tasks.", "motivation": "Existing fairness methods focus on binary classification and single attributes, but real-world applications require handling multiple protected attributes simultaneously and preventing disparities in intersectional subgroups, especially for continuous attributes like age.", "method": "Proposed distance covariance regularization to mitigate association between predictions and protected attributes, extended with two multivariate dependence measures (joint distance covariance and novel concatenated distance covariance) to address fairness gerrymandering.", "result": "The framework effectively captures both linear and nonlinear dependencies, works with various attribute types, and includes calibration methods for regularization strength using Jensen-Shannon divergence to quantify distribution differences across groups.", "conclusion": "The approach provides a comprehensive solution for fairness in both regression and classification tasks, demonstrated on COMPAS recidivism and motor insurance datasets, addressing critical gaps in handling multiple protected attributes and intersectional fairness."}}
{"id": "2509.08593", "categories": ["cs.AI", "stat.ML", "90C05, 68T27", "I.2.3; F.4.1"], "pdf": "https://arxiv.org/pdf/2509.08593", "abs": "https://arxiv.org/abs/2509.08593", "authors": ["Andr\u00e9s Corrada-Emmanuel"], "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges", "comment": "7 pages, 1 figure", "summary": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement.", "AI": {"tldr": "Using logical consistency between disagreeing LLM judges to detect misaligned grading without ground truth knowledge", "motivation": "When using LLMs as judges to evaluate other LLMs, infinite monitoring chains are inevitable without ground truth. There's a need to detect misaligned judges without knowing the correct answers.", "method": "Formalize logical consistency between disagreeing experts as a Linear Programming problem in integer response counts. Use agreement/disagreement patterns to compute possible evaluations of grading ability.", "result": "Developed no-knowledge alarms that can detect with no false positives when at least one member of an ensemble of judges violates specified grading ability requirements.", "conclusion": "Logical consistency analysis provides a reliable way to monitor LLM judges without ground truth knowledge, enabling detection of misaligned grading through disagreement patterns."}}
{"id": "2509.08176", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08176", "abs": "https://arxiv.org/abs/2509.08176", "authors": ["Honghui Du", "Leandro Minku", "Huiyu Zhou"], "title": "MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments", "comment": "Published in the 2020 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Concept drift is a major problem in online learning due to its impact on the\npredictive performance of data stream mining systems. Recent studies have\nstarted exploring data streams from different sources as a strategy to tackle\nconcept drift in a given target domain. These approaches make the assumption\nthat at least one of the source models represents a concept similar to the\ntarget concept, which may not hold in many real-world scenarios. In this paper,\nwe propose a novel approach called Multi-source mApping with tRansfer LearnIng\nfor Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge\nfrom multiple data sources in non-stationary environments even when source and\ntarget concepts do not match. This is achieved by projecting the target concept\nto the space of each source concept, enabling multiple source sub-classifiers\nto contribute towards the prediction of the target concept as part of an\nensemble. Experiments on several synthetic and real-world datasets show that\nMARLINE was more accurate than several state-of-the-art data stream learning\napproaches.", "AI": {"tldr": "MARLINE is a novel approach that enables knowledge transfer from multiple data sources in non-stationary environments, even when source and target concepts don't match, by projecting target concepts to source spaces and using ensemble predictions.", "motivation": "Existing approaches assume at least one source model represents a similar concept to the target, which may not hold in real-world scenarios. Concept drift in online learning impacts predictive performance of data stream mining systems.", "method": "Projects target concept to the space of each source concept, enabling multiple source sub-classifiers to contribute to target prediction as part of an ensemble system.", "result": "Experiments on synthetic and real-world datasets show MARLINE was more accurate than several state-of-the-art data stream learning approaches.", "conclusion": "MARLINE effectively handles concept drift by leveraging multiple data sources without requiring source-target concept matching, demonstrating superior performance over existing methods."}}
{"id": "2509.08682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08682", "abs": "https://arxiv.org/abs/2509.08682", "authors": ["Guoqing Ma", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Jiawei Shen", "Jingjiang Liu", "Yidan Liang"], "title": "Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference", "comment": null, "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.", "AI": {"tldr": "A novel failure attribution framework for multi-agent systems using multi-granularity causal inference, achieving 36.2% step-level accuracy and 22.4% average task success improvement.", "motivation": "Current diagnostic tools for multi-agent systems rely on statistical correlations and achieve less than 15% accuracy in locating root-cause failures, severely hampering practical deployment of MAS.", "method": "Two key technical contributions: (1) performance causal inversion principle that models performance dependencies by reversing data flow in execution logs with Shapley values for agent-level blame assignment; (2) CDC-MAS causal discovery algorithm that identifies critical failure steps by addressing non-stationary nature of MAS interaction data.", "result": "Achieves up to 36.2% step-level accuracy on Who&When and TRAIL benchmarks, significantly outperforming state-of-the-art methods. Generated optimizations boost overall task success rates by an average of 22.4%.", "conclusion": "Provides a principled and effective solution for debugging complex agent interactions, enabling more reliable and interpretable multi-agent systems through automated optimization loops validated by counterfactual simulations."}}
{"id": "2509.08180", "categories": ["cs.LG", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.08180", "abs": "https://arxiv.org/abs/2509.08180", "authors": ["Paul Curry"], "title": "The Domain Mixed Unit: A New Neural Arithmetic Layer", "comment": "7 pages, 5 tables, includes results on the NALM benchmark", "summary": "The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a\nsingle parameter gate that mixes between log-space and linear-space\nrepresentations while performing either addition (DMU add) or subtraction (DMU\nsub). Two initializations are proposed for the DMU: one covering addition and\nmultiplication, and another covering subtraction and division. The DMU achieves\nstate-of-the-art performance on the NALM Benchmark, a dataset designed to test\nthe ability of neural arithmetic units to generalize arithmetic operations,\nspecifically performing with the highest percentage solved over all seeds on\nmultiplication and division. The DMU will be submitted as a pull request to the\nopen-source NALM benchmark, and its code is available on GitHub at\nhttps://github.com/marict?tab=repositories", "AI": {"tldr": "DMU is a new neural arithmetic unit that mixes log-space and linear-space representations with a single parameter gate, achieving state-of-the-art performance on arithmetic generalization tasks.", "motivation": "To create a neural arithmetic unit that can better generalize arithmetic operations by combining different mathematical representations in a learnable way.", "method": "Proposes Domain Mixed Unit (DMU) with parameter gate mixing log-space and linear-space representations, with specific initializations for addition/multiplication and subtraction/division operations.", "result": "Achieves state-of-the-art performance on NALM Benchmark with highest percentage solved on multiplication and division across all seeds.", "conclusion": "DMU successfully demonstrates improved generalization for arithmetic operations and will be contributed to the open-source NALM benchmark."}}
{"id": "2509.08705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08705", "abs": "https://arxiv.org/abs/2509.08705", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases", "comment": "9 pages, 7 figures, 2 tables", "summary": "We introduce a novel Theory of Mind (ToM) framework inspired by dual-process\ntheories from cognitive science, integrating a fast, habitual graph-based\nreasoning system (System 1), implemented via graph convolutional networks\n(GCNs), and a slower, context-sensitive meta-adaptive learning system (System\n2), driven by meta-learning techniques. Our model dynamically balances\nintuitive and deliberative reasoning through a learned context gate mechanism.\nWe validate our architecture on canonical false-belief tasks and systematically\nexplore its capacity to replicate hallmark cognitive biases associated with\ndual-process theory, including anchoring, cognitive-load fatigue, framing\neffects, and priming effects. Experimental results demonstrate that our\ndual-process approach closely mirrors human adaptive behavior, achieves robust\ngeneralization to unseen contexts, and elucidates cognitive mechanisms\nunderlying reasoning biases. This work bridges artificial intelligence and\ncognitive theory, paving the way for AI systems exhibiting nuanced, human-like\nsocial cognition and adaptive decision-making capabilities.", "AI": {"tldr": "Novel dual-process Theory of Mind framework combining fast graph-based reasoning (System 1) with slow meta-adaptive learning (System 2) using context gate mechanism, validated on false-belief tasks and cognitive bias replication.", "motivation": "To bridge artificial intelligence and cognitive theory by creating AI systems that exhibit human-like social cognition and adaptive decision-making capabilities through dual-process reasoning.", "method": "Integrates graph convolutional networks (GCNs) for fast habitual reasoning (System 1) with meta-learning techniques for context-sensitive deliberative reasoning (System 2), using a learned context gate mechanism to dynamically balance both systems.", "result": "The model closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and successfully replicates cognitive biases including anchoring, cognitive-load fatigue, framing effects, and priming effects.", "conclusion": "This work successfully bridges AI and cognitive science, demonstrating that dual-process architectures can produce human-like reasoning patterns and cognitive biases, paving the way for more nuanced AI social cognition systems."}}
{"id": "2509.08181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08181", "abs": "https://arxiv.org/abs/2509.08181", "authors": ["Honghui Du", "Leandro Minku", "Aonghus Lawlor", "Huiyu Zhou"], "title": "Multi-Label Transfer Learning in Non-Stationary Data Streams", "comment": "Accepted at IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Label concepts in multi-label data streams often experience drift in\nnon-stationary environments, either independently or in relation to other\nlabels. Transferring knowledge between related labels can accelerate\nadaptation, yet research on multi-label transfer learning for data streams\nremains limited. To address this, we propose two novel transfer learning\nmethods: BR-MARLENE leverages knowledge from different labels in both source\nand target streams for multi-label classification; BRPW-MARLENE builds on this\nby explicitly modelling and transferring pairwise label dependencies to enhance\nlearning performance. Comprehensive experiments show that both methods\noutperform state-of-the-art multi-label stream approaches in non-stationary\nenvironments, demonstrating the effectiveness of inter-label knowledge transfer\nfor improved predictive performance.", "AI": {"tldr": "Two novel transfer learning methods for multi-label data streams that leverage knowledge transfer between labels to improve adaptation in non-stationary environments.", "motivation": "Label concepts in multi-label data streams often experience drift independently or in relation to other labels, and transferring knowledge between related labels can accelerate adaptation, but research on multi-label transfer learning for data streams remains limited.", "method": "Proposed two methods: BR-MARLENE leverages knowledge from different labels in both source and target streams for multi-label classification; BRPW-MARLENE extends this by explicitly modelling and transferring pairwise label dependencies.", "result": "Comprehensive experiments show both methods outperform state-of-the-art multi-label stream approaches in non-stationary environments.", "conclusion": "The methods demonstrate the effectiveness of inter-label knowledge transfer for improved predictive performance in multi-label data streams with concept drift."}}
{"id": "2509.08713", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.08713", "abs": "https://arxiv.org/abs/2509.08713", "authors": ["Ziming Luo", "Atoosa Kasirzadeh", "Nihar B. Shah"], "title": "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems", "comment": null, "summary": "AI scientist systems, capable of autonomously executing the full research\nworkflow from hypothesis generation and experimentation to paper writing, hold\nsignificant potential for accelerating scientific discovery. However, the\ninternal workflow of these systems have not been closely examined. This lack of\nscrutiny poses a risk of introducing flaws that could undermine the integrity,\nreliability, and trustworthiness of their research outputs. In this paper, we\nidentify four potential failure modes in contemporary AI scientist systems:\ninappropriate benchmark selection, data leakage, metric misuse, and post-hoc\nselection bias. To examine these risks, we design controlled experiments that\nisolate each failure mode while addressing challenges unique to evaluating AI\nscientist systems. Our assessment of two prominent open-source AI scientist\nsystems reveals the presence of several failures, across a spectrum of\nseverity, which can be easily overlooked in practice. Finally, we demonstrate\nthat access to trace logs and code from the full automated workflow enables far\nmore effective detection of such failures than examining the final paper alone.\nWe thus recommend journals and conferences evaluating AI-generated research to\nmandate submission of these artifacts alongside the paper to ensure\ntransparency, accountability, and reproducibility.", "AI": {"tldr": "AI scientist systems need scrutiny to prevent workflow flaws that undermine research integrity. This paper identifies four failure modes and demonstrates their presence in existing systems, recommending trace logs and code submission for better detection.", "motivation": "AI scientist systems can accelerate scientific discovery but their internal workflows haven't been closely examined, risking flaws that could compromise research integrity and trustworthiness.", "method": "Designed controlled experiments to isolate four potential failure modes (inappropriate benchmark selection, data leakage, metric misuse, post-hoc selection bias) and assessed two prominent open-source AI scientist systems.", "result": "Found presence of several failures across different severity levels that are easily overlooked in practice. Showed that access to trace logs and code enables far more effective failure detection than examining just the final paper.", "conclusion": "Journals and conferences should mandate submission of trace logs and code alongside AI-generated research papers to ensure transparency, accountability, and reproducibility."}}
{"id": "2509.08184", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.08184", "abs": "https://arxiv.org/abs/2509.08184", "authors": ["Francesco D'Angelo", "Francesco Croce", "Nicolas Flammarion"], "title": "Selective Induction Heads: How Transformers Select Causal Structures In Context", "comment": null, "summary": "Transformers have exhibited exceptional capabilities in sequence modeling\ntasks, leveraging self-attention and in-context learning. Critical to this\nsuccess are induction heads, attention circuits that enable copying tokens\nbased on their previous occurrences. In this work, we introduce a novel\nframework that showcases transformers' ability to dynamically handle causal\nstructures. Existing works rely on Markov Chains to study the formation of\ninduction heads, revealing how transformers capture causal dependencies and\nlearn transition probabilities in-context. However, they rely on a fixed causal\nstructure that fails to capture the complexity of natural languages, where the\nrelationship between tokens dynamically changes with context. To this end, our\nframework varies the causal structure through interleaved Markov chains with\ndifferent lags while keeping the transition probabilities fixed. This setting\nunveils the formation of Selective Induction Heads, a new circuit that endows\ntransformers with the ability to select the correct causal structure\nin-context. We empirically demonstrate that transformers learn this mechanism\nto predict the next token by identifying the correct lag and copying the\ncorresponding token from the past. We provide a detailed construction of a\n3-layer transformer to implement the selective induction head, and a\ntheoretical analysis proving that this mechanism asymptotically converges to\nthe maximum likelihood solution. Our findings advance the understanding of how\ntransformers select causal structures, providing new insights into their\nfunctioning and interpretability.", "AI": {"tldr": "Transformers can dynamically handle varying causal structures through Selective Induction Heads, enabling them to identify correct contextual dependencies and copy tokens from appropriate past positions.", "motivation": "Existing research on induction heads relies on fixed Markov Chain structures, which cannot capture the dynamic causal relationships found in natural language where token dependencies change with context.", "method": "Proposed a framework using interleaved Markov chains with different lags but fixed transition probabilities, enabling transformers to learn Selective Induction Heads that identify the correct causal structure in-context.", "result": "Transformers successfully learn to predict next tokens by identifying the correct lag and copying corresponding tokens from the past, with a 3-layer transformer implementation and theoretical proof showing asymptotic convergence to maximum likelihood solution.", "conclusion": "The work advances understanding of how transformers select causal structures dynamically, providing new insights into their functioning and interpretability beyond fixed Markov Chain approaches."}}
{"id": "2509.08188", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.08188", "abs": "https://arxiv.org/abs/2509.08188", "authors": ["Hritik Arasu", "Faisal R Jahangiri"], "title": "ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis", "comment": "16 Pages, 6 figures", "summary": "Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,\nchewing, and shiver -- confound automated analysis yet are costly to label at\nscale. We study whether modern generative models can synthesize realistic,\nlabel-aware artifact segments suitable for augmentation and stress-testing.\nUsing the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and\nfixed-length multi-channel windows (e.g., 250 samples) with preprocessing\ntailored to each model (per-window min-max for adversarial training;\nper-recording/channel $z$-score for diffusion). We compare a conditional\nWGAN-GP with a projection discriminator to a 1D denoising diffusion model with\nclassifier-free guidance, and evaluate along three axes: (i) fidelity via Welch\nband-power deltas ($\\Delta\\delta,\\ \\Delta\\theta,\\ \\Delta\\alpha,\\ \\Delta\\beta$),\nchannel-covariance Frobenius distance, autocorrelation $L_2$, and\ndistributional metrics (MMD/PRD); (ii) specificity via class-conditional\nrecovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation\neffects on artifact recognition. In our setting, WGAN-GP achieves closer\nspectral alignment and lower MMD to real data, while both models exhibit weak\nclass-conditional recovery, limiting immediate augmentation gains and revealing\nopportunities for stronger conditioning and coverage. We release a reproducible\npipeline -- data manifests, training configurations, and evaluation scripts --\nto establish a baseline for EEG artifact synthesis and to surface actionable\nfailure modes for future work.", "AI": {"tldr": "This paper compares two generative models (WGAN-GP and diffusion model) for synthesizing realistic EEG artifacts to address the high cost of manual labeling, finding that WGAN-GP achieves better spectral alignment but both models have weak class-conditional recovery.", "motivation": "Artifacts in EEG data (muscle, eye movement, electrode, chewing, shiver) confound automated analysis but are costly to label at scale. The study aims to determine if modern generative models can synthesize realistic, label-aware artifact segments for augmentation and stress-testing.", "method": "Using TUH EEG Artifact corpus with subject-wise splits and fixed-length multi-channel windows. Compared conditional WGAN-GP with projection discriminator to 1D denoising diffusion model with classifier-free guidance. Evaluated along fidelity (spectral metrics, covariance, autocorrelation), specificity (class-conditional recovery), and utility (augmentation effects).", "result": "WGAN-GP achieved closer spectral alignment and lower MMD to real data. Both models exhibited weak class-conditional recovery, limiting immediate augmentation gains. The study provides a reproducible pipeline for EEG artifact synthesis.", "conclusion": "While WGAN-GP shows better performance in spectral metrics, both generative models have limitations in class-conditional recovery, revealing opportunities for stronger conditioning and coverage improvements in future work on EEG artifact synthesis."}}
{"id": "2509.08191", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08191", "abs": "https://arxiv.org/abs/2509.08191", "authors": ["Robert Stephany", "Youngsoo Choi"], "title": "Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics", "comment": "6 pages, 2 figures", "summary": "Solving complex partial differential equations is vital in the physical\nsciences, but often requires computationally expensive numerical methods.\nReduced-order models (ROMs) address this by exploiting dimensionality reduction\nto create fast approximations. While modern ROMs can solve parameterized\nfamilies of PDEs, their predictive power degrades over long time horizons. We\naddress this by (1) introducing a flexible, high-order, yet inexpensive\nfinite-difference scheme and (2) proposing a Rollout loss that trains ROMs to\nmake accurate predictions over arbitrary time horizons. We demonstrate our\napproach on the 2D Burgers equation.", "AI": {"tldr": "A new method combining flexible finite-difference scheme and Rollout loss training to improve long-term predictive accuracy of reduced-order models for PDEs.", "motivation": "Traditional reduced-order models for partial differential equations suffer from degraded predictive power over long time horizons despite being computationally efficient.", "method": "Introduced (1) a flexible, high-order, inexpensive finite-difference scheme and (2) a Rollout loss that trains ROMs to maintain accuracy over arbitrary time horizons.", "result": "Demonstrated the approach successfully on the 2D Burgers equation.", "conclusion": "The proposed method enhances ROM performance for long-term PDE predictions while maintaining computational efficiency."}}
{"id": "2509.08194", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.08194", "abs": "https://arxiv.org/abs/2509.08194", "authors": ["Caio de Prospero Iglesias", "Kimberly Villalobos Carballo", "Dimitris Bertsimas"], "title": "Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization", "comment": null, "summary": "We address the problem of policy selection in contextual stochastic\noptimization (CSO), where covariates are available as contextual information\nand decisions must satisfy hard feasibility constraints. In many CSO settings,\nmultiple candidate policies--arising from different modeling paradigms--exhibit\nheterogeneous performance across the covariate space, with no single policy\nuniformly dominating. We propose Prescribe-then-Select (PS), a modular\nframework that first constructs a library of feasible candidate policies and\nthen learns a meta-policy to select the best policy for the observed\ncovariates. We implement the meta-policy using ensembles of Optimal Policy\nTrees trained via cross-validation on the training set, making policy choice\nentirely data-driven. Across two benchmark CSO problems--single-stage\nnewsvendor and two-stage shipment planning--PS consistently outperforms the\nbest single policy in heterogeneous regimes of the covariate space and\nconverges to the dominant policy when such heterogeneity is absent. All the\ncode to reproduce the results can be found at\nhttps://anonymous.4open.science/r/Prescribe-then-Select-TMLR.", "AI": {"tldr": "A modular framework called Prescribe-then-Select (PS) that first builds a library of feasible candidate policies and then learns a meta-policy to select the best policy for given covariates in contextual stochastic optimization problems.", "motivation": "In contextual stochastic optimization, multiple candidate policies from different modeling paradigms show heterogeneous performance across covariate space, with no single policy uniformly dominating others.", "method": "PS framework constructs a library of feasible candidate policies, then uses ensembles of Optimal Policy Trees trained via cross-validation to create a data-driven meta-policy for selecting the best policy based on observed covariates.", "result": "PS consistently outperforms the best single policy in heterogeneous regimes across two benchmark problems (newsvendor and shipment planning) and converges to the dominant policy when heterogeneity is absent.", "conclusion": "The Prescribe-then-Select framework provides an effective data-driven approach for policy selection in contextual stochastic optimization problems with heterogeneous performance across covariates."}}
{"id": "2509.08195", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08195", "abs": "https://arxiv.org/abs/2509.08195", "authors": ["Qiaobo Li", "Zhijie Chen", "Arindam Banerjee"], "title": "Sketched Gaussian Mechanism for Private Federated Learning", "comment": null, "summary": "Communication cost and privacy are two major considerations in federated\nlearning (FL). For communication cost, gradient compression by sketching the\nclients' transmitted model updates is often used for reducing per-round\ncommunication. For privacy, the Gaussian mechanism (GM), which consists of\nclipping updates and adding Gaussian noise, is commonly used to guarantee\nclient-level differential privacy. Existing literature on private FL analyzes\nprivacy of sketching and GM in an isolated manner, illustrating that sketching\nprovides privacy determined by the sketching dimension and that GM has to\nsupply any additional desired privacy.\n  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which\ndirectly combines sketching and the Gaussian mechanism for privacy. Using\nR\\'enyi-DP tools, we present a joint analysis of SGM's overall privacy\nguarantee, which is significantly more flexible and sharper compared to\nisolated analysis of sketching and GM privacy. In particular, we prove that the\nprivacy level of SGM for a fixed noise magnitude is proportional to\n$1/\\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for\nmoderate $b$) SGM can provide much stronger privacy guarantees than the\noriginal GM under the same noise budget. We demonstrate the application of SGM\nto FL with either gradient descent or adaptive server optimizers, and establish\ntheoretical results on optimization convergence, which exhibits only a\nlogarithmic dependence on the number of parameters $d$. Experimental results\nconfirm that at the same privacy level, SGM based FL is at least competitive\nwith non-sketching private FL variants and outperforms them in some settings.\nMoreover, using adaptive optimization at the server improves empirical\nperformance while maintaining the privacy guarantees.", "AI": {"tldr": "Sketched Gaussian Mechanism (SGM) combines sketching and Gaussian noise for federated learning, providing stronger privacy guarantees than isolated approaches while maintaining communication efficiency.", "motivation": "Address both communication cost and privacy in federated learning by jointly analyzing sketching compression and Gaussian noise mechanisms, rather than treating them separately.", "method": "Propose SGM that directly combines sketching (dimension reduction) with Gaussian noise addition, using R\u00e9nyi-DP for joint privacy analysis and applying to FL with gradient descent or adaptive optimizers.", "result": "SGM provides privacy proportional to 1/\u221ab (sketching dimension), offering stronger privacy than original Gaussian mechanism under same noise budget, with only logarithmic dependence on parameter count.", "conclusion": "SGM-based FL is competitive with non-sketching private variants and outperforms them in some settings, with adaptive optimization improving performance while maintaining privacy guarantees."}}
{"id": "2509.08225", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08225", "abs": "https://arxiv.org/abs/2509.08225", "authors": ["Matthew Nolan", "Lina Yao", "Robert Davidson"], "title": "Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition", "comment": "37 pages, 10 figures", "summary": "Human Activity Recognition (HAR) has seen significant advancements with the\nadoption of deep learning techniques, yet challenges remain in terms of data\nrequirements, reliability and robustness. This paper explores a novel\napplication of Ensemble Distribution Distillation (EDD) within a\nself-supervised learning framework for HAR aimed at overcoming these\nchallenges. By leveraging unlabeled data and a partially supervised training\nstrategy, our approach yields an increase in predictive accuracy, robust\nestimates of uncertainty, and substantial increases in robustness against\nadversarial perturbation; thereby significantly improving reliability in\nreal-world scenarios without increasing computational complexity at inference.\nWe demonstrate this with an evaluation on several publicly available datasets.\nThe contributions of this work include the development of a self-supervised EDD\nframework, an innovative data augmentation technique designed for HAR, and\nempirical validation of the proposed method's effectiveness in increasing\nrobustness and reliability.", "AI": {"tldr": "Self-supervised Ensemble Distribution Distillation framework for Human Activity Recognition that improves accuracy, uncertainty estimation, and robustness against adversarial attacks without increasing inference complexity.", "motivation": "Address challenges in Human Activity Recognition including data requirements, reliability, and robustness by leveraging unlabeled data through self-supervised learning.", "method": "Uses Ensemble Distribution Distillation within a self-supervised framework with partially supervised training strategy and innovative data augmentation techniques designed specifically for HAR.", "result": "Achieves increased predictive accuracy, robust uncertainty estimates, and substantial improvements in robustness against adversarial perturbations on multiple public datasets.", "conclusion": "The proposed self-supervised EDD framework significantly enhances reliability in real-world HAR scenarios while maintaining computational efficiency during inference."}}
{"id": "2509.08233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08233", "abs": "https://arxiv.org/abs/2509.08233", "authors": ["Kai Yi"], "title": "Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization", "comment": "PhD Dissertation", "summary": "Distributed and federated learning are essential paradigms for training\nmodels across decentralized data sources while preserving privacy, yet\ncommunication overhead remains a major bottleneck. This dissertation explores\nstrategies to improve communication efficiency, focusing on model compression,\nlocal training, and personalization. We establish a unified framework for\nbiased and unbiased compression operators with convergence guarantees, then\npropose adaptive local training strategies that incorporate personalization to\naccelerate convergence and mitigate client drift. In particular, Scafflix\nbalances global and personalized objectives, achieving superior performance\nunder both IID and non-IID settings. We further introduce privacy-preserving\npruning frameworks that optimize sparsity while minimizing communication costs,\nwith Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device\noverhead. Finally, SymWanda, a symmetric post-training pruning method, enhances\nrobustness under high sparsity and maintains accuracy without retraining.\nExtensive experiments on benchmarks and large-scale language models demonstrate\nfavorable trade-offs among accuracy, convergence, and communication, offering\ntheoretical and practical insights for scalable, efficient distributed\nlearning.", "AI": {"tldr": "This dissertation develops communication-efficient strategies for distributed and federated learning through model compression, adaptive local training with personalization, and privacy-preserving pruning frameworks, achieving improved performance and reduced overhead.", "motivation": "Communication overhead is a major bottleneck in distributed and federated learning paradigms that train models across decentralized data sources while preserving privacy.", "method": "Proposes a unified framework for biased/unbiased compression operators, adaptive local training strategies with personalization (Scafflix), privacy-preserving pruning frameworks (Cohort-Squeeze), and symmetric post-training pruning (SymWanda).", "result": "Achieves superior performance under both IID and non-IID settings, reduces communication costs while maintaining accuracy, and enhances robustness under high sparsity without retraining.", "conclusion": "The proposed methods offer favorable trade-offs among accuracy, convergence, and communication, providing theoretical and practical insights for scalable, efficient distributed learning."}}
{"id": "2509.08247", "categories": ["cs.LG", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.08247", "abs": "https://arxiv.org/abs/2509.08247", "authors": ["Xiaolong Luo", "Michael Lingzhi Li"], "title": "The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data", "comment": "15 pages, 9 figures", "summary": "While existing critical care EHR datasets such as MIMIC and eICU have enabled\nsignificant advances in clinical AI research, the CRITICAL dataset opens new\nfrontiers by providing extensive scale and diversity -- containing 1.95 billion\nrecords from 371,365 patients across four geographically diverse CTSA\ninstitutions. CRITICAL's unique strength lies in capturing full-spectrum\npatient journeys, including pre-ICU, ICU, and post-ICU encounters across both\ninpatient and outpatient settings. This multi-institutional, longitudinal\nperspective creates transformative opportunities for developing generalizable\npredictive models and advancing health equity research. However, the richness\nof this multi-site resource introduces substantial complexity in data\nharmonization, with heterogeneous collection practices and diverse vocabulary\nusage patterns requiring sophisticated preprocessing approaches.\n  We present CRISP to unlock the full potential of this valuable resource.\nCRISP systematically transforms raw Observational Medical Outcomes Partnership\nCommon Data Model data into ML-ready datasets through: (1) transparent data\nquality management with comprehensive audit trails, (2) cross-vocabulary\nmapping of heterogeneous medical terminologies to unified SNOMED-CT standards,\nwith deduplication and unit standardization, (3) modular architecture with\nparallel optimization enabling complete dataset processing in $<$1 day even on\nstandard computing hardware, and (4) comprehensive baseline model benchmarks\nspanning multiple clinical prediction tasks to establish reproducible\nperformance standards. By providing processing pipeline, baseline\nimplementations, and detailed transformation documentation, CRISP saves\nresearchers months of preprocessing effort and democratizes access to\nlarge-scale multi-institutional critical care data, enabling them to focus on\nadvancing clinical AI.", "AI": {"tldr": "CRITICAL dataset provides 1.95B records from 371K patients across 4 institutions, enabling full-spectrum critical care research. CRISP is a preprocessing pipeline that transforms raw data into ML-ready format with quality management, vocabulary standardization, and efficient processing.", "motivation": "Existing critical care datasets like MIMIC and eICU lack the scale, diversity, and longitudinal perspective needed for generalizable clinical AI models. CRITICAL addresses this with multi-institutional data capturing pre-ICU, ICU, and post-ICU patient journeys across diverse settings.", "method": "CRISP systematically processes Observational Medical Outcomes Partnership Common Data Model data through: (1) transparent data quality management with audit trails, (2) cross-vocabulary mapping to SNOMED-CT standards with deduplication and unit standardization, (3) modular architecture for efficient processing (<1 day), and (4) comprehensive baseline model benchmarks.", "result": "The pipeline enables complete dataset processing in under 1 day on standard hardware, provides ML-ready datasets, establishes reproducible performance standards through baseline benchmarks, and saves researchers months of preprocessing effort.", "conclusion": "CRISP democratizes access to large-scale multi-institutional critical care data by providing a comprehensive preprocessing solution, allowing researchers to focus on advancing clinical AI rather than data preparation challenges."}}
{"id": "2509.08255", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08255", "abs": "https://arxiv.org/abs/2509.08255", "authors": ["Wei Huang", "Anda Cheng", "Yinggui Wang"], "title": "Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning", "comment": "Accepted by emnlp2025", "summary": "Recent advancements in large language models (LLMs) have shown impressive\ncapabilities in various downstream tasks but typically face Catastrophic\nForgetting (CF) during fine-tuning. In this paper, we propose the\nForgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to\nbalance CF and downstream task performance. Our investigation reveals that the\ndegree to which task vectors (i.e., the subtraction of pre-trained weights from\nthe weights fine-tuned on downstream tasks) overlap with pre-trained model\nparameters is a critical factor for CF. Based on this finding, FAPM employs the\nratio of the task vector to pre-trained model parameters as a metric to\nquantify CF, integrating this measure into the pruning criteria. Importantly,\nFAPM does not necessitate modifications to the training process or model\narchitecture, nor does it require any auxiliary data. We conducted extensive\nexperiments across eight datasets, covering natural language inference, General\nQ&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results\ndemonstrate that FAPM limits CF to just 0.25\\% while maintaining 99.67\\%\naccuracy on downstream tasks. We provide the code to reproduce our results.", "AI": {"tldr": "FAPM is a novel pruning metric that reduces catastrophic forgetting to 0.25% while maintaining 99.67% downstream task accuracy by measuring task vector overlap with pre-trained parameters.", "motivation": "Large language models suffer from catastrophic forgetting during fine-tuning, which degrades their performance on original capabilities while adapting to new tasks.", "method": "Proposes Forgetting-Aware Pruning Metric (FAPM) that uses the ratio of task vectors to pre-trained parameters as a pruning criterion to quantify and mitigate catastrophic forgetting without changing training process or architecture.", "result": "Extensive experiments across 8 datasets show FAPM limits catastrophic forgetting to just 0.25% while maintaining 99.67% accuracy on downstream tasks.", "conclusion": "FAPM effectively balances catastrophic forgetting and downstream performance through a simple yet effective pruning-based approach that requires no architectural changes or auxiliary data."}}
{"id": "2509.08270", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08270", "abs": "https://arxiv.org/abs/2509.08270", "authors": ["Pranav Pawar", "Kavish Shah", "Akshat Bhalani", "Komal Kasat", "Dev Mittal", "Hadi Gala", "Deepali Patil", "Nikita Raichada", "Monali Deshmukh"], "title": "Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models", "comment": null, "summary": "As Vision-Language Models (VLMs) grow in sophistication, their ability to\nperform reasoning is coming under increasing supervision. While they excel at\nmany tasks, their grasp of fundamental scientific principles, such as physics,\nremains an underexplored frontier. To reflect the advancements in these\ncapabilities, we introduce a novel and accessible framework designed to\nrigorously evaluate VLMs on their understanding of 2D physics. Our framework\nfeatures a pragmatic scenario generator that creates a diverse testbed of over\n400 problems across four core domains: Projectile Motion, Collision Dynamics,\nMechanics, and Fluid Dynamics. Through comprehensive evaluation of four\nstate-of-the-art VLMs, we demonstrate a strong correlation between model scale\nand reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving\nan overall score of 0.815. We find that while models excel at formulaic\nproblems, they struggle significantly with domains requiring abstract spatial\nreasoning. By designing this framework, we aim to democratize the study of\nscientific reasoning in VLMs and foster deeper insights into their capabilities\nand limitations.", "AI": {"tldr": "A new framework evaluates Vision-Language Models' understanding of 2D physics through 400+ problems across four domains, showing correlation between model scale and performance but revealing struggles with abstract spatial reasoning.", "motivation": "As VLMs advance, their grasp of fundamental scientific principles like physics remains underexplored, requiring rigorous evaluation methods to assess their reasoning capabilities.", "method": "Developed a pragmatic scenario generator creating diverse testbed with over 400 problems across Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics domains to evaluate four state-of-the-art VLMs.", "result": "Strong correlation found between model scale and reasoning ability, with top model Qwen2.5-VL-7B achieving 0.815 overall score. Models excel at formulaic problems but struggle significantly with abstract spatial reasoning domains.", "conclusion": "The framework democratizes study of scientific reasoning in VLMs and provides deeper insights into their capabilities and limitations, particularly highlighting challenges in spatial reasoning tasks."}}
{"id": "2509.08277", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08277", "abs": "https://arxiv.org/abs/2509.08277", "authors": ["Dung T. Tran", "Huyen Ngoc Huyen", "Hong Nguyen", "Xuan-Vu Phan", "Nam-Phong Nguyen"], "title": "Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning", "comment": null, "summary": "Rainfall forecasting in Vietnam is highly challenging due to its diverse\nclimatic conditions and strong geographical variability across river basins,\nyet accurate and reliable forecasts are vital for flood management, hydropower\noperation, and disaster preparedness. In this work, we propose a Matrix\nProfile-based Weighted Ensemble (MPWE), a regime-switching framework that\ndynamically captures covariant dependencies among multiple geographical model\nforecasts while incorporating redundancy-aware weighting to balance\ncontributions across models. We evaluate MPWE using rainfall forecasts from\neight major basins in Vietnam, spanning five forecast horizons (1 hour and\naccumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results\nshow that MPWE consistently achieves lower mean and standard deviation of\nprediction errors compared to geographical models and ensemble baselines,\ndemonstrating both improved accuracy and stability across basins and horizons.", "AI": {"tldr": "MPWE framework for rainfall forecasting in Vietnam that dynamically weights ensemble models using matrix profile analysis to capture covariant dependencies and reduce redundancy", "motivation": "Accurate rainfall forecasting is crucial for flood management and disaster preparedness in Vietnam's diverse climatic conditions, but existing geographical models face challenges with variability across river basins", "method": "Matrix Profile-based Weighted Ensemble (MPWE) that uses regime-switching to dynamically capture covariant dependencies among multiple geographical forecasts with redundancy-aware weighting", "result": "MPWE consistently achieved lower mean and standard deviation of prediction errors compared to geographical models and ensemble baselines across 8 major basins and multiple forecast horizons", "conclusion": "The proposed MPWE framework provides both improved accuracy and stability in rainfall forecasting, making it valuable for flood management and disaster preparedness applications in Vietnam"}}
{"id": "2509.08300", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08300", "abs": "https://arxiv.org/abs/2509.08300", "authors": ["Yao Lu", "Chunfeng Sun", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "\\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition", "comment": null, "summary": "Deep learning-based Automatic Modulation Recognition (AMR) model has made\nsignificant progress with the support of large-scale labeled data. However,\nwhen developing new models or performing hyperparameter tuning, the time and\nenergy consumption associated with repeated training using massive amounts of\ndata are often unbearable. To address the above challenges, we propose\n\\emph{FoQuS}, which approximates the effect of full training by selecting a\ncoreset from the original dataset, thereby significantly reducing training\noverhead. Specifically, \\emph{FoQuS} records the prediction trajectory of each\nsample during full-dataset training and constructs three importance metrics\nbased on training dynamics. Experiments show that \\emph{FoQuS} can maintain\nhigh recognition accuracy and good cross-architecture generalization on\nmultiple AMR datasets using only 1\\%-30\\% of the original data.", "AI": {"tldr": "FoQuS reduces training overhead for AMR models by selecting core subsets (1%-30% of data) using training dynamics metrics, maintaining high accuracy and cross-architecture generalization.", "motivation": "Deep learning AMR models require massive labeled data, making repeated training for model development and hyperparameter tuning time-consuming and energy-intensive.", "method": "FoQuS records prediction trajectories during full-dataset training and constructs three importance metrics based on training dynamics to select a coreset from the original dataset.", "result": "Experiments show FoQuS maintains high recognition accuracy and good cross-architecture generalization on multiple AMR datasets using only 1%-30% of original data.", "conclusion": "FoQuS effectively approximates full training effects while significantly reducing computational overhead, making AMR model development more efficient."}}
{"id": "2509.08315", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.08315", "abs": "https://arxiv.org/abs/2509.08315", "authors": ["Bohan Yu", "Yekun Chai"], "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference", "comment": null, "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.", "AI": {"tldr": "EvolKV is an adaptive framework that uses evolutionary search to optimize layer-wise KV cache compression, achieving better performance than heuristic methods while using significantly less memory.", "motivation": "Existing KV cache compression methods rely on heuristics that ignore layer-specific feature patterns and task performance interactions, leading to degraded generalization.", "method": "Reformulates cache allocation as multi-objective optimization problem and uses evolutionary search to dynamically configure layer budgets while maximizing downstream performance.", "result": "Outperforms all baseline methods on 11 tasks, achieves up to 7% improvement on GSM8K, and surpasses full KV cache performance on code completion using only 1.5% of original budget.", "conclusion": "EvolKV demonstrates the untapped potential of learned compression strategies for KV cache budget allocation, showing superior memory efficiency and task performance over heuristic approaches."}}
{"id": "2509.08329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08329", "abs": "https://arxiv.org/abs/2509.08329", "authors": ["Lukas Toral", "Teddy Lazebnik"], "title": "Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing", "comment": null, "summary": "Reinforcement Learning (RL) algorithms often require long training to become\nuseful, especially in complex environments with sparse rewards. While\ntechniques like reward shaping and curriculum learning exist to accelerate\ntraining, these are often extremely specific and require the developer's\nprofessionalism and dedicated expertise in the problem's domain. Tackling this\nchallenge, in this study, we explore the effectiveness of pre-trained Large\nLanguage Models (LLMs) as tutors in a student-teacher architecture with RL\nalgorithms, hypothesizing that LLM-generated guidance allows for faster\nconvergence. In particular, we explore the effectiveness of reusing the LLM's\nadvice on the RL's convergence dynamics. Through an extensive empirical\nexamination, which included 54 configurations, varying the RL algorithm (DQN,\nPPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,\nSnake, Connect Four), our results demonstrate that LLM tutoring significantly\naccelerates RL convergence while maintaining comparable optimal performance.\nFurthermore, the advice reuse mechanism shows a further improvement in training\nduration but also results in less stable convergence dynamics. Our findings\nsuggest that LLM tutoring generally improves convergence, and its effectiveness\nis sensitive to the specific task, RL algorithm, and LLM model combination.", "AI": {"tldr": "LLM tutoring accelerates RL convergence while maintaining optimal performance, with advice reuse further reducing training time but causing less stable convergence.", "motivation": "RL algorithms require long training in complex environments, and existing acceleration techniques are domain-specific and require expert knowledge.", "method": "Student-teacher architecture using pre-trained LLMs as tutors for RL algorithms (DQN, PPO, A2C), with extensive testing across 54 configurations varying algorithms, LLM models, and environments.", "result": "LLM tutoring significantly accelerates RL convergence while maintaining comparable optimal performance. Advice reuse further improves training duration but results in less stable convergence dynamics.", "conclusion": "LLM tutoring generally improves RL convergence, but effectiveness depends on specific task, RL algorithm, and LLM model combination."}}
{"id": "2509.08342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08342", "abs": "https://arxiv.org/abs/2509.08342", "authors": ["Jiaming Yan", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.", "AI": {"tldr": "MoEpic is an efficient MoE inference system that vertically splits experts into top/bottom segments, caches top segments of hot experts to improve cache hit rate, prefetches experts for next layers, and uses adaptive cache configuration to reduce GPU memory requirements and inference latency.", "motivation": "Mixture-of-Experts (MoE) models have massive parameters that impose heavy GPU memory demands, hindering widespread adoption. Existing offloading approaches suffer from poor cache hit rates and significant expert loading latency during inference.", "method": "MoEpic vertically divides each expert into top and bottom segments, caches top segments of hot experts to store more experts under limited VRAM, predicts and prefetches activated experts for next layers, and uses a divide-and-conquer algorithm for adaptive cache configuration.", "result": "Extensive experiments show MoEpic saves about half of the GPU cost while lowering inference latency by 37.51%-65.73% compared to baselines.", "conclusion": "MoEpic effectively addresses GPU memory constraints in MoE LLMs through expert segmentation, intelligent caching, and adaptive configuration, enabling more efficient deployment of large MoE models."}}
{"id": "2509.08383", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08383", "abs": "https://arxiv.org/abs/2509.08383", "authors": ["Matan Avitan", "Moran Baruch", "Nir Drucker", "Itamar Zimerman", "Yoav Goldberg"], "title": "Efficient Decoding Methods for Language Models on Encrypted Data", "comment": null, "summary": "Large language models (LLMs) power modern AI applications, but processing\nsensitive data on untrusted servers raises privacy concerns. Homomorphic\nencryption (HE) enables computation on encrypted data for secure inference.\nHowever, neural text generation requires decoding methods like argmax and\nsampling, which are non-polynomial and thus computationally expensive under\nencryption, creating a significant performance bottleneck. We introduce cutmax,\nan HE-friendly argmax algorithm that reduces ciphertext operations compared to\nprior methods, enabling practical greedy decoding under encryption. We also\npropose the first HE-compatible nucleus (top-p) sampling method, leveraging\ncutmax for efficient stochastic decoding with provable privacy guarantees. Both\ntechniques are polynomial, supporting efficient inference in privacy-preserving\nsettings. Moreover, their differentiability facilitates gradient-based\nsequence-level optimization as a polynomial alternative to straight-through\nestimators. We further provide strong theoretical guarantees for cutmax,\nproving it converges globally to a unique two-level fixed point, independent of\nthe input values beyond the identity of the maximizer, which explains its rapid\nconvergence in just a few iterations. Evaluations on realistic LLM outputs show\nlatency reductions of 24x-35x over baselines, advancing secure text generation.", "AI": {"tldr": "Cutmax is an HE-friendly argmax algorithm that enables efficient greedy decoding under homomorphic encryption, reducing latency by 24x-35x compared to baselines for secure text generation.", "motivation": "Privacy concerns arise when processing sensitive data with LLMs on untrusted servers. Homomorphic encryption enables secure computation but existing decoding methods (argmax, sampling) are computationally expensive under encryption, creating performance bottlenecks.", "method": "Introduces cutmax (HE-friendly argmax algorithm) and the first HE-compatible nucleus (top-p) sampling method. Both techniques are polynomial and differentiable, supporting efficient inference and gradient-based sequence-level optimization.", "result": "Achieves latency reductions of 24x-35x over baselines on realistic LLM outputs. Cutmax converges globally to a unique two-level fixed point in just a few iterations.", "conclusion": "The proposed techniques enable practical greedy and stochastic decoding under encryption with provable privacy guarantees, significantly advancing secure text generation capabilities."}}
{"id": "2509.08359", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08359", "abs": "https://arxiv.org/abs/2509.08359", "authors": ["Haeun Jeon", "Hyunglip Bae", "Chanyeong Kim", "Yongjae Lee", "Woo Chang Kim"], "title": "Prediction Loss Guided Decision-Focused Learning", "comment": null, "summary": "Decision-making under uncertainty is often considered in two stages:\npredicting the unknown parameters, and then optimizing decisions based on\npredictions. While traditional prediction-focused learning (PFL) treats these\ntwo stages separately, decision-focused learning (DFL) trains the predictive\nmodel by directly optimizing the decision quality in an end-to-end manner.\nHowever, despite using exact or well-approximated gradients, vanilla DFL often\nsuffers from unstable convergence due to its flat-and-sharp loss landscapes. In\ncontrast, PFL yields more stable optimization, but overlooks the downstream\ndecision quality. To address this, we propose a simple yet effective approach:\nperturbing the decision loss gradient using the prediction loss gradient to\nconstruct an update direction. Our method requires no additional training and\ncan be integrated with any DFL solvers. Using the sigmoid-like decaying\nparameter, we let the prediction loss gradient guide the decision loss gradient\nto train a predictive model that optimizes decision quality. Also, we provide a\ntheoretical convergence guarantee to Pareto stationary point under mild\nassumptions. Empirically, we demonstrate our method across three stochastic\noptimization problems, showing promising results compared to other baselines.\nWe validate that our approach achieves lower regret with more stable training,\neven in situations where either PFL or DFL struggles.", "AI": {"tldr": "A novel gradient perturbation method that combines prediction and decision loss gradients to improve decision-focused learning stability and performance.", "motivation": "Traditional prediction-focused learning (PFL) optimizes prediction accuracy but ignores downstream decision quality, while decision-focused learning (DFL) directly optimizes decisions but suffers from unstable convergence due to flat-and-sharp loss landscapes.", "method": "Proposes perturbing the decision loss gradient using the prediction loss gradient with a sigmoid-like decaying parameter, creating an update direction that guides training without additional training requirements.", "result": "The method achieves lower regret with more stable training across three stochastic optimization problems, outperforming both PFL and DFL baselines in challenging scenarios.", "conclusion": "The approach successfully bridges the gap between stable optimization and decision quality optimization, providing a theoretically guaranteed convergence to Pareto stationary points while being compatible with any DFL solver."}}
{"id": "2509.08461", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.08461", "abs": "https://arxiv.org/abs/2509.08461", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.", "AI": {"tldr": "Vision Language Models (VLMs) outperform CNNs in neutrino interaction classification, offering better performance, interpretability, and flexibility for integrating multimodal data in high-energy physics.", "motivation": "To explore the application of Vision Language Models (specifically fine-tuned LLaMa 3.2) for identifying neutrino interactions in pixelated detector data from high-energy physics experiments, leveraging their multimodal reasoning capabilities beyond traditional CNN approaches.", "method": "Benchmarked a fine-tuned Vision Language Model (LLaMa 3.2 variant) against state-of-the-art convolutional neural network architectures used in NOvA and DUNE experiments for classifying electron and muon neutrino events in pixelated detector data.", "result": "VLMs outperformed CNNs in classification performance while providing greater interpretability through reasoning-based predictions and better flexibility for integrating auxiliary textual or semantic information.", "conclusion": "VLMs show strong potential as a general-purpose backbone for physics event classification due to their high performance, interpretability, and generalizability, opening new avenues for multimodal reasoning in experimental neutrino physics."}}
{"id": "2509.08372", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08372", "abs": "https://arxiv.org/abs/2509.08372", "authors": ["Kosuke Kihara", "Junki Mori", "Taiki Miyagawa", "Akinori F. Ebihara"], "title": "Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models", "comment": "Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge\n  Intelligence: Smart, Efficient, and Scalable Solutions for IoT, Wearables,\n  and Embedded Devices (SEEDS)", "summary": "Federated Learning (FL) offers a framework for training models\ncollaboratively while preserving data privacy of each client. Recently,\nresearch has focused on Federated Source-Free Domain Adaptation (FFREEDA), a\nmore realistic scenario wherein client-held target domain data remains\nunlabeled, and the server can access source domain data only during\npre-training. We extend this framework to a more complex and realistic setting:\nClass Imbalanced FFREEDA (CI-FFREEDA), which takes into account class\nimbalances in both the source and target domains, as well as label shifts\nbetween source and target and among target clients. The replication of existing\nmethods in our experimental setup lead us to rethink the focus from enhancing\naggregation and domain adaptation methods to improving the feature extractors\nwithin the network itself. We propose replacing the FFREEDA backbone with a\nfrozen vision foundation model (VFM), thereby improving overall accuracy\nwithout extensive parameter tuning and reducing computational and communication\ncosts in federated learning. Our experimental results demonstrate that VFMs\neffectively mitigate the effects of domain gaps, class imbalances, and even\nnon-IID-ness among target clients, suggesting that strong feature extractors,\nnot complex adaptation or FL methods, are key to success in the real-world FL.", "AI": {"tldr": "Replacing FFREEDA backbone with frozen vision foundation models improves accuracy and reduces costs in class-imbalanced federated learning without complex parameter tuning.", "motivation": "Address class imbalances in both source and target domains, label shifts between domains and clients in federated learning scenarios where target data remains unlabeled.", "method": "Replace FFREEDA backbone with frozen vision foundation model (VFM) instead of enhancing aggregation or domain adaptation methods.", "result": "VFMs effectively mitigate domain gaps, class imbalances, and non-IID-ness among target clients, improving overall accuracy.", "conclusion": "Strong feature extractors (VFMs) are more important than complex adaptation or FL methods for real-world federated learning success."}}
{"id": "2509.08515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08515", "abs": "https://arxiv.org/abs/2509.08515", "authors": ["Alicia Tierz", "Jad Mounayer", "Beatriz Moya", "Francisco Chinesta"], "title": "Variational Rank Reduction Autoencoders for Generative", "comment": null, "summary": "Generative thermal design for complex geometries is fundamental in many areas\nof engineering, yet it faces two main challenges: the high computational cost\nof high-fidelity simulations and the limitations of conventional generative\nmodels. Approaches such as autoencoders (AEs) and variational autoencoders\n(VAEs) often produce unstructured latent spaces with discontinuities, which\nrestricts their capacity to explore designs and generate physically consistent\nsolutions.\n  To address these limitations, we propose a hybrid framework that combines\nVariational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks\n(DeepONets). The VRRAE introduces a truncated SVD within the latent space,\nleading to continuous, interpretable, and well-structured representations that\nmitigate posterior collapse and improve geometric reconstruction. The DeepONet\nthen exploits this compact latent encoding in its branch network, together with\nspatial coordinates in the trunk network, to predict temperature gradients\nefficiently and accurately.\n  This hybrid approach not only enhances the quality of generated geometries\nand the accuracy of gradient prediction, but also provides a substantial\nadvantage in inference efficiency compared to traditional numerical solvers.\nOverall, the study underscores the importance of structured latent\nrepresentations for operator learning and highlights the potential of combining\ngenerative models and operator networks in thermal design and broader\nengineering applications.", "AI": {"tldr": "Hybrid framework combining VRRAEs and DeepONets for efficient generative thermal design with continuous latent spaces and accurate gradient prediction.", "motivation": "Address high computational costs of thermal simulations and limitations of conventional generative models (AEs/VAEs) that produce unstructured latent spaces with discontinuities.", "method": "VRRAE with truncated SVD for continuous, interpretable latent representations + DeepONet using latent encoding in branch network and spatial coordinates in trunk network for temperature gradient prediction.", "result": "Enhanced geometry generation quality, improved gradient prediction accuracy, and substantial inference efficiency gains compared to traditional numerical solvers.", "conclusion": "Structured latent representations are crucial for operator learning, and combining generative models with operator networks shows great potential for thermal design and engineering applications."}}
{"id": "2509.08592", "categories": ["cs.LG", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.08592", "abs": "https://arxiv.org/abs/2509.08592", "authors": ["Aadit Sengupta", "Pratinav Seth", "Vinay Kumar Sankarapu"], "title": "Interpretability as Alignment: Making Internal Understanding a Design Principle", "comment": "Pre-Print", "summary": "Large neural models are increasingly deployed in high-stakes settings,\nraising concerns about whether their behavior reliably aligns with human\nvalues. Interpretability provides a route to internal transparency by revealing\nthe computations that drive outputs. We argue that interpretability especially\nmechanistic approaches should be treated as a design principle for alignment,\nnot an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer\nintuitive but correlational explanations, while mechanistic techniques like\ncircuit tracing or activation patching yield causal insight into internal\nfailures, including deceptive or misaligned reasoning that behavioral methods\nlike RLHF, red teaming, or Constitutional AI may overlook. Despite these\nadvantages, interpretability faces challenges of scalability, epistemic\nuncertainty, and mismatches between learned representations and human concepts.\nOur position is that progress on safe and trustworthy AI will depend on making\ninterpretability a first-class objective of AI research and development,\nensuring that systems are not only effective but also auditable, transparent,\nand aligned with human intent.", "AI": {"tldr": "Interpretability should be a core design principle for AI alignment, not just a diagnostic tool, as mechanistic methods provide causal insights into internal failures that behavioral methods may miss.", "motivation": "Large neural models deployed in high-stakes settings need reliable alignment with human values, requiring internal transparency to understand their computations and ensure trustworthy behavior.", "method": "Advocates for mechanistic interpretability approaches (circuit tracing, activation patching) over post-hoc methods (LIME, SHAP) to gain causal understanding of model internals and detect deceptive/misaligned reasoning.", "result": "Identifies that mechanistic interpretability provides superior causal insights into model failures compared to correlational post-hoc methods, though faces scalability and representation challenges.", "conclusion": "Progress on safe AI depends on making interpretability a first-class objective in AI R&D to ensure systems are auditable, transparent, and aligned with human intent."}}
{"id": "2509.08401", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08401", "abs": "https://arxiv.org/abs/2509.08401", "authors": ["Xunkai Li", "Daohan Su", "Sicheng Liu", "Ru Zhang", "Rong-Hua Li", "Guoren Wang"], "title": "Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models", "comment": null, "summary": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios.", "AI": {"tldr": "MoT addresses domain generalization conflicts in graph foundation models by introducing Information Tinker and Regularization Tinker components to prevent model degradation and representation collapse, achieving state-of-the-art performance across multiple domains.", "motivation": "Graph foundation models suffer from domain generalization conflicts that cause imperceptible pitfalls - model degradation (encoder/codebook failing to capture input diversity) and representation collapse (embedding/codebook vectors losing semantic separability), which collectively impair decoder performance and create optimization dilemmas during pre-training.", "method": "Proposes MoT (Mixture-of-Tinkers) with two components: (1) Information Tinker using edge-wise semantic fusion and mixture-of-codebooks with domain-aware routing to improve information capacity, and (2) Regularization Tinker with two additional regularizations to improve gradient supervision. The architecture adheres to GFM scaling laws for controllable model scale.", "result": "Experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios compared to state-of-the-art baselines.", "conclusion": "MoT effectively addresses the Information Bottleneck and Regularization Deficit challenges in graph foundation models, providing a flexible architecture that prevents optimization dilemmas and enables robust cross-domain generalization capabilities."}}
{"id": "2509.08606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08606", "abs": "https://arxiv.org/abs/2509.08606", "authors": ["Alireza Sameh", "Mehrdad Rostami", "Mourad Oussalah", "Vahid Farrahi"], "title": "Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques", "comment": null, "summary": "Purpose: We compared the performance of deep learning (DL) and classical\nmachine learning (ML) algorithms for the classification of 24-hour movement\nbehavior into sleep, sedentary, light intensity physical activity (LPA), and\nmoderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access\ndata from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.\nParticipants were randomly divided into training, validation, and test sets\n(121, 15, and 15 participants each). Raw acceleration signals were segmented\ninto non-overlapping 10-second windows, and then a total of 104 handcrafted\nfeatures were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),\nBidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and\nOne-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw\nacceleration signals and with handcrafted features extracted from these signals\nto predict 24-hour movement behavior categories. The handcrafted features were\nalso used to train classical ML algorithms, namely Random Forest (RF), Support\nVector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression\n(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying\n24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an\noverall accuracy of approximately 85% when trained with raw acceleration\nsignals, and 1D-CNN an overall accuracy of approximately 80%. When trained on\nhandcrafted features, the overall accuracy for both DL and classical ML\nalgorithms ranged from 70% to 81%. Overall, there was a higher confusion in\nclassification of MVPA and LPA, compared to sleep and sedentary categories.\nConclusion: DL methods with raw acceleration signals had only slightly better\nperformance in predicting 24-hour movement behavior intensities, compared to\nwhen DL and classical ML were trained with handcrafted features.", "AI": {"tldr": "DL algorithms using raw acceleration signals achieved 80-85% accuracy for 24-hour movement behavior classification, slightly outperforming classical ML methods (70-81% accuracy) using handcrafted features.", "motivation": "To compare the performance of deep learning and classical machine learning algorithms for classifying 24-hour movement behaviors (sleep, sedentary, LPA, MVPA) from wrist-worn accelerometer data.", "method": "Used data from 151 adults wearing wrist accelerometers. Extracted 104 handcrafted features from 10-second windows. Compared 4 DL algorithms (LSTM, BiLSTM, GRU, 1D-CNN) using raw signals and handcrafted features against 6 classical ML algorithms (RF, SVM, XGBoost, LR, ANN, DT) using handcrafted features only.", "result": "LSTM, BiLSTM, and GRU achieved ~85% accuracy with raw signals, 1D-CNN achieved ~80%. Both DL and classical ML ranged 70-81% accuracy with handcrafted features. Higher confusion between MVPA and LPA categories compared to sleep/sedentary.", "conclusion": "DL methods with raw acceleration signals showed only slightly better performance than DL/classical ML trained with handcrafted features for movement behavior classification."}}
{"id": "2509.08697", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08697", "abs": "https://arxiv.org/abs/2509.08697", "authors": ["James Gong", "Raymond Luo", "Emma Wang", "Leon Ge", "Bruce Li", "Felix Marattukalam", "Waleed Abdulla"], "title": "Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective", "comment": "6 pages", "summary": "Backpropagation is the pivotal algorithm underpinning the success of\nartificial neural networks, yet it has critical limitations such as\nbiologically implausible backward locking and global error propagation. To\ncircumvent these constraints, the Forward-Forward algorithm was proposed as a\nmore biologically plausible method that replaces the backward pass with an\nadditional forward pass. Despite this advantage, the Forward-Forward algorithm\nsignificantly trails backpropagation in accuracy, and its optimal form exhibits\nlow inference efficiency due to multiple forward passes required. In this work,\nthe Forward-Forward algorithm is reshaped through its integration with\nsimilarity learning frameworks, eliminating the need for multiple forward\npasses during inference. This proposed algorithm is named Forward-Forward\nAlgorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical\nevaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST\nsubstantially improves accuracy, narrowing the gap with backpropagation. On\nCIFAR-10, FAUST achieves 56.22\\% accuracy with a simple multi-layer perceptron\narchitecture, approaching the backpropagation benchmark of 57.63\\% accuracy.", "AI": {"tldr": "FAUST integrates Forward-Forward algorithm with similarity learning to eliminate multiple forward passes during inference, improving accuracy while maintaining biological plausibility.", "motivation": "Address limitations of backpropagation (biologically implausible backward locking) and Forward-Forward algorithm (low accuracy and inference efficiency due to multiple forward passes).", "method": "Integrates Forward-Forward algorithm with similarity learning frameworks using tuplet loss, eliminating the need for multiple forward passes during inference.", "result": "Achieves 56.22% accuracy on CIFAR-10 with MLP, approaching backpropagation's 57.63%. Substantial improvements on MNIST, Fashion-MNIST datasets.", "conclusion": "FAUST successfully bridges the accuracy gap with backpropagation while maintaining biological plausibility and improving inference efficiency over original Forward-Forward."}}
{"id": "2509.08467", "categories": ["cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2509.08467", "abs": "https://arxiv.org/abs/2509.08467", "authors": ["Patrick J. Laub", "Tu Pho", "Bernard Wong"], "title": "An Interpretable Deep Learning Model for General Insurance Pricing", "comment": null, "summary": "This paper introduces the Actuarial Neural Additive Model, an inherently\ninterpretable deep learning model for general insurance pricing that offers\nfully transparent and interpretable results while retaining the strong\npredictive power of neural networks. This model assigns a dedicated neural\nnetwork (or subnetwork) to each individual covariate and pairwise interaction\nterm to independently learn its impact on the modeled output while implementing\nvarious architectural constraints to allow for essential interpretability (e.g.\nsparsity) and practical requirements (e.g. smoothness, monotonicity) in\ninsurance applications. The development of our model is grounded in a solid\nfoundation, where we establish a concrete definition of interpretability within\nthe insurance context, complemented by a rigorous mathematical framework.\nComparisons in terms of prediction accuracy are made with traditional actuarial\nand state-of-the-art machine learning methods using both synthetic and real\ninsurance datasets. The results show that the proposed model outperforms other\nmethods in most cases while offering complete transparency in its internal\nlogic, underscoring the strong interpretability and predictive capability.", "AI": {"tldr": "ANAM is an interpretable deep learning model for insurance pricing that uses separate neural networks for each covariate and interaction term, maintaining predictive power while providing full transparency.", "motivation": "To bridge the gap between traditional actuarial models' interpretability and neural networks' predictive power in insurance pricing applications.", "method": "Assigns dedicated neural networks to each covariate and pairwise interaction term with architectural constraints for sparsity, smoothness, and monotonicity requirements.", "result": "Outperforms traditional actuarial and state-of-the-art machine learning methods in most cases while offering complete transparency.", "conclusion": "The model successfully combines strong predictive capability with full interpretability, making it suitable for practical insurance pricing applications."}}
{"id": "2509.08698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08698", "abs": "https://arxiv.org/abs/2509.08698", "authors": ["Thorsten Wittkopp"], "title": "A layered architecture for log analysis in complex IT systems", "comment": "Dissertation", "summary": "In the evolving IT landscape, stability and reliability of systems are\nessential, yet their growing complexity challenges DevOps teams in\nimplementation and maintenance. Log analysis, a core element of AIOps, provides\ncritical insights into complex behaviors and failures. This dissertation\nintroduces a three-layered architecture to support DevOps in failure\nresolution. The first layer, Log Investigation, performs autonomous log\nlabeling and anomaly classification. We propose a method that labels log data\nwithout manual effort, enabling supervised training and precise evaluation of\nanomaly detection. Additionally, we define a taxonomy that groups anomalies\ninto three categories, ensuring appropriate method selection. The second layer,\nAnomaly Detection, detects behaviors deviating from the norm. We propose a\nflexible Anomaly Detection method adaptable to unsupervised, weakly supervised,\nand supervised training. Evaluations on public and industry datasets show\nF1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third\nlayer, Root Cause Analysis, identifies minimal log sets describing failures,\ntheir origin, and event sequences. By balancing training data and identifying\nkey services, our Root Cause Analysis method consistently detects 90-98% of\nroot cause log lines within the top 10 candidates, providing actionable\ninsights for mitigation. Our research addresses how log analysis methods can be\ndesigned and optimized to help DevOps resolve failures efficiently. By\nintegrating these three layers, the architecture equips teams with robust\nmethods to enhance IT system reliability.", "AI": {"tldr": "Three-layered AIOps architecture for automated log analysis: autonomous labeling, flexible anomaly detection, and root cause analysis to help DevOps teams efficiently resolve system failures.", "motivation": "Growing complexity of IT systems challenges DevOps teams in maintaining stability and reliability. Log analysis provides critical insights but requires automation to handle complex behaviors and failures efficiently.", "method": "Three-layer architecture: 1) Log Investigation - autonomous log labeling and anomaly classification with taxonomy; 2) Anomaly Detection - flexible method for unsupervised/weakly supervised/supervised training; 3) Root Cause Analysis - identifies minimal log sets describing failures and event sequences.", "result": "F1-scores of 0.98-1.0 for anomaly detection on public and industry datasets. Root cause analysis detects 90-98% of root cause log lines within top 10 candidates. Provides actionable insights for failure mitigation.", "conclusion": "The integrated three-layer architecture provides robust methods for DevOps teams to enhance IT system reliability through automated log analysis, enabling efficient failure resolution without manual effort."}}
{"id": "2509.08482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08482", "abs": "https://arxiv.org/abs/2509.08482", "authors": ["Andrea Maldonado", "Christian M. M. Frey", "Sai Anirudh Aryasomayajula", "Ludwig Zellner", "Stephan A. Fahrenkrog-Petersen", "Thomas Seidl"], "title": "SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms", "comment": null, "summary": "Process mining aims to extract and analyze insights from event logs, yet\nalgorithm metric results vary widely depending on structural event log\ncharacteristics. Existing work often evaluates algorithms on a fixed set of\nreal-world event logs but lacks a systematic analysis of how event log\ncharacteristics impact algorithms individually. Moreover, since event logs are\ngenerated from processes, where characteristics co-occur, we focus on\nassociational rather than causal effects to assess how strong the overlapping\nindividual characteristic affects evaluation metrics without assuming isolated\ncausal effects, a factor often neglected by prior work. We introduce SHAining,\nthe first approach to quantify the marginal contribution of varying event log\ncharacteristics to process mining algorithms' metrics. Using process discovery\nas a downstream task, we analyze over 22,000 event logs covering a wide span of\ncharacteristics to uncover which affect algorithms across metrics (e.g.,\nfitness, precision, complexity) the most. Furthermore, we offer novel insights\nabout how the value of event log characteristics correlates with their\ncontributed impact, assessing the algorithm's robustness.", "AI": {"tldr": "SHAining is a novel approach that quantifies how different event log characteristics impact process mining algorithm performance metrics, analyzing over 22,000 event logs to identify which characteristics most affect algorithms across various metrics.", "motivation": "Existing process mining research evaluates algorithms on fixed event logs but lacks systematic analysis of how specific event log characteristics individually impact algorithm performance, especially considering that characteristics co-occur in real-world processes.", "method": "The SHAining approach quantifies marginal contributions of varying event log characteristics to process mining metrics. Using process discovery as a downstream task, it analyzes over 22,000 event logs covering diverse characteristics to assess their impact on metrics like fitness, precision, and complexity.", "result": "The approach uncovers which event log characteristics affect algorithms across different metrics the most and provides insights about how characteristic values correlate with their contributed impact, assessing algorithm robustness.", "conclusion": "SHAining provides the first systematic quantification of how event log characteristics impact process mining algorithm performance, offering valuable insights into algorithm robustness and the relationship between characteristic values and their effects on evaluation metrics."}}
{"id": "2509.08734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08734", "abs": "https://arxiv.org/abs/2509.08734", "authors": ["Andreas Burger", "Luca Thiede", "Al\u00e1n Aspuru-Guzik", "Nandita Vijaykumar"], "title": "DEQuify your force field: More efficient simulations using deep equilibrium models", "comment": "AI4MAT-ICLR-2025 Spotlight https://openreview.net/forum?id=XACVRYePQQ", "summary": "Machine learning force fields show great promise in enabling more accurate\nmolecular dynamics simulations compared to manually derived ones. Much of the\nprogress in recent years was driven by exploiting prior knowledge about\nphysical systems, in particular symmetries under rotation, translation, and\nreflections. In this paper, we argue that there is another important piece of\nprior information that, thus fa,r hasn't been explored: Simulating a molecular\nsystem is necessarily continuous, and successive states are therefore extremely\nsimilar. Our contribution is to show that we can exploit this information by\nrecasting a state-of-the-art equivariant base model as a deep equilibrium\nmodel. This allows us to recycle intermediate neural network features from\nprevious time steps, enabling us to improve both accuracy and speed by\n$10\\%-20\\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ\nbase model. The training is also much more memory efficient, allowing us to\ntrain more expressive models on larger systems.", "AI": {"tldr": "Recasting equivariant force field models as deep equilibrium models to exploit temporal continuity in molecular dynamics, achieving 10-20% improvements in accuracy and speed.", "motivation": "To leverage the prior knowledge that molecular dynamics simulations are continuous and successive states are extremely similar, which hasn't been fully exploited in machine learning force fields.", "method": "Transform a state-of-the-art equivariant base model into a deep equilibrium model to recycle intermediate neural network features from previous time steps.", "result": "Achieved 10-20% improvements in both accuracy and speed on MD17, MD22, and OC20 200k datasets compared to non-DEQ base models, with more memory-efficient training.", "conclusion": "Exploiting temporal continuity through deep equilibrium models significantly enhances performance and efficiency in machine learning force fields for molecular dynamics simulations."}}
{"id": "2509.08483", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.08483", "abs": "https://arxiv.org/abs/2509.08483", "authors": ["Matias D. Cattaneo", "Boris Shigida"], "title": "Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis", "comment": null, "summary": "We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed\nmomentum parameter $\\beta \\in (0, 1)$ provides exponential decay of memory.\nBuilding on Kovachki and Stuart (2021), we prove that on an exponentially\nattractive invariant manifold the algorithm is exactly plain gradient descent\nwith a modified loss, provided that the step size $h$ is small enough. Although\nthe modified loss does not admit a closed-form expression, we describe it with\narbitrary precision and prove global (finite \"time\" horizon) approximation\nbounds $O(h^{R})$ for any finite order $R \\geq 2$. We then conduct a\nfine-grained analysis of the combinatorics underlying the memoryless\napproximations of HB, in particular, finding a rich family of polynomials in\n$\\beta$ hidden inside which contains Eulerian and Narayana polynomials. We\nderive continuous modified equations of arbitrary approximation order (with\nrigorous bounds) and the principal flow that approximates the HB dynamics,\ngeneralizing Rosca et al. (2023). Approximation theorems cover both full-batch\nand mini-batch HB. Our theoretical results shed new light on the main features\nof gradient descent with heavy-ball momentum, and outline a road-map for\nsimilar analysis of other optimization algorithms.", "AI": {"tldr": "Analysis shows Polyak heavy-ball momentum gradient descent is equivalent to plain gradient descent with a modified loss on attractive manifolds, with arbitrary precision approximation bounds and rich combinatorial structure.", "motivation": "To understand the mathematical structure and behavior of gradient descent with heavy-ball momentum, particularly its equivalence to modified loss gradient descent and the combinatorial patterns in its approximations.", "method": "Proved equivalence between HB and modified loss gradient descent on attractive manifolds, conducted fine-grained combinatorial analysis of memoryless approximations, derived continuous modified equations with rigorous bounds.", "result": "Established global approximation bounds O(h^R) for any finite order R\u22652, discovered rich polynomial family containing Eulerian and Narayana polynomials, generalized principal flow approximation for HB dynamics.", "conclusion": "HB momentum provides exact equivalence to modified loss gradient descent with exponential memory decay, revealing complex combinatorial structure and enabling high-order approximations for both full-batch and mini-batch settings."}}
{"id": "2509.08755", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08755", "abs": "https://arxiv.org/abs/2509.08755", "authors": ["Zhiheng Xi", "Jixuan Huang", "Chenyang Liao", "Baodai Huang", "Honglin Guo", "Jiaqi Liu", "Rui Zheng", "Junjie Ye", "Jiazheng Zhang", "Wenxiang Chen", "Wei He", "Yiwen Ding", "Guanyu Li", "Zehui Chen", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning", "comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL", "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.", "AI": {"tldr": "AgentGym-RL is a new reinforcement learning framework for training LLM agents from scratch without supervised fine-tuning, featuring modular architecture and supporting diverse real-world scenarios with a novel ScalingInter-RL training approach.", "motivation": "The community lacks a unified interactive RL framework that can effectively train autonomous LLM agents from scratch across diverse environments without relying on supervised fine-tuning.", "method": "Introduces AgentGym-RL framework with modular architecture and ScalingInter-RL training approach that balances exploration-exploitation by gradually increasing interaction horizons.", "result": "Agents match or surpass commercial models on 27 tasks across diverse environments, demonstrating stability and effectiveness.", "conclusion": "The framework provides key insights and will be open-sourced to empower development of next-generation intelligent agents through stable RL training."}}
{"id": "2509.08499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08499", "abs": "https://arxiv.org/abs/2509.08499", "authors": ["Chisom Chibuike", "Adeyinka Ogunsanya"], "title": "Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks", "comment": "11 pages, 4 figures", "summary": "Optimization has been an important factor and topic of interest in training\ndeep learning models, yet less attention has been given to how we select the\noptimizers we use to train these models. Hence, there is a need to dive deeper\ninto how we select the optimizers we use for training and the metrics that\ndetermine this selection. In this work, we compare the performance of 10\ndifferent optimizers in training a simple Multi-layer Perceptron model using a\nheart disease dataset from Kaggle. We set up a consistent training paradigm and\nevaluate the optimizers based on metrics such as convergence speed and\nstability. We also include some other Machine Learning Evaluation metrics such\nas AUC, Precision, and Recall, which are central metrics to classification\nproblems. Our results show that there are trade-offs between convergence speed\nand stability, as optimizers like Adagrad and Adadelta, which are more stable,\ntook longer time to converge. Across all our metrics, we chose RMSProp to be\nthe most effective optimizer for this heart disease prediction task because it\noffered a balanced performance across key metrics. It achieved a precision of\n0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.\nHowever, it was not the most stable. We recommend that, in less\ncompute-constrained environments, this method of choosing optimizers through a\nthorough evaluation should be adopted to increase the scientific nature and\nperformance in training deep learning models.", "AI": {"tldr": "Comparison of 10 optimizers for heart disease prediction using MLP, finding RMSProp most effective with balanced performance across precision, recall, AUC and training speed.", "motivation": "Lack of systematic approach for optimizer selection in deep learning, need to understand metrics that determine optimal optimizer choice for training.", "method": "Evaluated 10 optimizers on MLP model with heart disease dataset using consistent training paradigm, measuring convergence speed, stability, AUC, precision, and recall.", "result": "RMSProp performed best overall with precision 0.765, recall 0.827, AUC 0.841 and faster training time. Trade-offs observed between stability and convergence speed.", "conclusion": "Recommends thorough optimizer evaluation in less compute-constrained environments to improve scientific rigor and model performance in deep learning training."}}
{"id": "2509.08756", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.08756", "abs": "https://arxiv.org/abs/2509.08756", "authors": ["Zhaoxun \"Lorenz\" Liu", "Wagner H. Souza", "Jay Han", "Amin Madani"], "title": "Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform", "comment": null, "summary": "Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,\naccurate patient-hospital allocation decisions under extreme pressure. Here, we\ndeveloped and validated a deep reinforcement learning-based decision-support AI\nagent to optimize patient transfer decisions during simulated MCIs by balancing\npatient acuity levels, specialized care requirements, hospital capacities, and\ntransport logistics. To integrate this AI agent, we developed MasTER, a\nweb-accessible command dashboard for MCI management simulations. Through a\ncontrolled user study with 30 participants (6 trauma experts and 24\nnon-experts), we evaluated three interaction approaches with the AI agent\n(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI\nscenarios in the Greater Toronto Area. Results demonstrate that increasing AI\ninvolvement significantly improves decision quality and consistency. The AI\nagent outperforms trauma surgeons (p < 0.001) and enables non-experts to\nachieve expert-level performance when assisted, contrasting sharply with their\nsignificantly inferior unassisted performance (p < 0.001). These findings\nestablish the potential for our AI-driven decision support to enhance both MCI\npreparedness training and real-world emergency response management.", "AI": {"tldr": "AI agent using deep reinforcement learning optimizes patient transfer decisions in mass casualty incidents, significantly outperforming human experts and enabling non-experts to achieve expert-level performance.", "motivation": "Mass casualty incidents overwhelm healthcare systems and require rapid, accurate patient-hospital allocation decisions under extreme pressure, creating a critical need for decision support systems.", "method": "Developed and validated a deep reinforcement learning-based AI agent that balances patient acuity, care requirements, hospital capacities, and transport logistics. Created MasTER dashboard for simulations and conducted user study with 30 participants across different AI interaction approaches.", "result": "Increasing AI involvement significantly improves decision quality and consistency. AI outperforms trauma surgeons (p < 0.001) and enables non-experts to achieve expert-level performance when assisted, contrasting with their inferior unassisted performance (p < 0.001).", "conclusion": "The AI-driven decision support system shows strong potential to enhance both MCI preparedness training and real-world emergency response management by improving decision quality and enabling non-experts to perform at expert levels."}}
{"id": "2509.08814", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08814", "abs": "https://arxiv.org/abs/2509.08814", "authors": ["Zhanming Shen", "Zeyu Qin", "Zenan Huang", "Hao Chen", "Jiaqi Hu", "Yihong Zhuang", "Guoshan Lu", "Gang Chen", "Junbo Zhao"], "title": "Merge-of-Thought Distillation", "comment": null, "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students.", "AI": {"tldr": "Merge-of-Thought Distillation (MoT) is a lightweight framework that efficiently distills reasoning capabilities from multiple teachers into compact students by alternating teacher-specific fine-tuning and weight-space merging, achieving superior performance with minimal data.", "motivation": "Traditional reasoning distillation assumes a single oracle teacher, but multiple candidate teachers are practically available. Different students have different best teachers, and even for the same student the best teacher varies across datasets, requiring a method to unify multiple teachers' reasoning abilities while overcoming supervision conflicts.", "method": "Propose Merge-of-Thought Distillation (MoT) framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants, using only about 200 high-quality CoT samples.", "result": "On competition math benchmarks, MoT applied to Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1. Consistently outperforms best single-teacher distillation and naive multi-teacher union, raises performance ceiling while mitigating overfitting, shows robustness to distribution-shifted and peer-level teachers.", "conclusion": "MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics, cultivates better teachers, and demonstrates that consensus-filtered reasoning features transfer broadly. It provides a simple, scalable route to efficiently distill long CoT capabilities from diverse teachers into compact students."}}
{"id": "2509.08530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08530", "abs": "https://arxiv.org/abs/2509.08530", "authors": ["Wen-Bo Xie", "Xun Fu", "Bin Chen", "Yan-Li Lee", "Tao Deng", "Tian Zou", "Xin Wang", "Zhen Liu", "Jaideep Srivastavad"], "title": "Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures", "comment": null, "summary": "In this work, we focus on the efficiency and scalability of pairwise\nconstraint-based active clustering, crucial for processing large-scale data in\napplications such as data mining, knowledge annotation, and AI model\npre-training. Our goals are threefold: (1) to reduce computational costs for\niterative clustering updates; (2) to enhance the impact of user-provided\nconstraints to minimize annotation requirements for precise clustering; and (3)\nto cut down memory usage in practical deployments. To achieve these aims, we\npropose a graph-based active clustering algorithm that utilizes two sparse\ngraphs: one for representing relationships between data (our proposed data\nskeleton) and another for updating this data skeleton. These two graphs work in\nconcert, enabling the refinement of connected subgraphs within the data\nskeleton to create nested clusters. Our empirical analysis confirms that the\nproposed algorithm consistently facilitates more accurate clustering with\ndramatically less input of user-provided constraints, and outperforms its\ncounterparts in terms of computational performance and scalability, while\nmaintaining robustness across various distance metrics.", "AI": {"tldr": "A graph-based active clustering algorithm that uses two sparse graphs to reduce computational costs, minimize annotation requirements, and cut memory usage while maintaining high accuracy and scalability.", "motivation": "To address efficiency and scalability challenges in pairwise constraint-based active clustering for large-scale data processing in applications like data mining, knowledge annotation, and AI model pre-training.", "method": "Proposes a graph-based active clustering algorithm utilizing two sparse graphs: one for representing data relationships (data skeleton) and another for updating this skeleton, enabling refinement of connected subgraphs to create nested clusters.", "result": "The algorithm achieves more accurate clustering with dramatically fewer user-provided constraints, outperforms counterparts in computational performance and scalability, and maintains robustness across various distance metrics.", "conclusion": "The proposed approach successfully addresses the threefold goals of reducing computational costs, enhancing constraint impact, and cutting memory usage while delivering superior clustering performance for large-scale data applications."}}
{"id": "2509.08578", "categories": ["cs.LG", "q-bio.PE", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.08578", "abs": "https://arxiv.org/abs/2509.08578", "authors": ["Hong Liu"], "title": "MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization", "comment": null, "summary": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. To address this, we present MAESTRO, a Multi-modal\nAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves\nrobustness by adaptively fusing multi-modal inputs-including surveillance, web\nsearch trends, and meteorological data-and leveraging a comprehensive\nspectro-temporal architecture. The model first decomposes time series into\nseasonal and trend components. These are then processed through a hybrid\nfeature enhancement pipeline combining Transformer-based encoders, a Mamba\nstate-space model for long-range dependencies, multi-scale temporal\nconvolutions, and a frequency-domain analysis module. A cross-channel attention\nmechanism further integrates information across the different data modalities.\nFinally, a temporal projection head performs sequence-to-sequence forecasting,\nwith an optional estimator to quantify prediction uncertainty. Evaluated on\nover 11 years of Hong Kong influenza data (excluding the COVID-19 period),\nMAESTRO shows strong competitive performance, demonstrating a superior model\nfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.\nExtensive ablations confirm the significant contributions of both multi-modal\nfusion and the spectro-temporal components. Our modular and reproducible\npipeline is made publicly available to facilitate deployment and extension to\nother regions and pathogens.Our publicly available pipeline presents a\npowerful, unified framework, demonstrating the critical synergy of advanced\nspectro-temporal modeling and multi-modal data fusion for robust\nepidemiological forecasting.", "AI": {"tldr": "MAESTRO is a multi-modal ensemble model for robust influenza forecasting that combines surveillance, web search, and weather data using spectro-temporal analysis and achieves state-of-the-art performance with R-square of 0.956.", "motivation": "Timely and robust influenza incidence forecasting is critical for public health decision-making, requiring adaptive fusion of multi-modal data sources for accurate predictions.", "method": "Decomposes time series into seasonal/trend components, processes through hybrid pipeline with Transformer encoders, Mamba state-space model, temporal convolutions, frequency analysis, and cross-channel attention for multi-modal fusion.", "result": "Achieves state-of-the-art R-square of 0.956 on 11+ years of Hong Kong influenza data, with extensive ablations confirming contributions of multi-modal fusion and spectro-temporal components.", "conclusion": "Presents a powerful unified framework demonstrating the synergy of advanced spectro-temporal modeling and multi-modal data fusion for robust epidemiological forecasting, with publicly available modular pipeline."}}
{"id": "2509.08617", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08617", "abs": "https://arxiv.org/abs/2509.08617", "authors": ["Khawla Elhadri", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "title": "Towards Interpretable Deep Neural Networks for Tabular Data", "comment": null, "summary": "Tabular data is the foundation of many applications in fields such as finance\nand healthcare. Although DNNs tailored for tabular data achieve competitive\npredictive performance, they are blackboxes with little interpretability. We\nintroduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to\nlearn a dictionary of monosemantic features within the latent space used for\nprediction. Using an automated method, we assign human-interpretable semantics\nto these features. This allows us to represent predictions as linear\ncombinations of semantically meaningful components. Empirical evaluations\ndemonstrate that XNNTab attains performance on par with or exceeding that of\nstate-of-the-art, black-box neural models and classical machine learning\napproaches while being fully interpretable.", "AI": {"tldr": "XNNTab is an interpretable neural architecture for tabular data that uses sparse autoencoders to learn monosemantic features with human-interpretable semantics, achieving competitive performance while providing full interpretability.", "motivation": "Tabular data applications in fields like finance and healthcare require both high predictive performance and interpretability, but current DNNs for tabular data are blackboxes with little interpretability.", "method": "Uses a sparse autoencoder (SAE) to learn a dictionary of monosemantic features in the latent space, then employs an automated method to assign human-interpretable semantics to these features, representing predictions as linear combinations of semantically meaningful components.", "result": "Empirical evaluations show XNNTab achieves performance on par with or exceeding state-of-the-art black-box neural models and classical machine learning approaches.", "conclusion": "XNNTab provides a fully interpretable neural architecture for tabular data that maintains competitive predictive performance while offering transparent, semantically meaningful explanations for predictions."}}
{"id": "2509.08625", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08625", "abs": "https://arxiv.org/abs/2509.08625", "authors": ["Hugo Str\u00e4ng", "Tai Dinh"], "title": "An upper bound of the silhouette validation metric for clustering", "comment": null, "summary": "The silhouette coefficient summarizes, per observation, cohesion versus\nseparation in [-1, 1]; the average silhouette width (ASW) is a common internal\nmeasure of clustering quality where higher values indicate more coveted\nresults. However, the dataset-specific maximum of ASW is typically unknown, and\nthe standard upper limit 1 is often unattainable. In this work, we derive for\neach data point in a given dataset a sharp upper bound on its silhouette width.\nBy aggregating these individual bounds, we present a canonical data-dependent\nupper bound on ASW that often assumes values well below 1. The presented bounds\ncan indicate whether individual data points can ever be well placed, enable\nearly stopping of silhouette-based optimization loops, and help answer a key\nquestion: How close is my clustering result to the best possible outcome on\nthis specific data? Across synthetic and real datasets, the bounds are provably\nnear-tight in many cases and offer significant enrichment of cluster quality\nevaluation.", "AI": {"tldr": "This paper presents a method to derive data-dependent upper bounds for silhouette width and average silhouette width (ASW) that are often much lower than the theoretical maximum of 1, providing more realistic quality assessment for clustering results.", "motivation": "The standard upper limit of 1 for ASW is often unattainable in practice, making it difficult to assess how close a clustering result is to the best possible outcome for a specific dataset. There's a need for dataset-specific bounds to better evaluate clustering quality.", "method": "The authors derive sharp upper bounds for individual data points' silhouette width by analyzing the dataset structure, then aggregate these individual bounds to create a canonical data-dependent upper bound on ASW.", "result": "The proposed bounds are provably near-tight across synthetic and real datasets, often assuming values well below 1, and significantly enrich cluster quality evaluation by providing realistic performance benchmarks.", "conclusion": "These data-dependent bounds enable better assessment of clustering quality, help determine if individual points can be well-placed, allow for early stopping in optimization loops, and answer the key question of how close a clustering result is to the dataset-specific optimum."}}
{"id": "2509.08653", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08653", "abs": "https://arxiv.org/abs/2509.08653", "authors": ["Minqi Jiang", "Jo\u00e3o G. M. Ara\u00fajo", "Will Ellsworth", "Sian Gooding", "Edward Grefenstette"], "title": "Generative Data Refinement: Just Ask for Better Data", "comment": null, "summary": "For a fixed parameter size, the capabilities of large models are primarily\ndetermined by the quality and quantity of its training data. Consequently,\ntraining datasets now grow faster than the rate at which new data is indexed on\nthe web, leading to projected data exhaustion over the next decade. Much more\ndata exists as user-generated content that is not publicly indexed, but\nincorporating such data comes with considerable risks, such as leaking private\ninformation and other undesirable content. We introduce a framework, Generative\nData Refinement (GDR), for using pretrained generative models to transform a\ndataset with undesirable content into a refined dataset that is more suitable\nfor training. Our experiments show that GDR can outperform industry-grade\nsolutions for dataset anonymization, as well as enable direct detoxification of\nhighly unsafe datasets. Moreover, we show that by generating synthetic data\nthat is conditioned on each example in the real dataset, GDR's refined outputs\nnaturally match the diversity of web scale datasets, and thereby avoid the\noften challenging task of generating diverse synthetic data via model\nprompting. The simplicity and effectiveness of GDR make it a powerful tool for\nscaling up the total stock of training data for frontier models.", "AI": {"tldr": "GDR framework uses generative models to transform problematic datasets into safe, high-quality training data by generating synthetic alternatives that preserve diversity while removing private/unsafe content.", "motivation": "Address data exhaustion by leveraging user-generated content that's currently unusable due to privacy and safety concerns, enabling access to vast untapped data sources.", "method": "Uses pretrained generative models to create synthetic data conditioned on real examples, removing undesirable content while maintaining dataset diversity without complex prompting.", "result": "Outperforms industry-grade anonymization solutions, enables detoxification of highly unsafe datasets, and naturally matches web-scale dataset diversity.", "conclusion": "GDR provides a simple yet effective solution to scale training data for frontier models by safely incorporating currently unusable user-generated content."}}
{"id": "2509.08660", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08660", "abs": "https://arxiv.org/abs/2509.08660", "authors": ["Eric Eaton", "Marcel Hussing", "Michael Kearns", "Aaron Roth", "Sikata Bela Sengupta", "Jessica Sorrell"], "title": "Replicable Reinforcement Learning with Linear Function Approximation", "comment": null, "summary": "Replication of experimental results has been a challenge faced by many\nscientific disciplines, including the field of machine learning. Recent work on\nthe theory of machine learning has formalized replicability as the demand that\nan algorithm produce identical outcomes when executed twice on different\nsamples from the same distribution. Provably replicable algorithms are\nespecially interesting for reinforcement learning (RL), where algorithms are\nknown to be unstable in practice. While replicable algorithms exist for tabular\nRL settings, extending these guarantees to more practical function\napproximation settings has remained an open problem. In this work, we make\nprogress by developing replicable methods for linear function approximation in\nRL. We first introduce two efficient algorithms for replicable random design\nregression and uncentered covariance estimation, each of independent interest.\nWe then leverage these tools to provide the first provably efficient replicable\nRL algorithms for linear Markov decision processes in both the generative model\nand episodic settings. Finally, we evaluate our algorithms experimentally and\nshow how they can inspire more consistent neural policies.", "AI": {"tldr": "First provably efficient replicable reinforcement learning algorithms for linear function approximation settings, addressing the challenge of experimental result replication in ML.", "motivation": "Replicability is a fundamental challenge in scientific disciplines including ML. RL algorithms are particularly unstable in practice, and while replicable algorithms exist for tabular RL, extending these to practical function approximation settings remained an open problem.", "method": "Developed replicable methods for linear function approximation in RL. Introduced two efficient algorithms for replicable random design regression and uncentered covariance estimation, then leveraged these tools to create provably efficient replicable RL algorithms for linear MDPs in both generative model and episodic settings.", "result": "Successfully developed the first provably efficient replicable RL algorithms for linear function approximation. Experimental evaluation shows these algorithms can inspire more consistent neural policies.", "conclusion": "This work makes significant progress in addressing replicability challenges in RL by providing the first provably efficient replicable algorithms for linear function approximation settings, bridging the gap between theoretical replicability guarantees and practical RL applications."}}
{"id": "2509.08679", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08679", "abs": "https://arxiv.org/abs/2509.08679", "authors": ["Jingya Cheng", "Jiazi Tian", "Federica Spoto", "Alaleh Azhir", "Daniel Mork", "Hossein Estiri"], "title": "Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data", "comment": null, "summary": "\\textbf{Background:} Machine learning models trained on electronic health\nrecords (EHRs) often degrade across healthcare systems due to distributional\nshift. A fundamental but underexplored factor is diagnostic signal decay:\nvariability in diagnostic quality and consistency across institutions, which\naffects the reliability of codes used for training and prediction.\n  \\textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying\ndiagnostic data quality at the patient level in dementia, and to test SFI-aware\ncalibration for improving model performance across heterogeneous datasets\nwithout outcome labels.\n  \\textbf{Methods:} We built a simulation framework generating 2,500 synthetic\ndatasets, each with 1,000 patients and realistic demographics, encounters, and\ncoding patterns based on dementia risk factors. The SFI was derived from six\ninterpretable components: diagnostic specificity, temporal consistency,\nentropy, contextual concordance, medication alignment, and trajectory\nstability. SFI-aware calibration applied a multiplicative adjustment, optimized\nacross 50 simulation batches.\n  \\textbf{Results:} At the optimal parameter ($\\alpha$ = 2.0), SFI-aware\ncalibration significantly improved all metrics (p $<$ 0.001). Gains ranged from\n10.3\\% for Balanced Accuracy to 32.5\\% for Recall, with notable increases in\nPrecision (31.9\\%) and F1-score (26.1\\%). Performance approached reference\nstandards, with F1-score and Recall within 1\\% and Balanced Accuracy and\nDetection Rate improved by 52.3\\% and 41.1\\%, respectively.\n  \\textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model\ngeneralization. SFI-aware calibration provides a practical, label-free strategy\nto enhance prediction across healthcare contexts, particularly for large-scale\nadministrative datasets lacking outcome labels.", "AI": {"tldr": "Developed Signal Fidelity Index (SFI) to quantify diagnostic data quality in dementia EHRs and showed SFI-aware calibration significantly improves ML model performance across healthcare systems without needing outcome labels.", "motivation": "Machine learning models trained on EHRs degrade across healthcare systems due to distributional shift, particularly diagnostic signal decay - variability in diagnostic quality and consistency across institutions that affects code reliability.", "method": "Built simulation framework with 2,500 synthetic datasets (1,000 patients each) with realistic demographics and coding patterns. SFI derived from six components: diagnostic specificity, temporal consistency, entropy, contextual concordance, medication alignment, and trajectory stability. Applied SFI-aware calibration with multiplicative adjustment optimized across 50 simulation batches.", "result": "At optimal parameter (\u03b1=2.0), SFI-aware calibration significantly improved all metrics (p<0.001): 10.3% Balanced Accuracy, 32.5% Recall, 31.9% Precision, 26.1% F1-score. Performance approached reference standards with F1/Recall within 1% and Balanced Accuracy/Detection Rate improved by 52.3%/41.1%.", "conclusion": "Diagnostic signal decay is a tractable barrier to model generalization. SFI-aware calibration provides practical, label-free strategy to enhance prediction across healthcare contexts, especially for large administrative datasets lacking outcome labels."}}
{"id": "2509.08683", "categories": ["cs.LG", "cs.IT", "math.IT", "68P30"], "pdf": "https://arxiv.org/pdf/2509.08683", "abs": "https://arxiv.org/abs/2509.08683", "authors": ["Delio Jaramillo-Velez", "Charul Rajput", "Ragnar Freij-Hollanti", "Camilla Hollanti", "Alexandre Graell i Amat"], "title": "Perfectly-Private Analog Secure Aggregation in Federated Learning", "comment": "Comments welcome", "summary": "In federated learning, multiple parties train models locally and share their\nparameters with a central server, which aggregates them to update a global\nmodel. To address the risk of exposing sensitive data through local models,\nsecure aggregation via secure multiparty computation has been proposed to\nenhance privacy. At the same time, perfect privacy can only be achieved by a\nuniform distribution of the masked local models to be aggregated. This raises a\nproblem when working with real valued data, as there is no measure on the reals\nthat is invariant under the masking operation, and hence information leakage is\nbound to occur. Shifting the data to a finite field circumvents this problem,\nbut as a downside runs into an inherent accuracy complexity tradeoff issue due\nto fixed point modular arithmetic as opposed to floating point numbers that can\nsimultaneously handle numbers of varying magnitudes. In this paper, a novel\nsecure parameter aggregation method is proposed that employs the torus rather\nthan a finite field. This approach guarantees perfect privacy for each party's\ndata by utilizing the uniform distribution on the torus, while avoiding\naccuracy losses. Experimental results show that the new protocol performs\nsimilarly to the model without secure aggregation while maintaining perfect\nprivacy. Compared to the finite field secure aggregation, the torus-based\nprotocol can in some cases significantly outperform it in terms of model\naccuracy and cosine similarity, hence making it a safer choice.", "AI": {"tldr": "A novel secure aggregation method for federated learning using torus instead of finite fields, achieving perfect privacy without accuracy loss.", "motivation": "Address privacy risks in federated learning by ensuring perfect privacy through uniform distribution masking, while avoiding accuracy-complexity tradeoffs of finite field approaches.", "method": "Proposes secure parameter aggregation using the torus (rather than finite fields) to leverage uniform distribution properties, enabling perfect privacy preservation without the accuracy limitations of fixed-point modular arithmetic.", "result": "Experimental results show the torus-based protocol performs similarly to non-secure models while maintaining perfect privacy, and significantly outperforms finite field secure aggregation in model accuracy and cosine similarity.", "conclusion": "Torus-based secure aggregation provides a safer choice for federated learning by guaranteeing perfect privacy without sacrificing model accuracy, overcoming limitations of previous finite field approaches."}}
{"id": "2509.08703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08703", "abs": "https://arxiv.org/abs/2509.08703", "authors": ["Nikasadat Emami", "Amirhossein Khalilian-Gourtani", "Jianghao Qian", "Antoine Ratouchniak", "Xupeng Chen", "Yao Wang", "Adeen Flinker"], "title": "Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping", "comment": "Accepted at IEEE International Conference on Neural Engineering\n  (NER), 2025. This is the author's accepted manuscript", "summary": "Identifying cortical regions critical for speech is essential for safe brain\nsurgery in or near language areas. While Electrical Stimulation Mapping (ESM)\nremains the clinical gold standard, it is invasive and time-consuming. To\naddress this, we analyzed intracranial electrocorticographic (ECoG) data from\n16 participants performing speech tasks and developed machine learning models\nto directly predict if the brain region underneath each ECoG electrode is\ncritical. Ground truth labels indicating speech arrest were derived\nindependently from Electrical Stimulation Mapping (ESM) and used to train\nclassification models. Our framework integrates neural activity signals,\nanatomical region labels, and functional connectivity features to capture both\nlocal activity and network-level dynamics. We found that models combining\nregion and connectivity features matched the performance of the full feature\nset, and outperformed models using either type alone. To classify each\nelectrode, trial-level predictions were aggregated using an MLP applied to\nhistogram-encoded scores. Our best-performing model, a trial-level RBF-kernel\nSupport Vector Machine together with MLP-based aggregation, achieved strong\naccuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings\nhighlight the value of combining spatial and network information with\nnon-linear modeling to improve functional mapping in presurgical evaluation.", "AI": {"tldr": "Machine learning models using ECoG data can predict speech-critical brain regions with high accuracy, matching ESM performance non-invasively.", "motivation": "Electrical Stimulation Mapping (ESM) is invasive and time-consuming for identifying speech-critical cortical regions during brain surgery planning. A non-invasive alternative is needed.", "method": "Analyzed ECoG data from 16 participants performing speech tasks, developed ML models combining neural activity, anatomical regions, and functional connectivity features. Used ESM-derived ground truth labels to train classification models with trial-level predictions aggregated via MLP.", "result": "Best model (RBF-kernel SVM with MLP aggregation) achieved ROC-AUC: 0.87 and PR-AUC: 0.57 on held-out participants. Models combining region and connectivity features outperformed single-feature models.", "conclusion": "Combining spatial and network information with non-linear modeling provides effective non-invasive functional mapping for presurgical evaluation, potentially reducing reliance on invasive ESM."}}
{"id": "2509.08709", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08709", "abs": "https://arxiv.org/abs/2509.08709", "authors": ["Shun Takagi", "Satoshi Hasegawa"], "title": "Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing", "comment": "Accepted at PoPETs 2026", "summary": "In cross-device private federated learning, differentially private\nfollow-the-regularized-leader (DP-FTRL) has emerged as a promising\nprivacy-preserving method. However, existing approaches assume a semi-honest\nserver and have not addressed the challenge of securely removing this\nassumption. This is due to its statefulness, which becomes particularly\nproblematic in practical settings where clients can drop out or be corrupted.\nWhile trusted execution environments (TEEs) might seem like an obvious\nsolution, a straightforward implementation can introduce forking attacks or\navailability issues due to state management. To address this problem, our paper\nintroduces a novel server extension that acts as a trusted computing base (TCB)\nto realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral\nTEE module on the server side to produce verifiable proofs of server actions.\nSome clients, upon being selected, participate in auditing these proofs with\nsmall additional communication and computational demands. This extension\nsolution reduces the size of the TCB while maintaining the system's scalability\nand liveness. We provide formal proofs based on interactive differential\nprivacy, demonstrating privacy guarantee in malicious settings. Finally, we\nexperimentally show that our framework adds small constant overhead to clients\nin several realistic settings.", "AI": {"tldr": "Novel server extension using ephemeral TEE to achieve maliciously secure DP-FTRL with verifiable proofs and client auditing, maintaining privacy and scalability.", "motivation": "Existing DP-FTRL approaches assume semi-honest server and don't address malicious settings, while TEE implementations face forking attacks and availability issues due to state management problems.", "method": "Introduces a trusted computing base (TCB) with ephemeral TEE module on server to produce verifiable proofs of server actions, with selected clients auditing these proofs with minimal overhead.", "result": "Formal proofs demonstrate privacy guarantee in malicious settings, experimental results show small constant overhead to clients in realistic scenarios.", "conclusion": "The proposed framework successfully reduces TCB size while maintaining system scalability and liveness, providing maliciously secure DP-FTRL with practical efficiency."}}
{"id": "2509.08714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08714", "abs": "https://arxiv.org/abs/2509.08714", "authors": ["Ahmed Sadaqa", "Di Liu"], "title": "Compressing CNN models for resource-constrained systems by channel and layer pruning", "comment": "16 pages, 4 figures, the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases", "summary": "Convolutional Neural Networks (CNNs) have achieved significant breakthroughs\nin various fields. However, these advancements have led to a substantial\nincrease in the complexity and size of these networks. This poses a challenge\nwhen deploying large and complex networks on edge devices. Consequently, model\ncompression has emerged as a research field aimed at reducing the size and\ncomplexity of CNNs. One prominent technique in model compression is model\npruning. This paper will present a new technique of pruning that combines both\nchannel and layer pruning in what is called a \"hybrid pruning framework\".\nInspired by EfficientNet, a renowned CNN architecture known for scaling up\nnetworks from both channel and layer perspectives, this hybrid approach applies\nthe same principles but in reverse, where it scales down the network through\npruning. Experiments on the hybrid approach demonstrated a notable decrease in\nthe overall complexity of the model, with only a minimal reduction in accuracy\ncompared to the baseline model. This complexity reduction translates into\nreduced latency when deploying the pruned models on an NVIDIA JETSON TX2\nembedded AI device.", "AI": {"tldr": "A hybrid pruning framework combining channel and layer pruning to reduce CNN complexity while maintaining accuracy, enabling efficient deployment on edge devices.", "motivation": "The increasing size and complexity of CNNs pose challenges for deployment on edge devices, requiring effective model compression techniques.", "method": "Proposes a hybrid pruning framework inspired by EfficientNet, applying reverse scaling principles through combined channel and layer pruning to reduce network complexity.", "result": "Experiments showed significant complexity reduction with minimal accuracy loss, resulting in reduced latency when deployed on NVIDIA JETSON TX2 embedded AI device.", "conclusion": "The hybrid pruning approach effectively balances model compression and performance, making large CNNs more suitable for edge device deployment."}}
{"id": "2509.08731", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.08731", "abs": "https://arxiv.org/abs/2509.08731", "authors": ["Xuefeng Gao", "Jiale Zha", "Xun Yu Zhou"], "title": "Data-driven generative simulation of SDEs using diffusion models", "comment": null, "summary": "This paper introduces a new approach to generating sample paths of unknown\nstochastic differential equations (SDEs) using diffusion models, a class of\ngenerative AI models commonly employed in image and video applications. Unlike\nthe traditional Monte Carlo methods for simulating SDEs, which require explicit\nspecifications of the drift and diffusion coefficients, our method takes a\nmodel-free, data-driven approach. Given a finite set of sample paths from an\nSDE, we utilize conditional diffusion models to generate new, synthetic paths\nof the same SDE. To demonstrate the effectiveness of our approach, we conduct a\nsimulation experiment to compare our method with alternative benchmark ones\nincluding neural SDEs. Furthermore, in an empirical study we leverage these\nsynthetically generated sample paths to enhance the performance of\nreinforcement learning algorithms for continuous-time mean-variance portfolio\nselection, hinting promising applications of diffusion models in financial\nanalysis and decision-making.", "AI": {"tldr": "Diffusion models used to generate SDE sample paths without knowing drift/diffusion coefficients, outperforming traditional methods and enhancing financial portfolio optimization.", "motivation": "Traditional Monte Carlo methods for simulating stochastic differential equations require explicit knowledge of drift and diffusion coefficients, which may not be available in real-world applications. A model-free, data-driven approach is needed.", "method": "Uses conditional diffusion models to generate new synthetic paths from finite sets of existing SDE sample paths, taking a model-free approach that doesn't require explicit coefficient specifications.", "result": "The method demonstrates effectiveness through simulation experiments comparing with benchmark methods including neural SDEs, and shows improved performance in reinforcement learning for continuous-time mean-variance portfolio selection.", "conclusion": "Diffusion models show promising applications in financial analysis and decision-making, providing a powerful data-driven alternative to traditional SDE simulation methods."}}
{"id": "2509.08736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08736", "abs": "https://arxiv.org/abs/2509.08736", "authors": ["Dong Han", "Zhehong Ai", "Pengxiang Cai", "Shuzhou Sun", "Shanya Lu", "Jianpeng Chen", "Ben Gao", "Lingli Ge", "Weida Wang", "Xiangxin Zhou", "Xihui Liu", "Mao Su", "Wanli Ouyang", "Lei Bai", "Dongzhan Zhou", "Tao XU", "Yuqiang Li", "Shufei Zhang"], "title": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System", "comment": null, "summary": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery.", "AI": {"tldr": "ChemBOMAS is an LLM-enhanced multi-agent system that accelerates Bayesian optimization in chemistry through knowledge-driven coarse-grained optimization and data-driven fine-grained optimization, achieving 96% success in real-world pharmaceutical experiments compared to experts' 15%.", "motivation": "Bayesian optimization in chemistry faces limitations due to sparse experimental data and complex reaction mechanisms, requiring a more efficient approach to accelerate chemical discovery.", "method": "Uses LLMs to intelligently decompose search space (knowledge-driven coarse-grained optimization) and generate pseudo-data points to enhance BO process within candidate regions (data-driven fine-grained optimization).", "result": "Achieved 96% optimal objective value in wet-lab pharmaceutical experiments, significantly outperforming domain experts' 15%. Benchmark evaluations confirmed superior optimization effectiveness and efficiency compared to various BO algorithms.", "conclusion": "ChemBOMAS is a powerful tool that significantly accelerates chemical discovery through LLM-enhanced Bayesian optimization, demonstrating practical utility in real-world pharmaceutical applications."}}
{"id": "2509.08750", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08750", "abs": "https://arxiv.org/abs/2509.08750", "authors": ["Yuanchun Guo", "Bingyan Liu", "Yulong Sha", "Zhensheng Xian"], "title": "PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints", "comment": "Accepted by DAC2025", "summary": "Federating heterogeneous models on edge devices with diverse resource\nconstraints has been a notable trend in recent years. Compared to traditional\nfederated learning (FL) that assumes an identical model architecture to\ncooperate, model-heterogeneous FL is more practical and flexible since the\nmodel can be customized to satisfy the deployment requirement. Unfortunately,\nno prior work ever dives into the existing model-heterogeneous FL algorithms\nunder the practical edge device constraints and provides quantitative analysis\non various data scenarios and metrics, which motivates us to rethink and\nre-evaluate this paradigm. In our work, we construct the first system platform\n\\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical\nconstraints of edge devices, where diverse model heterogeneity algorithms are\nclassified and tested on multiple data tasks and metrics. Based on the\nplatform, we perform extensive experiments on these algorithms under the\ndifferent edge constraints to observe their applicability and the corresponding\nheterogeneity pattern.", "AI": {"tldr": "The paper introduces PracMHBench, the first system platform for evaluating model-heterogeneous federated learning under practical edge device constraints, analyzing various algorithms across different data scenarios and metrics.", "motivation": "Traditional FL assumes identical model architectures, but model-heterogeneous FL is more practical and flexible for edge devices with diverse resource constraints. No prior work has quantitatively analyzed existing model-heterogeneous FL algorithms under practical edge constraints.", "method": "The authors constructed PracMHBench platform to evaluate model-heterogeneous FL algorithms on practical edge device constraints. They classified diverse model heterogeneity algorithms and tested them on multiple data tasks and metrics.", "result": "Extensive experiments were performed on various algorithms under different edge constraints to observe their applicability and corresponding heterogeneity patterns.", "conclusion": "The work provides a systematic evaluation framework for model-heterogeneous FL, enabling better understanding of algorithm performance under practical edge device constraints."}}
{"id": "2509.08759", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.08759", "abs": "https://arxiv.org/abs/2509.08759", "authors": ["Mominul Rubel", "Adam Meyers", "Gabriel Nicolosi"], "title": "Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning", "comment": null, "summary": "We introduce the Fourier Learning Machine (FLM), a neural network (NN)\narchitecture designed to represent a multidimensional nonharmonic Fourier\nseries. The FLM uses a simple feedforward structure with cosine activation\nfunctions to learn the frequencies, amplitudes, and phase shifts of the series\nas trainable parameters. This design allows the model to create a\nproblem-specific spectral basis adaptable to both periodic and nonperiodic\nfunctions. Unlike previous Fourier-inspired NN models, the FLM is the first\narchitecture able to represent a complete, separable Fourier basis in multiple\ndimensions using a standard Multilayer Perceptron-like architecture. A\none-to-one correspondence between the Fourier coefficients and amplitudes and\nphase-shifts is demonstrated, allowing for the translation between a full,\nseparable basis form and the cosine phase--shifted one. Additionally, we\nevaluate the performance of FLMs on several scientific computing problems,\nincluding benchmark Partial Differential Equations (PDEs) and a family of\nOptimal Control Problems (OCPs). Computational experiments show that the\nperformance of FLMs is comparable, and often superior, to that of established\narchitectures like SIREN and vanilla feedforward NNs.", "AI": {"tldr": "FLM is a novel neural network architecture that represents multidimensional nonharmonic Fourier series using cosine activations to learn frequencies, amplitudes, and phase shifts as trainable parameters, outperforming established models like SIREN on PDEs and optimal control problems.", "motivation": "To create a neural network architecture that can represent complete, separable Fourier basis in multiple dimensions and adapt to both periodic and nonperiodic functions, addressing limitations of previous Fourier-inspired models.", "method": "Uses a feedforward structure with cosine activation functions where frequencies, amplitudes, and phase shifts are learned as trainable parameters, establishing one-to-one correspondence between Fourier coefficients and network parameters.", "result": "FLM achieves comparable and often superior performance to SIREN and vanilla feedforward NNs on benchmark PDEs and optimal control problems, demonstrating effective problem-specific spectral basis adaptation.", "conclusion": "The FLM architecture successfully represents multidimensional Fourier series, provides interpretable parameters, and shows strong performance on scientific computing tasks, establishing a new standard for Fourier-based neural networks."}}
{"id": "2509.08779", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.08779", "abs": "https://arxiv.org/abs/2509.08779", "authors": ["Ali Amini", "Mohammad Alijanpour", "Behnam Latifi", "Ali Motie Nasrabadi"], "title": "ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals", "comment": "29 pages, 7 figures. Preprint. Correspondence: alijanpour@ucf.edu", "summary": "Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in\nchildren that can persist into adulthood, affecting social, academic, and\ncareer life. Early diagnosis is crucial for managing these impacts on patients\nand the healthcare system but is often labor-intensive and time-consuming. This\npaper presents a novel method to improve ADHD diagnosis precision and\ntimeliness by leveraging Deep Learning (DL) approaches and electroencephalogram\n(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive\ntemporal-spatial characterization, attention modules, and explainability\ntechniques optimized for EEG signals. ADHDeepNet integrates feature extraction\nand refinement processes to enhance ADHD diagnosis. The model was trained and\nvalidated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),\nemploying nested cross-validation for robust performance. The proposed\ntwo-stage methodology uses a 10-fold cross-subject validation strategy.\nInitially, each iteration optimizes the model's hyper-parameters with inner\n2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various\nstandard deviations and magnification levels is applied for data augmentation.\nADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC\nsubjects. To clarify model explainability and identify key brain regions and\nfrequency bands for ADHD diagnosis, we analyzed the learned weights and\nactivation patterns of the model's primary layers. Additionally, t-distributed\nStochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding\nin interpreting the model's decisions. This study highlights the potential of\nDL and EEG in enhancing ADHD diagnosis accuracy and efficiency.", "AI": {"tldr": "ADHDeepNet - a deep learning model using EEG signals achieves near-perfect accuracy (99.17%) for ADHD diagnosis with 100% sensitivity, employing temporal-spatial characterization and explainability techniques.", "motivation": "Early ADHD diagnosis is crucial but labor-intensive. The paper aims to improve diagnosis precision and timeliness using deep learning and EEG signals to address the challenges in traditional diagnostic methods.", "method": "Developed ADHDeepNet DL model with comprehensive temporal-spatial characterization, attention modules, and explainability techniques. Used 121 participants (61 ADHD, 60 controls) with nested cross-validation, hyperparameter optimization, and Additive Gaussian Noise for data augmentation.", "result": "Achieved 100% sensitivity and 99.17% accuracy in ADHD/HC classification. Model explainability analysis identified key brain regions and frequency bands for ADHD diagnosis, with t-SNE visualization aiding decision interpretation.", "conclusion": "The study demonstrates the strong potential of deep learning combined with EEG signals for highly accurate and efficient ADHD diagnosis, offering a promising alternative to traditional labor-intensive diagnostic approaches."}}
{"id": "2509.08822", "categories": ["cs.LG", "I.2.6; I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2509.08822", "abs": "https://arxiv.org/abs/2509.08822", "authors": ["Willy Sucipto", "Jianlong Zhou", "Ray Seung Min Kwon", "Fang Chen"], "title": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management", "comment": "30 pages, 8 figures, 3 tables. Survey of TinyML and IoT applications\n  in beekeeping (datasets, benchmarking, deployment). Submitted to ACM\n  Computing Surveys (under review)", "summary": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement.", "AI": {"tldr": "Survey paper on TinyML applications in beekeeping, covering hive monitoring, behavior recognition, pest detection, and swarming prediction using edge AI for sustainable pollinator management.", "motivation": "Honey bee colonies face increasing threats but traditional monitoring methods are disruptive and cloud-based solutions are impractical for remote apiaries. TinyML offers low-power, real-time edge monitoring alternatives.", "method": "Synthesizes current innovations in TinyML for apiculture across four functional areas: hive condition monitoring, bee behavior recognition, pest/disease detection, and swarming event forecasting. Examines datasets, lightweight models, and benchmarking strategies.", "result": "Identifies critical limitations including data scarcity, generalization challenges, and deployment barriers in off-grid environments. Highlights emerging opportunities in efficient inference, adaptive edge learning, and dataset standardization.", "conclusion": "Provides foundation for scalable, AI-driven monitoring systems to support sustainable pollinator management through consolidated research and engineering practices in TinyML applications for beekeeping."}}
