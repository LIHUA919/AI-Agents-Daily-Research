<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: LLMs like GPT-4 and GPT-5 can predict and reproduce individual-level human cognitive biases in conversational decision-making scenarios, with differences in alignment to human behavior.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors like cognitive load interact with these biases, moving beyond just reproducing known biases.

Method: Adapted three well-established decision scenarios into a conversational setting, conducted a human experiment (N=1100) with a chatbot for decision-making via simple or complex dialogues, and simulated these conditions using participant demographics and dialogue transcripts with GPT-4 and GPT-5 LLMs.

Result: Human experiments revealed robust biases. LLMs reproduced human biases with precision, but showed notable differences between models in how they aligned with human behavior.

Conclusion: This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [2] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: Paper critiques linear AI progress, introduces familiar vs. strange intelligence and nonlinear model of general intelligence, with implications for adversarial testing.


<details>
  <summary>Details</summary>
Motivation: To challenge the linear model of AI progress and provide a more nuanced understanding of AI intelligence, addressing how AI capabilities may combine superhuman and subhuman performance across domains.

Method: Expands on Susan Schneider's critique, develops concepts of 'familiar intelligence' (human-like patterns) and 'strange intelligence' (defying human patterns), and constructs a nonlinear model where general intelligence is the ability to achieve broad goals in varied environments without reduction to a single linear quantity.

Result: Argues that AI intelligence is likely to be strange, combining superhuman insights with surprising errors. It implies that failures on seemingly obvious tasks do not disprove general intelligence, and excellent performance on specific tasks like IQ tests does not guarantee broad capacities.

Conclusion: AI should be evaluated using adversarial testing approaches, expecting systems to sometimes fail unexpectedly. A nonlinear model is more accurate for assessing AI capabilities, as general intelligence is not a unified linear capacity but a complex, domain-defying ability.

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [3] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead is a structure-aware, multi-turn document reasoning agent that improves long-document QA by using hierarchical and sequential priors, achieving better results than existing agentic search methods.


<details>
  <summary>Details</summary>
Motivation: Existing agentic search frameworks treat long documents as flat collections of chunks, underutilizing document-native priors like hierarchical organization and sequential discourse structure, limiting effectiveness in long-document QA.

Method: DeepRead converts PDFs to structured Markdown, indexes at paragraph level with coordinate-style metadata, and equips LLM with Retrieve and ReadSection tools for localization and contiguous reading based on document structure.

Result: DeepRead achieves significant improvements over Search-o1-style agentic search in document QA, with synergistic effect between tools, and shows human-like 'locate then read' behavior in fine-grained analysis.

Conclusion: DeepRead effectively operationalizes hierarchical and sequential document priors to enhance multi-turn reasoning and retrieval, demonstrating the value of structure-aware approaches for advanced long-document QA tasks.

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [4] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: MINT is a neuro-symbolic tree approach that optimizes AI agents to actively query humans for missing information in object-driven planning, improving rewards and success rates with few questions.


<details>
  <summary>Details</summary>
Motivation: Open-world planning often faces incomplete information (e.g., unknown objects, human goals), creating knowledge gaps that hinder joint human-AI planning. Addressing this requires strategies for AI to effectively elicit human inputs.

Method: Propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about knowledge gaps, using self-play to optimize elicitation strategies. It builds symbolic trees of possible interactions and consults neural planning policies to estimate uncertainty, then leverages LLMs to search reasoning and curate optimal queries.

Result: Evaluation on three benchmarks with unseen/unknown objects shows MINT-based planning achieves near-expert returns, significantly improved rewards and success rates, while issuing a limited number of questions per task.

Conclusion: MINT provides a framework for AI agents to actively elicit human inputs, effectively managing knowledge gaps in joint planning and enhancing performance in realistic object-driven scenarios.

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [5] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLMs perform well on solved graph theory problems but struggle with open problems, supporting educational use for exploration but not novel insight.


<details>
  <summary>Details</summary>
Motivation: To understand how reliably LLMs support mathematically rigorous thinking in computer science education, particularly for graph theory problems.

Method: Use an eight-stage evaluation protocol reflecting authentic mathematical inquiry on a solved and an open graph theoretic problem.

Result: LLM performed strongly on solved problem with correct proofs but could not solve open problem, though it avoided fabricating results.

Conclusion: LLMs can aid in exploring established material but are limited for novel mathematical insight, highlighting the need for independent verification in education.

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [6] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: This paper argues for shifting uncertainty quantification (UQ) research from single-turn question-answering to interactive LLM agents, proposing a new general formulation and a conditional uncertainty reduction perspective to guide UQ design in realistic agent setups.


<details>
  <summary>Details</summary>
Motivation: Current UQ research for large language models (LLMs) focuses on single-turn question-answering, but LLM agents are increasingly used in complex, interactive tasks, necessitating a principled framework for agent UQ that addresses open-world scenarios.

Method: The paper presents a general formulation of agent UQ that subsumes existing setups, analyzes prior works as implicitly treating UQ as uncertainty accumulation, and contrasts this with a proposed perspective of conditional uncertainty reduction that models reducible uncertainty over an agent's trajectory by emphasizing action interactivity.

Result: A conceptual framework is outlined to provide actionable guidance for designing UQ in LLM agent setups, shifting the focus from accumulation to reduction of uncertainty in interactive contexts.

Conclusion: The paper concludes with practical implications for frontier LLM development and domain-specific applications, highlighting the need for agent UQ and identifying open problems in the field.

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [7] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: This study proposes a reinforcement learning framework using masked PPO for adaptive collision avoidance in active debris removal missions with small satellites, focusing on multi-debris rendezvous and refueling.


<details>
  <summary>Details</summary>
Motivation: The orbital environment is increasingly crowded with debris, posing challenges for safe active debris removal operations and minimizing collision risks.

Method: A reinforcement learning framework based on a masked Proximal Policy Optimization algorithm is used to dynamically adjust maneuvers for efficient mission planning, fuel efficiency, and collision avoidance.

Result: Simulated scenarios from the Iridium 33 debris dataset show the RL framework reduces collision risk and improves mission efficiency compared to traditional heuristic methods.

Conclusion: The work provides a scalable solution for complex multi-debris active debris removal missions and is applicable to other autonomous space mission planning problems.

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [8] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: This study validates the VERA-MH evaluation framework for assessing AI safety in mental health chatbots, finding strong agreement between clinicians and an LLM judge in evaluating safe and unsafe behaviors.


<details>
  <summary>Details</summary>
Motivation: As millions use AI chatbots for psychological support, there's an urgent need for evidence-based automated safety benchmarks to ensure these tools are safe, particularly for high-risk situations like suicide prevention.

Method: The study simulated conversations between LLM-based user-agents and general-purpose AI chatbots, then had licensed mental health clinicians independently rate conversations for safety using a scoring rubric. An LLM-based judge used the same rubric to evaluate the same conversations. They compared rating alignment between individual clinicians and between clinician consensus and the LLM judge.

Result: Individual clinicians showed strong consistency in safety ratings (IRR: 0.77), establishing a gold-standard clinical reference. The LLM judge was strongly aligned with clinical consensus (IRR: 0.81) overall and within key conditions. Clinicians also perceived the user-agents to be realistic.

Conclusion: The findings support the clinical validity and reliability of VERA-MH as an open-source, fully automated AI safety evaluation for mental health, suggesting it can effectively assess chatbot safety in suicide risk scenarios. Further research will examine generalizability and robustness.

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [9] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: This paper compares three planners (nominal PPO, domain-randomized PPO, and MCTS) for active debris removal mission planning in low Earth orbit, highlighting trade-offs between speed and adaptability under varying constraints.


<details>
  <summary>Details</summary>
Motivation: Autonomous mission planning for Active Debris Removal must balance efficiency, adaptability, and strict feasibility constraints like fuel and duration, requiring robust methods that handle distribution shifts.

Method: The work evaluates three planners for multi-debris rendezvous: a nominal Masked PPO trained under fixed parameters, a domain-randomized Masked PPO for robustness, and a plain MCTS baseline, using a high-fidelity orbital simulation with refueling and randomized debris in 300 test cases across different scenarios.

Result: Nominal PPO performs best when conditions match training but degrades under distributional shift; domain-randomized PPO shows improved adaptability with moderate loss in nominal performance; MCTS handles constraint changes best but with much higher computation time.

Conclusion: There's a trade-off between learned policies' speed and search-based methods' adaptability, suggesting combining training diversity with online planning could lead to resilient ADR planners.

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [10] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS is a scalable and accessible graph-based simulation framework for multi-agent systems, designed to lower the barrier to entry and support fast prototyping and large-scale agent deployments.


<details>
  <summary>Details</summary>
Motivation: Intelligent systems and multi-agent coordination are central to real-world applications, but existing high-fidelity simulators are computationally expensive and not suited for rapid prototyping or large-scale deployments.

Method: Developed GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight and extensible simulation framework that supports environments represented as graphs, emphasizes scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding, and is agnostic to policy type (e.g., heuristic, optimization-based, learning-based agents including LLMs).

Result: GAMMS enables efficient simulation of complex domains like urban road networks and communication systems, integrates with external tools such as machine learning libraries and planning solvers, provides built-in visualization with minimal configuration, supports high-performance simulations on standard hardware, and is open-source.

Conclusion: GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling by lowering the barrier for researchers and enabling accessible simulations, making it a valuable tool for the community.

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [11] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: A study evaluates the reliability and bias of Large Language Models (LLMs) as evaluators in merchant risk assessment, using a structured multi-evaluator framework with a consensus-deviation metric. Findings reveal heterogeneous biases among LLMs, with some aligning better with human judgment and payment-network data.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to assess reasoning quality, but their reliability and bias in payments-risk settings, such as Merchant Category Code-based risk assessment, remain poorly understood. There's a need for a replicable framework to evaluate LLM-as-a-judge systems and understand biases in financial workflows.

Method: The study introduces a structured multi-evaluator framework with a five-criterion rubric and Monte-Carlo scoring to assess rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. A consensus-deviation metric is used to compare each judge's score to the mean of others, avoiding circularity. The evaluation includes input from 26 payment-industry experts for human consensus and validation with payment-network data.

Result: Results show substantial heterogeneity in bias: GPT-5.1 and Claude 4.5 Sonnet exhibit negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 show positive bias (+0.77, +0.71), with bias reduced by 25.8% under anonymization. LLM judges assign scores averaging +0.46 points above human consensus, and the negative bias of GPT-5.1 and Claude 4.5 Sonnet aligns more closely with human judgment. Ground-truth validation reveals four models have statistically significant alignment with payment-network data (Spearman rho = 0.56 to 0.77), confirming framework effectiveness.

Conclusion: The framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows, demonstrating that LLM evaluators can exhibit significant biases that vary across models and conditions. It highlights the need for bias-aware protocols in operational financial settings to ensure reliable and fair assessments.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [12] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: Democratic Preference Optimization (DemPO) uses algorithmic sortition to align AI with representative human preferences, improving model values through demographic representativeness in training.


<details>
  <summary>Details</summary>
Motivation: Current preference-based alignment methods like RLHF rely on human rater pools that are convenience samples, overrepresenting some demographics and underrepresenting others, leading to biased AI systems that may not reflect broader societal values.

Method: DemPO introduces two training schemes: Hard Panel (exclusive training on preferences from a quota-satisfying mini-public sampled via sortition) and Soft Panel (reweighting all data by each rater's inclusion probability under the sortition lottery), with Soft Panel proven to recover the Hard Panel objective in closed form.

Result: Evaluation on Llama models (1B to 8B parameters) using a public dataset with rater demographics and a representative U.S. panel constitution shows Hard Panel consistently ranks first, Soft Panel outperforms the unweighted baseline across six aggregation methods, and effect sizes increase with model capacity.

Conclusion: Enforcing demographic representativeness during preference collection, rather than post-hoc correction, produces AI models whose behavior better aligns with values from representative publics, enhancing fairness and societal reflection.

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [13] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: SocialVeil is a new social learning environment that simulates cognitive-difference-induced communication barriers (semantic vagueness, sociocultural mismatch, emotional interference) to evaluate LLMs' social intelligence in imperfect settings, finding barriers impair performance significantly with limited adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLMs' social intelligence often assume idealized communication, limiting diagnosis of their ability to maintain and repair interactions in more realistic, imperfect settings.

Method: The authors present SocialVeil, grounded in a systematic literature review of human communication challenges, to simulate social interaction under three representative disruption types. They introduce two barrier-aware evaluation metrics (unresolved confusion, mutual understanding) and conduct experiments across 720 scenarios with four frontier LLMs, including human evaluations for fidelity.

Result: Barriers consistently impair LLM performance: mutual understanding reduced by over 45% on average, confusion elevated by nearly 50%. Human evaluations show high fidelity (ICC≈0.78, Pearson r≈0.80). Adaptation strategies (Repair Instruction, Interactive learning) have only modest effects.

Conclusion: This work advances social interaction environments toward real-world communication, opening opportunities to explore LLM agents' social intelligence beyond idealized settings.

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [14] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT is a chaos-aware spatio-temporal framework for cross-city traffic prediction, improving few-shot learning with regime analysis and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Traffic prediction is difficult in data-scarce, cross-city settings due to nonlinear dynamics and domain shifts; existing methods often fail to capture the chaotic nature of traffic for effective few-shot learning.

Method: Proposes CAST-CKT, which uses a chaotic analyser to quantify predictability regimes, with chaos-aware attention, adaptive topology learning, and chaotic consistency alignment for cross-city knowledge transfer.

Result: Extensive experiments on four benchmarks show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while providing interpretable regime analysis and uncertainty quantification.

Conclusion: CAST-CKT offers a novel approach that improves generalization and interpretability in cross-city traffic prediction, making it suitable for practical deployment with code available.

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [15] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG enhances graph-based RAG with causal gating across hierarchical modules to reduce spurious answers and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods over-rely on surface-level matching and lack explicit causal modeling, leading to unfaithful or spurious answers, and suffer from scalability and cross-module reasoning issues due to modular graph structures.

Method: Propose HugRAG, a framework that rethinks knowledge organization through causal gating across hierarchical modules, explicitly modeling causal relationships to suppress spurious correlations and enable scalable reasoning over large-scale knowledge graphs.

Result: Extensive experiments show HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics.

Conclusion: HugRAG establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [16] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: A set of 10 research-level math questions is introduced to test AI systems, with answers temporarily encrypted.


<details>
  <summary>Details</summary>
Motivation: To evaluate the current capability of AI systems in handling advanced mathematics questions that arise from actual research.

Method: Shared a previously unpublished set of 10 math questions that have naturally occurred in the authors' research process.

Result: The questions are publicly available for testing AI systems, while the answers remain encrypted for a short period to maintain integrity.

Conclusion: This initiative provides a benchmark to assess AI's ability to answer research-level mathematics questions, highlighting current challenges and potential areas for improvement.

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [17] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: This paper addresses challenges in retrieval-augmented generation (RAG) for domain settings with multiple heterogeneous knowledge bases (KBs), using Chinese Tibetan medicine as a case study, and proposes two methods—DAKS for KB routing and budgeted retrieval, and an alignment graph for evidence fusion—to improve traceability, reduce hallucinations, and enhance cross-KB evidence coverage, validated on a benchmark with a lightweight generator.


<details>
  <summary>Details</summary>
Motivation: In retrieval-augmented generation (RAG), domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging, particularly when dense and easily matched encyclopedia entries can dominate retrieval even if more authoritative sources like classics or clinical papers are available, as seen in Chinese Tibetan medicine. This can lead to biased retrieval, hallucinations, and reduced cross-KB verification capabilities.

Method: The paper proposes two complementary methods: 1) DAKS, which performs KB routing and budgeted retrieval to mitigate density-driven bias and prioritize authoritative sources; and 2) an alignment graph to guide evidence fusion and coverage-aware packing for improved cross-KB evidence coverage without naive concatenation. Experiments are conducted with a 500-query benchmark covering single-KB and cross-KB questions, using a lightweight generator (openPangu-Embedded-7B).

Result: Experiments show consistent gains in routing quality and cross-KB evidence coverage. The full system achieves the best CrossEv@5 score while maintaining strong faithfulness and citation correctness, indicating improved traceability and reduced hallucinations.

Conclusion: The proposed methods effectively address retrieval challenges in multi-KB RAG settings, as demonstrated in Chinese Tibetan medicine, by mitigating bias, enhancing cross-KB verification, and improving overall system performance, suggesting potential for broader domain applications.

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [18] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: Surgery is a fine-tuning defense method using attention sink divergence to mitigate harmful fine-tuning in LLMs, improving safety benchmarks by 5.9-11.25%.


<details>
  <summary>Details</summary>
Motivation: Harmful fine-tuning can invalidate safety alignment of large language models, posing safety risks.

Method: Measure sink divergence for each attention head, propose separable sink divergence hypothesis, and apply a regularizer (Surgery) to suppress positive sink divergence during fine-tuning.

Result: Surgery improves defense performance by 5.90%, 11.25%, and 9.55% on BeaverTails, HarmBench, and SorryBench benchmarks respectively.

Conclusion: Surgery effectively reduces the model's tendency to learn harmful patterns by steering attention heads toward negative sink divergence, enhancing safety alignment.

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: Proposes using denoising diffusion probabilistic models (DDPMs) as unified conditional density estimators for tabular imaging-derived phenotypes (IDPs) to derive univariate centiles and deviation scores while modeling multivariate dependence, evaluated on synthetic data and UK Biobank with dimensions up to 200.


<details>
  <summary>Details</summary>
Motivation: Traditional neuroimaging normative modeling fits one model per IDP, which scales well but discards multivariate dependence that may indicate coordinated biological patterns, limiting clinical interpretability of deviation profiles.

Method: Utilizes DDPMs with two denoiser backbones: a FiLM-conditioned multilayer perceptron (MLP) and a tabular transformer (SAINT) with feature self-attention and intersample attention, conditioning on covariates like age via learned embeddings.

Result: For low dimensions, diffusion models provide well-calibrated per-IDP outputs comparable to baselines while modeling realistic dependence structure. At higher dimensions, the transformer backbone outperforms the MLP in calibration and preserves higher-order dependence, enabling scalable joint normative models compatible with standard pipelines.

Conclusion: Diffusion-based normative modeling is a practical approach for generating calibrated multivariate deviation profiles in neuroimaging, combining the benefits of joint modeling with compatibility to per-IDP analysis.

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [20] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: Causal Analyst framework uses LLMs and causal analysis to identify direct causes of jailbreaks in large language models, enabling improved attacks and defenses with a dataset of 35k jailbreak attempts.


<details>
  <summary>Details</summary>
Motivation: Existing studies on jailbreaks in LLMs often overlook causal relationships between interpretable prompt features and jailbreak occurrences, hindering understanding of mechanisms for safety enhancement.

Method: Propose Causal Analyst framework integrating LLMs into causal discovery with a dataset of 35k jailbreak attempts across 7 LLMs, annotated with 37 prompt features, using LLM-based encoding and GNN-based causal graph learning.

Result: Identified specific prompt features (e.g., 'Positive Character', 'Number of Task Steps') as direct causal drivers, developed Jailbreaking Enhancer boosting attack success rates and Guardrail Advisor for intent extraction, with experiments showing robustness over non-causal approaches.

Conclusion: Causal analysis of jailbreak features is an effective and interpretable approach for improving LLM reliability, as demonstrated through practical applications and validated experiments.

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [21] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [22] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: Feature steering in LLMs effectively controls target behaviors but severely degrades performance (accuracy, coherence) compared to prompt engineering, indicating a trade-off.


<details>
  <summary>Details</summary>
Motivation: Feature steering is promising for controlling LLM behavior via internal representations, but its practical effectiveness and potential trade-offs with output quality in real-world applications are poorly understood.

Method: Evaluated Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (innocuous and safety-relevant) on 171 MMLU questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control.

Result: Auto Steer successfully modifies target behaviors (e.g., scores of 3.33 vs. 2.98 for prompting on Llama-8B, 3.57 vs. 3.10 on Llama-70B) but causes dramatic performance degradation: MMLU accuracy drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B; coherence falls from 4.62 to 2.24 and 4.94 to 3.89 respectively. Prompting achieves the best overall balance.

Conclusion: Current feature steering methods have limitations for practical deployment where task performance cannot be sacrificed, highlighting fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [23] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER is a unified multimodal fusion framework that enhances robustness by compressing inputs to remove noise and forcing cross-modality integration, and reconstructs missing modalities using energy-based methods with uncertainty quantification, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Multimodal fusion is hindered by noisy inputs that degrade representation quality and missing modalities that cause prediction failures, necessitating a robust solution.

Method: The framework uses dual-stage compression: within-modality frequency transforms (wavelet for audio, DCT for video) to remove noise, and cross-modality bottleneck tokens to enforce genuine integration. For missing modalities, it employs energy-based reconstruction via gradient descent on a learned energy function.

Result: Experiments on CMU-MOSI, CMU-MOSEI, and CH-SIMS show state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion in both complete and high-missing conditions.

Conclusion: DCER effectively addresses robustness challenges in multimodal fusion through compression and reconstruction, offering improved performance and uncertainty quantification, with code available on Github.

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [24] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA enables inference-time adaptation for Laplacian-based time-series models using observed data, combining delay-coordinate embeddings, spectral learning, and frozen decoders with lightweight adapters.


<details>
  <summary>Details</summary>
Motivation: To enhance adaptation of time-series models under changing dynamics without extensive retraining, leveraging in-context learning with Laplacian spectral methods.

Method: Combines delay-coordinate embeddings and Laplacian spectral learning for diffusion-coordinate representations, uses a frozen nonlinear decoder, and adds lightweight latent-space residual adapters based on Gaussian-process regression or attention-like Markov operators over context windows.

Result: LISA improves over frozen baselines in forecasting and autoregressive rollout experiments, with greater benefits under changing dynamics.

Conclusion: This work connects in-context adaptation to nonparametric spectral methods for dynamical systems, demonstrating effective inference-time adaptation for time-series models.

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [25] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: Integrating physical ODE knowledge into causal discovery for dynamical systems improves identifiability and robustness via an SDE-based framework.


<details>
  <summary>Details</summary>
Motivation: Combine causal discovery and physics-based models to enhance identifiability, stability, and robustness in analyzing dynamical systems with feedback and non-stationarity.

Method: Propose an integrative framework modeling system evolution as a stochastic differential equation (SDE), with drift from known ODE dynamics and diffusion from unknown causal couplings, using a scalable sparsity-inducing MLE algorithm for parameter estimation.

Result: Experiments show improved causal graph recovery and more stable, physically consistent estimates compared to purely data-driven baselines.

Conclusion: Leveraging partial physical knowledge as an inductive bias effectively enhances causal discovery in complex dynamical systems.

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [26] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: Temporal Pair Consistency (TPC) is a lightweight variance-reduction method for continuous-time generative models that pairs velocity predictions at timesteps along the same probability path, reducing gradient variance without changing model architecture or probability paths, improving sample quality and efficiency in image generation tasks.


<details>
  <summary>Details</summary>
Motivation: Continuous-time generative models like diffusion and flow matching often suffer from high estimator variance and inefficient sampling due to independent timestep training objectives, leading to suboptimal performance.

Method: TPC couples velocity predictions at paired timesteps along the probability path, introducing trajectory-coupled regularization at the estimator level while preserving the flow-matching objective, with extensions to modern pipelines like noise-augmented training and rectified flow.

Result: TPC achieves lower FID scores than prior methods on CIFAR-10 and ImageNet at multiple resolutions, with equal or lower computational cost, and integrates seamlessly into state-of-the-art pipelines.

Conclusion: TPC effectively reduces variance in generative models, enhancing sample quality and efficiency without architectural changes, making it a practical and scalable solution for continuous-time models.

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [27] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO replaces static reference in DPO with dynamic adversarial anchor for robust LLM alignment under noisy supervision.


<details>
  <summary>Details</summary>
Motivation: Static references in methods like DPO cause distributional mismatch and amplify spurious signals under noise; reference-free variants suffer unconstrained reward drift.

Method: Uses geometric-aware anchor as pessimistic baseline via adversarial perturbations, adaptively reweights preference pairs with Anchor Gap metric.

Result: GAPO improves robustness across noise settings, matches or enhances performance on standard LLM alignment and reasoning benchmarks.

Conclusion: Dynamic anchors effectively mitigate noise sensitivity and distributional mismatch in LLM preference optimization.

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [28] [A logical re-conception of neural networks: Hamiltonian bitwise part-whole architecture](https://arxiv.org/abs/2602.04911)
*E Bowen,R Granger,A Rodriguez*

Main category: cs.LG

TL;DR: A novel architecture using graph-based relational encoding with low-precision arithmetic for symbolic-like computation and linear scaling, outperforming standard ANNs.


<details>
  <summary>Details</summary>
Motivation: To create a system that directly represents relations (e.g., part-whole) in a way distinct from standard artificial neural networks, enabling intrinsic relational encoding and symbolic computation characteristics.

Method: Encode arbitrary data as graphs with edges from a fixed set of primitive pairwise relations, use a graph-Hamiltonian operator to calculate energies (with ground states satisfying all constraints), and employ radically low-precision arithmetic for linear computational cost scaling with edges.

Result: The architecture processes standard ANN examples, identifies logical relational structures (e.g., part-of, next-to), builds hierarchical representations for abductive inference, and derives equivalent ANN operations with embedded vector encodings for semantic representation.

Conclusion: This simple system offers a novel approach to relational processing with advantages in efficiency and symbolic computation, inviting further tools and improvements for broader applications.

Abstract: We introduce a simple initial working system in which relations (such as part-whole) are directly represented via an architecture with operating and learning rules fundamentally distinct from standard artificial neural network methods. Arbitrary data are straightforwardly encoded as graphs whose edges correspond to codes from a small fixed primitive set of elemental pairwise relations, such that simple relational encoding is not an add-on, but occurs intrinsically within the most basic components of the system. A novel graph-Hamiltonian operator calculates energies among these encodings, with ground states denoting simultaneous satisfaction of all relation constraints among graph vertices. The method solely uses radically low-precision arithmetic; computational cost is correspondingly low, and scales linearly with the number of edges in the data. The resulting unconventional architecture can process standard ANN examples, but also produces representations that exhibit characteristics of symbolic computation. Specifically, the method identifies simple logical relational structures in these data (part-of; next-to), building hierarchical representations that enable abductive inferential steps generating relational position-based encodings, rather than solely statistical representations. Notably, an equivalent set of ANN operations are derived, identifying a special case of embedded vector encodings that may constitute a useful approach to current work in higher-level semantic representation. The very simple current state of the implemented system invites additional tools and improvements.

</details>


### [29] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A²-LLM is an end-to-end conversational audio avatar LLM that jointly reasons about language, audio prosody, and 3D facial motion, using the FLAME-QA dataset, to generate emotionally rich facial movements with real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Current conversational digital human systems rely on cascaded architectures, leading to accumulated errors, high latency, poor real-time performance, and a lack of emotional depth due to rigid lip-sync prioritizing over context.

Method: Propose A²-LLM, a unified framework that integrates language, audio prosody, and 3D facial motion reasoning. Introduce FLAME-QA, a multimodal dataset for training that aligns semantic intent with expressive facial dynamics in a QA format.

Result: The system achieves superior emotional expressiveness in facial movements, going beyond simple lip-synchronization, while maintaining real-time efficiency with 500 ms latency and 0.7 RTF (Real-Time Factor).

Conclusion: A²-LLM addresses the limitations of cascaded systems by enabling joint reasoning for more expressive and responsive conversational digital humans, demonstrating potential for enhanced human-computer interaction.

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [30] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY is a linear-time attention method using Yat kernels on a sphere, performing almost like softmax attention with O(L) complexity.


<details>
  <summary>Details</summary>
Motivation: To enable scalable Transformers without performance trade-offs by creating an efficient linear-time attention mechanism that closely approximates softmax attention.

Method: Constrain queries and keys to a unit sphere, use Bernstein's theorem to express spherical Yat-kernel as nonnegative mixture of polynomial-exponential product kernels, derive a positive random-feature approximation for linear-time O(L) attention.

Result: SLAY achieves performance nearly indistinguishable from standard softmax attention, outperforms prior methods like Performers and Cosformers, with linear time and memory scaling.

Conclusion: SLAY represents the closest linear-time approximation to softmax attention to date, enabling scalable Transformers without typical performance trade-offs of attention linearization.

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [31] [Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams](https://arxiv.org/abs/2602.04917)
*Soshi Kakio,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: HeteroComp: a method for summarizing heterogeneous tensor streams and detecting group anomalies, outperforming state-of-the-art in accuracy with efficient computation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing event tensor streams fail to handle heterogeneous attributes (categorical and continuous) and discretize timestamps, leading to poor performance in capturing temporal dynamics and detecting group anomalies like DoS attacks.

Method: Uses Gaussian process priors to model continuous attributes and temporal dynamics, directly estimating probability densities from data to summarize streams into components representing latent groups and their temporal changes.

Result: HeteroComp provides concise summarization and accurate group anomaly detection, outperforming existing algorithms in accuracy while maintaining computational efficiency independent of data stream length.

Conclusion: HeteroComp effectively addresses the limitations of previous methods, offering a robust approach for summarizing heterogeneous tensor streams and detecting group anomalies in real-time applications.

Abstract: Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.

</details>


### [32] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: LLMs exhibit 'compliance' by prioritizing in-context information over internal knowledge through geometric interference, not norm dilution.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanistic realization of compliance in LLMs, specifically how they resolve knowledge conflicts and whether it involves signal dilution or geometric changes.

Method: Conducted layer-wise geometric analysis on Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing residual stream updates from counter-factual contexts into radial and angular components.

Result: Rejected the 'Manifold Dilution' hypothesis; two architectures maintained stable residual norms despite performance degradation. Found compliance characterized by 'Orthogonal Interference', where conflicting contexts inject quasi-orthogonal steering vectors, rotating hidden states.

Conclusion: LLMs simulate adoption via geometric displacement, not unlearning, challenging scalar confidence metrics and highlighting need for vectorial monitoring to distinguish true knowledge from mimicry.

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [33] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: PTL is an iterative pruning method that gradually compresses LLMs through prune-tune loops, enabling compression to half size with minimal performance loss on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs have impressive reasoning capabilities but are computationally expensive. Direct pruning methods cause dramatic performance drops and require extensive retraining. A more gradual approach is needed to compress models without sacrificing reasoning performance.

Method: Proposed Gradual Compacting method with Prune-Tune Loop (PTL) - divides compression into multiple fine-grained iterations. At each stage, applies pruning followed by fine-tuning to incrementally reduce model size while restoring performance. Inspired by the "boiling frog" effect of gradual adaptation.

Result: PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to original models on reasoning tasks. Works with various pruning strategies (neuron/layer pruning) and post-training methods (continual pre-training/reinforcement learning). Effective across multiple tasks including mathematical reasoning and code generation.

Conclusion: PTL provides a flexible, effective approach for compressing LLMs while preserving their reasoning capabilities through gradual, iterative compression with minimal retraining overhead, demonstrating broad applicability across different tasks and compression methods.

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [34] [CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning](https://arxiv.org/abs/2602.04920)
*Ronghao Lin,Qiaolin He,Sijie Mai,Ying Zeng,Aolin Xiong,Li Huang,Yap-Peng Tan,Haifeng Hu*

Main category: cs.LG

TL;DR: CyIN is a framework that uses cyclic information bottleneck and cross-modal translation to handle missing modalities in multimodal learning, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal deployments often face unpredictable missing modalities, causing pre-trained models to suffer performance drops, unlike the perfectly paired data used in training.

Method: The method involves building an informative latent space via cyclic token- and label-level information bottleneck to capture task-related features, and cross-modal cyclic translation to reconstruct missing modalities from available ones.

Result: Extensive experiments on 4 multimodal datasets show CyIN achieves superior performance in both complete and diverse incomplete scenarios, demonstrating effectiveness.

Conclusion: CyIN successfully bridges complete and incomplete multimodal learning, offering a unified model that maintains robustness with dynamic missing modalities.

Abstract: Multimodal machine learning, mimicking the human brain's ability to integrate various modalities has seen rapid growth. Most previous multimodal models are trained on perfectly paired multimodal input to reach optimal performance. In real-world deployments, however, the presence of modality is highly variable and unpredictable, causing the pre-trained models in suffering significant performance drops and fail to remain robust with dynamic missing modalities circumstances. In this paper, we present a novel Cyclic INformative Learning framework (CyIN) to bridge the gap between complete and incomplete multimodal learning. Specifically, we firstly build an informative latent space by adopting token- and label-level Information Bottleneck (IB) cyclically among various modalities. Capturing task-related features with variational approximation, the informative bottleneck latents are purified for more efficient cross-modal interaction and multimodal fusion. Moreover, to supplement the missing information caused by incomplete multimodal input, we propose cross-modal cyclic translation by reconstruct the missing modalities with the remained ones through forward and reverse propagation process. With the help of the extracted and reconstructed informative latents, CyIN succeeds in jointly optimizing complete and incomplete multimodal learning in one unified model. Extensive experiments on 4 multimodal datasets demonstrate the superior performance of our method in both complete and diverse incomplete scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [35] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: This paper analyzes LLM-based multi-agent systems for inventory management, showing they can find optimal policies and introducing AIM-RM for enhanced adaptability.


<details>
  <summary>Details</summary>
Motivation: Large language model-based multi-agent systems offer potential for solving inventory management challenges, but their effectiveness and adaptability to diverse supply chain scenarios remain uncertain.

Method: The study examines an LLM-based MAS with a fixed-ordering strategy prompt and safe-stock strategy, and proposes AIM-RM, an agent that uses similarity matching of historical experiences for better adaptation.

Result: Empirical results show the LLM-based MAS can determine optimal ordering decisions in restricted scenarios, and AIM-RM outperforms benchmarks in various scenarios, demonstrating robustness and adaptability.

Conclusion: LLM-based multi-agent systems are promising for inventory management, with AIM-RM proving effective and adaptable, though further research is needed for broader applications.

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


### [36] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: LTS is a learned shared-memory mechanism that reduces redundant computation in parallel agentic systems by enabling selective cross-team information reuse.


<details>
  <summary>Details</summary>
Motivation: Parallel execution of multiple agent teams in agentic systems leads to significant computational cost due to overlapping computation on similar sub-problems, which current approaches do not address efficiently.

Method: Proposes Learning to Share (LTS), which includes a global memory bank accessible to all teams and a lightweight controller trained with stepwise reinforcement learning to decide which intermediate agent steps to add to memory for selective reuse.

Result: Experiments on AssistantBench and GAIA benchmarks show LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines.

Conclusion: Learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems, as demonstrated by LTS's performance.

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [37] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: PhysicsAgentABM combines LLM multi-agent reasoning with agent-based models for scalable, calibrated simulation using cluster-level inference and neuro-symbolic fusion.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems are expensive and poorly calibrated for timestep simulation, while classical ABMs struggle with individual-level complexity and non-stationary behaviors, requiring a scalable and calibrated approach.

Method: Uses cluster-based inference: symbolic agents encode mechanistic priors, a multimodal neural model captures dynamics, and uncertainty-aware fusion yields calibrated transition distributions. ANCHOR clustering reduces LLM calls via contrastive loss. Agents stochastically realize transitions under local constraints.

Result: Experiments in public health, finance, and social sciences show gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines, with LLM call reduction of up to 6-8 times.

Conclusion: PhysicsAgentABM re-architects generative ABMs for scalable, calibrated simulation using population-level inference with neuro-symbolic fusion, establishing a new paradigm for LLM-based simulation.

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>
